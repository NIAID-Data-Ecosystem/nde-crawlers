[{"_id": "SB_Public_Apps_admin/sbg-public-data/alignment-metrics-qc/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/alignment-metrics-qc/6", "applicationCategory": "Workflow", "name": "Alignment Metrics QC", "description": "Running this pipeline will provide you with useful statistics to help you judge the quality of your alignment. Provide aligned reads in the BAM format and the reference FASTA to which they were aligned as inputs. This pipeline will generate an easily-digestible table report with quality control information, including the proportion of your reads that could not be aligned and the percentage of reads that passed quality checks.\n\nThis pipeline works with output from genomic or transcriptomic aligners. Run this pipeline as part of your routine bioinformatic quality control or when optimizing alignment for your data by experimenting with aligner settings.", "input": [{"name": "Reference Genome FASTA", "encodingFormat": "application/x-fasta"}, {"name": "BAM_files"}], "output": [{"name": "Summary Metrics", "encodingFormat": "text/plain"}], "codeRepository": ["https://github.com/broadinstitute/picard"], "applicationSubCategory": ["SAM/BAM Processing", "Quality Control"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799487, "contributor": ["bix-demo", "sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_jiewho/bd-public-project/bd-rhapsody-analysis-pipeline/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/jiewho/bd-public-project/bd-rhapsody-analysis-pipeline/15", "applicationCategory": "Workflow", "name": "BD Rhapsody\u2122 Targeted Analysis Pipeline", "description": "The BD Rhapsody\u2122 assays are used to create sequencing libraries from single cell transcriptomes.\n\nAfter sequencing, the analysis pipeline takes the FASTQ files and a reference file for gene alignment. The pipeline generates molecular counts per cell, read counts per cell, metrics, and an alignment file.", "input": [{"name": "Exact Cell Count"}, {"name": "Disable Refined Putative Cell Calling"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "VDJ Species Version"}, {"name": "Putative Cell Calling"}, {"name": "AbSeq Reference"}, {"name": "Reference"}, {"name": "Run Name"}, {"name": "Subsample Reads"}, {"name": "Subsample Seed"}, {"name": "Tag Names"}, {"name": "Sample Tags Version"}], "output": [{"name": "VDJ"}, {"name": "Bioproduct Statistics"}, {"name": "Cell Label Filter"}, {"name": "Expression Matrix"}, {"name": "Final BAM File"}, {"name": "Final BAM Index"}, {"name": "Metrics Summary"}, {"name": "Data Tables"}, {"name": "Unfiltered Data Tables"}, {"name": "Unfiltered Expression Matrix"}, {"name": "Pipeline Logs"}, {"name": "Putative Cells Origin"}, {"name": "Protein Aggregates (Experimental)"}, {"name": "Immune Cell Classification (Experimental)"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/jiewho/bd-public-project/bd-rhapsody-analysis-pipeline/15.png", "version": "v1.10.1", "project": "BD Public Project", "softwareVersion": ["v1.0"], "dateModified": 1649980706, "dateCreated": 1507667967, "contributor": ["jdreux", "schen", "aberno", "jeremyadamsfisher"], "sdPublisher": "BD", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_jiewho/bd-public-project/bd-rhapsody-wta-analysis-pipeline/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/jiewho/bd-public-project/bd-rhapsody-wta-analysis-pipeline/7", "applicationCategory": "Workflow", "name": "BD Rhapsody\u2122 WTA Analysis Pipeline", "description": "The BD Rhapsody\u2122 WTA Analysis Pipeline is used to create sequencing libraries from single cell transcriptomes without having to specify a targeted panel.\n\nAfter sequencing, the analysis pipeline takes the FASTQ files, a reference genome file and a transcriptome annotation file for gene alignment. The pipeline generates molecular counts per cell, read counts per cell, metrics, and an alignment file.", "input": [{"name": "Exact Cell Count"}, {"name": "Disable Refined Putative Cell Calling"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "VDJ Species Version"}, {"name": "Putative Cell Calling"}, {"name": "AbSeq Reference"}, {"name": "Transcriptome Annotation", "encodingFormat": "application/x-gtf"}, {"name": "Reference Genome", "encodingFormat": "application/x-tar"}, {"name": "Supplemental Reference"}, {"name": "Run Name"}, {"name": "Subsample Reads"}, {"name": "Subsample Seed"}, {"name": "Tag Names"}, {"name": "Sample Tags Version"}], "output": [{"name": "VDJ"}, {"name": "Bioproduct Statistics"}, {"name": "Cell Label Filter"}, {"name": "Expression Matrix"}, {"name": "Final BAM File"}, {"name": "Final BAM Index"}, {"name": "Metrics Summary"}, {"name": "Data Tables"}, {"name": "Unfiltered Data Tables"}, {"name": "Unfiltered Expression Matrix"}, {"name": "Pipeline Logs"}, {"name": "Putative Cells Origin"}, {"name": "Protein Aggregates (Experimental)"}, {"name": "Immune Cell Classification (Experimental)"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/jiewho/bd-public-project/bd-rhapsody-wta-analysis-pipeline/7.png", "version": "v1.10.1", "project": "BD Public Project", "softwareVersion": ["v1.0"], "dateModified": 1649980559, "dateCreated": 1565291572, "contributor": ["aberno"], "sdPublisher": "BD", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-analysis-0-19-0/28", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-analysis-0-19-0/28", "applicationCategory": "Workflow", "name": "Bismark Analysis", "description": "**Bismark Analysis 0.19.0** is a workflow for analyzing DNA methylation, a type of epigenetic modification, by processing reads which can be obtained by using two technologies for library preparation and sequencing, Whole Genome Bisulfite Sequencing (WGBS) and Reduced Representation Bisulfite Sequencing (RRBS). It can be used for an input file of any size, but it is optimized for larger files (more than 15 GB).\n\nThe pipeline contains four main parts: a quality control and adapter trimming performed by **Trim Galore!**, alignment of BS-Seq reads performed by the **Bismark**, deduplication performed by **Bismark Deduplicate**, and extraction of methylated cytosines performed by **Bismark Methylation Extractor**.\n\nThe workflow takes the following input files:\n\n* **FASTQ files** - compressed or decompressed BS-seq\u00a0reads\u00a0(FASTQ/FASTQ.GZ) with **Paired-end** and **Sample ID** metadata field set\n* **Reference FASTA** - a reference genome FASTA file\n* **Bisulfite genome reference** - gzipped in-silico bisulfite-treated and indexed reference genome (TAR.GZ), which can be created with **Bismark Genome Preparation** tool (the appropriate alignment algorithm Bowie1/Bowtie2 have to be selected). Suggested file for that input is **ucsc\\_Bisulfite\\_Genome\\_bowtie2.tar.gz**, which is created by **Bismark Genome Preparation** tool with provided reference genome file ucsc.hg19.fasta and selected Bowtie2 option\n\nOutput files are generated within the four abovementioned segments of the workflow:\n\n* In the quality control and adapter trimming segment, graphical interfaces with all necessary information about reads, their quality, and adapter contamination are generated before and after the tool Trim Galore! (**Pre trimming FASTQC Analysis** and **Post trimming FASTQC Analysis**). \n* An alignment BAM file (**Bismark alignment file**) is produced by the **Bismark** in the alignment part of the workflow. This file is sorted by coordinate and has its own index BAI file. For further processing, the direct output of the Bismark is sorted by the name to be acceptable for the tool **Bismark Deduplicate**.\n* A **Sorted deduplicated BAM** is created in the deduplication process and is sorted by coordinate and has its own index BAI file. In **RRBS** mode (no deduplication), this file should be the same as **Bismark alignment file**.\n* In the methylation call segment, **Bismark Methylation Extractor** operates on BAM file and generates **Coverage file** which is applicable to further analysis (differential methylation). \n* Graphical HTML reports generated by the tool **Bismark2Report** contains all information obtained from reports generated by Bismark, Bismark Deduplicate, and Bismark Methylation Extractor.\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThe workflow is designed to analyze one WGBS or RRBS sample (FASTQ, FASTQ.GZ) with properly set metadata fields.\n\n* The user should provide FASTQ files to input **FASTQ files** and set metadata fields  **Sample ID** for single-end read and **Sample ID** and **Paired-end** for paired-end reads. \n* All workflow settings are optimized for the use of larger input files with sequences (more than 15 GB). For other use cases, please contact Seven Bridges for support.\n* If paired-end reads are provided, the option **Paired** in the tool Trim Galore! should be set. The option **Single end** in Bismark and Bismark Deduplicate should be set No value or False.\n* The option **Bowtie aligner** in Bismark must be selected according to the selected alignment algorithm in the tool Bismark Genome Preparation.\n* The reference genome files which are used for generating the gzipped bisulfite indexed genome reference file should be used in the workflow. \n* After running task of workflow has successfully completed, it creates **Coverage file** which contains methylated cytosines and can be used for further analysis of differential methylation, HTML reports of FASTQ reads before and after trimming, HTML report of complete process of methylation calling (alignment, deduplication and extraction methylation call) and BAM files after alignment and deduplication. BAM file and report generated by Bismark,  Deduplication report and Splitting report can be used for comparison alignment and methylation parameters between samples in the tool **Bismark2Summary**.\n\n### Changes Introduced by Seven Bridges\n\nThe realization of workflow is based on the idea of optimizing the workflow in a better way. \n\nThe technical configuration of workflow is:\n\n* AWS instance c4.8xlarge (36 CPU, 60GB)\n* maximum parallel instances set to 6. \n\nThree parts of the pipeline which contribute to faster task executions are as follows:\n\n* Trim Galore! scattered by split FASTQ files. SBG FASTQ Split splits FASTQ files into 30 parts. Reads can't be divided to the maximum number of CPUs (36), because some tools run in parallel with Trim Galore! and execution time won't be optimized. The number of parts should be divisible by the maximum number of instances which is 6.\n* Scattered Bismark by trimmed FASTQ files which are merged before into 6 parts. This tool runs on six parallel instances (c4.8xlarge).\n* Generated BAM file are split by chromosome name and provided to Bismark Deduplicate and Bismark Methylation Extractor which are scattered by chromosome bam files. \n* Outputs generated by workflow are finally merged.\n\n### Common Issues and Important Notes\n\n* Metadata fields for FASTQ files **Paired-end** and **Sample ID** should be properly set, fields Sample ID for single-end read and Sample ID and Paired-end for paired-end reads.\n* For RRBS library, option **RRBS** should be set in the tool Trim Galore!. Also, author of the Bismark recommends that deduplication process should not be applied to RRBS reads. Option in Bismark Deduplicate **RRBS** should be set to TRUE for bypassing deduplication for RRBS reads.\n* Option **Bowtie Aligner** in the tool Bismark should be specified depending on which Bowtie is used for indexing in Bismark Genome Preparation. The default value is Bowtie2.\n* Options **Memory in MB per job** and **Number of CPUs per job** in Bismark are related to the instance on which this tool is running. Those options should be set to the maximum of memory and number of CPU of specified instance. E.g. AWS instance c4.8xlarge has 36 CPU and 60 GB, **Memory in MB per job** is set to 61440 MB and **Number of CPUs per job** set to 36.\n* The **Multicore** option in Bismark tool represents the number of parallel jobs (multicores) which are run concurrently. It takes value 8 by default and should not be changed in the case of the reads from directional sequencing library and human genome reference (3GB). This value is set so the pipeline can be run on AWS instance c4.8xlarge (36CPUs, 60GB). If reference genome is a bigger size or more references are provided, this parameter should be set to less value (e.g. for the reference of 4.5GB multicore is set to 3 in order to fit on c4.8xlarge).\n* All the above settings are optimized for the case of larger input files with sequences (more than 15 GB). For other cases, please contact SBG support.\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\n### Performance Benchmarking\n\nAmong all tools in workflow, Bismark has the highest memory demands. Memory and CPU requirements for Bismark strongly correlate with **Sequencing library** (directional or non-directional), a number of multicores (parameter **Multicore**) and the size of the reference file. If reference genome file has the bigger size or there are more reference files provided, the multicore parameter should be set to a lower value. We tested for the human genome (3 GB), multicore was set to 6, it means that 10 GB per one multicore, for three genome reference files (4.5GB in total), multicore was 3, 20 GB per one multicore.\n\nBased on our experience we choose AWS instance c4.8xlarge (36 CPU, 60 GB) which was enough for high memory requirement.\nIn the following table, you can find estimates of Bismark Analysis 0.19.0 running time and cost.\n\n| FASTQ (size) | Genome Reference (size) | Multicore (Bismark) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- | --- |\n| 2 x 700 MB (GZ)| 3 GB | 6 | c4.8xlarge | 1h 9m | $6.78 |\n| 2 x 12 GB (GZ) | 3 GB | 6 | c4.8xlarge | 5h 11m | $26.67 |\n| 2 x 220 GB | 4.5 GB | 3 | c4.8xlarge | 20h 44m | $114.29 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14", "input": [{"name": "FASTQ files", "encodingFormat": "text/fastq"}, {"name": "RRBS"}, {"name": "Bisulfite genome reference", "encodingFormat": "application/x-tar"}, {"name": "Single end"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Yacht"}, {"name": "Ignore r2"}, {"name": "Ignore 3prime_r2"}, {"name": "Ignore 3prime"}, {"name": "Ignore"}, {"name": "CX Context"}, {"name": "CutOff"}, {"name": "Sequencing library"}, {"name": "Seed mismatches"}, {"name": "Nucleotide coverage"}, {"name": "Multicore"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Bowtie Aligner"}, {"name": "Barcode"}, {"name": "Adapter"}, {"name": "Adapter 2"}, {"name": "5' clip read 1"}, {"name": "5' clip read 2"}, {"name": "3' clip read 1"}, {"name": "3' clip read 2"}, {"name": "Paired"}], "output": [{"name": "Splitting report", "encodingFormat": "text/plain"}, {"name": "Nucleotide report", "encodingFormat": "text/plain"}, {"name": "Mbias report", "encodingFormat": "text/plain"}, {"name": "Deduplicate report", "encodingFormat": "text/plain"}, {"name": "Alignment report", "encodingFormat": "text/plain"}, {"name": "Bismark report"}, {"name": "Sorted deduplicated BAM", "encodingFormat": "application/x-sam"}, {"name": "Yacht file", "encodingFormat": "text/plain"}, {"name": "Coverage file"}, {"name": "BedGraph"}, {"name": "CpG context", "encodingFormat": "text/plain"}, {"name": "CHH context", "encodingFormat": "text/plain"}, {"name": "CHG context", "encodingFormat": "text/plain"}, {"name": "Post trimming FASTQC Analysis", "encodingFormat": "text/html"}, {"name": "Pre trimming FASTQC Analysis", "encodingFormat": "text/html"}, {"name": "Bismark alignment file", "encodingFormat": "application/x-bam"}], "applicationSubCategory": ["Methylation", "Alignment", "Methylation Extraction"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561064, "dateCreated": 1519392251, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/bms-bam-prep/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/bms-bam-prep/4", "applicationCategory": "Workflow", "name": "BMS BAM prep", "description": "**BMS BAM prep** is the obligatory first phase that must precede all variant discovery. It involves pre-processing the raw sequence data (provided in FASTQ format) to produce analysis-ready BAM files. This involves alignment to a reference genome, as well as some data cleanup operations to correct for technical biases and make the data suitable for analysis [1].\n\nThis workflow is a part of the project to transfer BMS variant calling procedures from custom bash scripts to the Platform, which started in October 2017. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThis workflow contains three main steps:\n* **Alignment** of raw reads to the human reference genome with **BWA MEM Bundle** (0.7.17). Besides the standard **BWA MEM** SAM output file, **BWA MEM** package has been extended to support two additional output options: a BAM file obtained by piping through **Sambamba view** while filtering out the secondary alignments, as well as a **Coordinate Sorted BAM** option that additionally pipes the output through **Sambamba sort**, along with an accompanying BAI file produced by **Sambamba sort**. This sorted BAM is the default output of BWA MEM.\n* **Deduplication** with **Picard MarkDuplicates** tool (2.9.2). This second processing step consists of identifying read pairs that are likely to have originated from duplicates of the same original DNA fragments through some artifactual processes [1].\n* **Base recalibration** consists of two tools: **GATK BaseRecalibrator** and **GATK apply BQSR**. The first tool is applying machine learning to detect patterns of systematic errors in the base quality scores, which are confidence scores emitted by the sequencer for each base, while the second one is applying recalibration to the deduplicated BAM file [1].\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named by sample IDs e.g.: *SampleID.sorted.dedup.realigned.recal.bam*\n* **Picard BEDToIntervalsList** tools are added for conversion of BED files into INTERVALS_LIST.\n* Alignment can be performed in batch mode. Batching is most often performed by sample ID metadata.\n\n### Common Issues and Important Notes\n\n* In order to successfully work, **BWA MEM Bundle** requires a FASTA reference file accompanied with BWA FASTA indices in a TAR file. \n* There is a known issue with **Samblaster** - it does not support processing when the number of sequences in the reference FASTA is larger than 32768. \n* If the reference file is provided in FASTA format instead of a TAR file (which already contains all the necessary indices), the runtime will be significantly increased.\n* Pipeline provides suggested reference files when running it for the first time in a project. These files are aimed for use with BMS kit, and should only be used for testing purposes. If using this pipeline for production, these files should be replaced with references corresponding to the input reads.\n\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of **BMS BAM prep** run times and costs. All samples are aligned against **hg19 human reference**. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:| ------------- | ------------- | ------------- |\n|16|30000| 2 X 150 MB     | WES | 10 minutes| $0.13 |    c4.4xlarge |\n|16|30000| 2 X 3.9 GB     | WES | 1 hour and 35 minutes| $1.26 |    c4.4xlarge |\n|16|30000| 2 X 8.3 GB     | WES | 4 hour 15 minute| $3.38 |    c4.4xlarge |\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"bait_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_reads\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"]))}\n# Creates draft task\ntask = api.tasks.create(name=\"BMS BAM prep - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/).\n\n\n### References\n[1 - GATK best practices](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165)", "input": [{"name": "Reference or TAR with BWA reference indices", "encodingFormat": "application/x-tar"}, {"name": "Bait BED", "encodingFormat": "text/x-bed"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "KgSnp Database", "encodingFormat": "application/x-vcf"}, {"name": "MgIndel Database", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Dedup Metrics", "encodingFormat": "text/plain"}, {"name": "Recal Table"}, {"name": "Alignment Metrics", "encodingFormat": "text/plain"}, {"name": "Picard HS Metrics", "encodingFormat": "text/plain"}, {"name": "Picard Per Target Coverage", "encodingFormat": "text/plain"}, {"name": "Output BAM", "encodingFormat": "application/x-bam"}], "applicationSubCategory": ["WES-(WXS)", "SAM/BAM-Processing"], "project": "BMS Public Apps", "creator": "BMS/SBG", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648557472, "dateCreated": 1568133331, "contributor": ["bristol-myers-squibb-publishin/bms.publish"], "sdPublisher": "BMS", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-normal-pipeline-hg19/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-normal-pipeline-hg19/2", "applicationCategory": "Workflow", "name": "BMS WES Tumor-Normal pipeline hg19", "description": "**BMS WES TN pipeline hg19** is a workflow made around the _Strelka 1.0.15_ somatic variant caller (SNVs and INDELs) and _GATK Mutect 1.1.6_ (SNVs) somatic variant callers, which perform variant calling on matched tumor - normal BAM files.\n\nThis workflow is a part of the project to transfer BMS variant calling procedures from custom bash scripts to the Platform, which started in October 2017. This workflow utilizes human reference genome hg19.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\nBAM files are used as inputs. **Tumor BAM file** and **Normal BAM file**, should be sorted and indexed. This can be done by using the **BMS BAM prep** workflow. The **Input tar with reference** input should contain the reference genome with all necessary indexes. For every VCF output that this workflow produces, Variant Calling Metrics are calculated and a MAF file is produced.\n\nThere are tools that performs annotation of output VCF files in this workflow. Annotation is done with: \n* _dbSNP_ \n* _SNPEff database_ \n* _1000G snps_ \n* _1000G indels_\n* _Mills and 1000G indels_ \n* _Cosmic database_\n* _ExAC database_\n\n\n### Changes Introduced by Seven Bridges\n\n* Every output has its own subset which contains only variants that are in BED file regions. These output are prefixed with **intersected**.\n\n* Only primary chromosomes are kept in VCF/MAF files.\n\n* Outputs are named by sample IDs e.g.: TumorID-NormalID.passed.somatic.strelka.snvs.vcf\n\n\n### Common Issues and Important Notes\n\n#### **IMPORTANT LICENSING INFORMATION**\n* MuTect is licensed by the Broad Institute and is made available for free to academic users for non-commercial use only pursuant to the licensing terms below, and to other authorized licensees pursuant to the terms of their respective licenses, in each case for use within this pipeline only. The full text of the academic license for non-commercial use of MuTect is available on [github](https://github.com/broadinstitute/mutect/blob/master/mutect.LICENSE.TXT). \n\n*   For commercial licensing information, please email [Broad institute](softwarelicensing@broadinstitute.org). \n \n*  For more information about Mutect, please visit the [Mutect website](https://software.broadinstitute.org/cancer/cga/mutect).\n  \n#### MuTect documentation resources and support\n\nGeneral MuTect documentation can be found on the [MuTect website](https://software.broadinstitute.org/cancer/cga/mutect). Users of this pipeline are welcome to ask MuTect-related questions and report problems that are not specific to this pipeline in the [MuTect forum](http://gatkforums.broadinstitute.org/categories/mutect).\n\n#### Strelka common issues\n\n- According to the [Strelka FAQ](https://sites.google.com/site/strelkasomaticvariantcaller/home/faq), the following BAM records are incompatible with Strelka (quoted):\n  - Alignments which use the match/mismatch (\"=\"/\"X\") CIGAR notation\n  - Records where the \"=\" character is used in the SEQ field\n  - BAM records with basecall quality values greater than 70 (not supported assuming that this indicates an offset error)\n- Unless input BAM files **Tumor BAM file** and **Normal BAM file** are supplied to Strelka (both sorted and indexed), Strelka will not finish successfully. \n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **Matched TN pipeline hg19** run times and costs. All samples are aligned against **hg19 human reference**. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:| ------------- | ------------- | ------------- |\n|36|60000| 5.2 GB + 4.9 GB     | WES | 42 minutes| $1.11 |    c4.8xlarge |\n|36|60000| 6.6 GB + 6.9 GB     | WES | 48 minutes| $1.27 |    c4.8xlarge |\n|36|60000| 14 GB + 14 GB     | WES | 1 hour 21 minutes| $2.14 |    c4.8xlarge |\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"tumor_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"normal_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"kgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"mgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"snpEff_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"cosmic_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"cache_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"annotation_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"ExAC_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"BMS WES TN pipeline hg19 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/).", "input": [{"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Tumor reads", "encodingFormat": "application/x-bam"}, {"name": "Normal reads", "encodingFormat": "application/x-bam"}, {"name": "SnpEff Database", "encodingFormat": "application/zip"}, {"name": "Cache File", "encodingFormat": "application/x-tar"}, {"name": "Input TAR with reference", "encodingFormat": "application/x-fasta"}, {"name": "KgSnp Database", "encodingFormat": "application/x-vcf"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "KgIndel Database", "encodingFormat": "application/x-vcf"}, {"name": "MgIndel Database", "encodingFormat": "application/x-vcf"}, {"name": "ExAC Database", "encodingFormat": "application/x-vcf"}, {"name": "Cosmic Database", "encodingFormat": "application/x-vcf"}, {"name": "Annotation Reference", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Annotated MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Annotated MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "TMB Output", "encodingFormat": "text/plain"}, {"name": "BED Intersected Annotated Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "Annotated Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Annotated Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "Annotated Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "MuTect MAF"}, {"name": "BED Intersected MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected MuTect MAF"}, {"name": "MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "MuTect VC Summary Metrics"}, {"name": "MuTect Stats"}, {"name": "BED Intersected MuTect VC Summary Metrics"}, {"name": "Strelka Passed Indels VC Summary Metrics"}, {"name": "Strelka Passed SNVs VC Summary Metrics"}, {"name": "Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "Strelka All SNVs", "encodingFormat": "application/x-vcf"}, {"name": "Strelka All INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka All SNVs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka All INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka Passed INDELs VC Summary Metrics"}, {"name": "BED Intersected Strelka Passed SNVs VC Summary Metrics"}, {"name": "Strelka Passed SNVs MAF"}, {"name": "Strelka Passed INDELs MAF"}, {"name": "BED Intersected Strelka Passed SNVs MAF"}, {"name": "BED Intersected Strelka Passed INDELs MAF"}], "applicationSubCategory": ["Variant-Calling"], "project": "BMS Public Apps", "creator": "SBG/BMS", "softwareVersion": ["sbg:draft-2"], "dateModified": 1569422197, "dateCreated": 1568134297, "contributor": ["bristol-myers-squibb-publishin/bms.publish"], "sdPublisher": "BMS", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-only-pipeline-hg19/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-only-pipeline-hg19/2", "applicationCategory": "Workflow", "name": "BMS WES tumor only pipeline hg19", "description": "BMS WES tumor only pipeline hg19 is a workflow which performs somatic variant calling on a tumor BAM file.\n\nIt is made around the _Strelka 1.0.15_ (SNVs and INDELs) and _GATK Mutect 1.1.6_ (SNVs) somatic variant callers which perform variant calling on a tumor BAM file.\n\nThis workflow is a part of the project to transfer BMS variant calling procedures from custom bash scripts to the Platform, which started in October 2017. The workflow utilizes human reference genome hg19.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nBAM file is used as input. **Tumor BAM file**, should be sorted and indexed. This can be done by using the **BMS BAM prep** workflow. The **Input tar with reference** input should contain the reference genome file with all necessary indexes. For every output that this workflow produces, Variant Calling Metrics are calculated and a MAF file is produced.\n\nThere are also tools that perform annotation of output VCF files in this workflow. Annotation is done with:\n* _dbSNP_ \n* _SNPEff database_ \n* _1000G snps_ \n* _1000G indels_\n* _Mills and 1000G indels_ \n* _Cosmic database_\n* _ExAC database_\n\n\n### Changes Introduced by Seven Bridges\n\n* **Strelka1.0.15** doesn't perform somatic calling in tumor-only mode. We used **NA12878** WES BAM sample aligned against hg19 human reference as  normal sample.\n\n* Every output has its own subset, which contains only variants that are in BED file regions. These outputs are prefixed with **intersected**.\n\n* Only primary chromosomes are kept in VCF/MAF files.\n\n* Outputs are named by sample IDs e.g.: TumorID.passed.somatic.strelka.snvs.vcf\n\n\n### Common Issues and Important Notes\n\n#### **IMPORTANT LICENSING INFORMATION**\n* MuTect is licensed by the Broad Institute and is made available for free to academic users for non-commercial use only pursuant to the licensing terms below, and to other authorized licensees pursuant to the terms of their respective licenses, in each case for use within this pipeline only. The full text of the academic license for non-commercial use of MuTect is available on [github](https://github.com/broadinstitute/mutect/blob/master/mutect.LICENSE.TXT). \n\n*   For commercial licensing information, please email [Broad institute](softwarelicensing@broadinstitute.org). \n \n*  For more information about Mutect, please visit the [Mutect website](https://software.broadinstitute.org/cancer/cga/mutect).\n  \n#### MuTect documentation resources and support\n\nGeneral MuTect documentation can be found on the [MuTect website](https://software.broadinstitute.org/cancer/cga/mutect). Users of this pipeline are welcome to ask MuTect-related questions and report problems that are not specific to this pipeline in the [MuTect forum](http://gatkforums.broadinstitute.org/categories/mutect).\n\n#### Strelka common issues\n\n- According to the [Strelka FAQ](https://sites.google.com/site/strelkasomaticvariantcaller/home/faq) the following BAM records are incompatible with Strelka (quoted):\n  - Alignments which use the match/mismatch (\"=\"/\"X\") CIGAR notation\n  - Records where the \"=\" character is used in the SEQ field\n  - BAM records with basecall quality values greater than 70 (not supported assuming that this indicates an offset error)\n- Unless input BAM files **Tumor BAM file** and **Normal BAM file** are supplied to Strelka (both sorted and indexed), Strelka will not finish successfully.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of **Tumor only pipeline hg19** run times and costs. All samples are aligned against **hg19 human reference**. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:| ------------- | ------------- | ------------- |\n|36|60000| 4.4 GB     | WES | 48 minutes| $1.27 |    c4.8xlarge |\n|36|60000| 5.4 GB     | WES | 50 minutes| $1.32 |    c4.8xlarge |\n|36|60000| 14 GB     | WES | 1 hour 1 minute| $1.62 |    c4.8xlarge |\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"cosmic_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"annotation_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"tumor_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"kgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"mgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"snpEff_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"ExAC_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"cache_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"normal_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"BMS WES tumor only pipeline hg19 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/).", "input": [{"name": "Annotation Reference", "encodingFormat": "application/x-fasta"}, {"name": "Input TAR with reference", "encodingFormat": "application/x-fasta"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Tumor reads", "encodingFormat": "application/x-bam"}, {"name": "KgSnp Database", "encodingFormat": "application/x-vcf"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "SnpEff Database", "encodingFormat": "application/zip"}, {"name": "Cache file", "encodingFormat": "application/x-tar"}, {"name": "Normal reads", "encodingFormat": "application/x-bam"}, {"name": "KgIndel Database", "encodingFormat": "application/x-vcf"}, {"name": "MgIndel Database", "encodingFormat": "application/x-vcf"}, {"name": "Cosmic Database", "encodingFormat": "application/x-vcf"}, {"name": "ExAC Database", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Annotated Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Annotated Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "Annotated Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Annotated Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Annotated MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "Annotated MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "TMB Output", "encodingFormat": "text/plain"}, {"name": "MuTect MAF"}, {"name": "BED Intersected MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected MuTect MAF"}, {"name": "MuTect Variants", "encodingFormat": "application/x-vcf"}, {"name": "MuTect VC Summary Metrics"}, {"name": "MuTect Stats"}, {"name": "Strelka Passed INDELs VC Summary Metrics"}, {"name": "Strelka Passed SNVs VC Summary Metrics"}, {"name": "Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "Strelka All SNVs", "encodingFormat": "application/x-vcf"}, {"name": "Strelka All INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka Passed SNVs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka Passed INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka All SNVs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka All INDELs", "encodingFormat": "application/x-vcf"}, {"name": "BED Intersected Strelka Passed INDELs VC Summary Metrics"}, {"name": "BED Intersected Strelka Passed SNVs VC Summary Metrics"}, {"name": "Strelka Passed SNVs MAF"}, {"name": "Strelka Passed INDELs MAF"}, {"name": "BED Intersected Strelka Passed SNVs MAF"}, {"name": "BED Intersected Strelka Passed INDELs MAF"}, {"name": "BED Intersected MuTect VC Summary Metrics"}], "applicationSubCategory": ["Variant-Calling"], "project": "BMS Public Apps", "creator": "BMS/SBG", "softwareVersion": ["sbg:draft-2"], "dateModified": 1569422246, "dateCreated": 1568133459, "contributor": ["bristol-myers-squibb-publishin/bms.publish"], "sdPublisher": "BMS", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practices-rna-seq-variant-calling-4-1-0-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practices-rna-seq-variant-calling-4-1-0-0/8", "applicationCategory": "Workflow", "name": "BROAD Best Practices RNA-Seq Variant Calling 4.1.0.0", "description": "This workflow represents the GATK Best Practices for SNP and INDEL calling on RNA-Seq data. \n\nStarting from an unmapped BAM file, it performs alignment to the reference genome, followed by marking duplicates, reassigning mapping qualities, base recalibration, variant calling and variant filtering. On the [GATK website](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891), you can find more detailed information about calling variants in RNA-Seq.\n\n###Common Use Cases\n- If you have raw sequencing reads in FASTQ format, you should convert them to an unmapped BAM file using the **Picard FastqToSam** app before running the workflow.\n- **BaseRecalibrator** uses **Known indels** and **Known SNPs** databases to mask out polymorphic sites when creating a model for adjusting quality scores. Also, the **HaplotypeCaller** uses the **Known SNPs** database to populate the ID column of the VCF output.\n- The **HaplotypeCaller** app uses **Intervals list** to restrict processing to specific genomic intervals. You can set the **Scatter count** value in order to split **Intervals list** into smaller intervals. **HaplotypeCaller** processes these intervals in parallel, which will significantly reduce workflow execution time  in some cases.\n- You can provide a pre-generated **STAR** reference index file or a genome reference file to the **Reference or STAR index** input.\n- **Running a batch task**: Batching is performed by **Sample ID** metadata field on the **Unmapped BAM** input port. For running analyses in batches, it is necessary to set **Sample ID** metadata for each unmapped BAM file.\n\n\n###Changes Introduced by Seven Bridges\nThis workflow represents the GATK Best Practices for SNP and indel calling on RNA-Seq data, and there are no modifications to the original workflow.\n\n\n###Common Issues and Important Notes\n- As the *(--known-sites)* is the required option for GATK BaseRecalibrator tool, it is necessary to provide at least one database file to the **Known INDELs** or **Known SNPs** input port.\n- If you are providing pre-generated STAR reference index make sure it is created using the adequate version of STAR (check the STAR version in the original [WDL file](https://github.com/gatk-workflows/gatk3-4-rnaseq-germline-snps-indels/blob/master/rna-germline-variant-calling.wdl)).\n- When converting FASTQ files to an unmapped BAM file using **Picard FastqToSam**, it is required to set the **Platform** (`PLATFORM=`) parameter.\n- This workflow allows you to process one sample per task execution. If you are planning to process more than one sample, it is required to run multiple task executions in batch mode. More about batch analyses can be found [here](https://docs.sevenbridges.com/docs/about-batch-analyses).\n \n\n###Performance Benchmarking\nThe default memory and CPU requirements for each app in the workflow are the same as in the original [GATK Best Practices WDL](https://github.com/gatk-workflows/gatk3-4-rnaseq-germline-snps-indels/blob/master/rna-germline-variant-calling.wdl). You can change the default runtime requirements for **STAR GenomeGenerate** and **STAR Align** apps. \n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  AWS Instance Cost (spot) | AWS Instance Cost (on-demand) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|\n|     RNA-Seq     |  1.3 GB |     Yes    |     16M     |     101     |   2h44min   | 0.79$ | 1.79$ | \n|     RNA-Seq     |  3.9 GB |     Yes    |     50M     |     101     |   4h38min   | 1.29$ | 2.71$ | \n|     RNA-Seq     | 6.5 GB |     Yes    |     82M    |     101     |  6h44min  | 1.85$ | 3.84$ | \n|     RNA-Seq     | 12.9 GB |     Yes    |     164M    |     101     |  12h4min  | 3.30$ | 6.99$ |\n\n\n###API Python Implementation\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id = \"your_username/project\"\nworkflow_id = \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n        \"input\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list']),\n        \"in_alignments\": api.files.query(project=project_id, names=['G26234.HCC1187_1Mreads.bam'])[0],\n        \"in_reference\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.fasta'])[0],\n        \"in_gene_annotation\": api.files.query(project=project_id, names=['star.gencode.v19.transcripts.patched_contigs.gtf'])[0],\n        \"in_reference_or_index\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.star.gencode.v19.transcripts.patched_contigs.star-2.5.3a_modified-index-archive.tar'])[0],\n        \"known_indels\": api.files.query(project=project_id, names=['Mills_and_1000G_gold_standard.indels.b37.sites.vcf',\n                                                                   'Homo_sapiens_assembly19_1000genomes_decoy.known_indels.vcf']),\n        \"known_snps\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.dbsnp138.vcf']),\n}\n\ntask = api.tasks.create(name='GATK4 RNA-Seq Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False)\n#For running a batch task\ntask = api.tasks.create(name='GATK4 RNA-Seq Workflow - API Batch Example', project=project_id, app=workflow_id, inputs=inputs, run=False, batch_input='in_alignments', batch_by = { 'type': 'CRITERIA', 'criteria': [ 'metadata.sample_id'] })\n```\n\nInstructions for installing and configuring the API Python client are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).", "input": [{"name": "Unmapped BAM", "encodingFormat": "application/x-bam"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Known INDELs", "encodingFormat": "application/x-vcf"}, {"name": "Known SNPs", "encodingFormat": "application/x-vcf"}, {"name": "Interval list", "encodingFormat": "application/x-vcf"}, {"name": "Reference or STAR index", "encodingFormat": "application/x-tar"}, {"name": "Gene annotation", "encodingFormat": "application/x-gtf"}, {"name": "Number of threads"}, {"name": "Memory per job"}, {"name": "Read length"}, {"name": "Number of threads"}, {"name": "Max number of collapsed junctions"}, {"name": "Memory per job"}], "output": [{"name": "Output recalibrated BAM/SAM/CRAM", "encodingFormat": "application/x-sam"}, {"name": "Output VCF", "encodingFormat": "application/x-vcf"}, {"name": "Output filtered VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ScatterFeatureRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/broad-best-practices-rna-seq-variant-calling-4-1-0-0/8.png", "codeRepository": ["https://github.com/broadinstitute/gatk/", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["Transcriptomics", "Variant Calling"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1571417180, "dateCreated": 1555083927, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practices-somatic-cnv-pair-workflow-4-1-0-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practices-somatic-cnv-pair-workflow-4-1-0-0/7", "applicationCategory": "Workflow", "name": "BROAD Best Practices Somatic CNV Pair Workflow 4.1.0.0", "description": "BROAD Best Practices Somatic CNV Pair is used for detecting copy number variants (CNVs) in a single sample.\n\n### Common Use Cases\n\nThe workflow denoises case sample alignment data against a panel of normals (PON), created by **GATK CNV Panel Workflow**, to obtain copy ratios and models segments from the copy ratios and allelic counts. The latter modeling incorporates data from a matched control sample. The same workflow steps apply to targeted exome and whole genome sequencing data [1].\n\nThe basis of copy number variant detection is formed by collecting coverage counts, while the resolution of the analysis is defined by the genomic intervals list. In the case of whole genome data, the reference genome is divided into equally sized intervals or bins, while for exome data, the target regions of the capture kit should be padded. In either case the **PreprocessIntervals** tool is used for preparing the intervals list which is then used for collecting the raw integer counts. For this step,**CollectReadCounts** is utilized, which counts reads that overlap the interval. Read counts are standardized and denoised against the PON with the **DenoiseReadCounts** tool. Standardized and denoised copy ratios are plotted using the **PlotDenoisedCopyRatios** tool [2].\n\nNext step in the workflow is segmentation, performed by the **ModelSegments** tool [3]. In segmentation, contiguous copy ratios are grouped together into segments. The tool performs segmentation for both copy ratios and for allelic copy ratios, given allelic counts. **CollectAllelicCounts** will tabulate counts of the reference allele and counts of the dominant alternate allele for each site in a given genomic intervals list (**Common sites**). Modeled copy ratio and allelic fraction segments are plotted using the **PlotModeledSegments** tool.\n\nThe **CallCopyRatioSegments** tool allows for systematic calling of copy-neutral, amplified and deleted segments. The **Neutral segment copy ratio lower bound** (default 0.9) and **Neutral segment copy ratio upper bound** (default 1.1) parameters together set the copy ratio range for copy-neutral segments [4].\n\nSome of the common input parameters are listed below:\n* **Input reads - tumor** - Tumor BAM/SAM/CRAM file. In case of BAM and CRAM formats BAI and CRAI index files are required.\n* **Input reads - normal** - Matched normal BAM/SAM/CRAM file. In case of BAM and CRAM formats BAI and CRAI index files are required.\n* **Panel of normals** - CNV panel of normals (PON) file in HDF5 format.\n* **Reference** - Reference genome in FASTA format along with FAI and DICT secondary files.\n* **Intervals** - Required for both WGS and WES cases. Accepted formats must be compatible with the GATK `-L` argument. For WGS, the intervals should simply cover the autosomal chromosomes (sex chromosomes may be included, but care should be taken to avoid creating panels of mixed sex, and to denoise case samples only with panels containing only individuals of the same sex as the case samples) [5].\n* **Bin length** - This argument is used by the **PreprocessIntervals** tool and must be set to the same value that was used to create the PON file. If intervals in PON do not exactly match the ones used to collect read counts for case sample, the workflow will produce an error. For WES analysis, this parameter should be set to 0.\n* **Common sites** - Sites at which allelic counts will be collected, used in the **CollectAllelicCounts** tool. The file must be compatible with the GATK -L argument. This is usually a dbsnp VCF or Mills gold standard (SNPs only) VCF file. In case of WES analysis, we advise using subset of this file with variants contained in target intervals. This would reduce execution time of **CollectAllelicCounts** tool and would require less resources (see *Common Issues and Important Notes*).\n\n### Changes Introduced by Seven Bridges\n* Outputs of several tools in the workflow are grouped together using the **SBG Group Outputs** tool. This does not affect the contents of the files nor execution performance, it is introduced with the purpose of keeping output files neatly organized.\n\n### Common Issues and Important Notes\n* For WGS and some cases of WES samples **CollectAllelicCounts** will require more memory than the default 13000 MB. If the entire set of variants from dbsnp is used as input for this tool, we advise allocating at least 100000 MB (100GB) of memory through the **Memory per job** parameter.\n* For WGS analysis, **ModelSegments** may require more memory than the default 13000 MB. We advise allocating at least 32000 MB (32GB) of memory through the **Memory per job** parameter.\n* This workflow is set to run on two instances in parallel to reduce execution time. This is achieved through the `sbg:maxNumberOfParallelInstances` instance hint. \n\n### Performance Benchmarking\n| Input Size | Experimental Strategy | Coverage | Duration | Cost (on demand) |\n| --- | --- | --- | --- | --- | --- |\n| 2 x 45GB | WGS | 8x | 1h 34min | $3.27 | \n| 2 x 120GB | WGS | 25x | 3h 23min | $7.08 |\n| 2 x 210GB | WGS | 40x | 4h 57min | $10.56 |\n| 2 x 420GB | WGS | 80x | 8h 58min | $19.96 |\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"intervals\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"sequence_dictionary\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_alignments_tumor\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"common_sites\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"count_panel_of_normals\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"GATK CNV Somatic Pair Workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n* [1] [https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_pair_workflow.wdl](https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_pair_workflow.wdl)\n* [2] [https://gatkforums.broadinstitute.org/dsde/discussion/11682](https://gatkforums.broadinstitute.org/dsde/discussion/11682)\n* [3] [https://gatkforums.broadinstitute.org/dsde/discussion/11683#6](https://gatkforums.broadinstitute.org/dsde/discussion/11683#6)\n* [4] [https://gatkforums.broadinstitute.org/dsde/discussion/11683#6](https://gatkforums.broadinstitute.org/dsde/discussion/11683#6)\n* [5] [https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists](https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists)", "input": [{"name": "Sequence dictionary"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Blacklisted intervals", "encodingFormat": "application/x-vcf"}, {"name": "Input reads - tumor", "encodingFormat": "application/x-sam"}, {"name": "Collect counts format"}, {"name": "Input reads - normal", "encodingFormat": "application/x-sam"}, {"name": "Common sites", "encodingFormat": "text/x-bed"}, {"name": "Memory CollectAllelicCounts"}, {"name": "Minimum base quality"}, {"name": "Number of eigensamples"}, {"name": "Panel of normals", "encodingFormat": "application/x-hdf5"}, {"name": "Window size"}, {"name": "Smoothing credible interval threshold copy ratio"}, {"name": "Smoothing credible interval threshold allele fraction"}, {"name": "Number of smoothing iterations per fit"}, {"name": "Number of samples copy ratio"}, {"name": "Number of samples allele fraction"}, {"name": "Number of changepoints penalty factor"}, {"name": "Number of burn in samples copy ratio"}, {"name": "Number of burn in samples allele fraction"}, {"name": "Minor allele fraction prior alpha"}, {"name": "Minimum total allele count normal"}, {"name": "Minimum total allele count case"}, {"name": "Memory ModelSegments"}, {"name": "Maximum number of smoothing iterations"}, {"name": "Maximum number of segments per chromosome"}, {"name": "Kernel variance copy ratio"}, {"name": "Kernel variance allele fraction"}, {"name": "Kernel scaling allele fraction"}, {"name": "Kernel approximation dimension"}, {"name": "Genotyping homozygous log ratio threshold"}, {"name": "Genotyping base error rate"}, {"name": "Outlier neutral segment copy ratio Z score threshold"}, {"name": "Neutral segment copy ratio upper bound"}, {"name": "Neutral segment copy ratio lower bound"}, {"name": "Calling copy ratio Z score threshold"}, {"name": "Minimum contig length"}, {"name": "Run Oncotator"}], "output": [{"name": "Tumor outputs", "encodingFormat": "application/x-hdf5"}, {"name": "Normal outputs", "encodingFormat": "application/x-hdf5"}, {"name": "Oncotator outputs", "encodingFormat": "application/x-hdf5"}, {"name": "Preprocessed Intervals"}, {"name": "Entity ID tumor"}, {"name": "Entity ID normal"}], "softwareRequirements": ["MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1649156263, "dateCreated": 1555085893, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practices-somatic-cnv-panel-workflow-4-1-0-0/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practices-somatic-cnv-panel-workflow-4-1-0-0/14", "applicationCategory": "Workflow", "name": "BROAD Best Practices Somatic CNV Panel Workflow 4.1.0.0", "description": "BROAD Best Practices Somatic CNV Panel is used for creating a panel of normals (PON) given a set of normal samples.\n\n### Common Use Cases\n\nFor CNV discovery, the PON is created by running the initial coverage collection tools individually on a set of normal samples and combining the resulting copy ratio data using a dedicated PON creation tool [1]. This produces a binary file that can be used as a PON. It is very important to use normal samples that are as technically similar as possible to the tumor samples (same exome or genome preparation methods, sequencing technology etc.) [2].\n \nThe basis of copy number variant detection is formed by collecting coverage counts, while the resolution of the analysis is defined by the genomic intervals list. In the case of whole genome data, the reference genome is divided into equally sized intervals or bins, while for exome data, the target regions of the capture kit should be padded. In either case, the **PreprocessIntervals** tool is used for preparing the intervals list which is then used for collecting raw integer counts. For this step **CollectReadCounts** is utilized, which counts reads that overlap the interval. Finally a CNV panel of normals is generated using the **CreateReadCountPanelOfNormals** tool. \n\nIn creating a PON, **CreateReadCountPanelOfNormals** abstracts the counts data for the samples and the intervals using Singular Value Decomposition (SVD), a type of Principal Component Analysis. The normal samples in the PON should match the sequencing approach of the case sample under scrutiny. This applies especially to targeted exome data because the capture step introduces target-specific noise [3].\n\nSome of the common input parameters are listed below:\n*  **Input reads** (`--input`) - BAM/SAM/CRAM file containing reads. In the case of BAM and CRAM files, secondary BAI and CRAI index files are required.\n* **Intervals** (`--intervals`) - required for both WGS and WES cases. Formats must be compatible with the GATK `-L` argument. For WGS, the intervals should simply cover the autosomal chromosomes (sex chromosomes may be included, but care should be taken to avoid creating panels of mixed sex, and to denoise case samples only with panels containing only individuals of the same sex as the case samples)[4].\n* **Bin length** (`--bin-length`). This parameter is passed to the **PreprocessIntervals** tool. Read counts will be collected per bin and final PON file will contain information on read counts per bin. Thus, when calling CNVs in Tumor samples, **Bin length** parameter has to be set to the same value used when creating the PON file.\n* **Padding** (`--padding`). Also used in the **PreprocessIntervals** tool, defines number of base pairs to pad each bin on each side.\n* **Reference** (`--reference`) - Reference sequence file along with FAI and DICT files.\n* **Blacklisted Intervals** (`--exclude_intervals`) will be excluded from coverage collection and all downstream steps.\n* **Do Explicit GC Correction** - Annotate intervals with GC content using the **AnnotateIntervals** tool.\n\n### Changes Introduced by Seven Bridges\n*The workflow in its entirety is per [best practice](https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_panel_workflow.wdl) specification.* \n\n### Performance Benchmarking\n\n| Input Size | Experimental Strategy | Coverage | Duration | Cost (on demand) | AWS Instance Type |\n| --- | --- | --- | --- | --- | --- | --- |\n| 2 x 45GB | WGS | 8x | 33min | $0.59 | c4.4xlarge 2TB EBS |\n| 2 x 120GB | WGS | 25x | 1h 22min | $1.47 | c4.4xlarge 2TB EBS |\n| 2 x 210GB | WGS | 40x | 2h 19min | $2.48 | c4.4xlarge 2TB EBS |\n| 2 x 420GB | WGS | 80x | 4h 15min | $4.54 | c4.4xlarge 2TB EBS |\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"sequence_dictionary\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"intervals\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_alignments\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])), \n\t\"in_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"output_prefix\": \"sevenbridges\"}\n# Creates draft task\ntask = api.tasks.create(name=\"GATK CNV Somatic Panel Workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n* [1] [https://github.com/gatk-workflows/gatk4-somatic-cnvs](https://github.com/gatk-workflows/gatk4-somatic-cnvs)\n* [2] [https://gatkforums.broadinstitute.org/gatk/discussion/11053/panel-of-normals-pon](https://gatkforums.broadinstitute.org/gatk/discussion/11053/panel-of-normals-pon)\n* [3] [https://gatkforums.broadinstitute.org/dsde/discussion/11682](https://gatkforums.broadinstitute.org/dsde/discussion/11682)\n* [4] [https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists](https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists)", "input": [{"name": "Sequence dictionary"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Blacklisted intervals", "encodingFormat": "application/x-vcf"}, {"name": "Mappability track", "encodingFormat": "text/x-bed"}, {"name": "Segmental duplication track", "encodingFormat": "text/x-bed"}, {"name": "Input reads", "encodingFormat": "application/x-sam"}, {"name": "PON entity id"}, {"name": "Do explicit GC correction"}], "output": [{"name": "Preprocessed Intervals"}, {"name": "Read counts", "encodingFormat": "application/x-hdf5"}, {"name": "Entity ID"}, {"name": "Panel of normals", "encodingFormat": "application/x-hdf5"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1649156262, "dateCreated": 1555085892, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_lizhang/adk-resources/bwa-mem-multi-lane/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/lizhang/adk-resources/bwa-mem-multi-lane/9", "applicationCategory": "Workflow", "name": "BWA MEM Multi-Lane", "description": "[Beta] BWA MEM workflow aligning and merging multiple pairs of FASTq files into single BAM output.\n\nThis workflow is used as part of the \"Sample QC\" automation example available on [GitHub](https://github.com/sbg/adk-examples/tree/master/examples/sample-qc). It takes multi-lane FASTQ files as input, uses BWA MEM to align reads to given reference genome, and then merges all per-lane BAM files into a single BAM file using \"Sambamba Merge\". Please note this workflow is still under development and not yet recommended for use in production pipelines.", "input": [{"name": "#FASTQ", "encodingFormat": "text/fastq"}, {"name": "Reference or TAR with BWA reference indices", "encodingFormat": "application/x-tar"}], "output": [{"name": "merged_bam", "encodingFormat": "application/x-bam"}, {"name": "aligned_reads", "encodingFormat": "application/x-bam"}], "project": "adk-resources", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1597679589, "dateCreated": 1554149671, "contributor": ["christian_frech", "lizhang"], "sdPublisher": "ADKDEMO", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-macs2/21", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-macs2/21", "applicationCategory": "Workflow", "name": "ChIP-seq BWA Alignment and Peak Calling MACS2 2.1.1", "description": "ChIP-seq (Chromatin immunoprecipitation followed by high-throughput sequencing technology) allows researchers to study the landscape of chromatin modifications or the binding patterns of transcription factors and other chromatin-associated proteins like RNA-polymerases.\n\nThis pipeline takes you all the way from unaligned raw sequencing reads (FASTQ) to ChIP-seq peak detection.\n\nSequenced reads are aligned with the BWA-backtrack (preferably for short reads). After alignment, BAM files are filtered (to remove low quality and unpaired reads) and de-duplicated. A number of QC metrics that evaluate the library complexity and signal enrichment are calculated. Peak calling is performed on the processed BAM files using MACS2 peak caller.\n\nRequired inputs:\n\n* Treatment FASTQ files (ID: treatment_files) - one FASTQ file for single-end data, or two files for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata **Paired-end** field as 1 for one input file and as 2 for the other input file.\n\n* Reference/Index files (ID: reference_index_tar) - TAR bundle of already generated BWA index files.\n\nOptional inputs:\n\n* Control FASTQ files (ID: control_files) - one FASTQ file for single-end data, or two files per for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata **Paired-end** field as 1 for one input file and as 2 for the other input file.\n\n* Blacklist file (ID: blacklist_file) - BED file containing genomic regions that have anomalous, unstructured, high signal/read counts in next-generation sequencing experiments. It could be used by BEDTools intersect to increase the accuracy of peak calling and downstream ChIP-seq analysis.\n\n###**Common Use Cases**\nBefore executing the workflow you should choose between narrow and broad peak analysis by setting the **Analysis type** parameter and organism to calculate effective genome size by setting **Organism** parameter within MACS2 app in Define App Settings. \n \nBlacklisting:\n\nWhen provided with adequate blacklist BED file, BEDTools intersect within this pipeline, will remove blacklisted regions from MACS2 *narrowPeak/broadPeak* output file. The exclusion of these regions, called blacklist regions, aims to remove sources of artifact signal caused by biases from chromatin accessibility and ambiguous alignment to increase the accuracy of both peak calling and downstream ChIP-seq analysis.\n\n###**Changes Introduced by Seven Bridges**\n\n* MACS2 **Analysis type** parameter is used for choosing between narrow and broad peak analysis. If narrow peak is selected, **QVALUE** (`--qvalue`) parameter will be set to 0.01. If broad peak is selected, **BROAD** (`--broad`) parameter will be set to True and **QVALUE** (`--qvalue`) parameter to 0.05. \n\n* MACS2 uses the estimated fragment length to extend reads in 5'->3' direction to fix-sized fragments. By default, the fragment length is estimated by MACS2 during the model building step. However, for this specific workflow (following ENCODE pipeline), the predominant fragment length is estimated in a previous step (*JSON output; from 'SBG ChIP-seq filter and QC') and used as an input via the parameter **EXTSIZE** (`--extsize`). The parameter **NOMODEL** (`--nomodel`) is also set to force MACS2 to bypass the building of the shifting model.\n\n* **KEEP_DUPS** (`--keep-dup`) parameter is set to 'all' to prevent MACS2 from searching for duplicated reads because we expect the input BAM files to be de-duplicated. If this is not the case, this parameter should be set to the default option '1' which is to keep one tag at the same location.\n\n* **spmr** (`--SPMR`) and **BDG** (`--bdg`) parameters are set to True to perform normalization per million reads for fragment pileup and save extended fragment pileup, and local lambda tracks into a bedGraph file.\n\n###**Common Issues and Important Notes**\n\n* **When providing treatment and control files to the input, a \"Case ID\" and \"Sample ID\" metadata should be properly set for each file. PE files should have the same \"Sample ID\" and \"Case ID\". Treatment FASTQ files (PE or SE) should have the same \"Case ID\" as corresponding control FASTQ files.**\n\n* In some situations read trimming is necessary, for instance, if read ends display poor quality values (most generally the right end) or if treatment and control FASTQ files have inconsistent read lengths. In these cases, read-trimming should be applied.\n\n*  Preferred values for library complexity are NRF > 0.9, PBC1 \u2265 0.9, and PBC2 \u2265 10.\n\n* Preferred values for NSC and RSC metric when analysing sharp histone marks are: NSC > 1.05 and RSC > 0.8 (Even if ChIP-seq data does not meet these guidelines, there could still be significant biological information. The users should evaluate the profiles of the cross-correlation plot to further assess the quality of their data).\n\n* Input (control) and negative control samples should have low NSC and RSC scores.\n\n* SBG ChIP-seq Set Metadata tool adds values 'control' and 'sample' to the metadata field **chip-seq** of the input files.\n\n* Along with suggested input files for Human genome, Mus musculus Reference/index TAR file and Blacklist file could be obtained from Public Reference Files.\n\n* If the total size of input FASTQ files in a single task exceeds 20GB a larger AWS instance should be allocated due to memory requirements.\n\n* In cases when analysing two biological replicates, outputs between replicates should be compared using IDR app. IDR is publicly available app which could be used to combine outputs of ChIP-seq pipeline to perform the comparative analysis between biological replicates in the separate task.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n###**Performance Benchmarking**\n\nThe  run time and cost strongly depend on the input file sizes and available resources. Based on our testing, BWA Aligment and Filtering tool has increasing demand for RAM memory with the increase of sequencing reads within FASTQ input files. We set c4.4xlarge AWS instance with 30GB of RAM as a default instance but in some cases where more memory is requred our suggestion is using c4.8xlarge AWS instance. Here are some examples of run times and costs for tasks running on AWS spot instances:\n\n|Input FASTQ file size|AWS Instance| Duration | Cost |\n| --- | --- | --- | --- |\n| 3.4 GB (SE, fastq.gz, treatment)  | c4.4xlarge | 1h27m | $0.49 |\n| 3.4 GB + 410.4 MB (SE, fastq.gz, treatment + control) | c4.4xlarge | 1h36m | $0.54 |\n| 6 GB + 6.4 GB (PE, fastq.gz, treatment) | c4.4xlarge | 6h50m | $2.31 |\n| 6 GB + 6.4 GB + 3.4 GB + 3.7 GB  (PE, fastq.gz, treatment + control) | c4.4xlarge | 9h51m | $3.33 |\n| 3.4 GB (SE, fastq.gz, treatment)  | c4.8xlarge | 1h24m | $0.83 |\n| 3.4 GB + 410.4 MB (SE, fastq.gz, treatment + control) | c4.8xlarge | 1h23m | $0.83 |\n| 6 GB + 6.4 GB (PE, fastq.gz, treatment) | c4.8xlarge | 6h15m | $3.73 |\n| 6 GB + 6.4 GB + 3.4 GB + 3.7 GB  (PE, fastq.gz, treatment + control) | c4.8xlarge | 6h41m | $3.98 |\n\n###**API Python Implementation**\n\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n    'reference_index_tar': api.files.query(project=project_id, names=reference_index_tar_file)[0],\n    'treatment_files': list(api.files.query(project=project_id, names=treatment_files)),\n    'control_files': list(api.files.query(project=project_id, names=control_files)),\n    'blacklist_file': api.files.query(project=project_id, names=blacklist_file)[0],\n    'analysis_type' : \"narrow peaks\",\n    'Organism' : \"hs\" \n    }\ntask = api.tasks.create(name='ChIP-seq BWA Alignment and Peak Calling MACS2 - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False)\n\n#For running a batch task\ntask = api.tasks.create(name='ChIP-seq BWA Alignment and Peak Calling MACS2 - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False, batch_input='treatment_files', batch_by = { 'type': 'CRITERIA', 'criteria': [ 'metadata.sample_id'] })\n```\nwhere `reference_index_tar`, `treatment_files`, `control_files` and `blacklist_file` are list objects containing the input files. \n\nInstructions for installing and configuring the API Python client, are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n**Running a batch task:**\nBatching is performed by **Sample ID** metadata on the treatment_files port. For running analysis in batches it is necessary to set **Sample ID** metadata for each file and for paired-end data both files must have the same **Sample ID**.", "input": [{"name": "control_files"}, {"name": "reference_index_tar", "encodingFormat": "application/x-tar"}, {"name": "blacklist_file", "encodingFormat": "text/x-bed"}, {"name": "treatment_files", "encodingFormat": "text/fastq"}, {"name": "If True, MACS will save signal per million  reads for fragment pileup profiles"}, {"name": "Buffer size"}, {"name": "Band width for picking regions to compute fragment size"}, {"name": "Analysis type"}, {"name": "When set, scale the small sample up to the bigger sample"}, {"name": "Tag Size"}, {"name": "The small nearby region in basepairs to calculate dynamic lambda"}, {"name": "(NOT the legacy --shiftsize option!) The arbitrary shift in bp"}, {"name": "Set the random seed while down sampling data"}, {"name": "When set, use a custom scaling ratio of ChIP/control (e.g. calculated using NCIS) for linear scaling"}, {"name": "Minimum FDR (q-value) cutoff for peak detection"}, {"name": "Pvalue cutoff for peak detection"}, {"name": "Name of organism to calculate effective genome size. 'hs' for human (2.7e9), 'mm' for mouse   (1.87e9), 'ce' for C. elegans (9e7) and 'dm' for fruitfly (1.2e8)"}, {"name": "If True, MACS will use fixed background lambda as local lambda for every peak region"}, {"name": "Whether or not to build the shifting model"}, {"name": "Select the regions within MFOLD range of high-confidence enrichment ratio against background to build model"}, {"name": "The large nearby region in basepairs to calculate dynamic lambda"}, {"name": "Keep duplicates"}, {"name": "Whether turn on the auto pair model process"}, {"name": "When set, the value will be used to filter out peaks with low fold-enrichment"}, {"name": "Experiment name which will be used to name the output files"}, {"name": "When set, random sampling method will scale down the bigger sample"}, {"name": "While set, MACS2 will analyze number or total length of peaks that can be called by different p-value cutoff then output a summary table to help user decide a better cutoff"}, {"name": "If set, MACS will use a more sophisticated signal processing approach to find subpeak summits in each enriched peak region"}, {"name": "Cutoff for broad region"}, {"name": "If set, MACS will try to call broad peaks by linking nearby highly enriched regions"}, {"name": "Save extended fragment pileup, and local lambda tracks into a bedgraph file."}, {"name": "Number of threads"}], "output": [{"name": "BROAD_PEAK"}, {"name": "NARROW_PEAK"}, {"name": "Rmodel"}, {"name": "EXCEL_FILE"}, {"name": "BAM index"}, {"name": "report_zip", "encodingFormat": "application/zip"}, {"name": "filtered BAM", "encodingFormat": "application/x-bam"}, {"name": "output_json"}, {"name": "final_output", "encodingFormat": "application/x-vcf"}, {"name": "SUMMIT_BED_FILE"}, {"name": "SAMPLE_BDG"}, {"name": "CONTROL_BDG"}, {"name": "output_html", "encodingFormat": "text/html"}], "applicationSubCategory": ["Epigenetics", "ChIP-Seq"], "project": "SBG Public Data", "creator": "Milos Jordanski, Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155967, "dateCreated": 1500041064, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-spp-1-14/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-spp-1-14/10", "applicationCategory": "Workflow", "name": "ChIP-seq BWA Alignment and Peak Calling SPP 1.14", "description": "**Description:**\n\nChIP-seq (Chromatin immunoprecipitation followed by high-throughput sequencing technology) allows researchers to study the landscape of chromatin modifications or the binding patterns of transcription factors (TF) and other chromatin-associated proteins like RNA-polymerases or chromatin modulators.\n\nThis pipeline takes you all the way from unaligned raw sequencing reads (FASTQ) to ChIP-seq peaks detection.\n\nSequenced reads are aligned with the BWA-backtrack tool (specific for read size < 100bp). After alignment, BAM files are filtered (to remove low quality and unpaired reads) and de-duplicated. A number of QC metrics that evaluate the library complexity and signal enrichment are calculated. Peak calling is performed on the processed BAM files using SPP which is suggested peak caller for analysing binding patterns of TF.\n\n**Blacklisting**\n\nPrior to further ChIP-seq analysis, genomic regions that are associated with artifact signal may be removed. The exclusion of these regions, called blacklist regions, aims to remove sources of artifact signal caused by biases from chromatin accessibility and ambiguous alignment in order to increase the accuracy of both peak calling and comparative ChIP analysis. BEDTools intersect tool within this pipeline, when provided with adequate blacklist file, will remove blacklisted regions from peak SPP output file. \n\n**Required inputs:**\n\n* Input FASTQ reads (ID: Sample FASTQ) - one fastq file per sample for single-end data, or two files per sample for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata 'paired-end' field as 1 for one input file, as 2 for the other input file.\n\n* Reference/Index files (ID: Reference index) - TAR bundle of already generated index files.\n\n**Optional input:**\n\n* Control FASTQ reads (ID: Control FASTQ) - one fastq file per sample for single-end data, or two files per sample for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata 'paired-end' field as 1 for one input file, as 2 for the other input file.\n\n* Blacklist file (ID: Blacklist file) - BED file containing genomic regions that have anomalous, unstructured, high signal/ read counts in next-generation sequencing experiments. It could be used by BEDTools intersect in order to increase the accuracy of peak calling and comparative ChIP analysis.\n\n**Outputs:**\n\n* Sample_FASTQ.qc.b64html: B64HTML file with a table of all QC metrics.\n\n* Sample_FASTQ_deduped.filter.srt.bam: BAM files that are filtered (to remove low quality and unpaired reads) and de-duplicated. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Control_FASTQ_deduped.filter.srt.bam: BAM files that are filtered (to remove low quality and unpaired reads) and de-duplicated. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Sample_FASTQ_deduped.filter.srt.bam.bai: BAI index files for processed BAM files. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Control_FASTQ_deduped.filter.srt.bam.bai: BAI index files for processed BAM files. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Sample_FASTQ_SPPpeaks.narrowPeak: a BED6+4 format file which contains the peak locations together with pvalue and qvalue. This output contains fixed width peaks.\n\n* Sample_FASTQ_SPPpeaks.regionPeak: a BED6+4 format file which contains the peak locations together with pvalue and qvalue.  This output contains variable width peaks with regions of enrichment around peak summits.\n\n* Sample_FASTQ_SPPmodel.Rdata: Rdata object which you can use to access the model and output results produced by SPP. \n\n* Sample_FASTQ_SPPxcorplot.pdf: The cross-correlation of stranded read density profiles plot saved in a PDF file.\n\n* Sample_FASTQ_SPPpeaks.blacklisted.narrowPeak: Sample FASTQ_SPPpeaks.narrowPeak file without blacklist regions.(This file is not created if Blacklist file is not provided on input).\n\n**Important notes:**\n\n* **When providing sample and control files to the input, a \"Case ID\" and \"Sample ID\" metadata should be properly set for each file. PE files should have the same \"Sample ID\" and \"Case ID\". Sample files (PE or SE) should have the same \"Case ID\" as corresponding control files.**\n\n* In some situations read trimming is necessary, for instance, if read ends display poor quality values (most generally the right end) or if control and ChIP-seq samples have inconsistent read lengths. In these cases, read-trimming should be applied.\n\n* Preferred values for library complexity are NRF>0.9, PBC1 \u2265 0.9, and PBC2 \u2265 10.\n\n* Preferred values for NSC and RSC metric are: NSC > 1.05 and RSC > 0.8 (Even if ChIP-seq data does not meet these guidelines, there could still be significant biological information. The users should evaluate the profiles of the cross-correlation plot to further assess the quality of their data).\n\n* Input and negative control samples should have low NSC and RSC scores.\n\n* SBG ChIP-seq Set Metadata tool adds values 'control' and 'sample' to the metadata field 'chip-seq' of the input files.\n\n**Important notes about SPP options:**\n\n* **run_spp_nodups:** In this workflow we set \u2018run_spp_nodups\u2019 because we expect the input BAM files to be de-duplicated (after 'SBG ChIP-seq Filter and QC' workflow). If this is not the case, this parameter should not be set (un-select it).\n\n* **fragLen**: SPP requires the user to provide the fragment-length cross-correlation peak strandshift value to the \u2014speak parameter. In this workflow, this argument is passed to SPP using the fragment length estimated in the previous step (*json output; from 'SBG ChIP-seq filter and QC').\n\n* **npeak**: The threshold on a number of peaks to call is set to 300,000 to allow for 'relaxed' peak calling. Peak sets thresholded in this way are not meant to be interpreted as definitive binding events, but are rather intended to be used as input for subsequent statistical comparison of replicates. Please use the fdr parameter and leave the npeak option in blank if a more stringent analysis is needed.\n\n* **call_narrowPeaks**: Select this option for \u2018narrowPeak\u2019 output if you require that fixed width peaks are outputted.\n\n* **call_regionPeaks**: Select this option for \u2018regionPeak\u2019 output if you require that variable width peaks with regions of enrichment around peak summits outputted (recommended).\n\n**Common issues and limitations of the workflow:**\n\n* If the total size of the input files in a single task exceeds 20GB perhaps a larger instance should be allocated due to memory requirements.\n\n* In cases when analysing two biological replicates, outputs between replicates should be compared using IDR app. IDR is publicly available app which could be used to combine outputs of chip-seq pipeline in order to perform the comparative analysis between biological replicates in the separate task.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.", "input": [{"name": "control_files"}, {"name": "sample_files", "encodingFormat": "text/fastq"}, {"name": "reference_index_tar", "encodingFormat": "application/x-tar"}, {"name": "blacklist_file", "encodingFormat": "text/x-bed"}, {"name": "spp_nodups"}, {"name": "FDR"}, {"name": "savn"}, {"name": "Number of threads"}], "output": [{"name": "BAM index"}, {"name": "filtered BAM", "encodingFormat": "application/x-bam"}, {"name": "output_json"}, {"name": "b64html", "encodingFormat": "text/html"}, {"name": "final_output", "encodingFormat": "application/x-vcf"}, {"name": "output_NarrowPeak"}, {"name": "output_Rdata"}, {"name": "out_crosscorr_pdf"}, {"name": "report_zip", "encodingFormat": "application/zip"}], "applicationSubCategory": ["Epigenetics", "ChIP-Seq"], "project": "SBG Public Data", "creator": "Nemanja Vucic, Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155967, "dateCreated": 1500041198, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/clustering-and-gene-marker-identification-with-seurat-3-2-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/clustering-and-gene-marker-identification-with-seurat-3-2-2/3", "applicationCategory": "Workflow", "name": "Clustering and Gene Marker Identification with Seurat 3.2.2", "description": "The workflow performs clustering and gene marker identification analysis starting from gene-cell UMI or read counts.\n\nThe **Clustering and Gene Marker Identification with Seurat 3.2.2** workflow is based on the **Seurat 3.2.2** R package [1] and it can be used to process gene-cell UMI or read counts produced with the following tools available on the Seven Bridges Platform: **Cell Ranger counts**, **Salmon Alevin**, **Kallisto BUStools Workflow**, **zUMIs**, **Single-Cell Smart-seq2 Workflow v3.0.0**, and **STAR** (STARsolo option). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nDepending on the quantification method used, the **Input type** parameter needs to be specified adequately and the **Gene-cell count matrices** input port needs to be provided with one of the following files:\n\n- Cell Ranger counts:  filtered or unfiltered feature_bc_matrix.tar.gz file (from the *Feature-barcode matrices TAR* output port); \n- Salmon Alevin:  alevin_output.tar.gz file (from the *Compressed output directory* output port);\n- Kallisto BUStools Workflow: Rdata file (from the *Counts table* output port);\n- zUMIs: RDS file (from the *Expression files* output port);\n- STAR: filtered or unfiltered .tar.gz file (from the *STARsolo matrix TAR* output port);\n- Single-Cell Smart-seq2 Workflow v3.0.0: gene_expression.results.mtx file (from the *RSEM gene expression matrix* output port).\n\nRemoving low quality cells and unexpressed genes from the data can significantly improve the quality of the clustering analysis. Some of the parameters that can be used for filtering are: **Minimum number of cells**, **Minimum number of genes**, **Minimal number of UMI counts detected per cell**, **Minimal number of genes detected per cell**, **Maximum percentage of mitochondrial genes** etc. For more details and the default values, please read the parameters description. \n\nThe quality of the clustering can be further improved by restricting the variation due to uninteresting (unwanted) sources. By default, the Seurat method `SCTransform` will regress out variation due to number of UMIs (sequencing depth), number of genes, mitochondrial expression, and cell cycle phase (the last mentioned can be avoided by setting the **Cell cycle regression** parameter to **FALSE**). \n\nTo overcome the extensive technical noise in the expression of any single gene for scRNA-seq data, the Seurat assigns cells to clusters based on their PCA scores derived from the expression of the integrated most variable genes. With the Seurat `SCTransform` method, selection of PCs is no longer as important as it used to be. In theory, the more PCs we choose, the more variation is accounted for when performing the clustering, but it takes longer to perform the clustering. The **Number of principal components** parameter defines the number of PCs that will be used for clustering (default = **30**).\n\nThe parameter of particular importance for this analysis is the **Clustering resolutions** parameter specifying which resolution values should be used by the clustering algorithm to determine the number of clusters (higher resolution value will result in higher number of clusters). Multiple values for the parameter can be set and the results will be produced for each provided value (default = **0.5**). \n\nIn addition, a clustering tree, showing how samples move as the number of clusters increases, may help in deciding what resolution to use. The **Clustree resolutions** parameter specifies the resolutions that will be used for making the clustering tree (default = **0.1**, **0.2**, **0.3**, **0.4**, **0.5**, **0.6**, **0.7**, **0.8**, **0.9**, and **1** + resolutions chosen using the **Clustering resolutions** parameter, if not already in the list).\n \n To identify gene markers of a single cluster (compared to all other cells), Seurat performs differential expression. The testing method can be set using the **Differential expression test** parameter (default = **wilcox**, a.k.a. The Mann Whitney U test). Two parameters are important for gene marker selection: the **Minimum percentage of expressed cells** parameter that requires a gene to be detected at a minimum percentage in either of the two groups of cells (default = **0.1**) and the **Minimum log fold change** parameter that requires a gene to be differentially expressed (on average) by certain amount between the two groups (default = **0.25**).\n\nBy default, only positive markers will be considered. If the **Only positive markers** parameter is set to **FALSE**, both positive and negative markers will be included in the analysis.\n\nAs a result, the workflow produces: \n- HTML report, containing quality metric plots, the UMAP plot showing the results of clustering, the clustering tree plot, the table with top 10 markers per cluster, and the heatmap plot showing the expression pattern of marker genes among clusters.\n- TSV table with detected biomarkers (one table for each resolution).\n- PDF report, containing multiple violin and feature plots showing the top 6 marker genes identified in each cluster (one PDF file for each resolution). This output can be omitted by setting the **Biomarker plots** parameter to **FALSE**.\n- Result Seurat object, allowing further exploration of the data and results locally.\n\n### Changes Introduced by Seven Bridges\nThe Clustering tree plot is produced using the **Clustree** R package.\n\n### Common Issues and Important Notes\n- Currently, only human and mouse samples are supported.\n- In most of the cases, it is hard to tell the optimal parameter values for best clustering results in advance. Thus, after analysing all the QC and clustering plots, re-running of the analysis with corrected (optimal) parameter values is recommended.\n- The **Minimum number of genes** parameter is applied before the Seurat object is created, thus removed cells won\u2019t be visible in the QC plots of the html report. Using the **Minimal number of genes detected per cell** and the **Minimal number of UMI counts detected per cell** parameters would then be a safer option for removing low quality cells because the result of filtering using these parameters will be visible.\n- While running the PCA, the total number of PCs to compute and store is 50 by default. Thus, the total number of cells after filtering should be greater than 50, otherwise the task will fail.\n- In some cases, the workflow won't be able to detect mitochondrial counts, thus the percent of mitochondrial counts won't be used as a criterium for filtering. This may happen due to differences in labelling of the mitochondrial genes comparing different model organisms and reference files. Currently, the workflow should be able to detect human and mouse mitochondrial gene ids.\n- If the size of the html or pdf report exceeds 10 MiB, the preview of the file's content will not be supported on the Seven Bridges Platform. In that case, the files can be downloaded and observed locally.\n\n\n### Performance Benchmarking\n\nRuntime and task cost for different number of analysed cells, using the default options:\n\n| # of cells | Quantifier used  | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|       1.2 k     |     Cell Ranger   |   5 min   |   $0.05    |    c4.2xlarge    |\n|       4.6 k     |     Cell Ranger  |   11 min   |   $0.10    |    c4.2xlarge    |\n|       8.7 k     |     Cell Ranger    |   18 min   |   $0.17    |    c4.2xlarge    |\n|       11.7 k     |    Cell Ranger    |   30 min   |   $0.28    |    c4.2xlarge    |\n\n      \n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'in_counts': list(api.files.query(project=project_id, names=['sample_kallisto_gene_counts.Rdata'])), \n        'input_type': \"kallisto_bus\"\n        }\n\n# Run the task\ntask = api.tasks.create(name='Seurat 3.2.2 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Seurat documentation](https://satijalab.org/seurat/)", "input": [{"name": "Gene-cell count matrices"}, {"name": "Input Type"}, {"name": "Output name prefix"}, {"name": "Minimum number of cells"}, {"name": "Minimum number of genes"}, {"name": "Project descriptor"}, {"name": "Minimal number of UMI counts detected per cell"}, {"name": "Maximal number of UMI counts detected per cell"}, {"name": "Minimal number of genes detected per cell"}, {"name": "Maximal number of genes detected per cell"}, {"name": "Maximum percentage of mitochondrial genes"}, {"name": "Novelty score (complexity)"}, {"name": "Cell cycle regression"}, {"name": "Number of principal components"}, {"name": "Clustering resolutions"}, {"name": "Minimum percentage of expressed cells"}, {"name": "Minimum log fold change"}, {"name": "Only positive markers"}, {"name": "Gene marker plots"}, {"name": "Differential expression test"}, {"name": "Clustree resolutions"}], "output": [{"name": "Output seurat object"}, {"name": "Clustering results table"}, {"name": "Report", "encodingFormat": "text/html"}, {"name": "Biomarker plots"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/clustering-and-gene-marker-identification-with-seurat-3-2-2/3.png", "codeRepository": ["https://github.com/satijalab/seurat"], "applicationSubCategory": ["Single Cell"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1612284091, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-pipeline-from-baseline-093/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-pipeline-from-baseline-093/13", "applicationCategory": "Workflow", "name": "CNVkit CNV Calling", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **CNVkit CNV Calling** workflow determines copy number alterations and variations.\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* For proper CNV detection in sex chromosomes,  corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n* For a deeper CNV analysis (eg. determining loss of heterozygosity regions), an additional step of calling the germline mutations in a tumor sample should be performed. This pipeline, unlike the **CNVkit CNV Calling with GATK HaplotypeCaller** doesn't have a variant caller implemented. Instead, a VCF created by another caller can be provided to the **SNP VCF** input.\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named based on the **Tumor BAM Sample ID** metadata field.\n\n### Common Issues and Important Notes\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* For proper CNV detection in sex chromosomes,  corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n### Performance Benchmarking\n\nThe instance set for this workflow is the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM and 256 GB of EBS (disk space).\n\n\n| BAM size (GB) | Type | Duration | Cost  |\n|---------------|------|----------|-------|\n|       12      | WES  | 9m       | $0.06 |\n|      144      | WGS  | 1h 26m       | $0.57 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"antitarget_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference_coverage_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit CNV Calling - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)", "input": [{"name": "Minimal variant depth"}, {"name": "SNP VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Zygosity freq"}, {"name": "Male reference"}, {"name": "Ploidy"}, {"name": "Gender"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Antitarget BED", "encodingFormat": "text/x-bed"}, {"name": "Reference coverage file"}, {"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Significance threshold"}, {"name": "Segmentation method"}, {"name": "Drop outliers"}, {"name": "Drop low coverage"}, {"name": "Y axis minimum"}, {"name": "Y axis maximum"}, {"name": "Width of margin"}, {"name": "Draw a trendline"}, {"name": "Segment color"}, {"name": "Gene(s)"}, {"name": "Chromosome"}, {"name": "By bin"}, {"name": "Background marker"}, {"name": "Antitarget marker"}, {"name": "Minimum mapping quality score"}, {"name": "Count read midpoints"}, {"name": "Skip RepeatMasker"}, {"name": "No GC"}, {"name": "No edge"}, {"name": "Hard thresholds for calling each integer copy number"}, {"name": "Purity of cellularity"}, {"name": "Filter"}, {"name": "Center at:"}, {"name": "Center"}, {"name": "Title"}, {"name": "Copy number threshold"}, {"name": "No shift XY"}, {"name": "Minimum number of covered probes"}], "output": [{"name": "CNV VCF"}, {"name": "CNV Calls"}, {"name": "CNV Scatter plot"}, {"name": "CNV Diagram"}], "codeRepository": ["https://github.com/etal/cnvkit"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Eric Talevich", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648048903, "dateCreated": 1535126713, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc/14", "applicationCategory": "Workflow", "name": "CNVkit CNV Calling with GATK HaplotypeCaller", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **CNVkit CNV Calling with GATK HaplotypeCaller** workflow determines copy number alterations and variations.\n\n* For a deeper CNV analysis (eg. determining loss of heterozygosity regions), a VCF containing germline mutations in a tumor must be provided. The germline mutations in a tumor are determined using the two GATK HaplotypeCallers. The first GATK HaplotypeCaller outputs a VCF with germline mutations in a normal sample. The created VCF is an input for the second GATK HaplotypeCaller, based on which the germline mutations are confirmed in a tumor sample.\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* For a proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named based on the **Tumor BAM** and **Normal BAM Sample ID** metadata field.\n\n### Common Issues and Important Notes\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* Matched Tumor-Normal BAMs are highly recommended as Tumor BAM and Normal BAM inputs, respectively.\n\n* A db SNP can be provided as an input in order to annotate known SNPs. \n\n* For a proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Performance Benchmarking\n\nThe instances set for this workflow are the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM with 1 TB of EBS (disk) and the c5.9xlarge instance with 36 vCPUs, 72 GiB of RAM with EBS set to 1 TB.\n\n\n| BAM size (GB) | Type | Duration | Cost  |\n|---------------|------|----------|-------|\n|       12      |  WES |    31m   | $0.79 |\n|      144      |  WGS |  8h 26m  | $12.9 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"antitarget_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference_coverage_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"normal_bam\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])), \n\t\"reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"bed_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit CNV Calling with GATK HaplotypeCaller - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)", "input": [{"name": "Minimal variant depth"}, {"name": "Zygosity freq"}, {"name": "Male reference"}, {"name": "Ploidy"}, {"name": "Gender"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Antitarget BED", "encodingFormat": "text/x-bed"}, {"name": "Reference coverage file"}, {"name": "Normal BAM", "encodingFormat": "application/x-bam"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Intervals file", "encodingFormat": "text/x-bed"}, {"name": "db SNP", "encodingFormat": "application/x-vcf"}, {"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Significance threshold"}, {"name": "Segmentation method"}, {"name": "Drop outliers"}, {"name": "Drop low coverage"}, {"name": "Y axis minimum"}, {"name": "Y axis maximum"}, {"name": "Width of margin"}, {"name": "Draw a trendline"}, {"name": "Segment color"}, {"name": "Gene(s)"}, {"name": "Chromosome"}, {"name": "By bin"}, {"name": "Background marker"}, {"name": "Antitarget marker"}, {"name": "Skip RepeatMasker"}, {"name": "No GC"}, {"name": "No edge"}, {"name": "Hard thresholds for calling each integer copy number"}, {"name": "Purity of cellularity"}, {"name": "Filter"}, {"name": "Center at:"}, {"name": "Center"}, {"name": "Title"}, {"name": "Copy number threshold"}, {"name": "No shift XY"}, {"name": "Minimum number of covered probes"}, {"name": "Minimum mapping quality score"}, {"name": "Count read midpoints"}], "output": [{"name": "CNV VCF"}, {"name": "CNV Calls"}, {"name": "CNV Scatter plot"}, {"name": "CNV Diagram"}], "codeRepository": ["https://github.com/etal/cnvkit"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Eric Talevich", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648048902, "dateCreated": 1535126714, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-reference/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-reference/13", "applicationCategory": "Workflow", "name": "CNVkit Reference", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n* **CNVkit Reference** creates a **Reference coverage file** from the normal samples. Also, the workflow creates a **Target BED file** of baited regions and an **Antitarget BED file** of regions not included in the Target BED file. All three files are prerequisites for further CNV Calling workflows.\n\n* A **Reference coverage file** should be constructed specifically for each target capture panel, using a BED file that lists the genomic coordinates of the baited regions. Ideally, the control or \u201cnormal\u201d samples that are used to build the reference should match the type of sample (e.g. FFPE-extracted or fresh DNA) and library preparation protocol or kit used for the test (e.g. tumor) samples.[2]\n\n* A **Reference coverage file** needs to be created for both genders in separate runs; one run with only normal female, the other one with only male normal samples. The parameter **Male reference** should be set to *True* or *False* in accordance with the gender of the input samples.\n\n* In order to obtain annotated CNV (name of the genes that are located in the CNV regions), an **Annotation File** input must be provided (eg. hg19 annotation - [refFlat19](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refFlat.txt.gz)).\n\n* If the sequencing type is **Amplicon**, parameter **Split** for the CNVkit-target tool and parameter **No edge** in the CNVkit reference tool  should be set to *True*.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made, apart from the automation of file naming.\n\n### Common Issues and Important Notes\n\n* The number of samples should be under 40, because exceeding this number would not make any statistical difference.\n\n* The total memory of samples can not exceed 4 TB (important when samples are high-coverage WES or WGS).\n\n\n### Performance Benchmarking\n\nThe instance set for this workflow is the AWS c5.9xlarge instance with 36 vCPUs, 72 GiB of RAM and EBS (disk space) set to 4 TB.\n\n| Number of BAM files  (average size in GB) | Type | Duration | Cost  |\n|-------------------------------------------|------|----------|-------|\n|                  12 (11)                  |  WES |    29m   | $0.73 |\n|                   18 (11)                 |  WES |    38m   | $0.97 |\n|                  36 (11)                  |  WES |  1h 12m  | $1.87 |\n|                  10 (159)                 |  WGS |  5h 15m  | $8 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n    \"fa_fname\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"data_type\": \"WXS\",\n    \"bam_files\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n    \"targets_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit Reference - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)\n\n[2] [CNVkit reference](https://cnvkit.readthedocs.io/en/stable/pipeline.html#reference)", "input": [{"name": "Reference Fasta", "encodingFormat": "application/x-fasta"}, {"name": "Annotation File", "encodingFormat": "text/x-bed"}, {"name": "Data type"}, {"name": "Count"}, {"name": "Min mapping quality"}, {"name": "Normal BAMs", "encodingFormat": "application/x-bam"}, {"name": "Interval File", "encodingFormat": "text/x-bed"}, {"name": "Minimum gap size"}, {"name": "Split"}, {"name": "Average size of split target bins"}, {"name": "Antitarget minimum size"}, {"name": "Target minimum size"}, {"name": "Target maximum size"}, {"name": "BP per bin"}, {"name": "Antitarget minimum size"}, {"name": "Antitarget maximum size"}, {"name": "Male reference"}, {"name": "No GC"}, {"name": "No edge"}, {"name": "No RMask"}], "output": [{"name": "Reference coverage file"}, {"name": "Antitarget BED", "encodingFormat": "text/x-bed"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}], "codeRepository": ["https://github.com/etal/cnvkit"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Eric Talevich", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649765577, "dateCreated": 1535126716, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc-to/22", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc-to/22", "applicationCategory": "Workflow", "name": "CNVkit Tumor-Only CNV Calling with GATK HaplotypeCaller", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **CNVkit CNV Calling** workflow determines copy number alterations and variations. \n\n* For a deeper CNV analysis (eg. determining loss of heterozygosity regions), an additional step of calling the germline mutations in a tumor sample must be performed. This pipeline employs **GATK HaplotypeCaller** as germline variant caller.\n\n* This workflow is to be run in the case where no matched normal sample for a tumor sample is available.\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this workflow.\n\n* For proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named based on the **Tumor BAM Sample ID** metadata field.\n\n### Common Issues and Important Notes\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this workflow.\n\n* For proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n* A db SNP file must be provided in order to exclude somatic mutations in a tumor sample.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\n### Performance Benchmarking\n\nThe instances set for this workflow are the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM with 1 TB of EBS (disk space) and the c5.9xlarge instance with 36 vCPUs, 72 GiB of RAM with EBS set to 1 TB.\n\n\n| BAM size (GB) | Type | Duration | Cost  |\n|---------------|------|----------|-------|\n|       12      | WES  | 21m       | $0.53 |\n|      144      | WGS  | 3h 42m       | $5.66 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"antitarget_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference_coverage_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"db_snp\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"bed_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit Tumor-Only CNV Calling with GATK HaplotypeCaller - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)", "input": [{"name": "Minimal variant depth"}, {"name": "Zygosity freq"}, {"name": "Male reference"}, {"name": "Ploidy"}, {"name": "Gender"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Antitarget BED", "encodingFormat": "text/x-bed"}, {"name": "Reference coverage file"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Intervals file", "encodingFormat": "text/x-bed"}, {"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Significance threshold"}, {"name": "Segmentation method"}, {"name": "Drop outliers"}, {"name": "Drop low coverage"}, {"name": "Y axis minimum"}, {"name": "Y axis maximum"}, {"name": "Width of margin"}, {"name": "Draw a trendline"}, {"name": "Segment color"}, {"name": "Gene(s)"}, {"name": "Chromosome"}, {"name": "By bin"}, {"name": "Background marker"}, {"name": "Antitarget marker"}, {"name": "Skip RepeatMasker"}, {"name": "No GC"}, {"name": "No edge"}, {"name": "Hard thresholds for calling each integer copy number"}, {"name": "Purity of cellularity"}, {"name": "Filter"}, {"name": "Center at:"}, {"name": "Center"}, {"name": "Title"}, {"name": "Copy number threshold"}, {"name": "No shift XY"}, {"name": "Minimum number of covered probes"}, {"name": "Exclude expression"}, {"name": "Minimum mapping quality score"}, {"name": "Count read midpoints"}], "output": [{"name": "CNV VCF"}, {"name": "CNV Calls"}, {"name": "CNV Scatter plot"}, {"name": "CNV Diagram"}], "codeRepository": ["https://github.com/etal/cnvkit"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Eric Talevich", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648048904, "dateCreated": 1535126717, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-reference-wf-no-normals/30", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-reference-wf-no-normals/30", "applicationCategory": "Workflow", "name": "CNVkit Tumor-only Reference", "description": "CNVkit analyzes copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n* Ideally, the control or \u201cnormal\u201d samples used to build the reference should match the type of sample (e.g. FFPE-extracted or fresh DNA) and library preparation protocol or kit used for the test (e.g. tumor) samples[1]. If this is not the case, **CNVkit Tumor-only Reference** creates a \u201cflat\u201d **Reference coverage file** of neutral copy number for each probe from the target and antitarget interval files. \n\n* In order to obtain annotated CNV (name of the genes that are located in the CNV regions), an annotation file for the **Annotation File** input must be provided (eg. hg19 annotation - [refFlat19](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refFlat.txt.gz)).\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made, apart from the automation of file naming.\n\n### Common Issues and Important Notes\n\n* The **Reference FASTA** has to be indexed. We recommend using the **SBG FASTA Indices** app to index the reference file if needed.\n\n### Performance Benchmarking\n\nThe instance set for this workflow is the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM and 50 GB of EBS (disk space). The running time is around 5 minutes with the cost of $0.03.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*   \n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n    \"interval_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"reference_fasta\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit Tumor-only Reference - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)", "input": [{"name": "Interval File", "encodingFormat": "text/x-bed"}, {"name": "Annotation File", "encodingFormat": "text/plain"}, {"name": "Reference Fasta", "encodingFormat": "application/x-fasta"}, {"name": "Minimum gap size"}, {"name": "Split"}, {"name": "Average size of split target bins"}, {"name": "Antitarget minimum size"}, {"name": "Average size of antitarget bins"}, {"name": "Male reference"}, {"name": "No GC"}, {"name": "No edge"}, {"name": "No RMask"}], "output": [{"name": "Antitarget BED", "encodingFormat": "text/x-bed"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Reference coverage file"}], "softwareRequirements": ["sbg:Metadata"], "codeRepository": ["https://github.com/etal/cnvkit"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Eric Talevich", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649765577, "dateCreated": 1535126719, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvnator-analysis/35", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvnator-analysis/35", "applicationCategory": "Workflow", "name": "CNVnator Analysis", "description": "CNVnator Analysis workflow performs copy number variation (CNVs) calling by doing read-depth (RD) analysis of the input BAM files. \n###Methods\nCNVnator tool has **five** major steps: \n\n1. Reads extraction \n\n2. Histogram generation \n\n3. Statistics calculation\n\n4. RD signal partition \n4'. Calculating average RD signal per bin \n\n5. CNVs identification\n###Inputs\nIn order to execute the workflow properly, a user should set the following input files: \n\n1. Input BAM files (bam_files) - one or more BAM files \n\n2. Reference genome file (ref_genome) - FASTA file matching the provided BAM files\n###Parameter settings\nFor the correct execution of the workflow it is **necessary** for the following settings to be provided: Evaluate RD, Histogram, Identifying CNVs, RD signal partitioning and Calculate statistics. Recommended value for these settings (bin size) is 100.\n\nThe No GC correction (calculate results without GC correction) can be set, but is not mandatory.\n###Outputs\nOn the output **three** files are generated: \n\n1. average_rd_output_file - contains average RD signal per bin. These results are calculated after the RD signal partition step. \n\n2. vcf file - This is the result of the conversion of the cnv_result_file.\n\n3. cnv_result_file - This file contains the result of the CNV calling in **nine** columns, they are: \n\ncolumn 1: CNV_type \ncolumn 2: coordinates\ncolumn 3: CNV_size \ncolumn 4: normalized_RD - normalized to 1  \ncolumn 5: e-val1 - is calculated using t-test statistics.  \ncolumn 6: e-val2 - is from the probability of RD values within the region to be in the tails of a gaussian distribution describing frequencies of RD values in bins. \ncolumn 7: e-val3 - same as e-val1 but for the middle of CNV  \ncolumn 8: e-val4 - same as e-val2 but for the middle of CNV \ncolumn 9: q0 - fraction of reads mapped with q0 quality\n###Common issues\nThere aren't any known common issues.", "input": [{"name": "ref_genome", "encodingFormat": "application/x-fasta"}, {"name": "no_gc_correction"}, {"name": "#bam_files", "encodingFormat": "application/x-bam"}, {"name": "Calculate statistics"}, {"name": "Histogram"}, {"name": "RD signal partitioning"}, {"name": "Evaluate RD"}, {"name": "Identifying CNVs"}], "output": [{"name": "cnv_results_file", "encodingFormat": "text/plain"}, {"name": "average_rd_output_file", "encodingFormat": "text/plain"}, {"name": "vcf", "encodingFormat": "text/plain"}], "codeRepository": ["https://github.com/abyzovlab/CNVnator", "https://github.com/abyzovlab/CNVnator/releases", "https://github.com/abyzovlab/CNVnator/wiki", "https://github.com/abyzovlab/CNVnator/releases/download/v0.3.2/CNVnator_v0.3.2.zip"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Alexej Abyzov, Alexander E. Urban, Michael Snyder, and Mark Gerstein", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648048903, "dateCreated": 1454448265, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/crispr-dav-crispr-ngs-data-analysis-and-visualization-pipeline/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/crispr-dav-crispr-ngs-data-analysis-and-visualization-pipeline/4", "applicationCategory": "Workflow", "name": "CRISPR-DAV: CRISPR NGS Data Analysis and Visualization Pipeline", "description": "**CRISPR-DAV** is a pipeline to analyze amplicon-based NGS data of CRISPR clones in a high throughput manner. \n\nIn the pipeline, BWA alignment and ABRA realignment are performed to detect insertion and deletion. The realignment with ABRA was found to have improved detection of large indels. A simplified measurement on a read level, % indel reads, was defined to evaluate NHEJ (Non-Homologous End Joining) efficiency. Homology Directed Repair (HDR) efficiency was assessed in the pipeline as well. Resutls are presented in a comprehensive set of charts and an interactive alignment view. The charts are also helpful to explain potential issues related to read quality, non-specific amplification and sample swapping.\n\n###Required Inputs\n\n1. **FASTQ files**: These are the raw FASTQ files. They must be gzipped with file extension .gz. \n\n2. **Sample sheet**: The samplesheet will be used to prepare the inputs for the pipeline. Information for this file include Gene Symbol, Genome Name, Amplicon Range, Guide Sequence, HDR Newbases, Sample Name, Sample ID, Sample Project, and FASTQ Dirpath. All coordinates are 1-based in this file. The last two columns are optional (Please consult the example samplesheet.txt.template in the Source Code link.)\n\n3. **Reference files**:  If a genome or amplicon is used as reference, a reference tar file that includes a FASTA file, BWA index, and RefGene coordinate file should be prepared (see Install-and-Run.md in the Source Code link). The app included references for these genomes: hg19, hg38, mm10, rn6, canFam3, criGri1, etc.\n\n###App settings\n\n1. **Wing length**: Number of bases on each side of sgRNA to view base changes (default: 40).\n\n2. **Realign flag**: Realign for large indel detection with ABRA: Y(default) or N.\n\n3. **Maximum read Ns percentage**: Remove reads with percentage of Ns over this value (default: 3).\n\n4. **Minimum read quality mean score**: Remove reads with quality score mean below this value (default: 30).\n\n5. **Minimum BWA mapping quality score**: Require minimum BWA mapping quality score (default: 20).\n\n6. **Minimum read length**: Remove reads with length less than this value (default: 50).\n\n7. **Cores per job**: Number of cores to use in processing one sample (default: 3).\n\n###Outputs\n\n1. **Deliverables**: Results are presented in an HTML report.\n\n2. **Align**: Contains the intermediate files used for making HTML report.\n\n###Common Issues and Important Notes\n\nIf an input file used space instead of the required tab as separator, the pipeline would report error of missing columns.\n\n### Changes Introduced by Seven Bridges\n\nCRISPR-DAV was meant to be used with cluster configuration, so samples can be processed in parallel. SBG instead runs multiple processes on several AWS instances. Based on how many cores are set to be used per sample, we cun run certain number of analysis on one instance.\n\n\n### Performance Benchmarking\n\nWorkflow can run maximum of 16 AWS instances in parallel (c4.4xlarge).\n \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input FASTQ files | Reference | Paired End | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:--------:|:--------:|:-----:|:----------:|\n|     CRISPR-DAV Pipeline     |  2 x 3.0 MB, 2 x 14.0 MB |     genomex (681.9 KB)   |     Yes     |   7min   | $0.04| c4.4xlarge(256GB) |\n|     CRISPR-DAV Pipeline     | 2 x 4.0 MB, 2 x 15.0 MB |     mm10 (7.0 GB), hg19 (8.0 GB)    |     Yes    |   24min  | $0.22 | 2 x c4.4xlarge(256GB) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'rawfastq': list(api.files.query(project=project_id, names=['sample1_R2.fastq.gz','sample1_R1.fastq.gz', 'sample2_R1.fastq.gz', 'sample2_R2.fastq.gz'])),\n        'genome': list(api.files.query(project=project_id, names=['crispr-genomex.tar'])),\n        'input_sheet': api.files.query(project=project_id, names=['example1-samplesheet.xlsx'])[0]\n        }\n\n# Run the task\ntask = api.tasks.create(name='CRISPR-DAV workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [CRISPR-DAV Repo](https://github.com/pinetree1/crispr-dav)   \n[2] [CRISPR-DAV Paper](https://www.ncbi.nlm.nih.gov/pubmed/28961906)", "input": [{"name": "FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Sample sheet"}, {"name": "Reference files", "encodingFormat": "application/x-tar"}, {"name": "Wing length"}, {"name": "Realign flag"}, {"name": "Maximum read Ns percentage"}, {"name": "Minimum read quality mean score"}, {"name": "Minimum BWA mapping quality score"}, {"name": "Minimum read length"}, {"name": "Cores per job"}], "output": [{"name": "Deliverables", "encodingFormat": "application/zip"}, {"name": "Align", "encodingFormat": "application/zip"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/bristol-myers-squibb-publishin/bms-public-apps/crispr-dav-crispr-ngs-data-analysis-and-visualization-pipeline/4.png", "codeRepository": ["https://github.com/pinetree1/crispr-dav", "https://github.com/pinetree1/crispr-dav"], "applicationSubCategory": ["Alignment"], "project": "BMS Public Apps", "creator": "Bristol-Myers Squibb, Xuning Wang", "softwareVersion": ["sbg:draft-2"], "dateModified": 1541092209, "dateCreated": 1540565969, "contributor": ["bristol-myers-squibb-publishin/bms.publish"], "sdPublisher": "BMS", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly2-workflow-0-7-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly2-workflow-0-7-1/3", "applicationCategory": "Workflow", "name": "Delly2 Workflow", "description": "Delly is a tool for predicting structural variants, i.e. deletions, duplications, translocations and inversions. It integrates short insert paired-ends, long-range mate-pairs and split-read alignments to accurately delineate genomic rearrangements at single-nucleotide resolution.", "input": [{"name": "exclude"}, {"name": "genome", "encodingFormat": "application/x-fasta"}, {"name": "vcfgeno"}, {"name": "#input_bam", "encodingFormat": "application/x-sam"}, {"name": "Type"}, {"name": "Exclude preset"}, {"name": "Min mapping quality"}, {"name": "MAD Cutoff"}, {"name": "Min flanking size"}, {"name": "Flanking"}, {"name": "No InDels"}, {"name": "InDel size"}, {"name": "Genotyping quality"}], "output": [{"name": "output"}], "codeRepository": ["https://github.com/tobiasrausch/delly", "https://github.com/tobiasrausch/delly/blob/master/README.md"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799643, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-best-practice-germline-workflow/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-best-practice-germline-workflow/13", "applicationCategory": "Workflow", "name": "Delly - Germline Structural Variation (SV) calling", "description": "**Delly** is a tool used for discovering and genotyping structural variants. \n\n\n###Delly - Germline  Structural Variation (SV) calling\n\n**Delly Germline workflow**  consists of five steps:\n\n**Germline SV calling** - At least **two normal samples**  are  **required**. The calling step is done by  **sample**.\n\n**Merging SV sites** into a unified site list.\n\n**Genotyping  merged SV site list across all samples**. This is run in parallel for each sample.\n\n**Merging all genotyped samples** to get a single VCF/BCF file.\n\nApplying the germline SV filter which **requires at least 20 unrelated samples**.\n\n\n\n### Common Use Cases\n\n* This workflow is designed to analyze germline WGS samples. \n**The user should use this workflow only if 20 unrelated normal samples are available**. In all other cases, the user should use **Delly 0.7.8 - Call** and **Delly 0.7.8 - Filter** tools.\n* **Pre-processing of BAM files** is highly recommended. BAM files need to be sorted and indexed. This can be done by using **Sambamba Sort**.\nIf multiple libraries are present for a single sample these need to be merged in a single BAM file with unique ReadGroup tags. This can be done by using **Sambamba Merge**. Prior marking of duplicates by using **Picard MarkDuplicates** is recommended.\n\n\n\n\n\n###Changes introduced by SBG\n\n\n* Conversion of the output BCF file to VCF file is already done internally in the app. The user can use both of these files.\n\n\n\n\n###Common Issues and Important Notes\n\n* **Delly** works well with WGS data. Paired-end mapping that **Delly** uses does not have much power when running on WES data. Small InDels (<500bp) may work using WES data.\n\n* The SV size that Delly can call depends on the sharpness of the insert size distribution. For an insert size of 200-300bp with a 20-30bp standard deviation, **Delly** starts to call reliable SVs >=300bp. **Delly** also supports calling of small InDels using soft-clipped reads only. In this mode, the smallest SV size called is 15bp.\n\n* If working with multiple different libraries/BAM files for a single sample, the user should first merge these BAMs using tools such as **Picard** and then tag each library with a unique ReadGroup.\n\n* If Delly is running too slowly, the user should  exclude telomere and centromere regions and also all unplaced contigs. Delly is shipped with such an exclude list for human and mouse samples. In addition, input reads can be filtered more stringently using -q 20 and -s 15. \n\n* If the user is interested only in large SVs, they should deactivate small InDel calling (`--noindels`)\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\nFor known limitations and more about this tool, please see the [DELLY github page](https://github.com/dellytools/delly).\n\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*           \n\n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 68                     | 10h        | 16    | 7.96     | c4.4xlarge |\n\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n \"genome \": api.files.query(project=project_id, names=['human_g1k_v37_decoy.fasta'])[0],\n \"exclude \": api.files.query(project=project_id, names=['exclude_regions.tsv'])[0],\n \"input_normals\": list(api.files.query(project=project_id, names=['normal1.bam', \n                                                             'normal2.bam'])),\n}\ntask = api.tasks.create(name='Metagenomics WGS Analysis - API Example', project=project_id, app=workflow_id,  inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [Delly- github](https://github.com/dellytools/delly)\n\n[2] [Delly - paper](https://academic.oup.com/bioinformatics/article/28/18/i333/245403/DELLY-structural-variant-discovery-by-integrated)", "input": [{"name": "Exclude regions"}, {"name": "Genome", "encodingFormat": "application/x-fasta"}, {"name": "Normals", "encodingFormat": "application/x-bam"}], "output": [{"name": "Germline VCF", "encodingFormat": "application/x-vcf"}, {"name": "Germline BCF"}], "codeRepository": ["https://github.com/dellytools/delly"], "applicationSubCategory": ["Structural Variant Calling"], "project": "SBG Public Data", "creator": "Tobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049435, "dateCreated": 1537977013, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/best-practice-delly-somatic-workflow/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/best-practice-delly-somatic-workflow/8", "applicationCategory": "Workflow", "name": "Delly - Somatic Structural Variation (SV) calling", "description": "**Delly** is a tool used for discovering and genotyping structural variants. \n\n###Delly - Somatic Structural Variation (SV) calling\n\n**Delly Somatic workflow** consists of four steps:\n\n**Somatic SV calling** - At least **one tumor sample** and a **matched control sample are required**. \n\n**Somatic pre-filtering** of every tumor/control pair, using a tab-delimited sample description file, which is generated automatically.\n\n**Re-genotype somatic sites** across a larger panel of control samples to efficiently filter false positives and germline SVs. \n\n**Post-filter** for somatic SVs using all control samples.\n\n\n\n### Common Use Cases\n\n* This workflow is designed to analyze somatic WGS samples. \n* The user should use this workflow only if a large panel of normals is available ( >10 ). In all other cases, the user should use **Delly 0.7.7 - Call & Filter** tool.\n* The metadata field **Case ID** must be set for all files found on the **Controls** and **Tumors** input nodes. This info is important for pairing files and batching the workflow. Batching is usually done by **Tumor files**. \n* The BAM files that Delly requires on the **Controls** and **Tumor files** inputs need to be sorted and indexed. **Delly** also requires a reference genome (**Genome** input) in order to to identify split-reads. \n* The output of the workflow is a BCF format file with a CSI index.  \n\n* **Pre-processing of BAM files** is highly recommended. BAM files need to be sorted and indexed. This can be done by using **Sambamba Sort**.\nIf multiple libraries are present for a single sample these need to be merged in a single BAM file with unique ReadGroup tags. This can be done by using **Sambamba Merge**. Prior marking of duplicates by using **Picard MarkDuplicates** is recommended.\n\n\n\n###Changes introduced by SBG\n\n* For performance reasons, the **Prefiltering step** uses Delly version 0.7.7. In this version, the user has to specify the **type** parameter. If the user wants to call all SV types (INS, DEL, INV, BND and DUP) at once, they should select all of these types. **SBG recommends calling all SV types at once, because the Prefiltering Step is scattered by type, which makes the whole workflow run much faster.**\n\n* The Prefiltering step (which contains **Delly call**) and **Delly filter** tool are integrated into one tool: **Delly - Call & Filter**. \n\n* Conversion of the output BCF file to VCF file is already done internally in the app. The user can use both of these files.\n\n* **Delly Call & Filter**  automatically generates **samples.tsv** file that is used for somatic calling. \n\n###Common Issues and Important Notes\n\n* **Delly** works well with WGS data. Paired-end mapping that **Delly** uses does not have much power when running on WES data . Small InDels (<500bp) may work using WES data.\n\n* The SV size that Delly can call depends on the sharpness of the insert size distribution. For an insert size of 200-300bp with a 20-30bp standard deviation, **Delly** starts to call reliable SVs >=300bp. **Delly** also supports calling of small InDels using soft-clipped reads only. In this mode, the smallest SV size called is 15bp.\n\n* If working with multiple different libraries/BAM files for a single sample, the user should first merge these BAMs using tools such as **Picard** and then tag each library with a unique ReadGroup.\n\n* If Delly is running too slowly, the user should  exclude telomere and centromere regions and also all unplaced contigs. Delly is shipped with such an exclude list for human and mouse samples. In addition, input reads can be filtered more stringently using -q 20 and -s 15. \n\n* If the user is interested only in large SVs, they should deactivate small InDel calling (`--noindels`)\n\nFor known limitations and more about this tool, please see the [DELLY github page](https://github.com/dellytools/delly).\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*         \n\n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 61                     | 3h 26 min         | 16    | 2.73     | c4.4xlarge |\n| 296                   | 18h 14 min            | 16     | 14.51    |  c4.4xlarge |\n| 448                     | 18h 45 min         | 16    | 14.925     | c4.4xlarge |\n\n\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n \"genome \": api.files.query(project=project_id, names=['human_g1k_v37_decoy.fasta'])[0],\n \"exclude \": api.files.query(project=project_id, names=['exclude_regions.tsv'])[0],\n \"input_normals\": list(api.files.query(project=project_id, names=['normal1.bam', \n                                                             'normal2.bam'])),\n\"input_tumor\": list(api.files.query(project=project_id, names=['tumor1.bam', \n                                                             'tumor2.bam']))\n\n}\ntask = api.tasks.create(name='Metagenomics WGS Analysis - API Example', project=project_id, app=workflow_id,  inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [Delly- github](https://github.com/dellytools/delly)\n\n[2] [Delly - paper](https://academic.oup.com/bioinformatics/article/28/18/i333/245403/DELLY-structural-variant-discovery-by-integrated)", "input": [{"name": "Controls", "encodingFormat": "application/x-bam"}, {"name": "Tumor files", "encodingFormat": "application/x-bam"}, {"name": "Type"}, {"name": "Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude regions"}], "output": [{"name": "Raw calls VCF", "encodingFormat": "application/x-vcf"}, {"name": "Final somatic VCF", "encodingFormat": "application/x-vcf"}, {"name": "Prefiltered VCF", "encodingFormat": "application/x-vcf"}, {"name": "All SV types prefiltered BCF", "encodingFormat": "application/x-vcf"}], "codeRepository": ["https://github.com/dellytools/delly"], "applicationSubCategory": ["Structural Variant Calling"], "project": "SBG Public Data", "creator": "Tobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049434, "dateCreated": 1537977069, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/differential-exon-usage-with-dexseq-1-36-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/differential-exon-usage-with-dexseq-1-36-0/5", "applicationCategory": "Workflow", "name": "Differential Exon Usage with DEXSeq 1.36.0", "description": "The Bioconductor package DEXSeq tests for **differential exon usage** in comparative RNA-Seq experiments.\n\nDifferential exon usage (DEU) refers to changes in the relative usage of exons caused by the experimental condition. In the case of an inner exon, a change in relative exon usage is typically due to a change in the rate with which this exon is spliced into transcripts (alternative splicing). Note, however, that DEU is a more general concept than alternative splicing, since it also includes changes in the usage of alternative transcript start sites and polyadenylation sites, which can cause differential usage of exons at the 5\u2019 and 3\u2019 boundary of transcripts.\n\nThe DEXSeq method uses a generalized linear model to model the differential usage of exons in different sample groups. It assumes that the read counts in the exons follow a negative binomial distribution and controls for false discovery rate (FDR) by estimating the biological variability for each exon. For more details please check the original publication [1] and the DEXSeq vignette [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\t\n\t\t\t\n### Common Use Cases\n\nTo be able to run an analysis, the workflow needs to be provided with:\n- **Gene annotation (GTF) file** or **Flattened GFF file** if already generated using the **DEXSeq prepare annotation** tool.   \n- **Aligned sorted BAM or SAM** files (alignment with **STAR** is recommended; if using **STAR** set the **Output sorting type** option to **SortedByCoordinate**).  \n- **Phenotype information.**\n\nThere are two options for providing phenotype information:\n\n**a)** By indicating API keys for metadata fields that need to be included in the design. For example, if the effect of **sample_type** is being analysed (for example: treated, untreated) on DEU controlling for **sex** (male, female) **sample_id**, **sample_type** and **sex** metadata fields need to be populated for all BAM/SAM files, and **Covariate of interest** and **Control variables** need to be set to **sample_type** and **sex** respectively. \n\n**b)** By including a CSV file (**Sample Table** input) - see the example below. Sample table must contain the **sample_id** column with values that match values from the metadata **sample_id** field of the count files. To run an analysis, parameters **Covariate of interest** and **Control variables** (optional) need to be set to the adequate column names. For example, entering **sample_type** for the value of the **Covariate of interest** parameter and **library** in **Control variables** will test for differential exon usage between treated and untreated samples, while controlling for effect of library preparation.\n\nExample CSV content:\n\n```\nsample_id,sample_type,library,sex\ntreated1,treated,paired-end,male\ntreated2,treated,single-end,male\ntreated3,treated,paired-end,female\nuntreated1,untreated,single-end,male\nuntreated2,untreated,paired-end,female\nuntreated3,untreated,paired-end,female\nuntreated4,untreated,paired-end,male\n\n```\nIn the process of forming the counting bins, the\u00a0**DEXSeq prepare annotation**\u00a0tool\u00a0might come across overlapping genes. If exons coming from two different genes overlap, the tool\u2019s default behaviour is to combine the genes into a single \u201caggregate gene\u201d which is subsequently referred to with the IDs of the individual genes, joined by a plus (\u2018+\u2019) sign. This behaviour\u00a0can be disabled by setting the\u00a0**Gene Aggregation**\u00a0parameter to\u00a0**no** and overlapping exons will be simply skipped.\n\nThe following parameters describing the input BAM/SAM files should be adequately set: **Paired-end data** (yes / no, default = **yes**), **BAM/SAM Sort Type** (pos / name, default = **pos**), **Strandedness** (yes / no / reverse, default = **no**), **Minimal alignment quality** (default **10**).\n\nBy using the **Denominator** parameter it is possible to set a value of the sample annotation (e.g. condition) to use as a denominator in the log2 fold change. For example, if **Covariate of interest** consists of values: `treated` and `untreated`, and we want to use `untreated` as the control group, the **Denominator** parameter should be set to `untreated`. If the parameter is not specified, the function will take the annotation of the first sample. \n\nAs a result of an analysis the workflow will output:\n- CSV results table containing for each exon / exon part (featureID): gene id (groupID), seqnames (chromosomes), start, end, strand, exonBaseMean (mean of the counts across samples in each feature/exon), dispersion, stat, p-value, adjusted p-value, exon usage coefficients, log2fold_change for the tested groups and list of transcripts that contain that exon.\n- CSV results table with the per-gene adjusted p-values (a per-gene adjusted p-value is computed using the `perGeneQValue` function, which aggregates evidence from multiple tests within a gene to a single p-value for the gene and then corrects for multiple testing across genes).\n- HTML report - an overview of all the significant results with the links to the plots. NOTE: To be able to use the HTML report please download it, as well as HTML report directory, unzip the directory, and be sure that both the report and the report directory are located in the same directory. The **FDR cutoff** parameter is set to **0.1** by default, but it can be changed. Output HTML report can be omitted by setting the **HTML report** parameter to **FALSE**.\n- Optionally, by setting the **Output result R object** parameter to **TRUE**, the result R object will be outputted, so DEXSeq functions for visualisation and further exploration of the data and results can be used locally.   \n\nTesting for DEU in multiple conditions. DEXSeq tests for differences in exon usage between different conditions, meaning that if there are more than two different conditions, DEXSeq will still output one p-value for each exon, where the null model would be \"the different conditions do not have an effect on exon usage\". Thus, significant p-values indicate that the null model is rejected: at least one of the conditions has an effect on exon usage, such that the exon usage is different with respect to the other conditions. The fold changes in DEXSeq result table are the result of comparing all the conditions with the one specified by the **Denominator** parameter. So, for three conditions, two fold change columns in the result table can be expected.\n\n### Changes Introduced by Seven Bridges\n- The workflow accepts a compressed GTF (GTF.GZ) file.\n- The type of input files (SAM or BAM) will be automatically recognised. \n\n### Common Issues and Important Notes\n- GTF files from sources other than Ensembl may deviate from the format expected by the script. Hence, if you need to use a GTF or GFF file from another source, you may need to convert it to the expected format. For more details please check the DEXSeq vignette, section 10.6 [2].\n- If you are using classic alignment methods (i.e. not pseudo-alignment approaches) it is important to align them to the genome, not to the transcriptome, and to use a splice-aware aligner such as STAR (recommended) or TopHat2 [2].\n- Input Bam/SAM files must be sorted by coordinate (position) or sorted by name.\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and to be populated in all of the samples (**Count Files**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in its header. Keep in mind that metadata keys are usually different from what is seen on the front-end. To match metadata keys to their corresponding values on the front-end, please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). Learn how to [add a custom metadata field](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#modifying-metadata-via-the-visual-interface) to expression data files.\n- **sample_id** metadata key must be populated with unique values for all the samples whether CSV is provided or not. \n- DEXSeq analysis for a large number of samples can take extremely long (~10h for 64 input BAM files). In that case (but also in general), filtering out genes / exonic regions with low levels of expression (thus reducing the number of tests) should help speed up the analysis. Using a larger instance (c5.18xlarge instead of the default c5.9xlarge) should also be considered. \n\n### Performance Benchmarking\n\nRuntime and task cost for different number of input BAM files using all the default options:\n\n| # of input files | File Sizes  | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|       4     |     400 MB   |   15 min   |   $0.44    |    c5.9xlarge     |\n|       4     |     1.5 GB |   35 min   |   $0.99    |    c5.9xlarge     |\n|       10     |     1 - 1.6 GB   |   53 min   |   $1.47    |    c5.9xlarge     |\n|       20     |    1 - 1.6 GB   |   1 h 12 min   |   $2.01    |    c5.9xlarge    |\n|       30     |    250 MB - 1.6 GB  |   1 h 34 min   |   $2.61    |    c5.9xlarge     |\n\n\nRuntime and task cost when flattened GFF is provided instead of GTF:\n\n| # of input files | File Sizes  | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|       4     |     400 MB   |   13 min   |   $0.36    |    c5.9xlarge     |\n|       4     |     1.5 GB  |   33 min   |   $0.92    |    c5.9xlarge     |\n\n      \n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'in_alignments': list(api.files.query(project=project_id, names=['sample_1.bam', 'sample_2.bam', 'sample_3.bam', 'sample_4.bam'])),\n        'in_gene_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_sample_table': list(api.files.query(project=project_id, names=['sample_table.csv'])),\n        }\n\n# Run the task\ntask = api.tasks.create(name='Differential Exon Usage with DEXSeq 1.36.0 - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [DEXSeq publication](https://genome.cshlp.org/content/22/10/2008.full)\n[2] [DEXSeq vignette](https://bioconductor.org/packages/3.12/bioc/vignettes/DEXSeq/inst/doc/DEXSeq.html)", "input": [{"name": "Aligned sorted BAM or SAM", "encodingFormat": "application/x-bam"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Sample Table"}, {"name": "Gene Agreggation"}, {"name": "Minimal alignment quality"}, {"name": "BAM/SAM Sort Type"}, {"name": "Strandedness"}, {"name": "Paired-end data"}, {"name": "Covariate of interest"}, {"name": "Control variables"}, {"name": "Output name prefix"}, {"name": "Output result R object"}, {"name": "Denominator"}, {"name": "HTML report"}, {"name": "FDR cutoff"}], "output": [{"name": "DEXSeq result R object"}, {"name": "HTML report directory", "encodingFormat": "application/zip"}, {"name": "DEXSeq Results"}, {"name": "HTML report", "encodingFormat": "text/html"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/differential-exon-usage-with-dexseq-1-36-0/5.png", "codeRepository": ["https://github.com/areyesq89/DEXSeq"], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1619093562, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/differential-methylation-workflow/29", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/differential-methylation-workflow/29", "applicationCategory": "Workflow", "name": "Differential Methylation workflow", "description": "**Differential methylation** is a workflow for analyzing DNA methylation, a type of epigenetic modification, by processing coverage files which can be obtained from the [Bismark Analysis Workflow](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/bismark-analysis-0-19-0/). Reads, from which coverage files are generated, can be obtained using two technologies for library preparation and sequencing, Whole Genome Bisulfite Sequencing (WGBS) and Reduced Representation Bisulfite Sequencing (RRBS).\n\nThe analysis is based on inferring differentially methylated regions (or positions) between control and test group. \n\nThe workflow takes the following input files:\n\n* **Control** - control coverage file(s)\n* **Test** - test coverage file(s)\n* **Promoters for the visualization** - a BED file with transcription start site (TSS) for genes that are to be visualized\n* **Promoters for the differential methylation** - a BED file with TSSs for genes to apply differential methylation analysis to\n* **CpGi** - a BED file with CpGi regions across the genome\n\nThe output files generated by this workflow are:\n\n* **Visualized coverage files** - coverage files represented by **SBG Visualize coverage file** (PDF)\n* **Methylation stats** - methylation statistics for each sample (PNG)\n* **Coverage stats** - coverage statistics for each sample (PNG)\n* **Unfiltered differential methylation** - raw differential methylation (TSV)\n* **Filtered differential methylation** - (TSV)\n* **Annotated filtered differential methylation** - differentially methylated regions annotated with gene names (TSV)\n* **Region methylation** - summarized methylated/unmethylated base counts over predefined regions (TSV)\n* **Tile methylation** - summarized methylated/unmethylated base counts over tilling windows across genome (TSV)\n* **Filtered methylation object** - filtered methylation MethylKit object stored in an RDS file\n\nThe workflow contains two main parts: visualization, performed by **SBG Visualize coverage file** and differential methylation, performed by the R package, **MethylKit**.\n\n**SBG Visualize coverage file** is developed in order to provide a better visual representation of methylation density from the coverage files (produced by the Bismark Methylation Extractor). This tool also facilitates visual comparison between normal and tumor coverage files for regions e.g. promoters provided in a BED file.\n\nThe required inputs are the same as the inputs for the workflow (apart from Promoters for the differential methylation). The required parameters are:\n\n* **Genes** - desired gene(s) to be plotted\n* **Start** - number of bases upstream from TSS\n* **End** - number of bases downstream from TSS\n\nSome of the optional parameters are: **Cut off for coverage**, which is the minimal coverage per position to be plotted and **sample number**, which represents the total number of samples to be plotted.\n\n**SBG Add gene names** annotates differentially methylated regions with genes. The inputs are: \n\n* **BED** file with TSSs for genes to apply differential methylation analysis to\n* **TSV** file with differentially methylated regions from the MethylKits getMethylDiff (described below).\n\nAppart from SBG tools, the workflow is comprised of **MethylKit tools**:\n\n* The analysis starts with **methRead**, a tool which reads a list of files or single files with methylation information for bases/region in the genome and creates a methylRawList object, which is stored in an RDS file. An RDS file is a specialized R file for storing R objects. \n* The obtained RDS file is processed with **filterByCoverage**, a tool that filters a methRead object based on lower read cutoff or high read cutoff. Higher read cutoff is useful to eliminate PCR effects, while lower read cutoff is useful for performing better statistical tests.\n* Filtered RDS file is an input for the **unite** tool. This tool unites methylRawList objects so that only bases with coverage from all samples are retained.\n* **RegionCounts** summarizes methylated/unmethylated base counts over predefined regions from a BED file. \n* The **getPromoterRegions** tool creates promoter regions from a BED file for the **regionCounts** tool.\n* **TileMethylCounts** summarizes methylated/unmethylated base counts over tilling windows across genome. Size and step of the window are the input parameters for this tool.\n\nDifferential methylation workflow has three options to be run on and next step depends on the chosen run option:\n\n*  If option \u201cper position\u201d is selected, regionCounts and tileMethylCounts will be skipped \n*  If option \u201cper region\u201d is selected, tileMethylCounts will be skipped \n*  If option \u201cper tile\u201d is selected, regionCounts will be skipped\n\nSteps that succeed are the same for all three run modes.\n\n* The **pool** tool sums up coverage, numCs and numTs values within each group so one representative sample for each group is created.\n* **CalculateDiffMeth** calculates differential methylation statistics between two groups of samples. The function uses either logistic regression test or Fisher\u2019s Exact test to calculate differential methylation.\n* Output from calculateDiffMeth is filtered by **getMethylDiff** based on predefined criteria (q-value and percent of differential methylation).\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nCoverage file needs to be provided as input data for the differential methylation pipeline and it can be acquired by providing FASTQ files which originate from either reduced representation bisulfite sequencing - RRBS or whole genome bisulfite sequencing - WGBS to Bismark aligner and consequently to Bismark methylation extractor.\n\n### Changes Introduced by Seven Bridges\n\n* Considering the fact that MethylKit's default BED file is [NCBI RefSeq](https://www.ncbi.nlm.nih.gov/refseq/), the toolkit was optimized to operate with a BED file from [Eukaryotic promoter database](http://epd.vital-it.ch/index.php)(EPD). The main difference between the NCBI RefSeq and the EPD BED file is that the former contains positions of genes, exons, introns, while the latter contains only TSS of genes. \nMethylKit's getPromoterRegion tool was modified to operate with a EPD BED file by removing the parts of code which are dealing with introns and exons.\n\n### Common Issues and Important Notes\n\n* The workflow was tested with up to 112 sample, average size being 250 MB per sample. \n\n* For **SBG Visualize coverage file**, the recommended input number for the **sample number**  parameter is up to 12, as a higher value will cause difficulties in result interpretation; if the number is 12, 6 normal and 6 tumors will be plotted; if the number is odd, tumor samples will have one plot more than normal samples. Also, for the same tool, genes in the **Genes** paramater needs to be separated only by a comma, without a blank space (eg. MGMT,SEPT9,BRCA).\n\n### Performance Benchmarking\nSince the memory demands are correlated with size and number of the coverage files, the number of processors and memory are optional inputs for the workflow. The default value for memory is 244000 MB (AWS instance r3.8xlarge).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*      \n\n| Coverage files (average size, normal v tumor) | Instance (AWS)          | Duration | Cost   |   |\n|-----------------------------------------------|-------------------------|----------|--------|---|\n| 6 x 250 MB (3v3)                              | c3.4xlarge              | 30m      | $0.23  |   |\n| 67 x 250 MB (30v37)                           | r3.8xlarge              | 4h       | $2.56  |   |\n| 112 x 250 MB (75v37)                          | r3.8xlarge, r4.16xlarge | 4h 48m   | $15.42 |   |\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\"controls\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n\"cpgi\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n\"bed_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n\"promoters_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n\"option\": \"per position\",\n\"feature_type\": \"All\",\n\"input_type\": \"amp\",\n\"start\": 1,\n\"gene_name\": \"sevenbridges\",\n\"end\": 1}\n# Creates draft task\ntask = api.tasks.create(name=\"Differential Methylation workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).", "input": [{"name": "Amount of memory in MB"}, {"name": "Number of CPUs"}, {"name": "Control samples", "encodingFormat": "text/plain"}, {"name": "CpG islands", "encodingFormat": "text/plain"}, {"name": "Promoters for the visualization", "encodingFormat": "text/plain"}, {"name": "Differential methylation running mode"}, {"name": "Test samples", "encodingFormat": "text/plain"}, {"name": "Promoters for the differential methylation", "encodingFormat": "text/plain"}, {"name": "Upflank offset"}, {"name": "Unique promoters"}, {"name": "Downflank offset"}, {"name": "Lower percentie"}, {"name": "Low coverage"}, {"name": "High percentile"}, {"name": "High coverage"}, {"name": "Plot"}, {"name": "Labels"}, {"name": "Both strands"}, {"name": "Plot"}, {"name": "Labels"}, {"name": "Both strands"}, {"name": "Strand aware"}, {"name": "Feature type"}, {"name": "Minimum bases covered per region"}, {"name": "Window size"}, {"name": "Step size"}, {"name": "MC cores"}, {"name": "Coverage bases"}, {"name": "Statistical test to determine difference in methylation"}, {"name": "Overdispersion method"}, {"name": "Mean methylation difference calculation method"}, {"name": "Method for correcting p-value"}, {"name": "Type of differentially methylated bases/regions"}, {"name": "Differential methylation qvalue cutoff"}, {"name": "Methylation change absolute value cutoff"}, {"name": "Minimum per group"}, {"name": "Destrand"}, {"name": "Resolution"}, {"name": "Minimal coverage"}, {"name": "Input type"}, {"name": "Context"}, {"name": "Window for smoothing"}, {"name": "Start"}, {"name": "Sample number"}, {"name": "The degree of a polynomial"}, {"name": "Genes"}, {"name": "End"}, {"name": "Cut off for coverage"}], "output": [{"name": "Filtered methylation object"}, {"name": "Methylation stats"}, {"name": "Visualized coverage files"}, {"name": "Coverage stats"}, {"name": "Annotated filtered differential methylation"}, {"name": "Filtered differential methylation"}, {"name": "Unfiltered differential methylation"}, {"name": "Tile methylation"}, {"name": "Region methylation"}], "applicationSubCategory": ["Epigenetics", "Methylation"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1625734998, "dateCreated": 1522166072, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/drop-seq-wf/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/drop-seq-wf/11", "applicationCategory": "Workflow", "name": "Drop-seq Workflow", "description": "Drop-seq tools is a software toolkit written for processing Drop\u00adseq sequence data into a \u201cdigital expression matrix\u201d that will contain integer counts of the number of transcripts for each gene, in each cell. This software pipeline performs many analyses including massive de\u00admultiplexing of the data, alignment of reads to a reference genome, and processing of cellular and molecular barcodes.\n\nDrop\u00adseq sequencing libraries produce paired\u00adend reads: read 1 contains both a cell barcode and a molecular barcode (also known as a UMI); read 2 is aligned to the reference genome. This software toolkit has been developed to convert these sequencing reads into a digital expression matrix that contains integer counts of the number of transcripts for each gene, in each cell.\n\nThe raw reads from the sequencer must be converted into a Picard\u00adqueryname\u00adsorted BAM file for each library in the sequencer run. Since there are many sequencers and pipelines available to do this, we leave this step to the user. For example, we use either Picard IlluminaBasecallsToSam (preceded by Picard ExtractIlluminaBarcodes for a library with sample barcodes); \u200bor Illumina\u2019s bcl2fastq \u200bfollowed by Picard FastqToSam. \u200bOnce you have an unmapped, queryname\u00adsorted BAM, you can use this toolkit to align your raw reads and create a BAM file that is suitable to produce digital gene expression (DGE) results.\n\n###Required inputs\nInput BAM (ID: *input\\_bam*) - Input BAM that \n\nReference or index (ID: *reference\\_or\\_index*) - Reference sequence to which to align the reads, or a TAR bundle containing already generated indices.\n\nSequence dictionary (ID: *sequence\\_dict*) - Sequence dictionary for the reference file created by Picard CreateSequenceDictionary.\n\nAnnotations file (ID: *annotations\\_file*) - Annotations file for the reference used in the run. These is the annotations set to use to label the read. It can be a GTF or a refFlat file.\n\n###Required parameters\n\nBarcoded read (ID: *barcoded\\_read*) - The sequence can be from the first or second read [1/2]. This should be set to same read both in cell and molecule tagging part.\n\nSequence (ID: *sequence*) - The sequence to look for at the start of reads.\n\nPrimer sequence (ID: *primer\\_sequence*) - The sequence of the primer.\n\nNumber of barcodes (ID: *num\\_barcodes*) - Find the top set of <NUM\\_BARCODES> most common barcodes by HQ reads and only use this set for analysis.\n\nNumber of cells in library (ID: *num\\_core\\_barcodes*) - Number of cells that you think are in the library.\n\n###Output files\nGenome (ID: *genome*) - Genome files comprise binary genome sequence, suffix arrays, text chromosome names/lengths, splice junctions coordinates, and transcripts/genes information.\n\nOutput BAM (ID: *output\\_bam*) - Final BAM file created at the end of the workflow once the BAM passes all pipeline steps.\n\nDigital expression matrix (ID: *digital\\_expression*) - Output file of DGE Matrix. Genes are in rows, cells in columns. The first column contains the gene name.\n\nTag value frequencies histogram (ID: *output*) - Histogram of tag value frequencies.\n\n###Common issues\n\n1. Sometimes, during the Drop-seq protocol, instead of one cell in a droplet we get more, which in case of that \"cell\" creates poor results.", "input": [{"name": "Reference or index", "encodingFormat": "application/x-tar"}, {"name": "Sequence dictionary"}, {"name": "Annotations file", "encodingFormat": "application/x-gtf"}, {"name": "Input BAM", "encodingFormat": "application/x-sam"}, {"name": "Number of bases below quality"}, {"name": "Base quality"}, {"name": "Barcoded read"}, {"name": "Number of bases below quality"}, {"name": "Base quality"}, {"name": "Barcoded read"}, {"name": "Sequence"}, {"name": "Number of bases"}, {"name": "Mismatches"}, {"name": "Number of bases"}, {"name": "Mismatches"}, {"name": "Primer sequence"}, {"name": "Number of barcodes"}, {"name": "Number of cells in library"}], "output": [{"name": "Genome", "encodingFormat": "application/x-tar"}, {"name": "Digital expression matrix", "encodingFormat": "text/plain"}, {"name": "Tag value frequencies histogram", "encodingFormat": "text/plain"}, {"name": "Output BAM", "encodingFormat": "application/x-sam"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/drop-seq-wf/11.png", "codeRepository": [], "applicationSubCategory": ["RNA"], "project": "SBG Public Data", "creator": "McCarroll Lab, Harvard Medical School", "softwareVersion": ["sbg:draft-2"], "dateModified": 1563374994, "dateCreated": 1499251338, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/encode-atac-seq-pipeline-1-9-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/encode-atac-seq-pipeline-1-9-2/6", "applicationCategory": "Workflow", "name": "ENCODE ATAC-seq Pipeline", "description": "ATAC-Seq analysis performs quality control and signal processing, producing alignments and measures of enrichment.\nThe Assay for Transposase-Accessible Chromatin followed by sequencing (ATAC-seq) experiment provides genome-wide profiles of chromatin accessibility. \nBriefly, the ATAC-seq method works as follows: loaded transposase inserts sequencing primers into open chromatin sites across the genome, and reads are then sequenced. \nThe ends of the reads mark open chromatin sites [1]. \n\nThe workflow is based on the ENCODE ATAC-seq pipeline, developed by the ENCODE Consortium. The four major steps of the ATAC-Seq analysis are: \npre-alignment quality control, alignment, post-alignment processing and advanced ATAC-seq-specific quality control, and peak calling in order to identify \naccessible regions (which is the basis for advanced downstream analysis). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**ENCODE ATAC-Seq Pipeline** processes replicated or unreplicated, single-end and paired-end, ATAC-Seq data. The workflow requires **Bowtie2 Index TAR**, **Chromosome sizes TSV**, \nand **Input FASTQ R1 files** input files. For paired-end data, read2 FASTQs should be provided on the **Input FASTQ R2 files** input port. Additionally, **MACS2's genome sizes** \n(hs for human, mm for mouse or sum of 2nd column chromosome sizes file) and **Paired-End** for sequencing endedness (True for paired-end or False for single-end data) are required \napp settings. To enable alignment on mitochondrial reference and execution of the **Frac Mito** tool (determines the fraction of mitochondrial reads in a sample), \n**Mitochondrial Bowtie2 Index TAR** should be provided. For advanced ATAC-Seq quality control, executions of the **TSS Enrich**, **Fraglen Stat PE**, **GC Bias**, **Annot Enrich**, and \n**Compare signal to roadmap** steps require the following input files and app settings:\n- **TSS file** (**TSS Enrich**)\n- **Paired-End** set to True (**Fraglen Stat PE**)\n- **Reference FASTA file** (**GC Bias**)\n- **Blacklist file** (and/or **Blacklist2 file**), **DNase file**, **Promoter regions file** and **Enhancer regions file** (**Annot Enrich**)\n- **Roadmap metadata file**, **Reg2map BED file**, **Reg2map file** (**Compare signal to roadmap**).\n\nOptional app settings, **Mitochondrial chromosome name** (defined mitochondrial reads will be filtered out during filtering BAMs, e.g., chrM, MT) and **Reg-ex for chromosomes** \n(reg-ex for chromosomes to keep while filtering peaks, e.g. chr[\\dXY]+) are often defined in the test input JSON files provided by the ENCODE Consortium, and should be considered \nwhen running the workflow [3].\n\nThe workflow recognizes biological and corresponding technical replicates. As the first step, sequencing adapters are automatically\ndetected (the pipeline can detect/trim three types of adapter sequences, Illumina: AGATCGGAAGAGC, Nextera: CTGTCTCTTATA, smallRNA: TGGAATTCTCGG) and trimmed, paired-end or single-end, \ntechnical FASTQ replicates are merged for each bio-replicate. Merged files are aligned with **Bowtie2** and sequencing statistics are generated for aligned reads with SAMStat. Unmapped/low-quality and duplicate reads are removed from previously generated BAM files using the **Filter** tool. \nIf **Enable Xcor execution** is set to True (default, False), the same BAM files are provided to the **Filter No Deduplication for Xcor** tool where unmapped/low-quality reads are filtered without removal of duplicate reads, and resulting not-de-duplicated BAM files are converted to TAG-ALIGN (6 col. BED) format for each of the bio-replicates and \ncross-correlation analysis (including a plot) on subsampled and filtered TAG-ALIGNs is performed with the **Xcor** tool.\n\nFiltered and de-duplicated BAMs are also converted to TAG-ALIGN format for each bio-replicate and provided to the **Call Peak** tool. Apart from peak calling, TAG-ALIGNs are processed with **Macs2 Signal Track** and **Count Signal Track** (**Enable Count Signal Track execution** should be set to True) tools which generate count signal tracks. \n \nIf the **True Replicates Only** parameter is set to False (default), the **Spr** tool creates self-pseudo-replicates and peak calling is then performed on \nself-pseudo-replicates of each bio-replicate. The MACS2 methods are used for narrow peak calling and peak processing.\n\nAs part of the advanced quality control, several tools take de-duplicated BAM files of each bio-replicate. \n**TSS Enrich** performs the TSS enrichment calculation, i.e., signal-to-noise calculation [2]. \n**Fraglen Stat PE** uses Picard CollectInsertSizeMetrics software that provides useful metrics for validating library construction, including the insert size distribution and read orientation of paired-end libraries. Picard's results are used to determine the fraction of reads in NFR (Nucleosome Free Regions), percentage of \nNFR vs mono-nucleosome reads, and peak locations (presence of NFR, mono-nucleosome, and di-nucleosome peaks). The **GC Bias** tool runs Picard's CollectGcBiasMetrics method that collects metrics regarding GC bias, i.e., information about the relative proportions of guanine (G) and cytosine (C) nucleotides in a sample.\n\nIn addition to the previously mentioned advanced QC steps, the **Preseq** tool executes Picard's EsimateLibraryComplexity tool which estimates the numbers of unique molecules in a \nsequencing library, and Preseq's lc_extrap method that computes the expected future yield of distinct reads and bounds on the number of total distinct reads in the library, \nand the associated confidence intervals. **Annot Enrich** takes TAG-ALIGNs and gets the fraction of reads sitting in the provided Blacklist, DNase, Promoter, and Enhancer BED files of regions. \nThe **Compare signal to roadmap** tool takes **Macs2 Signal Track's** P-value bigWig output, gets the bwAverageOverBed, and then compares that signal with the signal in the Roadmap regions.\n\nAs the next step in the workflow, the **Jsd** tool plots fingerprints on BAMs and calculates Jensen-Shannon distance. It first takes de-duplicated BAM files, filters them with \n**Blacklist file** (and/or **Blacklist2 file**) and creates an indexed BAI file. \nNext, the tool runs deepTools' plotFingerprint tool, which randomly samples genome regions of a specified length and sums the per-base coverage in indexed files that overlap with those regions. \nThese values are then sorted according to their rank, and the cumulative sum of read counts is plotted. \nIn the final step the plotFingerprint tool\u2019s quality metrics output is further parsed in order to get Jensen-Shannon distance QCs for each bio-replicate.\n\nIf there are at least two bio-replicates provided on input, pooling of the reads from different bio-replicates and pseudo-replicates \n(**True Replicates Only** parameter is set to False by default) is performed by **Pool Ta True Replicates** and **Pool Ta Pseudo Replicates**, respectively. \nPeaks are called on pooled true and pseudo TAG-ALIGNs and **Macs2 Signal Track Pooled True Replicates** and **Count Signal Track Pooled True Replicates** are used to generate count signal tracks.\n\nIn order to evaluate reproducibility of high-throughput ATAC-Seq experiments, by measuring consistency between two biological replicates within an experiment, the pipeline runs the \n**Idr True Replicates** and **Overlap True Replicates** tools on all pairs of two bio-replicates, **Idr Pseudo Replicates** and **Overlap Pseudo Replicates** on pseudo-replicates for each bio-replicate, and **Idr Pooled Pseudo Replicates** and **Overlap Pooled Pseudo Replicates** on pooled pseudo-replicates. \nIDR represents a statistical procedure that operates on the replicated peak set and compares consistency of ranks of these peaks in individual replicate/pseudo-replicate peak sets. \nPeaks with high rank consistency are retained. IDR can operate on peaks across a pair of true replicates resulting in a \u201cconservative\u201d output peak set, or across a pair of pseudo-replicates resulting in an \u201coptimal\u201c output peak set. \nPeaks in the conservative peak set can be interpreted as high confidence peaks, representing reproducible events across true biological replicates and accounting for true biological and technical noise. \nPeaks in the optimal set can be interpreted as high-confidence peaks, representing reproducible events and accounting for read sampling noise. \nThe optimal set is more sensitive, especially when one of the replicates has lower data quality than the other [2]. \nIn addition to the IDR framework, naive overlap is performed by utilizing the BEDTools' intersectBed tool, which gives an intersection between two BED files. Also, peaks are filtered with \n**Blacklist file** (and/or **Blacklist2 file**). Next, overlapping and IDR peaks are provided to **Reproducibility** and **Reproducibility Idr** tools, respectively, \nin order to obtain reproducibility QC metrics. \n\nFinally, **QC Report** collects all previously generated QC files and produces the final HTML and JSON QC reports.\n\n### Changes Introduced by Seven Bridges\n\n- For single-end data, FASTQ files for all bio-replicates and corresponding technical replicates should be provided on the **Input FASTQ R1 files** input port. \n  In the paired-end data case, read1 FASTQs for all bio-replicates and corresponding technical replicates should be provided on the **Input FASTQ R1 files** input port, while \n  read2 FASTQs should be provided on the **Input FASTQ R2 files** input port.\n- Bio-replicates are sorted with the **SBG Prepare for Scatter and Count Replicates** tool based on the **Sample ID** metadata field. Corresponding technical replicates are further sorted based on the **Case ID** metadata field within the **Align** and **Mito Align** tools.\n- **SBG Cross Peaks** is used to sort files and implement the scatter from [ENCODE's WDL implementation of the pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/6cc3a0bd61daba7cb459cb02c9d09801582b410e/atac.wdl#L1497) when running **Idr True Replicates** and **Overlap True Replicates** on all pairs of two bio-replicates.\n- Adapter sequences are automatically detected within the **Align** and **Mito Align** tools. The pipeline can detect/trim three types of adapter sequences, \n  Illumina: AGATCGGAAGAGC, Nextera: CTGTCTCTTATA, smallRNA: TGGAATTCTCGG.\n\n### Common Issues and Important Notes\n\n- All files provided to **Input FASTQ R1 files** and **Input FASTQ R2 files** inputs must have proper metadata fields set. \n  If providing single-end files, set **Sample ID** (e.g., bio_rep_1, bio_rep_2, ...) and **Case ID** (e.g., tech_rep_1, tech_rep_2, ...) \n  for each bio-replicate and corresponding technical replicates.\n  When providing paired-end samples, set the **Sample ID** and **Case ID** metadata fields by following the rules illustrated in the following example with two bio-replicates:\n  1. BIO-REPLICATE: bio_rep_1_tech_rep_1_R1.fastq.gz (**Sample ID**: bio_rep_1, **Case ID**: tech_rep_1), \n     bio_rep_1_tech_rep_2_R1.fastq.gz (**Sample ID**: bio_rep_1, **Case ID**: tech_rep_2), \n     bio_rep_1_tech_rep_1_R2.fastq.gz (**Sample ID**: bio_rep_1, **Case ID**: tech_rep_1),\n     bio_rep_1_tech_rep_2_R2.fastq.gz (**Sample ID**: bio_rep_1, **Case ID**: tech_rep_2)\n  2. BIO-REPLICATE: bio_rep_2_tech_rep_1_R1.fastq.gz (**Sample ID**: bio_rep_2, **Case ID**: tech_rep_3), \n     bio_rep_2_tech_rep_2_R1.fastq.gz (**Sample ID**: bio_rep_2, **Case ID**: tech_rep_4), \n     bio_rep_2_tech_rep_1_R2.fastq.gz (**Sample ID**: bio_rep_2, **Case ID**: tech_rep_3),\n     bio_rep_2_tech_rep_2_R2.fastq.gz (**Sample ID**: bio_rep_2, **Case ID**: tech_rep_4)\n- Execution of the **Preseq** tool, which is a part of the advanced quality metrics pipeline's branch, is disabled due to \"too many defects in the approximation, consider running in defect mode\" \n  error.\n- All genome specific reference and additional annotation files (**Bowtie2 Index TAR**, **Chromosome sizes TSV**, **Mitochondrial Bowtie2 Index TAR**, **TSS file**, **Reference FASTA file**, **Blacklist file**, **DNase file**, **Promoter regions file**, **Enhancer regions file**, **Roadmap metadata file**, **Reg2map BED file**, **Reg2map file**) for hg38, mm10, hg19, and mm9 genomes can be found in the ENCODE ATAC-seq pipeline [GitHub repository](https://github.com/ENCODE-DCC/atac-seq-pipeline). The URLs for genome TSV files can be found [here](https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/docs/input.md#reference-genome). Each genome TSV file contains URLs for all aforementioned reference and annotation files, which can be used to import files to the Seven Bridges Platform via [FTP/HTTP(S) import method](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server).\n- For more details on ENCODE's current standards for ATAC-Seq experiments, please visit [ATAC-seq Data Standards and Processing Pipeline - Current Standards](https://www.encodeproject.org/atac-seq/).\n\n### Performance Benchmarking\n\nThe most time-consuming step of the workflow is the alignment of bio-replicates. \nIncreasing the number of parallel instances to match the number of jobs of all the alignment steps, along with increasing **Align - Number of CPUs** based on input file sizes, \nwill speed up the execution. Additionally, the execution can be further optimized by increasing **Filter - Number of CPUs**, **Filter for Xcor - Number of CPUs**, **Xcor - Number of CPUs**, and **Call Peak - Number of CPUs**.\n\n|Replicates|FASTQs size (R1 / R2)|PE|Duration|Cost|Align - No. CPUs|Parallel instances|Instance (AWS)|\n|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n|3|8.2 / 8.7 GB|Yes|9 h, 27 min|$31.29|48|6|c5.12xlarge / c4.2xlarge / r5.xlarge / r5.large|\n|3|5.3 / 5.4 GB|Yes|4 h, 33 min|$17.88|36|6|c5.9xlarge / c4.2xlarge / r5.large|\n|2|4.8 / 4.9 GB|Yes|7 h, 35 min|$18.07|36|4|c5.9xlarge / c4.2xlarge / r5.large|\n|1|1.6 / 1.6 GB|Yes|4 h, 2 min|$6.38|24|2 (default)|c5.9xlarge / c4.2xlarge|\n|1|1.6 / 1.6 GB|Yes|4 h, 26 min|$4.74|12|2 (default)|c5.4xlarge / c4.2xlarge|\n|1|1.6 / 1.6 GB|Yes|5 h, 51 min|$4.68|6 (default)|2 (default)|c4.2xlarge|\n|1|1.0 / 1.1 GB|Yes|2 h, 21 min|$3.32|24|2 (default)|c5.9xlarge / c4.2xlarge|\n|1|1.0 / 1.1 GB|Yes|2 h, 30 min|$2.28|12|2 (default)|c5.4xlarge / c4.2xlarge|\n|1|1.0 / 1.1 GB|Yes|3 h, 6 min|$2.25|6 (default)|2 (default)|c4.2xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\u200b\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"in_idx_tar\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_reads_R1\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n\t\"in_chrsz\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"paired_end\": False, \n\t\"gensz\": \"hs\"}\n# Creates draft task\ntask = api.tasks.create(name=\"ENCODE ATAC-Seq Pipeline - Single-End - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\u200b\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\u200b\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [ENCODE ATAC-seq Data Standards and Processing Pipeline](https://www.encodeproject.org/atac-seq/)\n\n[2] [ENCODE Project - Terms and Definitions](https://www.encodeproject.org/data-standards/terms/)\n\n[3] [GitHub - ENCODE-DCC ATAC-seq pipeline - Test Workflow](https://github.com/ENCODE-DCC/atac-seq-pipeline/tree/master/dev/test/test_workflow)", "input": [{"name": "Input FASTQ R1 files", "encodingFormat": "text/fastq"}, {"name": "Input FASTQ R2 files", "encodingFormat": "text/fastq"}, {"name": "Paired-End"}, {"name": "Chromosome sizes TSV"}, {"name": "MACS2's genome sizes"}, {"name": "TSS file", "encodingFormat": "text/x-bed"}, {"name": "Promoter regions file", "encodingFormat": "text/x-bed"}, {"name": "Enhancer regions file", "encodingFormat": "text/x-bed"}, {"name": "DNase file", "encodingFormat": "text/x-bed"}, {"name": "Roadmap metadata file", "encodingFormat": "text/plain"}, {"name": "Reg2map BED file", "encodingFormat": "text/x-bed"}, {"name": "Reg2map file", "encodingFormat": "text/plain"}, {"name": "Mitochondrial Bowtie2 Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Enable Fraglen Stat PE execution"}, {"name": "Enable Xcor execution"}, {"name": "Enable Count Signal Track execution"}, {"name": "True Replicates Only"}, {"name": "Enable Jsd execution"}, {"name": "Enable IDR execution"}, {"name": "QC JSON Reference"}, {"name": "Blacklist2 file", "encodingFormat": "text/x-bed"}, {"name": "Blacklist file", "encodingFormat": "text/x-bed"}, {"name": "Pipeline version"}, {"name": "Title"}, {"name": "Description"}, {"name": "Genome name"}, {"name": "Mitochondrial chromosome name"}, {"name": "Reg-ex for chromosomes"}, {"name": "Align - Number of CPUs"}, {"name": "Filter for Xcor - Number of CPUs"}, {"name": "Filter - Number of CPUs"}, {"name": "Xcor - Number of CPUs"}, {"name": "Call Peak - Number of CPUs"}, {"name": "Enable Preseq execution"}, {"name": "Bowtie2 Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Peak bio-replicates"}, {"name": "QC Report JSON"}, {"name": "QC Report HTML", "encodingFormat": "text/html"}, {"name": "Filtered BAM bio-replicates", "encodingFormat": "application/x-bam"}, {"name": "Bowtie2 raw BAM bio-replicates", "encodingFormat": "application/x-bam"}, {"name": "Positive bigWig bio-replicates"}, {"name": "Negative bigWig bio-replicates"}, {"name": "P-value bigWig bio-replicates"}, {"name": "FC bigWig bio-replicates"}, {"name": "Positive bigWig pooled bio-replicates"}, {"name": "Negative bigWig pooled bio-replicates"}, {"name": "FC bigWig pooled bio-replicates"}, {"name": "P-value bigWig pooled bio-replicates"}, {"name": "Peak pooled bio-replicates"}, {"name": "Blacklist-filtered IDR peak bio-replicates"}, {"name": "Blacklist-filtered overlap peak bio-replicates"}, {"name": "Optimal peak IDR"}, {"name": "Optimal peak overlap"}, {"name": "QC JSON Reference match"}, {"name": "Peak 1st pooled pseudo-replicates"}, {"name": "Peak 2nd pooled pseudo-replicates"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/atac.wdl", "https://github.com/ENCODE-DCC/atac-seq-pipeline", "https://github.com/ENCODE-DCC/atac-seq-pipeline/archive/refs/heads/master.zip"], "applicationSubCategory": ["ATAC-Seq", "Epigenetics"], "project": "SBG Public Data", "creator": "ENCODE-DCC", "softwareVersion": ["v1.1"], "dateModified": 1648050811, "dateCreated": 1624632548, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/encode-chip-seq-pipeline-2/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/encode-chip-seq-pipeline-2/8", "applicationCategory": "Workflow", "name": "ENCODE ChIP-Seq Pipeline 2", "description": "**ChIP-Seq** Analysis studies chromatin modifications and binding patterns of transcription factors and other proteins.\nIt combines chromatin immunoprecipitation (ChIP) assays with standard NGS sequencing. The workflow is based on ChIP-Seq 2 pipeline, developed by Encode consortia [1].\nThe steps of the ChIP-Seq Analysis workflow consist of mapping of reads including duplicate removal, cross correlation analysis, peak calling with blacklist filtering and a statistical framework, applied to the replicated peaks at the end in order to assess concordance of biological replicates. \n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.*** \n\n### Common Use Cases\n\n\n* **ENCODE ChIP-Seq Pipeline 2** is designed for processing single-end and paired-end ChIP-Seq data. It takes **List of bio-replicates FASTQ files**,  **Reference file**, **Index TAR** file and a **Chromosome sizes** file as required inputs. Optionally, a **Blacklist interval file** file can also be provided. It contains blacklist regions, genomic regions that are associated with artifact signals and may need to be removed.\nRequired AppSettings are sequence endedness for **Paired-end bio-replicates** (**Paired-end control files**, if controls files are provided), **Pipeline type**, that defines the type of analysis that will be performed (**tf**, **histone** or **control**), and **True replicates only** parameter that disables all analyses related to pseudo-replicates. Also, **Enable count signal track**, **Enable GC bias** and **Enable Jsd** should be set to true in order to include **Count Signal Track**, **GC bias** and **Jsd** tools in the analysis. \n\n* Pipeline recognizes biological replicates, on which the analysis is performed, as well as control replicates, which are being used to increase the specificity in peak calling. Technical replicates for each biological replicate can be provided and they will be merged in the first step in the pipeline. Next, the input reads are aligned, and the deduplication is performed along with the removal of unmapped and low-quality reads. This step is performed for both biological replicates and control reads. Users can choose the **Aligner** to be used, Bowtie2 or BWA-MEM. Also, there are two **Marker for duplicate reads** options to choose from in the filtering step, Picard and sambamba. The cross-correlation analysis is done on a filtered (but not-deduped) and subsampled BAM. There is a special FASTQ trimming for cross-correlation analysis. Read1 FASTQ file is trimmed to 50bp and then separately mapped as SE. Reads are filtered but duplicates are not removed, according to the workflow design [2]. \n\n* After alignment and filtering, resulting BAM files are converted to TAG-ALIGN (6 col. BED) format for each of the bio-replicates. Simplified TAG-ALIGN files serve as inputs to several tools, apart from peak caller. **Count Signal Track** and **Macs2 Signal Track** tools generate signal tracks, whereas the **Spr** tool creates self-pseudoreplicate reads. If there are at least two bio-replicates provided on input, pooling of the reads from different bio-replicates and pseudoreplicates is performed by **Pool TA**. This way additional files are provided for the IDR framework in order to have an unbiased quantitative way to determine if the peak came from all input replicates. \n\n* **Choose Ctl** and **Subsample Ctl** tools are used as a means to select an appropriate control file for peak calling.     \n\n* Peak calling is then performed on all replicates, pooled data, self-pseudoreplicates of each bio-replicate and the pooled pseudoreplicates. The workflow supports calling of two general types of peaks: narrow and region.The MACS2 and SPP methods are used for peak calling. The MACS2 software has a mode for narrow peak calling and can be used for both tf and histone pipeline type, while SPP is preferably used for region peak calling when analysing transcription factor binding sites.\n\n\n* As a way to assess concordance of peak calls between replicates, the **IDR** tool is used as the next step, along with **Overlap**, providing input peak files for the **Reproducibility** tool in order to obtain reproducibility QC. The basic idea is that if two replicates measure the same underlying biology, the most significant peaks, which are likely to be genuine signals, are expected to have high consistency between replicates, whereas peaks with low significance, which are more likely to be noise, are expected to have low consistency. Finally, **QC Report** collects all previously generated QC files and produces the final HTML and JSON QC reports, in an attempt to assess the quality of performed analysis.\n\n\n### Changes Introduced by Seven Bridges\n* Input FASTQ files belonging to Read1 and Read2 should be provided at the same input port. **SBG Pair FASTQs by Metadata** tool is added at the beginning of the pipeline in order to properly separate R1 and R2 for the **Align** tool.\n* Choosing the adequate control file for peak calling on replicates and pooled replicates is performed inside the **Subsample Ctl** and **Subsample Pool Ctl** tools.\n* In order to ensure that all the combinations of pairs of replicates are provided to cross-validation analyses (IDR/overlap), **SBG Cross Files** tool is added after the peak calling step. \n* **SBG Scatter Prepare** is added to the pipeline to pair the TAGALIGN bio-replicate files to corresponding control files forming a dictionary where the keys are bio-replicate files basenames, and values are corresponding control files basenames if there is at least one control file provided, or empty strings if no controls were included in the analysis. Dictionaries are then forwarded as an input to peak callers in order to select an appropriate control file for each bio-replicate TAGALIGN file. Since there are viable use cases where the number of biological replicates and control files do not match, scatter cannot be performed on bio-replicates and controls simultaneously.  \n \n\n\n### Common Issues and Important Notes\n* This pipeline supports up to 10 biological replicates and 10 controls, but it is recommended to keep up to 4 biological replicates, as the cross-validation (IDR/overlap) step is comparing every pair of the provided replicates.\n*  All files provided to the **List of control FASTQ files** and **List of bio-replicates FASTQ files** must have  **Sample ID** and **Case ID** metadata fields set. **Paired-end** metadata field must be set appropriately just for paired-end samples; if providing single-end files, do not set the **Paired-end** metadata field. All biological replicates must have a unique **Sample ID**, which can be an arbitrary string. For each biological replicate a single or multiple technical replicates can be provided and they will be merged in the first step in the pipeline. All technical replicates that correspond to one bio-replicate must have the same **Sample ID**. In addition to that all technical replicates must have an integer **Case ID** set such that it represents the order in which the technical replicates will be merged. Same logic applies if control files are provided.\nExamples of correctly set metadata for different scenarios:\\\n**Scenario 1**: There are 3 bio-replicates with a single technical replicate each, which makes a 3 input FASTQ files in total for single-end (or 6 for paired-end)\\\nFile1(SE) - metadata:{**Sample id**:<bio-rep1>, **Case id**: 1}\\\nFile2(SE) - metadata:{**Sample id**:<bio-rep2>, **Case id**: 1}\\\nFile3(SE) - metadata:{**Sample id**:<bio-rep3>, **Case id**: 1}\\\n**Scenario2**: There is one bio-replicate with 3 technical replicates, which makes a 3 input FASTQ files in total for single-end (or 6 for paired-end)\\\nFile1(SE) - metadata:{**Sample id**:<bio-rep1>, **Case id**: 1}\\\nFile2(SE) - metadata:{**Sample id**:<bio-rep1>, **Case id**: 2}\\\nFile3(SE) - metadata:{**Sample id**:<bio-rep1>, **Case id**: 3}\n*  The reference index file provided at the **Index TAR** input must be in concordance with the chosen **Aligner type**. \n* User controls the analysis type, **tf**, **histone**, or **control** by choosing the appropriate peak caller and pipeline type.\n* Default peak caller is **macs2**, and can be used with both **tf** and **histone** analysis, while the **spp** is used only with **tf**. Regardless of pipeline type, spp peak caller requires control files, whereas the control files are optional for macs2.\n* If the **control** mode is chosen, input control files should be provided at the  **List of bio-replicates FASTQ files** input port, while the **List of control FASTQ files** input port should be left empty. \n* All genome specific reference and additional files (Bowtie2 Index TAR, Chromosome sizes TSV, Reference FASTA file, Blacklist file) for hg38, mm10, hg19, and mm9 genomes can be found in the ENCODE ChIP-seq pipeline GitHub repository.\n\n\n\n### Performance Benchmarking\n\nThe most time consuming step of the workflow is the alignment of the bio-replicates/controls. Increasing the number of parallel instances to match the number of jobs of all the alignment steps will speed up the execution.\n\n| No of bio-replicates/ controls | Pipelyne type | Size per fastq.gz (control) | paired-end | Peak caller | Execution time | Price | Instances  (AWS)               |No of parallel instances\n| ------------------------------ | ------------- | --------------------------- | ---------- | ----------- | -------------- | ----- | ------------------------- |--------------|\n| 2/2                            | tf            | 2.4 (3.5) GB                | true       | spp         | 2h 36min       |  $5.81 | c4.2xlarge | 6 |\n| 2/2                            | tf            | 2.4 (3.5) GB                | true       | spp         | 3h 29min       |  $5.62 | c4.2xlarge | 4 |\n| 2/2                            | tf            | 2.4 (3.5) GB                | true       | spp         | 5h 37min       | $5.51 | c4.2xlarge | 2 |\n| 2/2                            | tf            | 2.4 (3.5) GB                | true       | macs2       | 5h 6min       | $4.92 | c4.2xlarge | 2 | \n| 2/0                            | histone       | 2.4 (/) GB                  | true       | macs2       | 3h 7min        | $2.91 | c4.2xlarge | 2 |\n| 2/0                            | control       | 3.5 (/) GB                  | true       | /           | 2h 52m          | $3.04 | c4.2xlarge | 2 |\n| 2/2                            | tf            | 900 (450) MB                | false      | spp         | 1h 3min        | $1.45 | c4.2xlarge | 2 |\n| 3/3                            | tf            | 650 (850) MB                | false      | spp         | 2h 11min       | $2.24 | c4.2xlarge | 2 |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\u200b\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"in_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_index\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_chrsz\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_reads\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])), \n\t\"paired_end\": True, \n\t\"true_replicates_only\": True, \n\t\"enable_count_signal_track\": True, \n\t\"enable_gc_bias\": True, \n\t\"pipeline_type\": \"tf\", \n\t\"enable_jsd\": True}\n# Creates draft task\ntask = api.tasks.create(name=\"ChIP-Seq WF - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\u200b\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\u200b\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [https://github.com/ENCODE-DCC/chip-seq-pipeline2](https://github.com/ENCODE-DCC/chip-seq-pipeline2)\n\n[2] [chip.wdl](https://github.com/ENCODE-DCC/chip-seq-pipeline2/blob/master/chip.wdl#L2568)", "input": [{"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Chromosome sizes file"}, {"name": "List of control FASTQ files", "encodingFormat": "text/fastq"}, {"name": "List of bio-replicates FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Custom aligner script"}, {"name": "Aligner"}, {"name": "Mitochondrial chromosome name"}, {"name": "Paired-end control files"}, {"name": "Use bwa mem"}, {"name": "Subsample reads controls"}, {"name": "Paired-end bio-replicates"}, {"name": "True replicates only"}, {"name": "Enable count signal track"}, {"name": "Enable GC bias"}, {"name": "Subsample reads bio-replicates"}, {"name": "Pipeline type"}, {"name": "Trim bp"}, {"name": "Crop FASTQs"}, {"name": "Crop FASTQs tolerance"}, {"name": "Phred score encoding"}, {"name": "Marker for duplicate reads"}, {"name": "Chromosomes to be filtered"}, {"name": "Exclusion minimum for cross-correlation analysis"}, {"name": "Exclusion maximum for cross-correlation analysis"}, {"name": "Blacklist interval file", "encodingFormat": "text/x-bed"}, {"name": "Enable Jsd"}, {"name": "Peak caller"}, {"name": "Pipeline version"}, {"name": "Title"}, {"name": "Description"}, {"name": "Memory factor"}, {"name": "Trimmomatic max memory"}], "output": [{"name": "Filtered BAM bio-replicates", "encodingFormat": "application/x-bam"}, {"name": "Positive bigWig bio-replicates"}, {"name": "Negative bigWig bio-replicates"}, {"name": "Output report", "encodingFormat": "text/html"}, {"name": "Output QC"}, {"name": "Peak bio-replicates"}, {"name": "P-value bigWig bio-replicates"}, {"name": "FC bigWig bio-replicates"}, {"name": "Peak pooled replicates"}, {"name": "P-value bigWig pooled replicates"}, {"name": "FC bigWig pooled replicates"}, {"name": "Blacklist-filtered overlap peak"}, {"name": "Blacklist-filtered IDR peak"}, {"name": "Optimal peak overlap"}, {"name": "Optimal peak idr"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/ENCODE-DCC/chip-seq-pipeline2/blob/master/chip.wdl#L2568", "https://github.com/ENCODE-DCC/chip-seq-pipeline2"], "applicationSubCategory": ["Epigenetics", "ChIP-Seq"], "project": "SBG Public Data", "softwareVersion": ["v1.1"], "dateModified": 1649241663, "dateCreated": 1624626930, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/eqtl-analysis-with-fastqtl-gtex-v7/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/eqtl-analysis-with-fastqtl-gtex-v7/16", "applicationCategory": "Workflow", "name": "eQTL analysis with FastQTL gtex_v7", "description": "This workflow performs the expression quantitative trait loci (eQTL) analysis using the **FastQTL** tool. Additional tools are added to adapt the input files to be supported by **FastQTL** as well as to generate reports which graphically illustrate the obtained results. \n\nGenome variability has been the focus of many studies in recent years due to its relevance to the differential disease risk among individuals. One of the most prominent directions has been the extensive set of studies on expression quantitative trait loci (eQTLs), namely, the discovery of genetic variants that explain variation in gene expression levels. These studies are not only concerned with the characterization of functional sequence variation but also with the understanding of basic processes of gene regulation and interpretation of genome-wide association studies. \n\nMapping eQTLs consists of finding statistically significant associations between genetic variants and gene expression levels typically measured in tens or hundreds of individuals. This workflow contains the **FastQTL** tool which performs linear regression between genotypes and molecular phenotypes in order to find the best nominal association in cis (i.e. variants located within a specific window around a phenotype). The tool also proposes several efficient methods to accurately and rapidly correct for multiple-testing at both the genotype and phenotype levels [2]. In the [FastQTL paper](https://www.ncbi.nlm.nih.gov/pubmed/26708335) [2], you can find more detailed information about eQTL analysis using the **FastQTL** tool.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n- The **FastQTL** tool requires genotype data in standard VCF format on its **Input variants files** input, phenotype data in an extended UCSC BED format on its **Input expression file** input and the gene annotation file on the **Gene annotation file** input. The VCF and BED files are required to be indexed with **Tabix** to enable fast retrieval of specific genomic regions. Moreover, both files should be multisample files. This workflow supports genotype data provided as multiple VCF files from non-overlapping sample sets or as one multisample VCF file. In case multiple VCF files are provided, the **Bcftools Merge** tool merges these files into one multisample VCF file which is then passed to the **FastQTL** tool. Phenotype data can also be provided as multiple abundance estimates files outputted by tools like **RSEM**, **Kallisto** or **Salmon** or as one multisample abundance estimates file. **SBG Create Expression Matrix** takes multiple expression files and creates one multisample file with expressions, which is passed to **FastQTL**. On its **Output expression QTL file** output, the workflow generates candidates for the eQTLs and on its **HTML reports** output it generates two reports which graphically illustrate the obtained results.\n\n- **FastQTL** requires for input VCF and BED files to contain the same samples, so it is necessary to do some filtering steps before running the **FastQTL** tool. The filtering is done within the **FastQTL Preprocessing** tool which is based on the custom R script which retains only the intersection of samples between VCF and BED and moreover deals with GT values which are different from 0/0, 0/1, 1/0 or 1/1 with .|. (**FastQTL** treats .|. as missing entries which are internally imputed as mean dosage at the variant site).\n\n- Covariate data can also be provided on the **Covariates** (`--covariates`)  input in simple TXT format. Some properties of this file are:\n\n1. The file is TAB delimited\n2. First row gives the sample ID and each additional one corresponds to a single covariate\n3. First column gives the covariate ID and each additional one corresponds to a sample\n4. The file should have S+1 rows and C+1 columns where S and C are the numbers of samples and covariates, respectively.\n\nBoth quantitative and qualitative covariates are supported. Quantitative covariates are assumed when only numeric values are provided. Qualitative covariates are assumed when only non-numeric values are provided. In practice, qualitative covariates with F factors are converted in F-1 binary covariates.\n\n- **FastQTL** supports a permutation-based analysis by specifyng the **Permute** (`--permute`) parameter. Usage example:\n\n```\nfastQTL\n   --vcf genotypes.vcf.gz  \\\n   --bed phenotypes.bed.gz  \\\n   --region 22:17000000-18000000 \n   --permute 1000  \\\n   --out permutations.default.txt.gz\n```\n\n- An adaptive permutation pass on the example data set (i.e. with a number of permutations that varies in function of the significance) is also supported. To run between 100 and 100,000 permutations, use:\n\n```\nfastQTL \n   --vcf genotypes.vcf.gz  \\\n   --bed phenotypes.bed.gz  \\\n   --region 22:17000000-18000000  \\\n   --permute 100 100000  \\\n   --out permutations.adaptive.txt.gz\n```\n\n- To change the cis-window size (i.e. the maximal distance spanned by phenotype-variant pairs to be considered for testing) from default value which is 1e6 bp, set the **Window** (`--window--`) option to the desired size.\n\n- Samples or variants can be excluded from the analysis using the **Exclude samples** (`--exclude_samples`) or **Exclude sites** (`--exclude_sites`) options, respectively.\n\n- This workflow contains a modified version of the **FastQTL** mapping software [3], which is a python wrapper for multi-threaded execution, used in the **GTEX eQTL analysis** project and which has the following enhancements:\n\n1. Options for filtering by minor allele frequency, **MAF threshold** (`--maf_threshold`) and minor allele sample count, **MA sample threshold** (`--ma-sample-threshold`)\n2. Calculation of q-values for FDR estimation using the **False discovery rate** (`--fdr`) option\n3. Minor allele information is reported in output\n\nFor more information, see [Multithreaded FastQTL](https://github.com/francois-a/fastqtl).\n\n###Changes Introduced by Seven Bridges\n\nThe workflow contains the **FastQTL** tool used in the **GTEX eQTL analysis** project, without any modifications.\n\n###Common Issues and Important Notes\n\n- **FastQTL** requires the **GT** field in the input VCF file. Missing entries (./., ./0 or ./1) are internally imputed as mean dosage at the variant site.\n\n- Make sure that the chromosome IDs are the same across all files. A very common mistake is to have chromosomes as 1-22 in the genotype file while they are chr1-chr22 in the phenotype or sequence data. In such cases, **FastQTL** will not be able to find correspondence.\n\n###Performance Benchmarking\n\nThe **eQTL analysis with FastQTL** workflow was tested using the RNA sequencing data set of 462 unrelated human lymphoblastoid cell line samples from the 1000 Genomes sample collection which was created by the Geuvadis consortium and 1000 genome genotype data for 445 samples. The execution time was about 6 hours on AWS m4.4xlarge instance.\n\nCost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n###API Python Implementation\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id = \"your_username/project\"\nworkflow_id = \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n        \"input_files\": api.files.query(project=project_id, names=['chr22.vcf.gz']),\n        \"abundance_estimates\": api.files.query(project=project_id, names=['geuvadis.linc.protein.cov.50PC.bed']),\n        \"in_gene_annotation\": api.files.query(project=project_id, names=['gencode.v19.annotation_withproteinids_GRCh37_liftover.gtf'])[0],\n}\n\ntask = api.tasks.create(name='eQTL analysis with FastQTL - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [FastQTL documentation](http://fastqtl.sourceforge.net/)\n\n[2] [FastQTL paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4866519/)\n\n[3] [Multithreaded FastQTL](https://github.com/francois-a/fastqtl)", "input": [{"name": "Input expressions file"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Output prefix"}, {"name": "Permute"}, {"name": "Create reports"}, {"name": "Input variants files", "encodingFormat": "application/x-vcf"}, {"name": "Best variants only"}, {"name": "Window"}, {"name": "Threshold"}, {"name": "MAF threshold"}, {"name": "MA sample threshold"}, {"name": "Interaction MAF threshold"}, {"name": "False discovery rate"}, {"name": "Seed"}, {"name": "Exclude samples"}, {"name": "Exclude sites"}, {"name": "Number of threads"}, {"name": "Qvalue lambda"}, {"name": "CPU per job"}], "output": [{"name": "FastQTL reports", "encodingFormat": "text/html"}, {"name": "Output expression QTL file"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/eqtl-analysis-with-fastqtl-gtex-v7/16.png", "codeRepository": ["https://github.com/francois-a/fastqtl", "https://github.com/francois-a/fastqtl", "https://github.com/francois-a/fastqtl"], "applicationSubCategory": ["eQTL Analysis"], "project": "SBG Public Data", "creator": "veliboka_josipovic, svetozar_nesic", "softwareVersion": ["v1.0"], "dateModified": 1649165281, "dateCreated": 1572009804, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/eqtl-analysis-with-matrixeqtl/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/eqtl-analysis-with-matrixeqtl/5", "applicationCategory": "Workflow", "name": "eQTL analysis with MatrixEQTL ", "description": "Expression quantitative trait loci (eQTLs) are genomic variants related to variation in expression levels of mRNAs. These loci could be either **cis**, in the neighborhood of a gene transcription start site (TSS) or **trans**, distant eQTLs. **MatrixEQTL** is designed for fast eQTL analysis on large datasets. It uses standard mapping methods that test the linkage between variation in expression and genetic polymorphisms. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n\n\n### Common Use Cases\n\n* Designed for eQTL analysis of large datasets.\n* Performs testing for all or only local transcript-SNP pairs\n* Ultra-fast, no loss of precision.\n* Equally fast for models with covariates.\n* Supports:\n    * Linear additive and ANOVA models. Supports testing for the effect of genotype-covariate interaction.\n    * Covariates to account for sex, population structure, surrogate variables, etc.\n    * Correlated and heteroskedastic errors.\n    * Correction for multiple testing using FDR external link.\n    * Separate p-value thresholds and FDR control for local and distant eQTLs.\n\n\n\n### Changes Introduced by Seven Bridges\n\n\n\n**Input files**\n\n * Multisample VCF file or a list of single-sample VCF files. \n * Tab-separated expression matrix file (containing gene name or gene id and expression level matrix of samples) or a list of files with expressions per sample from many of the popular quantification tools (Kallisto, RSEM, Salmon..). \n * GTF file to extract gene data (chromosome, position, strand, etc.). Gene names from the expression matrix file must match the gene names in GTF.\n\n\n**Output files**\n\n * Tab-separated file with local gene-SNP pairs (cis).  \n * Tab-separated file with distant gene-SNP pairs (trans).  \n * Two html report files (manhattan plot and the most significant SNP-gene pair) \n * Filtered data in a tar file (description below). \n\n\n\nData preprocessing is a part of the tool to allow for VCF and expression matrix file as input files. VCF file should contain genotype information for all samples and expression matrix file should contain gene_name or gene_id together with the expression data for all samples. Only samples that exist in both VCF and BED are analyzed.  \n\nAs **MatrixEQTL** always performs analysis for both local and distant gene-SNP pairs (cis and trans analysis), **p-value threshold** and **Cis p-value threshold** are always greater than zero. Hence, if any of these two values is set to zero it will be automatically reset to default value to make sure that cis and trans file are created.  \n\nA large number of samples implies a very large VCF file (2000 samples VCF could be larger than 100GB). To allow for exploratory analysis on local machines the information of all candidates (filtered by p-value lower than a given threshold) and genes is extracted from the input files and saved in the following tab separated format files: \n\n * snpid_imp.tsv contains SNP data: snpid, chromosome and position.\n * zygosity_imp.tsv contains genotype data: snpid, genotype per sample.\n * gene_positions.tsv contains gene data: gene_id (or gene_name), chromosome and left and right position.\n * gene_expression.tsv contains expression data: gene_id (or gene_name) and expression levels per sample. \n * manhattan_plots.tsv and manhattan_plots_selected.tsv are R DataFrames that provide data for the Manhattan plots in manhattanPlotsMatrixEQTL.html report.  \n\nThese files are then archived in a tar file. \n\n\n### Common Issues and Important Notes\n\n**MatrixEQTL** requires GT in the input VCF file. Missing entries (./., ./0 or ./1) are internally imputed as mean dosage at the variant site.\nMake sure that the chromosome IDs are the same across all files. A very common mistake is to have chromosomes as 1-22 in the genotype file while they are chr1-chr22 in the phenotype or sequence data. In such cases, **MatrixEQTL** will not be able to find correspondence.\n\neQTL analysis is performed with separate treatment of local and distant eQTLs. Therefore, the false discovery rate is calculated separately for these two groups of eQTLs and two outputs are produced (*.cis.tsv and *.trans.tsv). By default,  **Cis p-value threshold** and **p-value threshold** are set to 0.00001.\n\n\n### Performance Benchmarking \n\nBenchmarking is performed on On-Demand AWS instances\n\n\n| Input size VCF.GZ [Gb] | Duration [min] |  Cost [$] |   Instance  |\n|:---------------:|:----------:|:--------------:|:------------:|:-----------:|\n|     1.1     |     97   |    1.66  |   m4.4xlarge  |\n|     0.7     |     64   |    1.1  |   m4.4xlarge  |\n|     0.5     |     44   |    0.77  |   m4.4xlarge  |\n|     0.2     |     23   |    0.4  |   m4.4xlarge  |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for the corresponding platform, visit our documentation.\n```python\nfrom sevenbridges import Api\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id = \"your_username/project\"\nworkflow_id = \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n       \"input_files\": api.files.query(project=project_id, names=['chr22.vcf.gz']),\n       \"abundance_estimates\": api.files.query(project=project_id, names=['geuvadis.linc.protein.cov.50PC.bed']),\n       \"in_gene_annotation\": api.files.query(project=project_id, names=['gencode.v19.annotation_withproteinids_GRCh37_liftover.gtf'])[0],\n}\ntask = api.tasks.create(name='eQTL analysis with MatrixEQTL - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client are provided on GitHub. For more information about using the API Python client, consult sevenbridges-python documentation. More examples are available here.\n\n\n### References\n\n [1] [MatrixEQTL](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3348564/)", "input": [{"name": "Input VCF files", "encodingFormat": "application/x-vcf"}, {"name": "Abundance estimates", "encodingFormat": "text/plain"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "p-value threshold"}, {"name": "p-value threshold cis"}, {"name": "ANOVA"}, {"name": "Cis distance"}, {"name": "Select_cov"}, {"name": "Exclude covariates"}, {"name": "Covariates", "encodingFormat": "text/plain"}], "output": [{"name": "Output TSVs For Plots"}, {"name": "Output eQTL file"}, {"name": "HTML Reports", "encodingFormat": "text/html"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/eqtl-analysis-with-matrixeqtl/5.png", "codeRepository": [], "applicationSubCategory": ["eQTL Analysis"], "project": "SBG Public Data", "creator": "Andrey Shabalin", "softwareVersion": ["v1.0"], "dateModified": 1649165282, "dateCreated": 1572016481, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/exome-coverage-qc-1-0/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/exome-coverage-qc-1-0/1", "applicationCategory": "Workflow", "name": "Exome Coverage QC 1.0", "description": "Exome Coverage QC calculates histograms, per-base reports and BedGraph summaries of feature coverage (aligned sequences for example) for a given genome. This extended version additionally extracts and creates a text file containing summary coverage stats.", "input": [{"name": "bed"}, {"name": "bam"}], "output": [{"name": "summary"}, {"name": "coverage"}], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453798784, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fastqc-analysis/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fastqc-analysis/10", "applicationCategory": "Workflow", "name": "FastQC Analysis", "description": "The FastQC tool, developed by the Babraham Institute, analyzes sequence data from FASTQ, BAM, or SAM files. It produces a set of metrics and charts that help identify technical problems with the data. \n\nUse this pipeline on files you receive from a sequencer or a collaborator to get a general idea of how well the sequencing experiment went. Results from this pipeline can inform if and how you should proceed with your analysis.\n\n###Required inputs\n\n1. FASTQ Reads (ID: *FASTQ_reads*) - one or more FASTQ files. *Note*: In order to process these files efficient, set the number of threads on FastQC app. If it is not set, it will be set automatically based on number of input files, one CPU core per file. If the number of the files is too big (greater than the number of CPU cores on instance) the task will fail. Therefore it's advised that the user should set the \"threads\" argument of FastQC.\n\n###Outputs\n\n1. Report ZIP (ID: *report_zip*)  - ZIP archive containing FastQC html report with dependancies.\n2. FastQC Charts (ID: *b64html*) - Self-contained b64html file, enabling users to see FastQC reports on Seven Bridges platform.\n\n###Common issues\n\n1. In order to process these files efficient, set the number of threads on FastQC app. If it is not set, it will be set automatically based on number of input files, one CPU core per file. If the number of the files is too big (greater than the number of CPU cores on instance) the task will fail. Therefore it's advised that the user should set the \"threads\" argument of FastQC.\n2. If processing large number of big FASTQ files, you might hit the limit of available disk space. Before starting the workflow, check if the total input size is less than available instance disk space. If not, set the different instance, or reduce number of inputs.", "input": [{"name": "FASTQ Reads", "encodingFormat": "application/x-sam"}, {"name": "limits_file", "encodingFormat": "text/plain"}, {"name": "contaminants_file", "encodingFormat": "text/plain"}, {"name": "adapters_file", "encodingFormat": "text/plain"}], "output": [{"name": "Report ZIP", "encodingFormat": "application/zip"}, {"name": "FastQC Charts", "encodingFormat": "text/html"}], "codeRepository": [], "applicationSubCategory": ["FASTQ Processing", "Quality Control"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799047, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/functional-equivalence-wgs-cwl1-0/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/functional-equivalence-wgs-cwl1-0/14", "applicationCategory": "Workflow", "name": "Functional Equivalence WGS", "description": "**Functional Equivalence WGS** workflow processes WGS data according to the functional equivalence standard [1,2].\n\nFor user convenience, a germline variant calling step with HaplotypeCaller is included, with parameters matching current Broad Institute Best Practices WGS workflow [3]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Functional Equivalence WGS** workflow can be used to process single or multi-read group WGS data in FASTQ (FASTQ, FASTQ.GZ, FQ, FQ.GZ) format and obtain alignment files (CRAM) and single-sample germline variant calls (GVCF.GZ).\n\n### Changes Introduced by Seven Bridges\n\n* As the functional equivalence standard does not specify a fixed set of tools for all workflow steps, selection was made based on tool performance and speed.\n* Processing of both query-name sorted and query-name grouped alignments is supported.\n* Read-group level BAM files are merged with **Picard MarkDuplicates**.\n* Adding mate tags is optional and controlled via the **Add mate tags** input parameter.\n* Alignments (**Output pre-BQSR CRAM file**) are output before BQSR, not after, as specified in the standard. A BQSR table (**Merged BQSR report**) is output alongside alignments, for users wishing to apply BQSR with these settings.\n* A germline variant calling step using **HaplotypeCaller** is added, for user convenience.\n* The RG ID field is populated by the **BWA-MEM Bundle 0.7.17 CWL1.0** tool in the workflow by combining the values of **Library ID** and **Platform Unit ID** metadata fields of the input FASTQ files. If **Library ID** field is absent, **Platform Unit ID** field value will be used. In the absence of both fields, the RG ID will be assigned the value of '1'.\n\n### Common Issues and Important Notes\n\n1. The following inputs are required:\n    * **List of FASTQ files** - Input reads data in FASTQ format, with appropriate metadata fields set (see below).\n    * **Reference TAR archive for BWA-MEM** - Reference archive containing the FASTA file and its associated indices, used by BWA-MEM\n    * **Reference** - Reference FASTA file with FAI and DICT secondary files\n    * **Reference dictionary** - Reference FASTA file DICT index, used for generating intervals to scatter BQSR tools\n    * **Known SNPs** - One or more files with known SNPs used for BQSR (e.g. Homo_sapiens_assembly38.dbsnp.vcf)\n    * **Known indels** - One or more files with known indels used for BQSR (e.g. Homo_sapiens_assembly38.known_indels.vcf, Mills_and_1000G_gold_standard.indels.hg38.vcf )\n    * **Input TAR archive with calling intervals** - TAR archive containing INTERVAL_LIST files to use for variant calling (scattering of HaplotypeCaller)\n    * **Add mate tags** - If turned on, **samblaster** will be used to add mate tags to reads output by BWA-MEM\n    * **Query-name sort output alignments** - If turned on, **Sambamba sort** will be used to query-name sort BWA-MEM outputs. If left off, the outputs are grouped by query name.\n\n2. **List of FASTQ files** should have the following metadata fields set:\n    * **Platform** - this file is necessary for some of the GATK tools\n    * **Paired-end** - used during alignment\n    * **Sample ID** - required for HaplotypeCaller\n    * **Platform Unit ID** - only necessary if read-group level pairing of inputs is needed. To ensure proper input pairing, please ensure that read group-level files can be uniquely paired based on the combination of **Platform**, **Library ID**, **Platform Unit ID** and **File Segment** (if used) fields.\n    * **Library ID** - Please make sure to set this field if processing sample data from several libraries in the same task.\n\n3. In order to ensure that read-group level information is properly propagated through the workflow, when processing data with multiple read groups, please make sure to set **Platform Unit ID** and (for multiple libraries) **Library ID** metadata fields on all files supplied as **List of FASTQ files** input.\n\n4. Inputs **Known SNPs** and **Known Indels** should be in VCF format and accompanied by IDX index files. \n\n5. **Input TAR archive with calling intervals** should contain INTERVAL_LIST files. The number of these files will determine the number of HaplotypeCaller jobs.\n\n\n### Performance Benchmarking\n\nPerformance of the workflow greatly depends on input sizes/coverage and the selected resources.\n\nThe sample sets mentioned in the table are:\n\n1. 50x file set:\n   * GRCh38-aligned NA12878 (converted from the GRCh38 1000g CRAM) BAM   (50x) FASTQ.GZ files for 50x Illumina platinum pedigree NA12878: ERR194147 (48 GB)\n2. 30x file set:\n   * Simons dataset HGDP01078 sample realigned to GRCh38 by Seven Bridges  (30x) FASTQ.GZ files obtained with biobambam2 bamtofastq (11.6 GB)\n  \n     \n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| 30x file set | 14 h 14 min |$17.55 + $1.36 | c5.9xlarge and c4.2xlarge - 700 GB EBS | \n| 50x file set | 21 h 40 min | $27.33 + $3.0 | c5.9xlarge and c4.2xlarge - 700 GB EBS |  \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### References\n\n[1] [Functional Equivalence publication](https://www.nature.com/articles/s41467-018-06159-4)\n\n[2] [Functional Equivalence GitHub repository](https://github.com/CCDG/Pipeline-Standardization/blob/master/PipelineStandard.md)\n\n[3] [Broad Institute Best Practices Germline Variant Calling](https://github.com/gatk-workflows/gatk4-germline-snps-indels/commit/c8d0d63baa878ea4991e3108dd04e2c3bdbae6d2)", "input": [{"name": "Reference TAR archive for BWA-MEM", "encodingFormat": "application/x-tar"}, {"name": "List of FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Memory per job [MB]"}, {"name": "Reference dictionary"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Known SNPs", "encodingFormat": "application/x-vcf"}, {"name": "Known indels", "encodingFormat": "application/x-vcf"}, {"name": "Memory per job"}, {"name": "Memory per job"}, {"name": "Contamination fraction to filter"}, {"name": "Input TAR archive with calling intervals", "encodingFormat": "application/x-tar"}, {"name": "Memory per job"}, {"name": "CPUs per job"}, {"name": "Number of threads"}, {"name": "Optical duplicate pixel distance"}, {"name": "Memory per job"}, {"name": "Use original qualities"}, {"name": "Use original qualities"}, {"name": "Threads"}, {"name": "Total memory"}, {"name": "Reserved number of threads on the instance"}, {"name": "Sambamba threads"}, {"name": "Add mate tags"}, {"name": "Query-name sort output alignments"}, {"name": "PCR indel model"}], "output": [{"name": "Duplication metrics file"}, {"name": "Output pre-BQSR CRAM file", "encodingFormat": "application/x-bam"}, {"name": "Output md5sums for alignment files"}, {"name": "Output merged GVCF.GZ file", "encodingFormat": "application/x-vcf"}, {"name": "Output md5sums for variant files"}, {"name": "Merged BQSR report"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": [], "applicationSubCategory": ["WGS"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["v1.0"], "dateModified": 1648047365, "dateCreated": 1611328524, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fusion-transcript-detection-chimerascan/34", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fusion-transcript-detection-chimerascan/34", "applicationCategory": "Workflow", "name": "Fusion Transcript Detection - ChimeraScan", "description": "Fusion Transcript Detection - ChimeraScan detects and identifies fusion transcripts from paired-end RNA-Seq data using ChimeraScan.\n\nFusion genes or chimeras are gene alterations resulting from chromosomal rearrangements combining exons from genes located on the same or different chromosomes. Fusion gene products may have a new or different function than the two fusion partners. Fusion transcripts are frequently found in diverse types of carcinomas including breast, lung, and prostate cancers, as well as melanomas and lymphomas. Detection of known (and identification of novel) gene fusions can lead to a better understanding of the triggering mechanism and progression of carcinogenesis. Moreover, recent studies suggest that fusion gene products may represent a novel therapeutic target for the treatment of human cancers.\n\nMethod:\nThis pipeline uses ChimeraScan software package that to detect fusion genes (1). Besides the primary program that detects fusion genes, the toolkit consists of an accessory tool that prepares references for proper indexing in upstream analysis and a tool for preparing output files in HTML table format. ChimeraScan is found to exhibit strong performance with low rate of false positive fusion detections, but only accepts paired-end RNA-Seq data. Addition of Chimera (2) and Oncofuse (3) to this pipeline serves to provide additional control of detected fusion genes. Finally, graphical representation of identified fusion genes is provided by the genomic coordinate visualization tool Circos (4).\n\nInputs:\nReads (paired-end): This pipeline accepts one pair of paired-end RNA-Seq data in FASTQ format (plain text or compressed files). If reads for samples are present in multiple files, the Merge FASTQ Files Public Pipeline can be used to consolidate them before alignment. If both reads are given as a gzipped archive, one can use SBG Unpack FASTQs to unpack and set the paired-end metadata fields to 1 and 2 respectively, which is obligatory.\n\nGenome reference: FASTA file containing reference genome. For human samples we recommend ucsc.hg19.fasta.\nTranscriptome reference: Transcriptome reference file containing all known transcripts in GTF format. For human samples we recommend human_hg19_genes_2015.gtf. NOTE: ChimeraScan Index does not handle well the GENCODE GTF releases.\n\nFalse Positive Chimeras (optional): List of known false positive chimeras. For human samples we recommend hg19_bodymap_false_positive_chimeras.txt.\n\nTools and suggested parameter settings:\n* ChimeraScan v0.4.5 - Software package for detection of gene fusions in paired-end RNA-Seq datasets (1). ChimeraScan uses Bowtie to align paired-end reads to a combined genome/transcriptome reference, aiming to discover discordant reads, predict an optimal fusion breakpoint location, and detect chimeras. Software package includes an indexing program, ChimeraScan Index, which creates the combined index from genomic reference sequences (FASTA format) and custom transcriptome reference format (UCSC GenePred format). ChimeraScan GTF to genePred has been added to this pipeline to convert GTF file in a format acceptable by ChimeraScan Index. ChimeraScan HTML Table creates an HTML page with links to detailed descriptions of the chimeric genes. Additionally, SBG ChimeraScan4Circos generates output files needed visualization tool Circos. \nParameters are set to default values based on the best practice suggested by the ChimeraScan authors.\n\nTips for reducing false positives: \nOnly consider annotated genes (unless, of course, you are looking for fusion between unannotated transcripts).\nProvide a file of known likely false positives during task execution. We recommend, for human samples, hg19_bodymap_false_positive_chimeras.txt.\n* Chimera v1.12.0 - Software package for downstream processing that accepts ChimeraScan BEDPE output and enables further filtering of detected fusion transcripts (2). This tool generates a detected and a filtered fusion genes file, and produces the R workspace file obtained from pipeline execution. \n\nParameters that can be adjusted while filtering fusion candidates: \nmin.support (default: 10), minimal number of reads spanning a specific fusion.\nfilterList type: (default: annotated genes) Additional filtering allows predicted chimeras to be discarded based on the following criteria: \nspanning reads - if it has less spanning reads than a set value\nfusion names - if particular fusion (or gene) name is in the given list (e.g. ABL1:BCR)\nintronic - if the intronic regions are included in the fusion\nannotated genes - if the partner genes are not annotated (currently set as default)\nread through - if gene partners are the same\n* Oncofuse v1.1.0 serves for prioritization of fusions based on their oncogenic potential, i.e. the probability of being 'driver' event (3). Further, a node that converts output from Oncofuse into an html table sorted by driver probability is added.\n* Circos v0.68 - Tool for visual representation of identified fusion genes (4). Graphical visualization relies on circular layout of the genome using fixed configuration file. \n\nOutputs:\nThe chimeras.bedpe file contains information about the chromosomal regions, transcript IDs, genes, and statistics for each identified chimera. \nSortable table of detected chimeras in a user-friendly HTML page, for web browser viewing, with links to detailed descriptions of the chimeric genes. \nIndex file as a gzipped archive. NOTE: In case of subsequent runs of the pipeline with the same genome/transcriptome reference, one should reuse this file (in order to save approximately 4h needed for creating an index) and modify the pipeline accordingly (to start with Chimerascan Run). \nDetected and filtered fusion genes files generated by Chimera tool, provided as an additional control for true fusion detection; R workspace contains all saved R objects during execution of Chimera - it is useful for further analysis and for additional details on detected fusions.\nOncofuse output with functional prediction scores (oncogenic potential) of detected fusions. \nCircos plots for visual representation of fusion genes.\n\nAdditional suggestions:\nThe execution time of this pipeline can be measured in hours, even dozens of hours for big FASTQs (say, bigger than 10 GB). Remember that only creating an index takes around 4 hours. \nFeel free to customize this pipeline by removing or adding new nodes based on goal of your study.\n\n\nReferences:\n(1) Iyer, M. K. et al. ChimeraScan: a tool for identifying chimeric transcription in sequencing data. Bioinformatics 27, 2903\u20132904 (2011)\n(2) Beccuti M, Carrara M, Cordero F, Lazzarato F, Donatelli S, Nadalin F, Policriti A and Calogero RA \"Chimera: a Bioconductor package for secondary analysis of fusion products.\" Bioinformatics, 0, pp. 3 (2014)\n(3) Mikhail Shugay, Inigo Ortiz de Mend\u00edbil, Jose L. Vizmanos and Francisco J. Novo. Oncofuse: a computational framework for the prediction of the oncogenic potential of gene fusions. Bioinformatics, 29 (20): 2539-2546 (2013)\n(4) Krzywinski, M. et al. Circos: an information aesthetic for comparative genomics. Genome Res. 19, 1639\u20131645 (2009)", "input": [{"name": "reference", "encodingFormat": "application/x-tar"}, {"name": "genes", "encodingFormat": "application/x-gtf"}, {"name": "false_positives", "encodingFormat": "text/plain"}, {"name": "reads", "encodingFormat": "text/fastq"}, {"name": "Sort based on the column"}, {"name": "Header"}, {"name": "The organism to be used for annotation"}, {"name": "Define detected fusions by minimum supporting reads"}, {"name": "The Fusion Finder Tool"}, {"name": "Filter detected fusions: by minimum supporting reads"}, {"name": "FilterList type"}, {"name": "Filter detected fusions: by fusion partner"}, {"name": "Human karyotype"}, {"name": "tissue"}, {"name": "Human karyotype"}, {"name": "Output format"}, {"name": "Num Processors"}], "output": [{"name": "html_file", "encodingFormat": "text/html"}, {"name": "oncofuse_out", "encodingFormat": "text/plain"}, {"name": "index", "encodingFormat": "application/x-tar"}, {"name": "chimeras_bedpe"}, {"name": "circos_chimera"}, {"name": "circos_oncofuse"}, {"name": "filtered_fusions", "encodingFormat": "text/plain"}, {"name": "detected_fusions", "encodingFormat": "text/plain"}, {"name": "filtered_annotated_fusions"}, {"name": "b64html", "encodingFormat": "text/html"}], "applicationSubCategory": ["RNA-Seq", "Gene Fusion"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468218, "dateCreated": 1453799625, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practice-data-pre-processing-workflow-4-1-0-0/30", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practice-data-pre-processing-workflow-4-1-0-0/30", "applicationCategory": "Workflow", "name": "GATK Best Practice Data Pre-processing 4.1.0.0", "description": "**BROAD Best Practice Data Pre-processing Workflow 4.1.0.0**  is used to prepare data for variant calling analysis. \n\nIt can be divided into two major segments: alignment to reference genome and data cleanup operations that correct technical biases [1].\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n* **BROAD Best Practice Data Pre-processing Workflow 4.1.0.0**  is designed to operate on individual samples.\n* Resulting BAM files are ready for variant calling analysis and can be further processed by other BROAD best practice pipelines, like **Generic germline short variant per-sample calling workflow** [2], **Somatic CNVs workflow** [3] and **Somatic SNVs+Indel workflow** [4].\n\n\n### Changes Introduced by Seven Bridges\n\nThis pipeline represents the CWL implementation of BROADs [original WDL file](https://github.com/gatk-workflows/gatk4-data-processing/pull/14) available on github. Minor differences are introduced in order to successfully adapt to the Seven Bridges Platform. These differences are listed below:\n* **SamToFastqAndBwaMem** step is divided into elementary steps: **SamToFastq** - converting unaligned BAM file to interleaved  FASTQ file, **BWA Mem** - performing alignment and **Samtools View** - used for converting SAM file to BAM.\n*  A boolean parameter **Ignore default RG ID** is added to **BWA MEM Bundle** tool. When used, this parameter ensures that **BWA MEM Bundle** does not add read group information (RG) in the BAM file. Instead, RG ID information obtained from uBAM is added by **GATK MergeBamAlignment** afterwards. \n* **SortAndFixTags** is divided into elementary steps: **SortSam** and **SetNmMdAndUqTags**\n* Added **SBG Lines to Interval List**: this tool is used to adapt results obtained with **CreateSequenceGroupingTSV**  for platform execution, more precisely for scattering.\n\n\n\n### Common Issues and Important Notes\n\n* **BROAD Best Practice Data Pre-processing Workflow 4.1.0.0**  expects unmapped BAM (uBAM) file format as the main input. One or more read groups, one per uBAM file, all belonging to a single sample (SM).\n* **Input Alignments** (`--in_alignments`) - provided uBAM file should be in query-sorted order and all reads must have RG tags. Also, input uBAM files must pass validation by **ValidateSamFile**.\n* For each tool in the workflow, equivalent parameter settings to the one listed in the corresponding WDL file are set as defaults. \n\n### Performance Benchmarking\nSince this CWL implementation is meant to be equivalent to GATKs original WDL, there are no additional optimisation steps beside instance and storage definition. \nThe c5.9xlarge AWS instance hint is used for WGS inputs and attached storage is set to 1.5TB.\nIn the table given below one can find results of test runs for WGS and WES samples. All calculations are performed with reference files corresponding to assembly 38.\n\n*Cost can be significantly reduced by spot instance usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Input Size | Experimental Strategy | Coverage| Duration | Cost (on-demand) | AWS Instance Type |\n| --- | --- | --- | --- | --- | --- | \n| 6.6 GiB | WES | 70 |1h 19min | $2.61 | c5.9 |\n|3.4 GiB | WES |  40 | 42min   | $1.40 | c5.9 |\n| 111.3 GiB| WGS | 30 |22h 41min | $43.86 | c5.9 |\n| 37.2 GiB  | WGS | 10 | 7h 21min | $14.21 | c5.9 |\n\n\n### References\n\n[1] [Data Pre-processing](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165)\n[2] [Generic germline short variant per-sample calling](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145)\n[3] [Somatic CNVs](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11147)\n[4] [Somatic SNVs+Indel pipeline ](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11146)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "BWA index archive", "encodingFormat": "application/x-tar"}, {"name": "FASTA reference", "encodingFormat": "application/x-fasta"}, {"name": "DICT file"}, {"name": "Known sites", "encodingFormat": "text/x-bed"}], "output": [{"name": "Output BAM file", "encodingFormat": "application/x-bam"}, {"name": "MD5 file"}, {"name": "Duplication metrics"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/gatk-workflows/gatk4-data-processing", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["Alignment", "WGS"], "project": "SBG Public Data", "creator": "BROAD", "softwareVersion": ["v1.0"], "dateModified": 1648552783, "dateCreated": 1572002717, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-best-practice-generic-germline-short-variant-per-sample-cal/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-best-practice-generic-germline-short-variant-per-sample-cal/13", "applicationCategory": "Workflow", "name": "GATK Broad Best Practice Germline snps and indels variant calling 4.1.0.0", "description": "This workflow represents the GATK Best Practices for SNP and INDEL calling on DNA data.\n\nStarting from a processed **BAM** file, the workflow performs variant calling with respect to the reference genome. Depending on **HaplotypeCaller's** output file type (**VCF** or **g.VCF**, the resulting file of this workflow can be used as a stand alone result for single-sample analysis, or as one of the cohort files downstream joint calling analysis. On the GATK website you can find more detailed information about calling germline variants for single sample or joint calling analysis [1].\n\n### Common Use Cases\n\n* The **haplotypecaller-gvcf-gatk4** (original WDL name) workflow [1] runs the **HaplotypeCaller** tool from GATK4 in GVCF mode on a single sample according to GATK Best Practices. \n* To run HaplotypeCaller in a mode appropriate for joint calling analysis, one needs to set (`--emit_ref_confidence`) parameter to GVCF. \n* When executed, the workflow scatters the **HaplotypeCaller** tool over the **Calling intervals** file (`--in_intervals`). \n* The resulting g.VCF files are merged with **GATK MergeVCF**. \n* The output file produced will be a single g.VCF file which can be further processed in joint-discovery workflow. \n* By default, the output file is compressed with gzip, leading to a G.VCF.GZ extension.\n\n* This workflow can also be used for single sample analysis. For that purpose, it produces a VCF file which is obtained by setting (the `--emit_ref_confidence`) parameter to NONE. \n\n\n### Changes Introduced by Seven Bridges\n\n* The original **Generic germline variant per-sample calling** WDL implementation has a step called **CramToBamTask** which accepts CRAM files and converts them to BAM files with **samtools view**, while also indexing them. In this CWL implementation, this step is skipped as **GATK HaplotypeCaller** has the option to work with CRAM files. Keep in mind that CRAM files need to be indexed. \n\n* To enable scattering of **GATK Haplotypecallier** tool, we have introduced the **GATK IntervalListTool**, solution given in **GATK Production Germline short variant per-sample calling** [2]. \n\n### Common Issues and Important Notes\n\n* The **HaplotypeCaller** app uses **Intervals list** to restrict processing to specific genomic intervals. You can set the **Scatter count** value in order to split **Intervals list** into smaller intervals. **HaplotypeCaller** processes these intervals in parallel, which can significantly reduce workflow execution time in some cases.\n\n* The workflow accepts multiple flowcell BAMs on input, however, they must all share the same sample ID. Otherwise, some GATK tools will fail.\n\n* Running a **batch task**: Batching is performed by **Sample ID** metadata field on the **Aligned and Processed BAM** input port. For running analyses in batches, it is necessary to set **Sample ID** metadata for each **Processed and aligned BAM** file.\n\n### Performance Benchmarking\n                   \n|  BAM Input size | Experiment type | Coverage | Duration | Cost | Instance |\n|-----------------------|-----------------       |------------   |-------------|--------|--------------|\n| 55.8GiB              |  WGS           (scatter count = 20)     | ~50x            |  17h 35min   | $9.42           | c4.2xlarge |\n| 55.8GiB              |  WGS           (scatter count = 80)     | ~50x            |  10h 32min   | $5.64           | c4.2xlarge |\n| 24.6GiB              |  WGS           (scatter count = 80)     | ~10x            |  4h 12min     | $2.25           | c4.2xlarge |\n| 3.5GiB                |  WES            (scatter count = 1)    | ~70x             |  17min          | $0.16           | c4.2xlarge |\n| 1.9GiB                |  WES            (scatter count = 1)    | ~40x             |  11min          | $0.11           | c4.2xlarge | \n| 1.1GiB                |  WES            (scatter count = 1)    | ~20x             |  9min            | $0.08           | c4.2xlarge | \n| 434MiB               |  WES            (scatter count = 1)    | ~10x             |  6min            | $0.06           | c4.2xlarge | \n\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n        \"in_reference\": api.files.query(project=project_id, names=[\"Homo_sapiens_assembly38.fasta\"])[0], \n\t\"in_alignments\": list(api.files.query(project=project_id, names=[\"HCC1143BL_WES_1.processed.bam\"])), \n\t\"in_intervals\": list(api.files.query(project=project_id, names=[\"wgs_calling_regions.hg38.interval_list\"]))}\n\n# Creates draft task\ntask = api.tasks.create(name=\"GATK Best Practice Germline snps and indels 4.1.0.0 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Broad germline SNPS and INDELS](https://github.com/gatk-workflows/gatk4-germline-snps-indels)\n\n[2] [Broad Producion WGS germline SNPs and INDELs](https://github.com/gatk-workflows/broad-prod-wgs-germline-snps-indels)", "input": [{"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Calling intervals", "encodingFormat": "application/x-vcf"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Contamination fraction to filter"}, {"name": "Create output variant index"}, {"name": "Emit ref confidence"}, {"name": "Output mode"}, {"name": "Output VCF extension"}, {"name": "Output file format"}, {"name": "Output prefix"}], "output": [{"name": "VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/gatk-workflows/gatk4-germline-snps-indels", "https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/haplotypecaller-gvcf-gatk4.wdl", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["Variant Calling", "WGS"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648468219, "dateCreated": 1572002720, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-broad-best-practice-variant-calling-from-ubam-4-1-0-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-broad-best-practice-variant-calling-from-ubam-4-1-0-0/7", "applicationCategory": "Workflow", "name": "GATK Broad Best Practice Variant Calling From uBAM 4.1.0.0", "description": "**GATK Broad Best Practice Variant Calling From uBAM 4.1.0.0**  is used to prepare data for variant calling analysis. \n\nIt can be divided into two major segments: alignment to reference genome and data cleanup operations that correct technical biases [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n* **GATK Broad Best Practice Variant Calling From uBAM 4.1.0.0**  is designed to operate on individual samples.\n* When executed, the workflow scatters the HaplotypeCaller tool over the **Interval list** file (--in_intervals). Default number of intervals is 50.\n* The resulting VCF files are merged with **GATK MergeVCF**.\n* By default, the output file is VCF, but can be compressed with gzip by setting extension for **HaplotypeCaller** and **MergeVCF** to VCF.GZ extension.\n* To run **HaplotypeCaller** in a mode appropriate for joint calling analysis, one needs to set (--emit_ref_confidence) parameter to GVCF.\n\n\n\n### Changes Introduced by Seven Bridges\n\nThis pipeline represents the CWL implementation of two  BROADs: one for [pre-processing](https://github.com/gatk-workflows/gatk4-data-processing/pull/14) and other for [variant calling]() available on github. Minor differences are introduced in order to successfully adapt to the Seven Bridges Platform. These differences are listed below:\n* **SamToFastqAndBwaMem** step is divided into elementary steps: **SamToFastq** - converting unaligned BAM file to interleaved  FASTQ file, **BWA Mem** - performing alignment and **Samtools View** - used for converting SAM file to BAM.\n*  A boolean parameter **Ignore default RG ID** is added to **BWA MEM Bundle** tool. When used, this parameter ensures that **BWA MEM Bundle** does not add read group information (RG) in the BAM file. Instead, RG ID information obtained from uBAM is added by **GATK MergeBamAlignment** afterwards. \n\n* **SortAndFixTags** is divided into elementary steps: **SortSam** and **SetNmMdAndUqTags**.\n* Added **SBG Lines to Interval List**: this tool is used to adapt results obtained with **CreateSequenceGroupingTSV**  for platform execution, more precisely for scattering.\n* The original Generic germline variant per-sample calling WDL implementation has a step called **CramToBamTask** which accepts CRAM files and converts them to BAM files with **Samtools view**, while also indexing them. In this CWL implementation, this step is skipped as **GATK HaplotypeCaller** has the option to work with CRAM files. Keep in mind that CRAM files need to be indexed.\n* To enable scattering of **GATK HaplotypeCaller** tool, we have introduced the **GATK IntervalListTool**, solution given in GATK Production Germline short variant per-sample calling [2].\n\n\n### Common Issues and Important Notes\n\n* **GATK Broad Best Practice Variant Calling From uBAM 4.1.0.0**  expects unmapped BAM file format as the main input.\n* **Input Alignments** (`--in_alignments`) - provided an unmapped BAM (uBAM) file should be in query-sorter order and all reads must have RG tags. Also, input uBAM files must pass validation by **ValidateSamFile**.\n* For each tool in the workflow, equivalent parameter settings to the one listed in the corresponding WDL file are set as defaults. \n\n### Performance Benchmarking\nSince this CWL implementation is meant to be equivalent to GATKs original WDL, there are no additional optimisation steps beside instance and storage definition. \nThe c5.9xlarge AWS instance hint is used for WGS inputs and attached storage is set to 1.5TB.\nIn the table given below one can find results of test runs for WGS and WES samples. All calculations are performed with reference files corresponding to assembly 38.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Input Size | Experimental Strategy | Coverage| Duration | Cost (on-demand) | AWS Instance Type |\n| --- | --- | --- | --- | --- | --- | \n| 4.6 GiB | WES  | 70 |1h 23min | $2.68 | c5.9 |\n|2.3 GiB  | WES  |  40 | 45min   | $1.47 | c5.9 |\n| 89GiB.  | WGS  | 30 |1 day 3h 24min | $52.98 | c5.9 |\n| 29.8 GiB | WGS | 10 |10h 1min   | $19.38 | c5.9 |\n\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [Data Pre-processing](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165)\n\n[2] [Generic germline short variant per-sample calling](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145)\n\n[3] [Somatic CNVs](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11147)\n\n[4] [Somatic SNVs+Indel pipeline ](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11146)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "BWA index archive", "encodingFormat": "application/x-tar"}, {"name": "FASTA reference", "encodingFormat": "application/x-fasta"}, {"name": "DICT file"}, {"name": "Known sites", "encodingFormat": "text/x-bed"}, {"name": "Include the header in the output"}, {"name": "Interval list", "encodingFormat": "application/x-vcf"}, {"name": "Scatter count"}, {"name": "Output VCF extension"}, {"name": "Output file format"}, {"name": "Memory per job"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Emit ref confidence"}, {"name": "Include the header in the output"}, {"name": "Include the header in the output"}], "output": [{"name": "MD5 file"}, {"name": "Duplication metrics"}, {"name": "bwa_sam", "encodingFormat": "application/x-bam"}, {"name": "mba_sam", "encodingFormat": "application/x-sam"}, {"name": "pp_sam", "encodingFormat": "application/x-sam"}, {"name": "Output merged VCF or BCF file", "encodingFormat": "application/x-vcf"}, {"name": "Output BAM file", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/gatk-broad-best-practice-variant-calling-from-ubam-4-1-0-0/7.png", "codeRepository": ["https://github.com/gatk-workflows/gatk4-data-processing", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["WGS", "Variant Calling"], "project": "SBG Public Data", "creator": "BROAD", "softwareVersion": ["v1.0"], "dateModified": 1648476172, "dateCreated": 1611606890, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-generic-germline-short-variant-per-sample-calling-4-2-0-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-generic-germline-short-variant-per-sample-calling-4-2-0-0/3", "applicationCategory": "Workflow", "name": "GATK Generic Germline Short Variant Per-Sample Calling 4.2.0.0", "description": "**GATK Generic Germline Short Variant Per-Sample Calling 4.2.0.0**  calls germline variants in a sample [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**GATK Generic Germline Short Variant Per-Sample Calling 4.2.0.0** can be used to call germline variants in a WGS sample with **GATK HaplotypeCaller** [1], starting from an an analysis-ready BAM file (**Input alignments**).\n\n### Changes Introduced by Seven Bridges\n\n* Optional `CramToBam` step from the WDL implementation was omitted from the workflow. The starting alignments are assumed to be in BAM format.\n*  Interval files for variant calling should be provided together in an archive (**Archive with calling interval files**), which will be unpacked in the task.\n\n### Common Issues and Important Notes\n\n*  **Input alignments**, **Archive with calling interval files** and **Reference sequence** inputs are required.\n\n### Performance Benchmarking\n\nThe workflow was tested with a WGS NA12878 BAM file (48.4 GB) aligned to GRCh38 and the Broad Institute recommended set of 50 variant calling interval lists.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 48.4 GB BAM | 203 min |$5.18 + $0.09| c5.9xlarge  - 200 GB EBS | \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**GATK Generic Germline Short Variant Per-Sample Calling 4.2.0.0** was tested with cwltool 3.1.20210628163208.\n\n### References\n\n[1] [GATK Generic Germline Short Variant Per-Sample Calling workflow documentation](https://github.com/gatk-workflows/gatk4-germline-snps-indels/tree/4d8130f88e2e1f00c57122832721ecc78a5b0102)", "input": [{"name": "Archive with calling interval files", "encodingFormat": "application/zip"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}], "output": [{"name": "Merged variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/4d8130f88e2e1f00c57122832721ecc78a5b0102/haplotypecaller-gvcf-gatk4.wdl", "https://github.com/gatk-workflows/gatk4-germline-snps-indels/releases/tag/2.3.1", "https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/4d8130f88e2e1f00c57122832721ecc78a5b0102/README.md"], "applicationSubCategory": ["Variant Calling", "WGS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2", "v1.0"], "dateModified": 1648047366, "dateCreated": 1634721729, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-pre-processing-for-variant-discovery-4-2-0-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-pre-processing-for-variant-discovery-4-2-0-0/3", "applicationCategory": "Workflow", "name": "GATK Pre-Processing For Variant Discovery 4.2.0.0", "description": "**GATK Pre-Processing For Variant Discovery 4.2.0.0**  is used to prepare data for variant calling analysis. The workflow can be divided into two major segments: alignment to reference genome and data cleanup operations that correct technical biases [1].\n\n**A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.**\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n* **GATK Pre-Processing For Variant Discovery 4.2.0.0**  is designed to operate on individual samples.\n* Resulting BAM files are ready for variant calling analysis and can be further processed by other BROAD best practice pipelines, like **Generic germline short variant per-sample calling workflow** [2], **Somatic CNVs workflow** [3] and **Somatic SNVs+Indel workflow** [4].\n\n\n### Changes Introduced by Seven Bridges\n\nThis pipeline represents the CWL implementation of BROADs [original WDL file](https://github.com/gatk-workflows/gatk4-data-processing/blob/d216fddc20ca60677081cc524215b449e2939265/processing-for-variant-discovery-gatk4.wdl) available on github. Minor differences are introduced in order to successfully adapt to the Seven Bridges Platform. These differences are listed below:\n* **SortAndFixTags** is divided into elementary steps: **SortSam** and **SetNmMdAndUqTags**.\n\n### Common Issues and Important Notes\n\n* **GATK Pre-Processing For Variant Discovery 4.2.0.0**  expects unmapped BAM (uBAM) file format as the main input. One or more read groups, one per uBAM file, all belonging to a single sample (SM).\n* **Input Alignments** (`--in_alignments`) - provided uBAM file should be in query-sorted order and all reads must have RG tags. Also, input uBAM files must pass validation by **ValidateSamFile**.\n* For each tool in the workflow, equivalent parameter settings to the one listed in the corresponding WDL file are set as defaults. \n* If **Sample ID** is set in the **Input alignments** metadata, it will be used for output naming. Otherwise, name prefix from the first uBAM file will be used.\n* For most optimal task cost and performance, the following instance and storage hints are set in the workflow: c5.2xlarge with 600GB storage is set for **GATK MarkDuplicates**, **GATK SortSam** and **GATK SetNmMdAndUqTags**; c5.9xlarge with 500GB storage is set for all other tools in the workflow. Adequate Google and Azure instances are also set. Set storage size is sufficient for processing regular WGS files. For extreme cases, with extremely high coverage, storage hints should be edited or a workflow-level hint can be set through task execution settings (will likely lead to more expensive tasks).\n\n### Performance Benchmarking\n\nPerformance benchmark was run on WGS data for sample NA12878 from [Broad public dataset](https://console.cloud.google.com/storage/browser/broad-public-datasets/NA12878/unmapped) (24 unmapped BAM files, 2.7GB average file size). All calculations were performed with GRCh38 reference files.\n\n| Experiment type  | Duration | Cost (Instances + Attached Disks) | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS - 24 x 2.7GB |13h 30min | $15.41 ($14.43 + $0.98) | c5.2xlarge - 600GB EBS and c5.9xlarge - 500GB EBS |\n\n\n*Cost can be significantly reduced by spot instance usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n**GATK Pre-Processing For Variant Discovery 4.2.0.0** was tested with cwltool version 3.1.20210816212154. The `in_alignments`, `in_reference`, `reference_index_tar`, `known_sites` and `reference_dict` inputs were provided in the job.yaml/job.json file and used for testing. All other input parameters are set to the same values as in the WDL implementation inside the workflow and can't be modified by the user on the input.\n\n### References\n\n[1] [Data Pre-processing](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165)\n\n[2] [Generic germline short variant per-sample calling](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145)\n\n[3] [Somatic CNVs](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11147)\n\n[4] [Somatic SNVs+Indel pipeline ](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11146)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Known sites", "encodingFormat": "text/x-bed"}, {"name": "Reference dictionary"}], "output": [{"name": "MD5 file"}, {"name": "Output BAM file", "encodingFormat": "application/x-bam"}, {"name": "Output metrics file"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/gatk-workflows/gatk4-data-processing", "https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip"], "applicationSubCategory": ["Alignment", "CWLtool Tested"], "project": "SBG Public Data", "creator": "BROAD", "softwareVersion": ["v1.2"], "dateModified": 1648039727, "dateCreated": 1634722536, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-rnaseq-short-variant-discovery-4-2-0-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-rnaseq-short-variant-discovery-4-2-0-0/5", "applicationCategory": "Workflow", "name": "GATK RNAseq short variant discovery 4.2.0.0", "description": "**GATK RNAseq short variant discovery 4.2.0.0** workflow is an analysis flow for SNP and INDEL calling on RNASeq data. \n\nStarting from an unmapped BAM file, it performs alignment to the reference genome, followed by marking of duplicates, reassigning of mapping qualities, base recalibration, variant calling and variant filtering. \n\n**GATK RNAseq short variant discovery 4.2.0.0** represents a cwl implementation of the official GATK workflow given in WDL for RNASeq variant discovery. In  [1](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891), you can find detailed information about calling variants in RNASeq.\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n###Common Use Cases\n\n\n- If you have raw sequencing reads in FASTQ format, you should convert them to an unmapped BAM file using **Picard FastqToSam** before running the workflow.\n\n-  Batching is performed by **Sample ID** metadata field on the **Unaligned BAM** input port.\n\n\n- By providing a pre-generated **STAR** reference index file to the **Reference FASTA / Index TAR file** input, execution time will reduce.\n\n- **GATK BaseRecalibrator** uses indels and  SNPs databases provided on **Known sites** input, to mask out polymorphic sites when creating a model for adjusting quality scores. \n\n- **GATK HaplotypeCaller** uses the **dbSNP** database to populate the ID column of the VCF output.\n\n\n- **GATK HaplotypeCaller** uses exonic intervals produced by **GATK gtfToCallingIntervals** and organized with **GATK IntervalList Tools**  to restrict processing to specific genomic regions. **GATK IntervalList Tools** uses **Scatter count** value to split exonic interval list into smaller intervals; default value is 6.  **GATK HaplotypeCaller** processes these intervals in parallel, which can significantly reduce workflow execution time in some cases.\n\n- All input parameters are set to values defined in [1]. Still, some parameters are exposed so you can tailor resulting file names by setting **Output prefix for generated files** and format of resulting  files generated by **GATK MergeVcfs** and **GATK VariantFiltration** by setting **Output VCF file type** to VCF or VCF.GZ.\n\n\n\n\n### Changes Introduced by Seven Bridges\n\n\n**GATK RNAseq short variant discovery 4.2.0.0**  workflow represents the GATK Best Practices for SNP and indel calling on RNASeq data, and there are no modifications to the original workflow. \nThis implementation enables the user to choose compressed or uncompressed outputs of **GATK MergeVcfs** and **GATK VariantFiltration** as well as to define common prefixes of generated files, while all other parameters are set to default values given in [1]. \n\n\n### Common Issues and Important Notes\n\n- When converting FASTQ files to an unmapped BAM file using **Picard FastqToSam**, it is required to set the **Platform** (`PLATFORM=`) parameter.\n\n- As *(--known-sites)* is a required option for **GATK BaseRecalibrator**, it is necessary to provide at least one database file to the **Known sites** input port.\n\n- If you are providing a pre-generated STAR reference index, make sure it is created using the adequate version of STAR (check the STAR version in the original [WDL file](https://github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/blob/master/gatk4-rna-best-practices.wdl )). Also, make sure that the TAR file that *STAR Align* will use in the alignment process is created using the same *Read length* that is provided on workflow input. Please note that **Read length** is an optional parameter and that if not given, default value will be used. \n\n- This workflow allows you to process one sample per task execution. If you are planning to process more than one sample, it is required to run multiple task executions in batch mode. For running analyses in batches, it is necessary to set **Sample ID** metadata for each unmapped BAM file. Read more about [batch analyses](https://docs.sevenbridges.com/docs/about-batch-analyses).\n \n\n### Performance Benchmarking\n\nThe default memory and CPU requirements for each app in the workflow are the same as in the original [GATK Best Practices WDL](https://github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/blob/master/gatk4-rna-best-practices.wdl). You can change the default runtime requirements for **STAR GenomeGenerate** and **STAR Align** apps. \n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  AWS Instance Cost (on-demand) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|\n|     RNA-Seq     |  1.3 GB |     Yes    |     16M     |     101     |   1h56min   |  $1.09 | \n|     RNA-Seq     |  3.9 GB |     Yes    |     50M     |     101     |   3h44min   | $2.07 | \n|     RNA-Seq     | 6.6 GB |     Yes    |     82M    |     101     |  6h  | $3.32 | \n|     RNA-Seq     | 13.1 GB |     Yes    |     164M    |     101     |  11h7min  | $6.13 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**GATK RNAseq short variant discovery 4.2.0.0** was tested with cwltool version 3.1.20210816212154. The `out_haplotypecaller_vcf_extension` , `output_prefix`, `scatter_count` and `standard_min_confidence_threshold_for_calling`\ninputs were provided in the job.yaml/job.json file and used for testing.\n\n### References \n[1] [GATK Best Practice: RNAseq short variant discovery (SNPs + Indels)](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)", "input": [{"name": "Reference FASTA / Index TAR file", "encodingFormat": "application/x-tar"}, {"name": "Unaligned BAM file", "encodingFormat": "application/x-bam"}, {"name": "Read length"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Known sites", "encodingFormat": "application/x-vcf"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Sequence dictionary"}, {"name": "Scatter count"}, {"name": "Output prefix for generated files"}, {"name": "Output VCF file type"}, {"name": "Number of threads"}, {"name": "Memory per job [MB]"}, {"name": "Memory per job [MB]"}, {"name": "Number of threads"}, {"name": "Collapsed junctions max number"}, {"name": "Standard min confidence threshold for calling"}], "output": [{"name": "VariantFiltration: Output filtered variants", "encodingFormat": "application/x-vcf"}, {"name": "MarkDuplicates: Output metrics file"}, {"name": "MergeVcfs: Output merged variants", "encodingFormat": "application/x-vcf"}, {"name": "ApplyBQSR: Output recalibrated alignments", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/gatk-rnaseq-short-variant-discovery-4-2-0-0/5.png", "codeRepository": ["https://github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels"], "applicationSubCategory": ["Variant Calling", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1646129641, "dateCreated": 1646129640, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-somatic-create-mutect2-panel-of-normals-4-1-9-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-somatic-create-mutect2-panel-of-normals-4-1-9-0/4", "applicationCategory": "Workflow", "name": "GATK Somatic Create Mutect2 Panel of Normals 4.1.9.0", "description": "This workflow creates a panel of normals (germline and artifactual sites) for use in other GATK workflows.\n\n**GATK Somatic Create Mutect2 Panel of Normals** (**PON**) workflow takes multiple normal sample callsets produced by **GATK Somatic SNVs and INDELs 4.1.9.0** (**Mutect2** workflow) tumor-only mode (although it is called tumor-only, normal samples are given as the input) and collates sites present in two or more samples into a sites-only VCF. \n\nThe workflow is composed in reference to the official GATK's WDL [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n**PON** workflow captures common artifactual and germline variant sites and outputs them in the form of a VCF file. Main **Mutect2** workflow then uses that file to filter variants at the site-level. \n\nThe main workflow steps are:\n\n- The first step of this workflow is to run the **GATK Somatic SNVs and INDELs 4.1.9.0** in tumor-only mode for each normal sample.\u00a0\n- The next step is creating a GenomicDB from the normal calls.\u00a0\n- GATK **Genomics DB Import**\u00a0tool is used to combine multiple single-sample GVCF/VCF files before joint genotyping, on the specified genomic interval. \n- The output of the **Genomics DB Import** is then used by **Create Somatic Panel Of Normals** tool.\n\n### Common Issues and Important Notes\n\n- **BAM inputs** should be sorted and indexed. This can be done using **Bamtools**,  **Picard** or **Sambamba** tools which are all available in the Public Apps Gallery.\n- **Reference FASTA and index** require FAI and DICT index files. These files can be generated by using **SBG FASTA Indices** tool.\n- **Germline resource (gnomAD)** file and its index -  database of known germline variants, (see [Broad's Institute Site - gnomad downloads](http://gnomad.broadinstitute.org/downloads)) is optional but a recommended input.\n\n### Changes Introduced by Seven Bridges\n\n-  **GATK Somatic SNVs and INDELs 4.1.9.0** workflow inside the **GATK Somatic Create Mutect2 Panel of Normals 4.1.9.0** is parallelized (scatter) by **Normal reads**.\n- **Genomics DB Import** and **Create Somatic Panel Of Normals** tools are wrapped as an inner workflow inside the main workflow so that they can be parallelized together.\n\n### Performance Benchmarking \n\n\nIn the following table, you can find estimates of **GATK Somatic Create a Mutect2 Panel of Normals 4.1.9.0** running time and cost.\n\n| BAM size              \t| Type \t| Duration \t| Cost  \t| Instance (AWS)   \t|\n|-----------------------\t|------\t|----------\t|-------\t|------------\t|\n| 41 x avg. 11 GB       \t| WGS  \t| 4h 31min  \t| $3.07 \t| c5.4xlarge \t|\n| 32.5 + 29.2 + 26.8 GB \t| WGS  \t| 1h 39min  \t| $1.07 \t| c5.4xlarge \t|\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\nGet project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\nGet file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"in_normal_alignments\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"in_reference_and_index\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n    # You can add more inputs here\n}\ntask = api.tasks.create(name='GATK Somatic Create Mutect2 Panel of Normals 4.1.9.0 - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n[1] [GATK4 Create Mutect2 Panel of Normals](https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2_pon.wdl)", "input": [{"name": "Normal reads", "encodingFormat": "application/x-bam"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Interval Files", "encodingFormat": "application/x-vcf"}, {"name": "Germline Resource", "encodingFormat": "application/x-vcf"}, {"name": "Variants for contamination", "encodingFormat": "application/x-vcf"}, {"name": "Min contig size"}, {"name": "Number of contigs"}, {"name": "Compress output"}, {"name": "Scatter count"}, {"name": "Extra Arguments"}], "output": [{"name": "Output merged VCF or BCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/gatk-somatic-create-mutect2-panel-of-normals-4-1-9-0/4.png", "codeRepository": ["https://github.com/broadinstitute/gatk/", "https://github.com/broadinstitute/gatk/releases/download/4.1.6.0/gatk-4.1.6.0.zip"], "applicationSubCategory": ["Somatic Calling"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1614887116, "dateCreated": 1614884925, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-somatic-snvs-and-indels-mutect2-4-1-9-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-somatic-snvs-and-indels-mutect2-4-1-9-0/2", "applicationCategory": "Workflow", "name": "GATK Somatic SNVs and INDELs (Mutect2) 4.1.9.0", "description": "**Mutect2** is a somatic variant caller that uses local assembly and realignment to detect SNVs and INDELs.\n\n**Mutect2** tool together with few other tools is compiled into the **Mutect2** workflow (**GATK Somatic SNVs and INDELs 4.1.9.0**).\n\n**Mutect2 workflow** can detect SNVs and INDELs in one or more tumor samples from a single individual, with or without a matched normal sample. Assembly implies whole haplotypes and read pairs, rather than single bases, as the atomic units of biological variation and sequencing evidence, improving variant calling. Beyond local assembly and alignment, **Mutect2** tool is based on several probabilistic models for genotyping and filtering that work well for all sequencing depths. \n\nThis workflow is composed in reference to the official GATK's WDL [1].\n \n**Mutect2** workflow can also be used for\u00a0mitochondrial variant calling and detection of somatic mosaicism.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- **Mutect2** workflow runs on a single tumor-normal pair or on a single tumor sample and performs additional filtering and functional annotation tasks. It is mostly used to detect variants present in a tumor sample (somatic variants).\n\n### Common Issues and Important Notes\n\n- If you are using an intervals file, make sure that it is compatible with the reference that you are using.\n- **BAM inputs** should be sorted and indexed. This can be done using **Bamtools**, **Picard** or **Sambamba** tools which are all available in the Public Apps Gallery on the Seven Bridges Platform.\n- **Reference FASTA and index** require index files - fai and dict. These files can be generated by using **SBG FASTA Indices** tool.\n- Make sure that **Data Sources** matches **Reference FASTA** file if running **Funcotator**.\n- If **Run Funcotator** parameter is set to **True**, **Data Sources** file must be included as an input.\n\n\nThe following resources are optional but strongly recommended:\n\n- **Panel of normals** file and its index - probable technical artifacts (false positives)\n- **Germline resource (gnomAD)** file and its index - database of known germline variants, see [Broad's Institute Site - gnomad downloads](http://gnomad.broadinstitute.org/downloads)\n- **Variants for contamination** file and its index - common variants with allele frequencies for calculating contamination\n\n### Changes Introduced by Seven Bridges\n\n- This workflow has been tested, benchmarked and adapted for use with AWS instances. This means that most tools have had their GCS inputs removed.\n- **Mutect2** (M2) step from WDL has been broken down to it's components: **GetSampleName** (for both tumor and normal samples), **Mutect2**, **GetPileupSummaries** (for both tumor and normal samples).\n- **MergeBamOuts** step from WDL has been broken down to it's components: **GatherBamFiles**, **SortSam**, **BuildBamIndex**.\n\n### Performance Benchmarking \n\nIn the following table you can find estimates of GATK 4.1.6.0 Somatic SNVs and INDELs running time and cost.\n\n\n| BAM size (Tumor-Normal) | Type | Duration | Cost | Instance (AWS)   |\n|----------------------------------|------|----------|----------|------------|\n| 173.1-156.2 GB                 | WGS  | 1h 24min  | $ 0.95     | c5.4xlarge |\n| 129.5-218.5 MB                   | WGS  | 13min    | $0.15     | c5.4xlarge |\n| 23.5-24.5 GB                     | WGS  | 30min    | $0.34     | c5.4xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\nGet project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\nGet file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"in_tumor_alignments\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"in_reference_and_index\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n    # You can add more inputs here\n}\ntask = api.tasks.create(name='GATK4 Mutect2 Somatic Workflow - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n[1] [GATK4 Somatic SNVs and INDELs](https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl)", "input": [{"name": "Reference Fasta", "encodingFormat": "application/x-fasta"}, {"name": "Normal reads", "encodingFormat": "application/x-bam"}, {"name": "Tumor reads", "encodingFormat": "application/x-bam"}, {"name": "Variants for contamination", "encodingFormat": "application/x-vcf"}, {"name": "Realignment index bundle"}, {"name": "Data sources", "encodingFormat": "application/x-tar"}, {"name": "Panel of normals", "encodingFormat": "application/x-vcf"}, {"name": "Germline resource (gnomAD)", "encodingFormat": "application/x-vcf"}, {"name": "Interval files", "encodingFormat": "application/x-vcf"}, {"name": "Compress output"}, {"name": "Alleles (GGA VCF)", "encodingFormat": "application/x-vcf"}, {"name": "Arguments file"}, {"name": "Mutect2 task memory per job"}, {"name": "Mutect2 CPU per job"}, {"name": "Filter Alignment Artifacts CPU per job"}, {"name": "Filter Alignment Artifacts Memory Per Job"}, {"name": "Learn Read Orientation Memory Per Job"}, {"name": "Learn Read Orientation CPU per job"}, {"name": "Scatter count"}, {"name": "Af of alleles not in resource"}, {"name": "Create output bam index"}, {"name": "Create output variant index"}, {"name": "Genotype germline sites"}, {"name": "Genotype pon sites"}, {"name": "Read validation stringency"}, {"name": "Genotype filtered alleles"}, {"name": "Max mnp distance"}, {"name": "Make BAM output"}, {"name": "Make f1r2 file"}, {"name": "Extra Arguments"}, {"name": "Output"}, {"name": "Run Orientation Bias Mixture Model Filter"}, {"name": "Output"}, {"name": "Output filename"}, {"name": "Output filename"}, {"name": "Output file name"}, {"name": "Output filename"}, {"name": "Output file format"}, {"name": "Reference version"}, {"name": "Remove filtered variants"}, {"name": "Annotation Default: Sequencing center"}, {"name": "Annotation Default: Sequence source"}, {"name": "Annotation Default: Case ID"}, {"name": "Run Funcotator"}], "output": [{"name": "Output filtering stats"}, {"name": "Funcotated variants", "encodingFormat": "application/x-vcf"}, {"name": "Output Filtered VCF", "encodingFormat": "application/x-vcf"}, {"name": "Output contamination table"}, {"name": "Output merged Mutect2 stats"}, {"name": "Output BAI index"}, {"name": "Indexed data", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/gatk-somatic-snvs-and-indels-mutect2-4-1-9-0/2.png", "codeRepository": ["https://github.com/broadinstitute/gatk/", "https://github.com/broadinstitute/gatk/releases/download/4.1.6.0/gatk-4.1.6.0.zip"], "applicationSubCategory": ["Variant Calling", "CWL1.0"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1614887070, "dateCreated": 1614884977, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/aggregate-association-testing/37", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/aggregate-association-testing/37", "applicationCategory": "Workflow", "name": "GENESIS Aggregate Association Testing", "description": "**Aggregate Association Testing workflow** runs aggregate association tests, using Burden, SKAT [1], fastSKAT [2], SMMAT [3], or SKAT-O [4] to aggregate a user-defined set of variants. Association tests are parallelized by segments within chromosomes.\n\nDefine segments splits the genome into segments and assigns each aggregate unit to a segment based on the position of its first variant. Note that number of segments refers to the whole genome, not a number of segments per chromosome. Association testing is then for each segment in parallel, before combining results on chromosome level. Finally, the last step creates QQ and Manhattan plots.\n\nAggregate tests are typically used to jointly test rare variants. The **Alt freq max** parameter allows specification of the maximum alternate allele frequency allowable for inclusion in the test. Included variants are usually weighted using either a function of allele frequency (specified via the **Weight Beta** parameter) or some other annotation information (specified via the **Variant weight file** and **Weight user** parameters). \n\nWhen running a burden test, the effect estimate is for each additional unit of burden; there are no effect size estimates for the other tests. Multiple alternate alleles for a single variant are treated separately.\n\nThis workflow utilizes the *assocTestAggregate* function from the GENESIS software.\n\n\n### Common Use Cases\n * This workflow is designed to perform multi-variant association testing on a user-defined groups of variants.\n\n\n### Common Issues and important notes:\n* The null model input file should be created using GENESIS Null model. Please ensure that you use a Null model file and not one of Null model report only files also available in outputs. \n* The phenotype input file should be created using GENESIS Null model workflow. It is listed in the Null model Phenotypes file output field.  \n\n* This pipeline expects that **GDS Files**, **Variant Include Files**, and **Variant group files** are separated per chromosome, and that files are properly named. It is expected that chromosome is included in the filename in following format: chr## , where ## is the name of the chromosome (1-24 or X, Y). Chromosome can be included at any part of the filename.  Examples: data_subset_chr1.vcf,  data_chr1_subset.vcf, chr1_data_subset.vcf.\n\n* If **Weight Beta** parameter is set, it needs to follow proper convention, two space-delimited floating point numbers.\n\n* **Number of segments** parameter, if provided, needs to be equal or higher than number of chromosomes.\n\n* Testing showed that default parameters for **CPU** and **memory GB** (8GB) are sufficient for testing studies (up to 50k samples), however different null models might increase the requirements.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. \n\n| Samples &nbsp; &nbsp; &nbsp; &nbsp;|  | Rel. matrix in NM &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; | Test &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp;  | Parallel instances &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Instance type &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;| Instance  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp;  | CPU &nbsp; &nbsp; | RAM (GB) &nbsp; &nbsp; &nbsp; &nbsp;| Time    &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;    &nbsp;| Cost  &nbsp;  &nbsp; |\n| ------- | -------- | -------------------- | ------ | --------------------- | ---------------- | ------------- | --- | -------- | ----------- | ---- |\n| 10K     |        | w/o                  | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 8        | 1 h, 11 min | 16$  |\n| 10K     |         | Sparse               | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 8        | 1 h, 10min  | 17$  |\n| 10K     |         | Dense                | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 8        | 1 h, 12 min | 16$  |\n| 36K     |         | w/o                  | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 8        | 1 h, 46 min | 28$  |\n| 36K     |         | Sparse               | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 9        | 1 h, 50 min | 31$  |\n| 36K     |         | Dense                | Burden | 8                     | On Dm            | r5.12xlarge   | 4   | 36       | 2h, 59 min  | 66$  |\n| 50K     |         | w/o                  | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 8        | 2 h, 28 min | 40$  |\n| 50K     |         | Sparse               | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 9        | 2 h, 11 min | 43$  |\n| 50K     |         | Dense                | Burden | 8                     | On Dm            | r5.24xlarge   | 8   | 70       | 4 h, 47 min | 208$ |\n| 50K     |         | Dense                | Burden | 8                     | On Dm            | r5.24xlarge   | 8   | 70       | 4 h, 30 min | 218$ |\n| 50K     |         | Dense                | Burden | 8                     | On Dm            | r5.12xlarge   | 8   | 70       | 9 h         | 218$ |\n| 10K     |         | w/o                  | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 8        | 1 h, 55 min | 16$  |\n| 10K     |         | Sparse               | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 8        | 2 h         | 17$  |\n| 10K     |         | Dense                | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 8        | 2 h, 40 min | 16$  |\n| 36K     |         | w/o                  | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 8        | 2 h, 17 min | 30$  |\n| 36K     |         | Sparse               | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 9        | 2 h, 30 min | 30$  |\n| 36K     |         | Dense                | Burden | 8                     | On Dm            | n1-highmem-32 | 4   | 36       | 6 h         | 91$  |\n| 50K     |         | w/o                  | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 8        | 5 h, 50 min | 43$  |\n| 50K     |         | Sparse               | Burden | 8                     | On Dm            | n1-highmem-32 | 1   | 9        | 5 h, 50 min | 40$  |\n| 50K     |         | Dense                | Burden | 8                     | On Dm            | n1-highmem-96 | 8   | 70       | 6 h         | 270$ |     \n| 10K |  | w/o    | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 15 min | 16$  |\n| 10K |  | Sparse | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 15 min | 17$  |\n| 10K |  | Dense  | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 17 min | 17$  |\n| 36K |  | w/o    | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 47 min | 27$  |\n| 36K |  | Sparse | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 9   | 1 h, 50 min | 31$  |\n| 36K |  | Dense  | SKAT | 8 | On Dm | r5.12xlarge   | 6  | 48  | 5 h, 5 min  | 110$ |\n| 50K |  | w/o    | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 2 h, 27 min | 40$  |\n| 50K |  | Sparse | SKAT | 8 | On Dm | r5.12xlarge   | 1  | 9   | 2 h, 23 min | 44$  |\n| 50K |  | Dense  | SKAT | 8 | On Dm | r5.24xlarge   | 13 | 100 | 11 h, 2 min | 500$ |\n| 50K |  | Dense  | SKAT | 8 | On Dm | r5.24xlarge   | 12 | 90  | 9 h         | 435$ |\n| 50K |  | Dense  | SKAT | 8 | On Dm | r5.12xlarge   | 12 | 90  | 18 h        | 435$ |\n| 10K |  | w/o    | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 1 h, 50 min | 17$  |\n| 10K |  | Sparse | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h         | 16$  |\n| 10K |  | Dense  | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h, 50 min | 17$  |\n| 36K |  | w/o    | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h, 45 min | 30$  |\n| 36K |  | Sparse | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 9   | 2 h, 20 min | 30$  |\n| 36K |  | Dense  | SKAT | 8 | On Dm | n1-highmem-32 | 6  | 48  | 12 h        | 162$ |\n| 50K |  | w/o    | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 5 h         | 45$  |\n| 50K |  | Sparse | SKAT | 8 | On Dm | n1-highmem-32 | 1  | 9   | 5 h         | 45$  |\n| 50K |  | Dense  | SKAT | 8 | On Dm | n1-highmem-96 | 13 | 100 | 14 h        | 620$ |\n| 50K |  | Dense  | SKAT | 8 | On Dm | n1-highmem-96 | 12 | 90  | 14 h        | 620$ |\n| 10K |  | w/o    | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 15 min  | 16$  |\n| 10K |  | Sparse | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 16 min  | 16$  |\n| 10K |  | Dense  | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 18 min  | 17$  |\n| 36K |  | w/o    | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 45 min  | 28$  |\n| 36K |  | Sparse | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 9   | 1 h, 48 min  | 32$  |\n| 36K |  | Dense  | SMMAT | 8 | On Dm | r5.12xlarge   | 6  | 48  | 5h           | 111$ |\n| 50K |  | w/o    | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 2 h, 30 min  | 40$  |\n| 50K |  | Sparse | SMMAT | 8 | On Dm | r5.12xlarge   | 1  | 9   | 2 h, 47 min  | 44$  |\n| 50K |  | Dense  | SMMAT | 8 | On Dm | r5.24xlarge   | 13 | 100 | 11 h, 30 min | 500$ |\n| 50K |  | Dense  | SMMAT | 8 | On Dm | r5.24xlarge   | 12 | 90  | 9 h          | 435$ |\n| 50K |  | Dense  | SMMAT | 8 | On Dm | r5.12xlarge   | 12 | 90  | 18 h         | 435$ |\n| 10K |  | w/o    | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 1 h 30 min   | 15$  |\n| 10K |  | Sparse | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h          | 16$  |\n| 10K |  | Dense  | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h, 50 min  | 17$  |\n| 36K |  | w/o    | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h, 43 min  | 30$  |\n| 36K |  | Sparse | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 9   | 2 h, 25 min  | 30$  |\n| 36K |  | Dense  | SMMAT | 8 | On Dm | n1-highmem-32 | 6  | 48  | 12 h         | 160$ |\n| 50K |  | w/o    | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 5 h          | 42$  |\n| 50K |  | Sparse | SMMAT | 8 | On Dm | n1-highmem-32 | 1  | 9   | 5 h          | 50$  |\n| 50K |  | Dense  | SMMAT | 8 | On Dm | n1-highmem-96 | 13 | 100 | 14 h         | 620$ |\n| 50K |  | Dense  | SMMAT | 8 | On Dm | n1-highmem-96 | 12 | 90  | 14 h         | 620$ |\n| 10K |  | w/o    | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 14 min  | 16$  |\n| 10K |  | Sparse | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 15 min  | 16$  |\n| 10K |  | Dense  | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 17 min  | 17$  |\n| 36K |  | w/o    | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 8   | 1 h, 50 min  | 28$  |\n| 36K |  | Sparse | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 9   | 1 h, 40 min  | 34$  |\n| 36K |  | Dense  | Fast SKAT | 8 | On Dm | r5.12xlarge   | 6  | 50  | 5 h, 30 min  | 135$ |\n| 50K |  | w/o    | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 10  | 1 h, 30 min  | 40$  |\n| 50K |  | Sparse | Fast SKAT | 8 | On Dm | r5.12xlarge   | 1  | 10  | 1 h, 30 min  | 43$  |\n| 50K |  | Dense  | Fast SKAT | 8 | On Dm | r5.24xlarge   | 13 | 100 | 11 h, 41 min | 501$ |\n| 10K |  | w/o    | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 1 h, 30 min  | 16$  |\n| 10K |  | Sparse | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 1 h, 30 min  | 16$  |\n| 10K |  | Dense  | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 2 h, 50 min  | 17$  |\n| 36K |  | w/o    | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 8   | 3 h          | 30$  |\n| 36K |  | Sparse | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 9   | 4 h, 30min   | 32$  |\n| 36K |  | Dense  | Fast SKAT | 8 | On Dm | n1-highmem-32 | 6  | 50  | 11 h         | 160$ |\n| 50K |  | w/o    | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 10  | 3 h          | 45$  |\n| 50K |  | Sparse | Fast SKAT | 8 | On Dm | n1-highmem-32 | 1  | 10  | 3 h          | 45$  |\n| 50K |  | Dense  | Fast SKAT | 8 | On Dm | n1-highmem-96 | 13 | 100 | 14 h         | 650$ |\n\nIn tests performed we used **1000G** (tasks with 2.5k participants) and **TOPMed freeze5** datasets (tasks with 10k or more participants). All these tests are done with applied **MAF < 1% filter.** There are **70 mio** variants with MAF <= 1% in **1000G** and **460 mio** in **TOPMed freeze5 dataset**. Typically, aggregate tests only use a subset of these variants; e.g. grouped by gene. Computational performance will vary depending on how many total variants are tested, and how many variants are included in each aggregation unit.\n\n*For more details on **spot/preemptible instances** please visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances).*     \n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://f4c.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"input_gds_files\": api.files.query(project=project_id, names=[\"basename_chr1.gds\", \"basename_chr2.gds\", ..]),\n    \"variant_group_files\": api.files.query(project=project_id, names=[\"variant_group_chr1.RData\", \"variant_group_chr2.RData\", ..]),\n    \"phenotype_file\": api.files.query(project=project_id, names=[\"name_of_phenotype_file\"])[0],\n    \"null_model_file\": api.files.query(project=project_id, names=[\"name_of_null_model_file\"])[0]\n}\ntask = api.tasks.create(name='Aggregate Association Testing - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n[1] [SKAT](https://dx.doi.org/10.1016%2Fj.ajhg.2011.05.029)  \n[2] [fastSKAT](https://doi.org/10.1002/gepi.22136)  \n[3] [SMMAT](https://doi.org/10.1016/j.ajhg.2018.12.012)  \n[4] [SKAT-O](https://doi.org/10.1093/biostatistics/kxs014)  \n[5] [GENESIS](https://f4c.sbgenomics.com/u/boris_majic/genesis-pipelines-dev/apps/doi.org/10.1093/bioinformatics/btz567)", "input": [{"name": "Segment length"}, {"name": "Number of segments"}, {"name": "Genome build"}, {"name": "Variant group files"}, {"name": "Group ID"}, {"name": "Aggregate type"}, {"name": "Weight user"}, {"name": "Weight Beta"}, {"name": "Variant Weight file"}, {"name": "Test"}, {"name": "Rho"}, {"name": "Phenotype file"}, {"name": "Pass only"}, {"name": "Null model file"}, {"name": "Memory GB"}, {"name": "CPU"}, {"name": "Alt Freq Max"}, {"name": "Thin N points"}, {"name": "Thin N bins"}, {"name": "Known hits file"}, {"name": "Disable Thin"}, {"name": "GDS files"}, {"name": "Output prefix"}, {"name": "Truncate pval threshold"}, {"name": "Plot MAC threshold"}, {"name": "Variant Include Files"}], "output": [{"name": "Association test results"}, {"name": "Association test plots"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/UW-GAC/analysis_pipeline"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "TOPMed DCC", "softwareVersion": ["v1.2"], "dateModified": 1649763750, "dateCreated": 1602755594, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/null-model/28", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/null-model/28", "applicationCategory": "Workflow", "name": "GENESIS Null Model", "description": "**Null Model** workflow fits the regression or mixed effects model under the null hypothesis of no genotype effects. i.e., The outcome variable is regressed on the specified fixed effect covariates and random effects. The output of this null model is then used in the association tests.\n\nQuantitative and binary outcomes are both supported. Set parameter **family** to gaussian, binomial or poisson depending on the outcome type. Fixed effect covariates from the **Phenotype file** are specified using the **Covariates** parameter, and ancestry principal components can be included as fixed effects using the **PCA Files** and **Number of PCs to include as covariates** parameters. A kinship matrix (KM) or genetic relationship matrix (GRM) can be provided using the **Relatedness matrix file** parameter to account for genetic similarity among samples as a random effect. \n\nWhen no **Relatedness matrix file** is provided, standard linear regression is used if the parameter **family** is set to gaussian, logistic regression is used if the parameter **family** is set to binomial and poisson regression is used in case when **family** is set to poisson. When **Relatedness matrix file** is provided, a linear mixed model (LMM) is fit if **family** is set to gaussian, or a generalized linear mixed model (GLMM) is fit using the [GMMAT method](https://doi.org/10.1016/j.ajhg.2016.02.012) if **family** is set to binomial or poisson. For either the LMM or GLMM, the [AI-REML algorithm](https://doi.org/10.1111/jbg.12398) is used to estimate the variance components and fixed effects parameters.\n \nWhen samples come from multiple groups (e.g., study or ancestry group), it is common to observe different variances by group for quantitative traits. It is recommended to allow the null model to fit heterogeneous residual variances by group using the parameter group_var. This often provides better control of false positives in subsequent association testing. Note that this only applies when **family** is set to gaussian.\n\nRank-based inverse Normal transformation is supported for quantitative outcomes via the inverse_normal parameter. This parameter is TRUE by default. When **inverse normal** parameter is set to TRUE, (1) the null model is fit using the original outcome values, (2) the marginal residuals are rank-based inverse Normal transformed, and (3) the null model is fit again using the transformed residuals as the outcome; fixed effects and random effects are included both times the null model is fit. It has been shown that this fully adjusted two-stage procedure provides better false positive control and power in association tests than simply inverse Normalizing the outcome variable prior to analysis [(**reference**)](https://doi.org/10.1002/gepi.22188).\n\nThis workflow utilizes the *fitNullModel* function from the [GENESIS](doi.org/10.1093/bioinformatics/btz567) software.\n\nWorkflow consists of two steps. First step fits the null model, and the second one generates reports based on data. Reports are available both in RMD and HTML format. If **inverse normal** is TRUE, reports are generated for the model both before and after the transformation.\nReports contain the following information: Config info, phenotype distributions, covariate effect size estimates, marginal residuals, adjusted phenotype values and session information.\n\n### Common use cases:\nThis workflow is the first step in association testing. This workflow fits the null model and produces several files which are used in the association testing workflows:\n* Null model file which contains adjusted outcome values, the model matrix, the estimated covariance structure, and other parameters required for downstream association testing. This file will be input in association testing workflows.\n* Phenotype file which is a subset of the provided phenotype file, containing only the outcome and covariates used in fitting null model.\n* *Reportonly* null model file which is used to generate the report for the association test\n\n\nThis workflow can be used for trait heritability estimation.\n\nIndividual genetic variants or groups of genetic variants can be directly tested for association with this workflow by including them as fixed effect covariates in the model (via the **Conditional Variant File** parameter). This would be extremely inefficient genome-wide, but is useful for follow-up analyses testing variants of interest.\n\n\n### Common issues and important notes:\n* The null model report only output files are used to generate diagnostic reports to examine the null model fit. They are smaller versions of the full null model file and do not contain all elements necessary for association testing.\n\n* If **PCA File** is not provided, the **Number of PCs to include as covariates** parameter **must** be set to 0.\n\n* **PCA File** must be an RData object output from the *pcair* function in the GENESIS package.\n\n* The null model job can be very computationally demanding in large samples (e.g. > 20K). GENESIS supports using sparse representations of matrices in the **Relatedness matrix file** via the R Matrix package, and this can substantially reduce memory usage and CPU time.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost on spot instances. \n      \n| Samples &nbsp; &nbsp;| Relatedness matrix &nbsp; &nbsp; | Instance type &nbsp; &nbsp;| Instance &nbsp;  &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;  &nbsp; &nbsp;&nbsp; &nbsp;| Time   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Cost  |\n| ------- | --------------------- | ---------------- | -------------- | ----------- | ----- |\n| 2.5K    | w/o                   | Spot             | c4xlarge       | 5 min       | 0.01$ |\n| 10K     | w/o                   | Spot             | r4.8xlarge     | 6 min       | 0.06$ |\n| 36K     | w/o                   | Spot             | r4.8xlarge     | 7 min       | 0.06$ |\n| 50K     | w/o                   | Spot             | r4.8xlarge     | 7 min       | 0.07$ |\n| 2.5K    | Sparse                | Spot             | c4xlarge       | 5 min       | 0.01$ |\n| 10K     | Sparse                | Spot             | r4.8xlarge     | 8 min       | 0.06$ |\n| 36K     | Sparse                | Spot             | r4.8xlarge     | 40 min      | 0.40$ |\n| 50K     | Sparse                | Spot             | r4.8xlarge     | 50 min      | 0.50$ |\n| 2.5K    | Dense                 | Spot             | c4.xlarge      | 5 min       | 0.01$ |\n| 10K     | Dense                 | Spot             | c4.2xlarge     | 13 min      | 0.06$ |\n| 36K     | Dense                 | Spot             | r4.4xlarge     | 55 min      | 0.45$ |\n| 50K     | Dense                 | Spot             | r4.8xlarge     | 2 h         | 1.50$ |\n| 2.5K    | w/o                   | Preemtible       | n1-standard-4  | 5 min       | 0.01$ |\n| 10K     | w/o                   | Preemtible       | n1-standard-32 | 6 min       | 0.05$ |\n| 36K     | w/o                   | Preemtible       | n1-hihgmem-32  | 8 min       | 0.06$ |\n| 50K     | w/o                   | Preemtible       | n1-hihgmem-32  | 7 min       | 0.06$ |\n| 2.5K    | Sparse                | Preemtible       | n1-standard-4  | 5 min       | 0.01$ |\n| 10K     | Sparse                | Preemtible       | n1-hihgmem-32  | 7 min       | 0.06$ |\n| 36K     | Sparse                | Preemtible       | n1-hihgmem-32  | 40 min      | 0.30$ |\n| 50K     | Sparse                | Preemtible       | n1-hihgmem-32  | 50 min      | 0.45$ |\n| 2.5K    | Dense                 | Preemtible       | n1-highcpu-8   | 10 min      | 0.01$ |\n| 10K     | Dense                 | Preemtible       | n1-highcpu-16  | 12 min      | 0.04$ |\n| 36K     | Dense                 | Preemtible       | n1-standard-32 | 43 min      | 1$    |\n| 50K     | Dense                 | Preemtible       | n1-standard-96 | 2 h, 30 min | 12$   |\n\n\n*Cost shown here were obtained with **spot/preemptible instances** enabled. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Sample include file"}, {"name": "Relatedness matrix file"}, {"name": "Phenotype file"}, {"name": "PCA File"}, {"name": "Output prefix"}, {"name": "GDS Files"}, {"name": "Two stage model"}, {"name": "Conditional variant file"}, {"name": "Rescale residuals"}, {"name": "Outcome"}, {"name": "Norm by group"}, {"name": "Number of PCs to include as covariates"}, {"name": "Group variate"}, {"name": "CPU"}, {"name": "Covariates"}, {"name": "Family"}, {"name": "Number of categories in boxplot"}], "output": [{"name": "Null model Phenotypes file"}, {"name": "Rmd files"}, {"name": "HTML Reports", "encodingFormat": "text/html"}, {"name": "Null model file"}, {"name": "Null model report only files"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/UW-GAC/analysis_pipeline"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "TOPMed DCC", "softwareVersion": ["v1.2"], "dateModified": 1649763750, "dateCreated": 1602755594, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/single-variant-association-testing/39", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/single-variant-association-testing/39", "applicationCategory": "Workflow", "name": "GENESIS Single Variant Association Testing", "description": "**Single Variant workflow** runs single-variant association tests. It consists of several steps. Define Segments divides genome into segments, either by a number of segments, or by a segment length. Note that number of segments refers to whole genome, not a number of segments per chromosome. Association test is then performed for each segment in parallel, before combining results on chromosome level. Final step produces QQ and Manhattan plots.\n\nThis workflow uses the output from a model fit using the null model workflow to perform tests for all variants individually. The reported effect estimate is for the alternate allele, and multiple alternate alleles for a single variant are tested separately. \n\nWhen testing a binary outcome, the saddlepoint approximation (SPA) for p-values [1][2] can be used by specifying **Test type** = \u2018score.spa\u2019; this is generally recommended. SPA will provide better calibrated p-values, particularly for rarer variants in samples with case-control imbalance. \n\nWhen testing a binary outcome, the BinomiRare test is also available[3]. This is a \u201ccarriers only\u201d exact test that compares the observed number of variant carriers who are cases to the expected number based on the probability of being a case under the null hypothesis of no association between outcome and variant. This test may be useful when testing association of very rare variants with rare outcomes.\n\nWhen using the test_type \u2018score\u2018 or \u2018score.spa\u2018, the fast approximation to the score standard error (SE) implemented by Zhou et al. (2018) in their SAIGE software [5] is available by using a null_model_file prepared with the Update Null Model for Fast Score Test workflow. This approximation may be much faster than computing the true score SE in large samples, as it replaces the full covariance matrix in the calculation with the product of a diagonal matrix and a scalar correction factor (se.correction) in the updated null model output.\n\nIf your genotype data has sporadic missing values, they are mean imputed using the allele frequency observed in the sample.\n\nOn the X chromosome, males have genotype values coded as 0/2 (females as 0/1/2).\n\nThis workflow utilizes the *assocTestSingle* function from the GENESIS software [4].\n\n### Common Use Cases\n\nSingle Variant Association Testing workflow is designed to run single-variant association tests using GENESIS software. Set of variants on which to run association testing can be reduced by providing **Variant Include Files** - One file per chromosome containing variant IDs for variants on which association testing will be performed.\n\n\n### Common issues and important notes\n* The null model input file should be created using GENESIS Null model/GENESIS Update Null Model for Fast Score Test. Please ensure that you use a Null model file and not one of Null model report only files also available in outputs. \n* The phenotype input file should be created using GENESIS Null model workflow. It is listed in the Null model Phenotypes file output field.  \n\n* Association Testing - Single job can be very memory demanding, depending on number of samples and null model used. We suggest running with at least 5GB of memory allocated for small studies, and to use approximation of 0.5GB per thousand samples for larger studies (with more than 10k samples), but this again depends on complexity of null model. If a run fails with *error 137*, and with message killed, most likely cause is lack of memory. Memory can be allocated using the **memory GB** parameter.\n\n* This workflow expects **GDS** files split by chromosome, and will not work otherwise. If provided, **variant include** files must also be split in the same way. Also GDS and Variant include files should be properly named. It is expected that chromosome is included in the filename in following format: chr## , where ## is the name of the chromosome (1-24 or X, Y). Chromosome can be included at any part of the filename. Examples for GDS: data_subset_chr1.gds, data_chr1_subset.gds. Examples for Variant include files: variant_include_chr1.RData, chr1_variant_include.RData.\n\n* Some input arguments are mutually exclusive, for more information, please visit workflow [github page](https://github.com/UW-GAC/analysis_pipeline/tree/v2.5.0)\n\n### Changes introduced by Seven Bridges\nThere are no changes introduced by Seven Bridges.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. \n        \n\n| Samples &nbsp; &nbsp; |    | Rel. Matrix &nbsp; &nbsp;|Parallel instances &nbsp; &nbsp; | Instance type  &nbsp; &nbsp; &nbsp; &nbsp;| Spot/On Dem. &nbsp; &nbsp; |CPU &nbsp; &nbsp; | RAM &nbsp; &nbsp; | Time  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Cost |\n|--------------------|---------------|----------------|------------------------|--------------------|--------------------|--------|--------|---------|-------|\n| 2.5k   |                 |   w/o          | 8                           |  r4.8xlarge | Spot     |1  | 2   | 1 h, 8 min   | $5  |\n| 2.5k   |               |   Dense     | 8                           |  r4.8xlarge | Spot     |1  | 2   | 1 h, 8 min   | $5  |\n| 10k   |                 |   w/o           | 8                           |  c5.18xlarge | On Demand     |1  | 2   | 50 min   | $10  |\n| 10k   |                |   Sparse     | 8                           |  c5.18xlarge | On Demand     |1  | 2   | 58 min   | $11  |\n| 10k   |                |   Sparse     | 8                           |  r4.8xlarge | On Demand     |1  | 2   | 1 h, 30 min   | $11  |\n| 10k   |                 |   Dense      | 8                           |  r5.4xlarge | On Demand     |1  | 8   | 3 h   | $24  |\n| 36k  |                  |   w/o           | 8                           |  r5.4xlarge | On Demand     |1  | 5   | 3 h, 20 min   | $27  |\n| 36k  |                  |   Sparse     | 8                           |  r5.4xlarge | On Demand     |1  | 5   | 4 h   | $32  |\n| 36k   |                  |   Sparse     | 8                           |  r5.12xlarge | On Demand     |1  | 5   | 1 h, 20 min   | $32  |\n| 36k   |                  |   Dense      | 8                           |  r5.12xlarge | On Demand     |1  | 50   | 1 d, 15 h   | $930  |\n| 36k   |                 |   Dense      | 8                           |  r5.24xlarge | On Demand     |1  | 50   | 17 h   | $800  |\n| 50k   |                  |   w/o           | 8                           |  r5.12xlarge | On Demand     |1  | 8   | 2 h   | $44  |\n| 50k   |                  |   Sparse     | 8                           |  r5.12xlarge | On Demand     |1  | 8   | 2 h   | $48 |\n| 50k   |                  |   Dense      | 8                           |  r5.24xlarge | On Demand     |48  | 100   | 11 d   | $13500  |\n| 2.5k   |                  |   w/o          | 8                           |  n1-standard-64 | Preemptible    |1  | 2   | 1 h   | $7  |\n| 2.5k   |                  |   Dense     | 8                           |  n1-standard-64 | Preemptible    |1  | 2   | 1 h   | $7  |\n| 10k   |                  |   w/o           | 8                           |  n1-standard-4 | On Demand     |1  | 2   | 1 h, 12 min  | $13  |\n| 10k   |                  |   Sparse     | 8                           |  n1-standard-4 | On Demand     |1  | 2   | 1 h, 13  min   | $14 |\n| 10k  |                  |   Dense      | 8                           |  n1-highmem-32 | On Demand     |1  | 8   | 2 h, 20  min   | $30  |\n| 36k   |                  |   w/o           | 8                           |  n1-standard-64 | On Demand     |1  | 5   | 1 h, 30  min   | $35  |\n| 36k   |                 |   Sparse     | 8                           |  n1-highmem-16 | On Demand     |1  | 5   | 4 h, 30  min   | $35  |\n| 36k   |                  |   Sparse     | 8                           |  n1-standard-64 | On Demand     |1  | 5   | 1 h, 30  min   | $35  |\n| 36k   |                  |   Dense      | 8                           |  n1-highmem-96 | On Demand     |1  | 50   | 1 d, 6  h   | $1300  |\n| 50k   |                  |   w/o           | 8                           |  n1-standard-96 | On Demand     |1  | 8    | 2  h   | $73  |\n| 50k   |                  |   Sparse     | 8                           |  n1-standard-96 | On Demand     |1  | 8    | 2  h   | $73  |\n| 50k   |                  |   Dense      | 8                           |  n1-highmem-96 | On Demand     |16  | 100    | 6  d   | $6600  |\n\nIn tests performed we used 1000G (tasks with 2.5k participants) and TOPMed freeze5 datasets (tasks with 10k or more participants). \nAll these tests are done with applied **MAF >= 1%** filter. The number of variants that have been tested is **14 mio in 1000G** and **12 mio in TOPMed freeze 5** dataset. \n\nAlso, a common filter in these analysis is **MAC>=5**. In that case the number of variants would be **32 mio for 1000G** and **92 mio for TOPMed freeze5** data. Since for single variant testing, the compute time grows linearly with the number of variants tested the execution time and price can be easily estimated from the results above.\n\n*For more details on **spot/preemptible instances** please visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances).*   \n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://f4c.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"input_gds_files\": api.files.query(project=project_id, names=[\"basename_chr1.gds\", \"basename_chr2.gds\", ..]),\n    \"phenotype_file\": api.files.query(project=project_id, names=[\"name_of_phenotype_file\"])[0],\n    \"null_model_file\": api.files.query(project=project_id, names=[\"name_of_null_model_file\"])[0]\n}\ntask = api.tasks.create(name='Single Variant Association Testing - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n - [1] [SaddlePoint Approximation (SPA)](https://doi.org/10.1016/j.ajhg.2017.05.014)  \n - [2] [SPA - additional reference](https://doi.org/10.1038/s41588-018-0184-y)  \n - [3] [BinomiRare](https://pubmed.ncbi.nlm.nih.gov/28393384/)  \n - [4] [GENESIS toolkit](doi.org/10.1093/bioinformatics/btz567/) \n - [5] [Efficiently controlling for case-control imbalance and sample relatedness in large-scale genetic association studies](https://www.nature.com/articles/s41588-018-0184-y)", "input": [{"name": "Segment length"}, {"name": "Number of segments"}, {"name": "Genome build"}, {"name": "Variant block size"}, {"name": "Test type"}, {"name": "Phenotype file"}, {"name": "Pass only"}, {"name": "Null model file"}, {"name": "memory GB"}, {"name": "MAF threshold"}, {"name": "MAC threshold"}, {"name": "CPU"}, {"name": "Disable Thin"}, {"name": "Known hits file"}, {"name": "Thin N bins"}, {"name": "Thin N points"}, {"name": "Output prefix"}, {"name": "GDS files"}, {"name": "Truncate pval threshold"}, {"name": "Plot MAC threshold"}, {"name": "Variant Include Files"}, {"name": "Genetic model"}], "output": [{"name": "Association test results"}, {"name": "Association test plots"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/UW-GAC/analysis_pipeline"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "TOPMed DCC", "softwareVersion": ["v1.2"], "dateModified": 1649763750, "dateCreated": 1602755592, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sliding-window-association-testing/34", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sliding-window-association-testing/34", "applicationCategory": "Workflow", "name": "GENESIS Sliding Window Association Testing", "description": "**Sliding Window Association Testing workflow** runs sliding-window association tests. It can use Burden, SKAT [1], fastSKAT [2], SMMAT [3], or SKAT-O [4] to aggregate all variants in a window.\n\nSliding window Workflow consists of several steps. Define Segments divides genome into segments, either by a number of segments, or by a segment length. Note that number of segments refers to whole genome, not a number of segments per chromosome Association test is then performed for each segment in parallel, before combining results on chromosome level. Final step produces QQ and Manhattan plots.\n\nAggregate tests are typically used to jointly test rare variants. The **Alt freq max** parameter allows specification of the maximum alternate allele frequency allowable for inclusion in the test. Included variants are usually weighted using either a function of allele frequency (specified via the **Weight Beta** parameter) or some other annotation information (specified via the **Variant weight file** and **Weight user** parameters). \n\nWhen running a burden test, the effect estimate is for each additional unit of burden; there are no effect size estimates for the other tests. Multiple alternate alleles for a single variant are treated separately.\n\nThis workflow utilizes the *assocTestAggregate* function from the GENESIS software [5].\n\n\n### Common Use Cases\n\nSliding Window Association Testing workflow is designed to run multi-variant, sliding window, association tests using GENESIS software. Set of variants on which to run association testing can be reduced by providing **Variant Include Files** - One file per chromosome containing variant IDs for variants on which association testing will be performed.\n\n### Changes Introduced by Seven Bridges:\n\nThere are no changes introduced by Seven Bridges.\n\n### Common Issues and Important Notes:\n* The null model input file should be created using GENESIS Null model. Please ensure that you use a Null model file and not one of Null model report only files also available in outputs. \n* The phenotype input file should be created using GENESIS Null model workflow. It is listed in the Null model Phenotypes file output field. \n\n\n* This workflow expects **GDS** files split by chromosome, and will not work otherwise. If provided, **variant include** files must also be split in the same way. Also GDS and Variant include files should be properly named. It is expected that chromosome is included in the filename in following format: chr## , where ## is the name of the chromosome (1-24 or X, Y). Chromosome can be included at any part of the filename. Examples for GDS: data_subset_chr1.gds, data_chr1_subset.gds. Examples for Variant include files: variant_include_chr1.RData, chr1_variant_include.RData.\n\n* **Note:** Memory requirements varies drastically based on input size, and on complexity of null model (for example inclusion of **Relatedness Matrix file** when fitting a null model, in *Null Model* workflow). For example, total input size of **GDS Files** of 100GB, requires 80GB of RAM if GRM is used, and 5 GB of RAM if GRM is not used. For this reason, **memory GB** parameter is exposed for users to adjust. If you need help with adjustments, fell free to contact our support. Running out of memory often, but not always, manifests as *error code 137*.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **hg38 human reference** with default options. \n\n| Samples &nbsp; &nbsp; |  | Rel. matrix &nbsp; &nbsp;| Test   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Parallel instances &nbsp; &nbsp;| Instance type &nbsp; &nbsp; &nbsp;  | Instance   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   | CPU &nbsp; &nbsp; | RAM (GB) &nbsp; &nbsp; | Time  &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;  &nbsp; &nbsp;&nbsp; &nbsp;   | Cost |\n| ------- | -------- | -------------------- | ------ | --------------------- | ---------------- | ------------- | --- | -------- | ----------- | ---- |\n| 10K     |          | w/o                  | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 5       | 4 h, 30 min | $50  |\n| 10K     |          | Sparse               | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 5        | 4 h, 30min  | $50  |\n| 10K     |          | Dense                | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 5        | 4 h, 30 min | $50  |\n| 36K     |          | w/o                  | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 5        | 9 h, 15 min | $115  |\n| 36K     |          | Sparse               | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 5        | 9 h, 15 min | $115  |\n| 36K     |          | Dense                | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 50       | 1 d | $500  |\n| 50K     |          | w/o                  | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 20        | 2 d, 5 h | $650  |\n| 50K     |          | Sparse               | Burden | 8                     | On Dm            | c5.9xlarge   | 1   | 20        | 2 d, 5 h | $650  |\n| 50K     |          | Dense                | Burden | 8                     | On Dm            | r5.12xlarge   | 1   | 100       | 2 d | $1400 |\n| 10K     |          | w/o                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5       | 20 h | $250  |\n| 10K     |          | Sparse                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5       | 20 h | $250  |\n| 10K     |          | Dense                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5       | 20 h | $250  |\n| 36K     |          | w/o                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5        | 5 d | $1400  |\n| 36K     |          | Sparse                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5        | 5 d | $1400  |\n| 36K     |          | Dense                | SKAT | 8                     | On Dm            | r5.12xlarge   | 1   | 50       | 11 d | $6300  |\n| 50K     |          | w/o                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 36   | 72        | 12 d | $3300  |\n| 50K     |          | Sparse                  | SKAT | 8                     | On Dm            | c5.9xlarge   | 36   | 72        | 12 d | $3300  |\n| 50K     |          | Dense                | SKAT | 8                     | On Dm            | r5.12xlarge   | 6   | 100       | 8 d | $4700 |\n| 10K     |          | w/o                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5       | 20 h | $250  |\n| 10K     |          | Sparse                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5       | 20 h | $250  |\n| 10K     |          | Dense                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5       | 20 h | $250  |\n| 36K     |          | w/o                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5        | 5 d | $1100  |\n| 36K     |          | Sparse                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 12   | 5        | 5 d | $1100  |\n| 36K     |          | Dense                | SMMAT | 8                     | On Dm            | r5.12xlarge   | 1   | 50       | 11 d | $6300  |\n| 50K     |          | w/o                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 36   | 72        | 13 d | $4000  |\n| 50K     |          | Sparse                  | SMMAT | 8                     | On Dm            | c5.9xlarge   | 36   | 72        | 13 d | $4000  |\n| 50K     |          | Dense                | SMMAT | 8                     | On Dm            | r5.12xlarge   | 6   | 100       | 7 d | $4000 |\n| 10K     |          | w/o                  | Fast-SKAT | 8                     | On Dm            | c5.9xlarge   | 5   | 5       | 14 h  | $110  |\n| 10K     |          | Sparse                  | Fast-SKAT | 8                     | On Dm            | c5.9xlarge   | 5   | 5       | 14 h  | $110  |\n| 10K     |          | Dense                | Fast-SKAT | 8                     | On Dm            | c5.9xlarge   | 12   | 10        | 20 h   | $240  |\n| 36K     |          | w/o                  | Fast-SKAT | 8                     | On Dm            | c5.9xlarge   | 18   | 36        | 3 d, 12 h | $1050  |\n| 36K     |          | Sparse                  | Fast-SKAT | 8                     | On Dm            | c5.9xlarge   | 18   | 36        | 3 d, 12 h | $1050  |\n| 36K     |          | Dense                | Fast-SKAT | 8                     | On Dm            | r5.12xlarge   | 1   | 50       | 2 d | $1000  |\n| 50K     |          | w/o                  | Fast-SKAT | 8                     | On Dm            | r5.12xlarge   | 18   | 36        | 4 d | $2600  |\n| 50K     |          | Sparse                  | Fast-SKAT | 8                     | On Dm            | r5.12xlarge   | 18   | 36        | 4 d, 12 h | $2600  |\n| 50K     |          | Dense                | Fast-SKAT | 8                     | On Dm            | r5.12xlarge   | 1   | 186       | 12 d | $7000 |\n| 10K     |          | w/o                  | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 5       | 3 h | $40  |\n| 10K     |          | Sparse               | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 5        | 3 h  | $40  |\n| 10K     |          | Dense                | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 5        | 3 h | $40  |\n| 36K     |          | w/o                  | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 5        | 7 h | $100  |\n| 36K     |          | Sparse               | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 5        | 8 h | $110  |\n| 36K     |          | Dense                | Burden | 8                     | On Dm            | n1-standard-96   | 1   | 50       | 1 d, 6 h | $100  |\n| 50K     |          | w/o                  | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 20        | 2 d | $500  |\n| 50K     |          | Sparse                  | Burden | 8                     | On Dm            | n1-standard-32   | 1   | 20        | 2 d | $500  |\n| 50K     |          | Dense                | Burden | 8                     | On Dm            | n1-standard-96   | 1   | 100       | 3 d | $2600 |\n| 10K     |          | w/o                  | SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 1 d, 12 h | $400  |\n| 10K     |          | Sparse                  | SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 1 d, 12 h | $400  |\n| 10K     |          | Dense                  | SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 1 d, 12 h | $400  |\n| 36K     |          | w/o                  | SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5        | 5 d | $1400  |\n| 36K     |          | Sparse                  | SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5        | 5 d | $1400  |\n| 36K     |          | Dense                | SKAT | 8                     | On Dm            | n1-standard-96   | 1   | 50       | 6 d | $5100  |\n| 50K     |          | w/o                  | SKAT | 8                     | On Dm            | n1-standard-64   | 36   | 72        | 14 d | $8000  |\n| 50K     |          | Sparse                  | SKAT | 8                     | On Dm            | n1-standard-32   | 18   | 36        | 15 d | $4500  |\n| 50K     |          | Dense                | SKAT | 8                     | On Dm            | n1-standard-96   | 6   | 100       | 9 d | $8300 |\n| 10K     |          | w/o                  | SMMAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 2 d | $560  |\n| 10K     |          | Sparse                  | SMMAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 2 d | $560  |\n| 10K     |          | Dense                  | SMMAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 2 d | $560  |\n| 36K     |          | w/o                  | SMMAT | 8                     | On Dm            | n1-standard-32   | 12   | 5        | 5 d | $1400  |\n| 36K     |          | Sparse                  | SMMAT | 8                     | On Dm            | n1-standard-32   | 12   | 5        | 5 d | $1400  |\n| 36K     |          | Dense                | SMMAT | 8                     | On Dm            | n1-standard-96   | 1   | 50       | 6 d | $5100  |\n| 50K     |          | w/o                  | SMMAT | 8                     | On Dm            | n1-standard-64   | 36   | 72        | 14 d | $8000  |\n| 50K     |          | Sparse                  | SMMAT | 8                     | On Dm            | n1-standard-32   | 18   | 36        | 17 d | $5000  |\n| 50K     |          | Dense                | SMMAT | 8                     | On Dm            | n1-standard-96   | 6   | 100       | 13 d | $11500 |\n| 10K     |          | w/o                  | Fast-SKAT | 8                     | On Dm            | n1-standard-32   | 5   | 5       | 16 h | $200  |\n| 10K     |          | Sparse                  | Fast-SKAT | 8                     | On Dm            | n1-standard-32   | 5   | 5       | 16 h | $200  |\n| 10K     |          | Dense                  | Fast-SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5       | 2 d | $560  |\n| 36K     |          | w/o                  | Fast-SKAT | 8                     | On Dm            | n1-standard-32   | 18   | 36        | 8 d | $2400  |\n| 36K     |          | Sparse                  | Fast-SKAT | 8                     | On Dm            | n1-standard-32   | 12   | 5        | 8 d | $2400  |\n| 36K     |          | Dense                | Fast-SKAT | 8                     | On Dm            | n1-standard-96   | 1   | 50       | 2 d, 12 h | $2100  |\n| 50K     |          | w/o                  | Fast-SKAT | 8                     | On Dm            | n1-standard-96   | 1   | 100        | 5 d | $4700  |\n| 50K     |          | Sparse                  | Fast-SKAT | 8                     | On Dm            | n1-standard-96   | 1   | 100        | 5 d | $4700  |\n| 50K     |          | Dense                | Fast-SKAT | 8                     | On Dm            | n1-standard-96   | 1   | 100       | 30 d | $26300 |\n\nIn tests performed we used **1000G** (tasks with 2.5k participants) and **TOPMed freeze5** datasets (tasks with 10k or more participants). All these tests are done with applied **MAF < 1% filter, window size 50kb** and **window step 20kb**. The number of variants that have been tested in **1000G** dataset is **175 mio (140k windows, 1k variants/window)**. The number of variants tested in **TOPMed freeze 5** is **1bio variants (140k windows, 7k variants/window)**.\n\nPlease note that we run all tests with default values for window size and window step (meaning 50kb and 20kb respectively). In this set up windows are overlapping and this is why we have so high number of variants tested. \n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://f4c.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"input_gds_files\": api.files.query(project=project_id, names=[\"basename_chr1.gds\", \"basename_chr2.gds\", ..]),\n    \"phenotype_file\": api.files.query(project=project_id, names=[\"name_of_phenotype_file\"])[0],\n    \"null_model_file\": api.files.query(project=project_id, names=[\"name_of_null_model_file\"])[0]\n}\ntask = api.tasks.create(name='Sliding Window Association Testing - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n - [1] [SKAT](https://dx.doi.org/10.1016%2Fj.ajhg.2011.05.029)  \n - [2] [fastSKAT](https://doi.org/10.1002/gepi.22136)  \n - [3] [SMMAT](https://doi.org/10.1016/j.ajhg.2018.12.012)  \n - [4] [SKAT-O](https://doi.org/10.1093/biostatistics/kxs014)  \n - [5] [GENESIS](https://f4c.sbgenomics.com/u/boris_majic/genesis-pipelines-dev/apps/doi.org/10.1093/bioinformatics/btz567)", "input": [{"name": "Segment length"}, {"name": "Number of segments"}, {"name": "Genome build"}, {"name": "Window step"}, {"name": "Window size"}, {"name": "Weight user"}, {"name": "Weight Beta"}, {"name": "Variant weight file"}, {"name": "Test"}, {"name": "Rho"}, {"name": "Phenotype file"}, {"name": "Pass only"}, {"name": "Null model file"}, {"name": "Memory GB"}, {"name": "CPU"}, {"name": "Alt freq max"}, {"name": "Thin N points"}, {"name": "Thin N bins"}, {"name": "Known hits file"}, {"name": "Disable Thin"}, {"name": "Output prefix"}, {"name": "GDS files"}, {"name": "Truncate pval threshold"}, {"name": "Plot MAC threshold"}, {"name": "Variant Include Files"}], "output": [{"name": "Association test results"}, {"name": "Association test plots"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/UW-GAC/analysis_pipeline"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "TOPMed DCC", "softwareVersion": ["v1.2"], "dateModified": 1649763750, "dateCreated": 1602755595, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/genesis-null-model-update-for-fast-score-test/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/genesis-null-model-update-for-fast-score-test/10", "applicationCategory": "Workflow", "name": "GENESIS Update Null Model for Fast Score Test", "description": "**GENESIS Update Null Model for Fast Score Test** updates **GENESIS Null model** outputs for fast score testing.\n\nThe **GENESIS Update Null Model for Fast Score Test** workflow takes a null model previously fit with the **GENESIS Null Model** workflow and updates it so that it can be used by the **GENESIS Single Variant Association Testing** workflow with the fast approximation to the score standard error (SE) implemented by Zhou et al. (2018) in their SAIGE software [1]. This approximation may be much faster than computing the true score SE in large samples, as it replaces the full covariance matrix in the calculation with the product of a diagonal matrix and a scalar correction factor (se.correction) in the updated null model output.\n\nThis approach can be used for a null model with any supported family, but it only applies to null models that are mixed models (i.e. a relatedness matrix was included when fitting the original null model). \n\nThis workflow utilizes the calcScore and nullModelFastScore functions from the GENESIS software.\n\nThe workflow consists of two steps. The first step uses the calcScore function to compute the score, true score SE, and the fast score SE on a set of variants; this is parallelized across chromosomes. The second step uses the nullModelFastScore function to combine the results across chromosomes, compute the scalar se.correction factor (the average across variants of the true SE to the fast SE), and updates the null model format. The output is an updated null model.  \n\nThis workflow should be used after the **GENESIS Null Model** workflow and before the **GENESIS Single Variant Association Testing** workflow.\n\n**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n### Common use cases\n\n  - This workflow is used for updating the null model from the **GENESIS Null Model** workflow so that it can be used by the **GENESIS Single Variant Association Testing** workflow with the fast approximation to the score standard error (SE).\n\n### Changes introduced by Seven Bridges\n  - There are no changes introduced by Seven Bridges.\n\n### Common issues and important notes\n  - This approach only applies to null models that are mixed models (i.e. a relatedness matrix was included when fitting the original null model).\n\n### Performance Benchmarking\nThe workflow has been tested with the small 1KG phase3 subset files. The execution time/price on spot AWS c4.2xlarge instance was 4 min/$0.02.\n\nCost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://f4c.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. \ninputs = {\n    \"input_gds_file\": api.files.query(project=project_id, names=[\"basename_chr1.gds\", \"basename_chr2.gds\", ..]),\n    \"phenotype_file\": api.files.query(project=project_id, names=[\"name_of_phenotype_file\"])[0],\n    \"null_model_file\": api.files.query(project=project_id, names=[\"name_of_null_model_file\"])[0]\n}\ntask = api.tasks.create(name='GENESIS Update Null Model for Fast Score Test - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).", "input": [{"name": "GDS files"}, {"name": "Null model file"}, {"name": "Phenotype file"}, {"name": "Variant include file"}, {"name": "CPU"}, {"name": "Genome build"}, {"name": "The number of variants"}, {"name": "Minimum MAC"}, {"name": "Pass only"}, {"name": "Output prefix"}], "output": [{"name": "Null model"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": [], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1649763751, "dateCreated": 1621600277, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/graf-germline-variant-detection-workflow-1-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/graf-germline-variant-detection-workflow-1-0/3", "applicationCategory": "Workflow", "name": "GRAF Germline Variant Detection Workflow", "description": "The GRAF Germline Variant Detection Workflow enables accurate alignment and variant calling by utilizing a **genome graph** reference that can address the bias and other limitations inherent in linear genome references. Seven Bridges has constructed a comprehensive **pan-genome graph** that incorporates the diverse genetic composition of all populations around the world. By using this pan-genome graph, the GRAF Germline Variant Detection Workflow makes graph technology applicable at the whole genome level, enabling [highly accurate and fast read alignment and variant calling](https://www.nature.com/articles/s41588-018-0316-4). The current version of the workflow is 1.0 and supports both GRCh37 and GRCh38 versions of the Pan-Genome graphs.\n\n## Methods and Algorithms\n#### Graph reference\nThe graph reference augments the linear representation of the human genome (GRCh37 or GRCh38) with additional information on the genetic diversity of various human populations. The pan-genome graphs provided by Seven Bridges contain single nucleotide polymorphisms, insertions, deletions and other structural variations observed with significant frequency in a large number of populations.\n#### Alignment\nThe GRAF Aligner is a fast and accurate short read aligner capable of aligning sequencing reads to the genome graph reference. It is designed to process single and paired reads from NGS sequencing technologies.\n#### Variant calling\nThe GRAF Variant Caller is designed to work in tandem with the GRAF Aligner and detect both small variants and structural variants with assistance from the data available on population genome variability.\n#### Filtering\nThe following hard filtering criteria is applied to the variants detected by the GRAF Variant Caller. Variants satisfying the criteria are marked as false positives but not removed from the VCF file.\n* SNPs: AD_Ratio[1] < 0.20, MBQ[1] < 15, QD < 1, MQRankSum < -8, FS > 50\n* Indels: AD_Ratio[1] < 0.15\n\n## Inputs\n#### Reads\nNGS sequencing reads. Several formats are supported:\n* A single FASTQ or FASTQ.GZ file with single end (unpaired) reads.\n* A pair of FASTQ or FASTQ.GZ files with paired end reads. The *Paired-end* metadata field on the input files must be set as *1* and *2* to denote the pairs of reads prior to task execution.\n* A single BAM or CRAM file with either single end or paired end reads. Pair property is determined from flag 0x1 (see SAM specification). When the input file is a CRAM file encoded relative to a reference, the indexed reference file should be provided as **CRAM reference** input.\n\n#### Linear reference\nA FASTA file representing the linear reference used as the basis for genome graph construction. This file must be indexed, with .fai index available in the same path as the FASTA file. Valid references files (*GRCh38.GRAF.Linear_Reference.v1.fasta* and *GRCh37.GRAF.Linear_Reference.v1.fasta*) and their indices are available in the public reference files.\n#### Graph reference\nA VCF.GZ file containing the variants used to construct the genome graph reference. This file must be indexed, with .tbi index available in the same path as the VCF.GZ file. The variants in the file must be represented relative to the FASTA file passed to **Linear reference** input. Valid pan-genome graphs (*GRCh38.GRAF.Pan_Genome_Reference.v1.vcf.gz* and *GRCh37.GRAF.Pan_Genome_Reference.v1.vcf.gz*) and their indices are available in the public reference files. The current pipeline does not accept custom graphs.\n#### Intervals\nThe target regions for variant calling in BED format. This BED file is also used to parallelize variant calling in multithreaded environment. Suggested files for the whole genome (*GRCh38.GRAF.Genome_Intervals.v1.bed* and *GRCh37.GRAF.Genome_Intervals.v1.bed*) are available in the public reference files.\n\n## Outputs\n#### Alignments\nRead alignments as output from the GRAF Aligner. The alignment file is coordinate-sorted and indexed, with .bai / .crai index as a secondary file with the alignment file.\n#### Variants\nA VCF file containing the final list of variants detected by the GRAF Variant Caller. Variants that do not pass the hard filtering criteria are marked with FP in the FILTER column.", "input": [{"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "Linear reference", "encodingFormat": "application/x-fasta"}, {"name": "Graph reference", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "text/x-bed"}, {"name": "CRAM reference", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Alignments", "encodingFormat": "application/x-bam"}, {"name": "Filtered variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/graf-germline-variant-detection-workflow-1-0/3.png", "codeRepository": ["https://github.com/sbg/docs/raw/master/GRAF/Seven_Bridges_GRAF_User_Guide.pdf", "https://github.com/sbg/docs/raw/master/GRAF/Seven_Bridges_GRAF_End_User_License_Agreement.pdf"], "applicationSubCategory": ["Alignment", "Variant Calling", "Graph", "Genomics", "CWL1.0"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["v1.0"], "dateModified": 1592502614, "dateCreated": 1592486867, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-purple-linx-workflow/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-purple-linx-workflow/5", "applicationCategory": "Workflow", "name": "GRIDSS/PURPLE/LINX Workflow", "description": "The **GRIDSS/PURPLE/LINX Workflow** is used for somatic genomic rearrangement detection and classification on WGS data. \n\nIt takes a pair of matched tumor/normal BAM files and produces allele specific copy number of every base of the genome, overall sample purity and ploidy, annotated SV clusters and gene fusion predictions. Moreover, it outputs detailed visualisations of the rearrangements in the tumor genome via integrated Circos plots showing copy number changes, clustered SVs, derivative chromosome predictions and impacted genes [1]. \n\nThe workflow is based on the following tools:\n\n- GRIDSS, used for structural variant calling;\n- PURPLE, used for allele specific copy number calling;\n- LINX, used for event classification, and visualisation [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nTo run the workflow, the following input parameters are required:\n\n- **Tumor BAM file** - Tumor BAM file with BAI index file is required.\n- **Matched normal BAM file** - Matched normal BAM file with BAI index file is required.\n- **Reference FASTA** - Reference genome in FASTA format along with FAI and DICT secondary files.\n- **Human virus reference FASTA** - Human virus reference sequence, BWA indexed and gridss-cached.\n- **BWA reference** - BWT index file to complement the FASTA reference. The following secondary files are required: .amb, .ann, .pac and .sa.\n- **GC profile** - GC profile file.\n- **Breakend PON** - Single breakend PON BED file.\n- **Breakpoint PON** - Paired breakpoint PON BEDPE file.\n- **Breakpoint hotspot** - Paired breakpoint hotspot BEDPE file.\n- **VCF file containing likely heterozygous sites** - BAF loci VCF file containing likely heterozygous sites.\n- **Sample name** - Name of the sample being processed.\n- **Reference version** - Reference genome version (19, 37 or 38).\n\n    Reference data (reference genome and virus sequence with all associated indices, GC content, PONs and BAF VCF) can be obtained from our Public Reference Files section (gpl_ref_data_37.gz and gpl_ref_data_38.gz, depending on the reference genome used). For more information on how the files were generated, you can visit each tool's \n    Github page.\n\nAfter successful execution, the workflow produces the following outputs corresponding to each of the tools:\n1. **Amber output directory**\n2. **Cobalt output directory**\n3. **GRIDSS VCF file**\n4. **GRIDSS assembly BAM file**\n5. **GRIDSS filtered VCF file**\n6. **GRIPSS filtered VCF file**\n7. **GRIPSS hard filtered VCF file**\n8. **Purple output directory**\n9. **Linx output directory**\n10. **Linx visualiser data directory**\n11. **Linx visualiser plot directory**\n \n Consult the tool documentation for details of the output file formats [3][4][5].\n\n### Changes Introduced by Seven Bridges\n\n- PURPLE can optionally persist its output to a SQL database, however this option is not available on the Platform.\n- AMBER Multiple Reference / Donor mode (using multiple reference BAM files) is not supported on the Seven Bridges Platform.\n- AMBER instance hint was changed to c4.8xlarge, due to failing of some tasks during testing caused by lack of memory requirements.\n\n\n\n### Common Issues and Important Notes\n\n1. **Somatic VCF** input is recommended, since a high quality set of somatic SNV and INDEL calls can improve the accuracy and utility of PURPLE. If provided, the variants are used for enhancing the purity and ploidy fit in 2 ways. Firstly, each solution receives a penalty for the proportion of somatic variants which have implied ploidies that are inconsistent with the minor and major allele ploidy. Secondly, for highly diploid samples, the VAFs of the somatic variants are used directly to calculate a somatic variant implied purity. For both purposes, accurate VAF estimation is essential thus PURPLE requires the \u2018AD\u2019 (Allelic Depth) field in the VCF. High quality filtering of artifacts and false positive calls is also critical to achieving an accurate fit [3]. \n\n2. If **Somatic VCF** is supplied, sample IDs in the VCF header must match normal and tumor sample names used in the rest of the workflow. These names will be either taken from **Matched normal sample name** and **Tumor sample name** input parameters or generated by default by appending '_N' and '_T' to the **Sample name** value for normal and tumor sample name, respectively. \n\n3. Input files (BAM files, reference genome, GC content, BED/BEDPE and VCF files) must use the same chromosome notation (e.g. 1 vs chr1).\n\n4. If **Driver catalog** parameter is set to True, **Driver gene panel** and **Hotspots** VCF file must be provided. Note that generating the driver catalog for SNVs assumes that the VCF has been annotated with SNPEFF [3]. \n\n5. Smoothing, somatic fit and fitting arguments are optional and changing these values without a thorough understanding of the system is not recommended [3]. \n\n\n\n### Performance Benchmarking\n\n\n| Experimental strategy |  Input size | Duration |  Cost |  Instance (AWS on-demand) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|\n|     WGS     |  2 x 110GB |   5h 25min   | $8.55 | **NOTE** |\n|     WGS     | 2 x 220GB |   9h 54min  | $16.27 | **NOTE** |\n\n**NOTE**: The workflow uses multiple types of instances, since the memory resources are allocated dynamically, as predicted via CWL code.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\nDue to high memory consumption, the workflow could not be tested locally with cwltool. Instead, cwltool --validate option is used for ensuring that workflow's CWL code satisfies the CWL 1.1 specification.\n\n### References\n\n[1] [GRIDSS/PURPLE/LINX publication](https://www.biorxiv.org/content/10.1101/781013v1.full)\n\n[2] [GRIDSS/PURPLE/LINX Github repository](https://github.com/hartwigmedical/gridss-purple-linx)\n\n[3] [PURPLE Github repository](https://github.com/hartwigmedical/hmftools/tree/master/purple)\n\n[4] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)\n\n[5] [LINX Github repository](https://github.com/hartwigmedical/hmftools/tree/master/linx)", "input": [{"name": "Sample name"}, {"name": "Tumor BAM file", "encodingFormat": "application/x-bam"}, {"name": "Matched normal BAM file", "encodingFormat": "application/x-bam"}, {"name": "Somatic VCF", "encodingFormat": "application/x-vcf"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Reference version"}, {"name": "BWA reference"}, {"name": "Reference cache gridss"}, {"name": "Human virus reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "GC profile"}, {"name": "Breakend PON", "encodingFormat": "text/x-bed"}, {"name": "Breakpoint PON"}, {"name": "Breakpoint hotspot"}, {"name": "Validation stringency"}, {"name": "Driver gene panel", "encodingFormat": "text/plain"}, {"name": "Matched normal sample name"}, {"name": "Tumor sample name"}, {"name": "Blacklist", "encodingFormat": "text/x-bed"}, {"name": "Configuration file"}, {"name": "VCF file containing likely heterozygous sites", "encodingFormat": "application/x-vcf"}, {"name": "Hotspots", "encodingFormat": "application/x-vcf"}, {"name": "Fragile sites", "encodingFormat": "text/x-bed"}, {"name": "Line elements", "encodingFormat": "text/x-bed"}, {"name": "Replication origins file", "encodingFormat": "text/x-bed"}, {"name": "Viral hosts", "encodingFormat": "text/x-bed"}, {"name": "Ensembl reference files directory"}, {"name": "Known fusion reference data", "encodingFormat": "text/x-bed"}, {"name": "Use proper pair"}, {"name": "Threads - Gridss"}, {"name": "Threads - Linx visauliser"}, {"name": "Threads - Cobalt"}, {"name": "Threads - Amber"}, {"name": "GRIDSS steps"}, {"name": "Somatic minimum peak"}, {"name": "Somatic minimum variants"}, {"name": "Somatic minimum purity spread"}, {"name": "Somatic minimum purity"}, {"name": "Somatic penalty weight"}, {"name": "Highly diploid percentage"}, {"name": "Minimum diploid tumor ratio count"}, {"name": "Minimum diploid tumor ratio count centromere"}, {"name": "Minimum purity"}, {"name": "Maximum purity"}, {"name": "Purity increment"}, {"name": "Maximum norm factor"}, {"name": "Minimum norm factor"}, {"name": "Norm factor increments"}, {"name": "Ploidy penalty factor"}, {"name": "Minimum ploidy penalty"}, {"name": "Minimum ploidy penalty standard deviation"}, {"name": "Ploidy penalty standard deviation"}, {"name": "No charts"}, {"name": "Maximum ploidy"}, {"name": "Minimum ploidy"}, {"name": "Threads - Purple"}, {"name": "External aligner"}, {"name": "Maximum coverage"}, {"name": "Concordant read pair distribution"}, {"name": "Keep intermediate files"}, {"name": "Minimum quality"}, {"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Minimum percentage of median depth"}, {"name": "Maximum percentage of median depth"}, {"name": "Minimum heterozygous allelic frequency percentage"}, {"name": "Maximum heterozygous allelic frequency percentage"}, {"name": "Threads - Gridss annonation"}, {"name": "Hard maximum normal absolute support"}, {"name": "Hard maximum normal relative support"}, {"name": "Hard minimum tumor qual"}, {"name": "Maximum homology length short inversion"}, {"name": "Maximum inexact homology length short DEL"}, {"name": "Maximum short strand bias"}, {"name": "Minimum length"}, {"name": "Minimum normal coverage"}, {"name": "Minimum qual break end"}, {"name": "Minimum qual break point"}, {"name": "Minimum qual rescue mobile element insertions"}, {"name": "Minimum tumor allelic frequency"}, {"name": "Maximum normal support"}, {"name": "Discover and annotate gene fusions"}, {"name": "Driver annotation"}, {"name": "Minimum distance to cluster SVs"}, {"name": "Chaining SV limit"}, {"name": "Log reportable fusion only"}, {"name": "Fusion gene distance"}, {"name": "Restricted fusion genes"}, {"name": "Logs in debug mode"}, {"name": "Innermost radius"}, {"name": "Outermost radius"}, {"name": "Gap radius"}, {"name": "Exon rank radius"}, {"name": "Gene relative size"}, {"name": "Segment relative size"}, {"name": "Copy number alterations relative size"}, {"name": "Minimum label size"}, {"name": "Maximum label size"}, {"name": "Maximum gene characters"}, {"name": "Maximum distance labels"}, {"name": "Maximum position labels"}, {"name": "Exact position"}, {"name": "Minimum line size"}, {"name": "Maximum line size"}, {"name": "Glyph size"}, {"name": "Interpolate copy number positions"}, {"name": "Interpolate exon positions"}, {"name": "Chromosome range height"}, {"name": "Chromosome range columns"}, {"name": "Fusion height"}, {"name": "Fusion legend rows"}, {"name": "Fusion legend height per row"}, {"name": "Cluster ID"}, {"name": "Chromosome"}, {"name": "Include line elements"}, {"name": "Gene"}, {"name": "Driver catalog"}], "output": [{"name": "GRIDSS VCF file", "encodingFormat": "application/x-vcf"}, {"name": "GRIPSS filtered VCF file", "encodingFormat": "application/x-vcf"}, {"name": "GRIPSS hard filtered VCF file", "encodingFormat": "application/x-vcf"}, {"name": "GRIDSS assembly BAM file", "encodingFormat": "application/x-bam"}, {"name": "Amber output directory"}, {"name": "Cobalt output directory"}, {"name": "Purple output directory"}, {"name": "Linx output directory"}, {"name": "Linx visualiser plot directory"}, {"name": "Linx visualiser data directory"}], "softwareRequirements": ["LoadListingRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/hartwigmedical/gridss-purple-linx", "https://github.com/umccr/gridss-purple-linx/blob/cwl-workflow/cwl/workflows/gridss-purple-linx/latest/gridss-purple-linx.latest.cwl", "https://github.com/umccr/gridss-purple-linx/blob/cwl-workflow/cwl/workflows/gridss-purple-linx/latest/gridss-purple-linx.latest.cwl"], "applicationSubCategory": ["Copy Number Analysis", "Structural Variant Calling", "Annotation"], "project": "SBG Public Data", "creator": "Hartwig Medical Foundation Tools", "softwareVersion": ["v1.1"], "dateModified": 1648049434, "dateCreated": 1639152933, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gseapreranked-workflow-4-1-0/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gseapreranked-workflow-4-1-0/9", "applicationCategory": "Workflow", "name": "GSEAPreranked Workflow", "description": "**GSEAPreranked Workflow** performs Gene Set Enrichment Analysis (GSEA) using the **GSEAPreranked** tool.\n\nAlthough genome-wide expression analysis has become a routine method in genomic research, it is still a major challenge to interpret these results and to extract insights into biological mechanisms from such information. GSEA is an analytical method that determines whether an a priori defined set of genes shows statistically significant differences between two biological states. The method analyzes gene sets that represent groups of genes that share common biological function, chromosomal location, or regulation and is fully described in the [GSEA PNAS 2005](https://www.pnas.org/content/102/43/15545.full) paper [1].\n\nThe **GSEAPreranked Workflow** consists of two tools, **GSEA Input Prepare** and **GSEAPreranked**. The **GSEAPreranked** tool performs gene set enrichment analysis and represents a wrapper around the command-line tool that was developed by the BROAD Institute [2].  The **GSEA Input Prepare** tool is based on the Python script developed by the Seven Bridges team to prepare the required input file formats for the **GSEAPreranked** tool.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n###Common Use Cases\n\n* The **GSEAPreranked Workflow**\u00a0is generated with an assumption that a differential expression analysis has been done before using the **DESeq2** tool which is publicly available on the Seven Bridges Platform. More information about how to perform differential expression analysis with the **DESeq2** tool can be found\u00a0[here](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/deseq2-1-26-0/). The **Differential expression data** input of the **GSEAPreranked Workflow** should be the **DESeq2 analysis results** output of the **DESeq2** tool which is a standard DESeq2 report with multiple columns. More information on the results columns can be found [here](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) (Section: *More information on results columns*). However, the **GSEAPreranked** tool requires a ranked list of differentially expressed genes that contains only two columns on its input. The first column of this list should contain feature identifiers while the second column should contain the chosen class-difference metric for this feature. The list needs to be sorted in descending numerical order [3] (Section: *Ranked Gene Lists*). To prepare this list from the DESeq2 report, a custom tool has been developed, **GSEA Input Prepare**, based on the Python script. The chosen metric for each gene is computed by multiplying the direction (sign) of the log2FoldChange value and the negative logarithm of the p-value.\u00a0After ranking the genes based on such obtained metric values, genes at the top of the ranked list are more highly expressed in one class of the samples (e.g., treated) while genes at the bottom are more highly expressed in another class of samples (e.g. untreated).\n\n* Besides a ranked list of differentially expressed genes that are going to be tested for enrichment, **GSEAPreranked** requires also gene sets file(s) that should be provided on either **Gene sets local** or **Gene sets database** workflow inputs.\n\n* The **Gene sets database** input lists all gene sets files contained in the **Molecular Signatures Database v7.2** (MSigDB), that is hosted and maintained by the GSEA team [4]. By choosing *msigdb.v7.2.symbols.gmt* for the **Gene sets database** input, all gene sets contained within MSigDB will be tested for enrichment. Additionally, the user can provide their own gene sets file(s) on the **Gene sets local** input. For a more detailed description of how to create the custom gene sets file, see [3] (section: *Gene Set Database Formats*). One or more user-created gene sets files can be analyzed during one run. Furthermore, multiple files created by the user and multiple files from the MSigDB can be tested for enrichment in the same run.\n\n* If the user has already prepared the ranked list of differentially expressed genes according to the file format required by the **GSEAPreranked** tool [3] (Section: Ranked Gene Lists), the **Gsea Input Prepared** tool will be skipped and gene set enrichment analysis will be performed using this user-supplied ranked list. If this is the case and the user wants to test gene sets from the MSigDB for enrichment, it is very important to use gene symbols as gene identifiers in the ranked list of genes in order to preserve compatibility with the gene sets identifiers used in this database.\n\n* The GSEA algorithm automatically normalizes the enrichment scores for variation in gene set size, as described in [GSEA Statistics](gsea-msigdb.org/gsea/doc/GSEAUserGuideTEXT.htm#_GSEA_Statistics). However, the normalization is not very accurate for extremely small or extremely large gene sets. For example, for gene sets with fewer than 10 genes, just 2 or 3 genes can generate significant results. Therefore, by default, GSEA ignores gene sets that contain fewer than 15 genes or more than 500 genes (defaults that are appropriate for datasets with 10,000 to 20,000 features). To change these default values, use the **Max gene sets size** and **Min gene sets size** parameters of the **GSEAPreranked Workflow**. However, keep in mind the possibility of inflated scorings for very small gene sets and inaccurate normalization for large ones.\n\n* The main outputs of the **GSEAPreranked Workflow** are two reports that contain up-regulated gene sets and down-regulated gene sets (produced on **GSEA up-regulated gene sets report** and **GSEA down-regulated gene sets report** outputs, respectively). Besides these two files, **GSEAPreranked** outputs the summary GSEA report on the **GSEA summary report** output and a ZIP file that contains all the files produced by this tool on the **GSEA ZIP report** output. This file should be downloaded to the local environment in order to analyze all results in detail and it contains:\n \n1. The summary report (*index.html*) \n2. The files that contain more detailed results (all of them are addressed and linked in the summary report)\n3. Subfolder named *edb* that contains a machine-readable version of the report\n\nMore information about gene set enrichment results can be found [here](https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideTEXT.htm#_Interpreting_GSEA_Results).\n\n### Changes introduced by Seven Bridges\n\nThis tool represents the command line wrapper of the *GSEAPreranked Analysis* developed by the BROAD Institute [2], and there are no modifications to the original tool.\n\n### Common Issues and Important Notes\n\n* If the user provides their own ranked list of differentially expressed genes, it is very important to choose the right ranking metric. It is strongly recommended that the data do not include duplicate ranking values because GSEA does not resolve ties. In case of a tie, the order of genes will be arbitrary, which may produce erroneous results.\n\n* It is crucial that the ranked list of genes and gene sets contain the same feature identifiers (gene symbols or gene ids). If MSIgDB gene sets are used for testing the enrichment, a ranked list of differentially expressed genes must contain gene symbols as gene identifiers.\n\n* Within GSEA, HGNC gene symbols are case sensitive which means that the identifiers \u201cTestGene1\u201d and \u201cTESTGENE1\u201d are not the same.\n\n###Performance Benchmarking\n\nThe execution time of the **GSEAPreranked Workflow** will depend on the number of genes included in the input dataset list, on the number and size of gene sets that are tested for enrichment and on the permutation number used.\n\nIn a test run with 15,195 genes contained in the input dataset, the execution time was about 48 minutes on an AWS c4.2xlarge instance (with the price around $0.30) while for 35,228 genes contained in the input dataset, the execution time was twice longer for the same parameters and gene sets files. Please note that in both runs, all gene sets contained in MSigDB v7.2 were tested for enrichment (more than 31,000) but in most cases, only certain target collections will be tested for enrichment and this will decrease the execution time. Moreover, the permutation number (the number of gene set permutations to perform in assessing the statistical significance of the enrichment score) was set to 1000 which is the value recommended by the GSEA team. However, during the testing phase, it is best to start with a small number, such as 10, since this number greatly prolongs execution time. After the analysis completes successfully, run it again with a full set of permutations. \n\nThe GSEA software works on a single thread and requires about 4 GB of RAM memory for the default permutation number (1000). If the number of permutations is set to a higher value, more memory may be required. To increase the memory, refer to the **Memory per job** parameter of the **GSEAPreranked** tool.\n\nThe cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n###API Python Implementation\n\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id = \"your_username/project\"\nworkflow_id = \"your_username/project/workflow\"\n\n# Get file names from files in your project.\ninputs = {\n        \"in_file\": api.files.query(project=project_id, names=['DESeq2.output.DF2.4.csv'])[0],\n        \"gene_sets_db\": [\"msigdb.v7.2.symbols.gmt\"]\n\n}\ntask = api.tasks.create(name='GSEAPreranked Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False)\n\n```\n\nInstructions for installing and configuring the API Python client are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [GSEA PNAS 2005](https://www.pnas.org/content/102/43/15545.full)\n\n[2] [GSEAPreranked Analysis](https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideTEXT.htm#_GSEAPreranked_Page)\n\n[3] [GSEA data formats](http://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats)\n\n[4] [Molecular Signatures Database v7.2](https://www.gsea-msigdb.org/gsea/msigdb/index.jsp)\n\n[5] [GSEA statistics](gsea-msigdb.org/gsea/doc/GSEAUserGuideTEXT.htm#_GSEA_Statistics)\n\n[6] [GSEA results](https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideTEXT.htm#_Interpreting_GSEA_Results)", "input": [{"name": "Input expression file"}, {"name": "Gene sets local"}, {"name": "Gene sets database"}], "output": [{"name": "GSEA ZIP report", "encodingFormat": "application/zip"}, {"name": "GSEA summary report", "encodingFormat": "text/html"}, {"name": "GSEA up-regulated gene sets report", "encodingFormat": "text/html"}, {"name": "GSEA down-regulated gene sets report", "encodingFormat": "text/html"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/gseapreranked-workflow-4-1-0/9.png", "codeRepository": [], "applicationSubCategory": ["Gene Set Enrichment Analysis", "Transcriptomics"], "project": "SBG Public Data", "creator": "BROAD Institute", "softwareVersion": ["v1.0"], "dateModified": 1638296939, "dateCreated": 1626442130, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-stringtie/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-stringtie/11", "applicationCategory": "Workflow", "name": "HISAT2-StringTie Workflow", "description": "The __HISAT2-StringTie Workflow__ can be used to perform a gene abundance estimation of RNA-Seq data (i.e. quantification) of a unified set of genes common for all samples in the analysis. The workflow is based on the\u00a0Nature protocol paper [1] with the  last step (differential expression testing) omitted.\n\nThe first part of the workflow indexes and aligns RNA-Seq reads to the reference using tools from the __HISAT2 2.2.1__ toolkit. The second part performs quantification and, if specified, transcriptome assembly using tools from the\u00a0__StringTie 2.1.3__\u00a0toolkit. Depending on the value of the __Estimate novel isoform abundance?__ required parameter, analysis will be continued in one of two possible ways:\n\n1) If set to 'True' then aligned reads are fed to the first run of __StringTie__ which performs 'reference guided' transcriptome assembly for each sample. Assembled transcripts and reference annotation transcripts are then merged into a uniform set of transcripts. The second run of __StringTie__ performs a quantification of merged transcripts and genes in each sample. For each sample, the workflow outputs the abundance estimation in the __Ballgown__ input format (one TAR bundle containing 5 .ctab tables), two files containing transcript and gene expression values in the __DESeq2__ input format, and a TAB file containing FPKM, TPM, and Coverage values for each gene. In addition to files containing the abundance estimation, the workflow also outputs a GTF file with merged transcripts, __GffCompare__ output files with comparison results between reference annotation transcripts and merged transcripts.\n\n2) If set to 'False' then aligned reads are only fed to  __StringTie__ once, this run performs quantification in each sample. For each sample, the workflow outputs the abundance estimation in the __Ballgown__ input format (one TAR bundle containing 5 .ctab tables), two files containing transcript and gene expression values in the __DESeq2__ input format, and a TAB file containing FPKM, TPM, and Coverage values for each gene. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n- The workflow consists of five steps: __HISAT2 Build__, __HISAT2__, __StringTie__, __StringTie Merge__ and __GffCompare__.\n- __Reads__ is the required input port that accepts raw sequencing reads in FASTQ or FASTA format. Could be also gzip\u2019ed or bzip2'ed.\n- __Reference or Index files__ is also required and accepts a reference FASTA file(s) or a pre-built __HISAT2__ index TAR bundle.\n- __Reference annotation file__ is used for guiding the assembly process and for extracting exons and splice sites to create an index that will enable __HISAT2__ to be a more accurate aligner.\n- This workflow can be used to perform the quantification of RNA-Seq data of a unified set of genomic features across all samples in an analysis suitable for a downstream differential expression step. Once quantification values are obtained, downstream differential expression analysis can be performed using **DESeq2** or **Ballgown** tools on the Seven Bridges Platform.\n\n\n### Changes Introduced by Seven Bridges\n\n* In order to facilitate and accelerate further RNA-Seq analysis, an additional toolkit __Sambamba (0.7.1)__ is integrated into the same Seven Bridges tool with __HISAT2__. Besides the standard __HISAT2__ SAM output, __HISAT2__ has been extended to provide sorted BAM files as well. Thus, the sorting step as described in the protocol paper [1] is not visible in the workflow\u2019s main page.\n\n* An additional, Seven Bridges-designed tool, __SBG Pair FASTQs by Metadata__, is added at the beginning of the workflow in order to enable the processing of multiple samples in a single run.\n\n* The workflow\u2019s indexing step is optional. __HISAT2 Build__ is implemented in a way that it can accept a TAR bundle containing an already indexed reference instead of reference FASTA file(s). If a TAR bundle is provided, the indexing step is skipped and the TAR bundle is forwarded to the output, thus reducing execution costs. You can find pre-built index files (grch38\\_tran.tar.gz, grch37\\_tran.tar.gz, hg38_tran.tar.gz) on the Seven Bridges Platform under the Public Reference Files section. Pre-built index files for organisms other than human can be found [here](https://daehwankimlab.github.io/hisat2/download/).\n\n* In order to enable __StringTie__ to produce quantification tables tailored for __DESeq2__ or [edgeR](http://bioconductor.org/packages/release/bioc/html/edgeR.html), the [`prepDE.py`](https://ccb.jhu.edu/software/stringtie/dl/prepDE.py) Python script [2] that extracts raw counts from **Ballgown** input tables is embedded within the __StringTie__ tool.\n\n### Common Issues and Important Notes\n\n* For paired-end reads, the __Paired-end__ metadata field should be set appropriately in all FASTQ (or FASTA) files that are found in the **Reads** input node.\n\n* All input FASTQ files (or FASTA files depending on the format of input reads) must have the __Sample ID__ metadata field appropriately set.\n\n* The GTF and reference FASTA files need to have compatible chromosome namings (i.e. >1, >2, ... or >chr1, >chr2, ...). If pre-built HISAT2 index is used (reference FASTA file(s) unknown), parameter __Remove 'chr' string__ (`--remove-chrname`) or __Add 'chr' string__ (`--add-chrname`) should be set to provide the same naming convention with GTF file.\n\n### Limitations\n\nThis workflow is optimised for eukaryotic genomes. When running with prokaryotic samples, whose genomes contain no introns and there are no splicing events, the index building step will fail. Please build the index separately with __HISAT2 Build__ default settings, without  __Splice sites file__ (`--ss`) and __Exon file__ (`--exon`) options, and run the workflow with index file in TAR format instead of the genome reference.\n\n### Performance Benchmarking\n\nWorkflow is optimised to be run in scatter mode, so default instance is set to r5.8xlarge with 2 GB of EBS (AWS). The most intensive part in the workflow is reference indexing. For example, building hg38 human reference index using reference transcriptome requires 160 GB of RAM (for more details see the [HISAT2 homepage](https://daehwankimlab.github.io/hisat2/manual/)).\n\nIn the following table you can find estimates of the workflow runtime and its cost when the __Estimate novel isoform abundance?__ parameter is set to 'True'. Indexing is not performed, a TAR index archive (size 5.4 GB) is provided. Size of the gene annotation file is 1.2 GB. All tasks were executed on the on-demand default AWS instance.\n\nNote: Input size and number of reads refer to the average value per sample.\n\n| # of samples | Input format | Input size | Paired-end | # of reads | Read length | Duration  | Cost   |\n|--------------|--------------|------------|------------|------------|-------------|-----------|--------|\n| 400          | FASTQ.GZ     | 287.5 MB     | No         | 11M      | 36          | 1h 41min  | $17.67 |\n| 100          | FASTQ.GZ     | 300 MB      | No         | 11M      | 36          | 36min  | $5.64 |\n| 1           | FASTQ        | 3.8 GB     | Yes        | 16M    | 101         | 13min  | $0.53 |\n| 50           | FASTQ.GZ     | 3.9 GB     | Yes        | 60M     | 75          | 3h 37min  | $20.91 |\n| 200          | FASTQ.GZ     | 4 GB     | Yes        | 60M     | 75          | 8h 23min | $66.07 |\n| 16           | FASTQ        | 21 GB     | No        | 79M    | 75         | 2h 48min  | $9.50 |\n| 16          | FASTQ        |  35.9 GB     | Yes        | 160M    | 50          | 4h 37min | $15.91 |\n| 1           | FASTQ        |  37 GB     | Yes        | 163M    | 101          | 1h 29 min | $3.44 |\n| 10*          | FASTQ     | 62.5 GB     | Yes        | 190M     | 101          | 6h 40min | $21.68 |\n*EBS increased to 3 GB due to large input size\n\nRuntime and task cost for one of the samples above when indexing is performed (size of reference file - 2.9 GB, size of GTF file - 1.2 GB):\n\n| # of samples | Input format | Input size | Paired-end | # of reads | Read length | Duration  | Cost   |\n|--------------|--------------|------------|------------|------------|-------------|-----------|--------|\n| 1           | FASTQ        | 3.8 GB     | Yes        | 16M    | 101         | 1h 6min  | $2.56 |\n\n*The cost of running __HISAT2-StringTie Workflow__ can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. The file names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_gene_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_references_or_index': list(api.files.query(project=project_id, names=['reference_fasta_file.fa'])),\n         'discover_novel_isoform': True\n        }\n\n# Run the task\ntask = api.tasks.create(name='HISAT2-StringTie Workflow - API Run', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [HISAT, StringTie, Ballgown protocol paper](https://www.nature.com/articles/nprot.2016.095)\n\n[2] [StringTie manual page - using StringTie with DESeq2 and edgeR](https://ccb.jhu.edu/software/stringtie/index.shtml?t=manual#deseq)", "input": [{"name": "Reference annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Estimate novel isoform abundance?"}, {"name": "Reference or Index files", "encodingFormat": "application/x-tar"}, {"name": "Reads", "encodingFormat": "application/x-fasta"}, {"name": "Remove 'chr' string"}, {"name": "Add 'chr' string"}], "output": [{"name": "GffCompare stats files", "encodingFormat": "application/x-gtf"}, {"name": "Merged transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Gene abundance estimation"}, {"name": "DESeq2 transcript count matrix"}, {"name": "DESeq2 gene count matrix"}, {"name": "Archived ballgown input tables", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": [], "applicationSubCategory": ["Alignment", "Quantification", "RNA-Seq"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648049434, "dateCreated": 1612220554, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/joint-analysis-of-rna-seq-and-atac-chip-seq-data/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/joint-analysis-of-rna-seq-and-atac-chip-seq-data/7", "applicationCategory": "Workflow", "name": "Joint analysis of RNA-Seq and ATAC/ChIP-Seq data", "description": "This workflow is an example of joint analysis that combines **ATAC-Seq** or **ChIP-Seq** with **RNA-Seq** data. It starts with comparing the transcript count and peak set data to identify differentially expressed genes and binding regions. The detected differential features are then overlapped and used for gene set enrichment analysis.\n\nThe workflow consists of three functional parts - peak sets downstream analysis, gene expression data downstream analysis and combined enrichment analysis.\n\nDownstream analysis of called peak sets is performed as differential binding analysis with **DiffBind** [1] and peak annotation with **ChIPseeker** [2]. **DiffBind** is an R Bioconductor package that is used for identifying sites that are\u00a0differentially enriched between two or more sample groups. It works primarily with sets of peak calls (\u2018peaksets\u2019), which are sets of genomic intervals representing candidate protein binding sites for each sample. It includes functions that support the processing of peaksets, including\u00a0overlapping and merging peak sets across an entire dataset, counting sequencing reads in overlapping intervals in peaksets, and identifying statistically significantly differentially bound sites. **ChIPseeker** package implements functions to retrieve the nearest genes around the peak and annotate the genomic region of the peak. Several visualization functions are implemented to summarize genomic annotation, distance to TSS, and overlap of peaks or genes.\u00a0\n\nGene expression data downstream analysis is represented by differential expression analysis with **DESeq2** [3]. The Bioconductor/R package **DESeq2** provides a set of functions for importing data, performing exploratory analysis and finally testing for differential expression. It analyzes estimated read counts from several samples, each belonging to one of two or more conditions under study, searching for systematic changes between conditions, as compared to within-condition variability.\n\nCombined enrichment analysis consists of the **Genes overlap** tool that filters DE genes that are also present in differentially bound peaks set, and enrichment analysis with [**GSEAPreranked**](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/gseapreranked-workflow-4-1-0/) sub-workflow. **Genes overlap** is a custom SBG R script made for the purpose of this workflow.\u00a0**GSEAPreranked**\u00a0sub-workflow, consists of\u00a0two tools,\u00a0**GSEA Input Prepare**\u00a0and\u00a0**GSEAPreranked**. The\u00a0**GSEAPreranked**\u00a0tool performs gene set enrichment analysis and represents a wrapper around the command-line tool that was developed by the BROAD Institute [4]. The\u00a0**GSEA Input Prepare**\u00a0tool is based on the Python script developed by the Seven Bridges team to prepare the required input file formats for the\u00a0**GSEAPreranked**\u00a0tool.\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common use cases\n\n***Required inputs for downstream analysis of called peak sets part:***\n\n* Sets of peaks (**Input peak files**) in narrowPeak format coming from a peak caller and accompanying peak alignment files (**Input BAM files**). \n\n* **Metadata sample sheet** in CSV format that should have a structure compatible with the **DiffBind** loading function and contain a row for each sample and three columns - SampleID, Condition and Replicate.\n\nExample Metadata sample sheet content below:\n```\nSampleID,Condition,Replicate\ntreated1,treated,1\ntreated2,treated,2\ntreated3,treated,3\nuntreated1,untreated,1\nuntreated2,untreated,2\nuntreated3,untreated,3\n\n```\n***Required inputs for gene expression data downstream analysis part:***\n\n* **Gene or transcript abundances** for each sample, obtained by RNA-Seq quantification tool (please use **HTSeq**, **RSEM** or **StringTie** for gene-level abundance estimates; **Salmon**, **Sailfish** or **Kallisto** for transcript-level abundance estimates).\n If the abundance estimates provided are on a transcript level, **Gene annotation** file is also required, to summarize them for gene-level analysis. \n\n* There are two options for providing phenotype information:\nBy indicating API keys for metadata fields that need to be included in the design. Phenotype information will then consist of variables you listed as **Covariate of interest** and **Control variables**.\nBy including a CSV file (**Phenotype data** input) that contains a row for each sample, with **Sample ID** in the first column. These **Sample ID**s need to match those in input files metadata. Also, a single line header with variable names should be included. \nExample Phenotype data content below:\n```\nsample_id,library,sex,condition\ntreated1,paired-end,male,treated\ntreated2,single-end,male,treated\ntreated3,paired-end,female,treated\nuntreated1,single-end,male,untreated\nuntreated2,paired-end,female,untreated\nuntreated3,paired-end,female,untreated\nuntreated4,paired-end,male,untreated\n\n```\n\nSupplying a CSV like this while entering \"condition\" for the value of the **Covariate of interest** parameter and \"library\" in **Control variables** will test for differential expression between treated and untreated samples, while controlling for effect of library preparation.\nThe information about a sample belonging to the treated or the untreated group can also be kept in the metadata. To use a metadata field for splitting the samples into groups for testing, enter its metadata key for the **Covariate of interest** parameter. All the input files need to have this metadata field populated. To control for possible confounders, enter their API keys as **Control variables**.\n\n***Required inputs for combined enrichment analysis part:***\n\n* The **Gene sets database** parameter lists all gene sets files contained in the **Molecular Signatures Database v7.2** (MSigDB), that is hosted and maintained by the GSEA team [5]. By choosing msigdb.v7.2.symbols.gmt for the **Gene sets database** input, all gene sets contained within MSigDB will be tested for enrichment. Additionally, the user can provide their own gene sets file(s) on the **Gene sets local** input. For a more detailed description of how to create the custom gene sets file and other information about this part of the analysis, visit [GSEAPreranked page](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/gseapreranked-workflow-4-1-0/).\n\n### Changes Introduced by Seven Bridges\n***Downstream analysis of called peak sets***\n\n* **DiffBind** can work only with narrowPeak files. [ENCODE ATAC-seq Pipeline](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/encode-atac-seq-pipeline-1-9-2/) or [ENCODE ChIP-Seq Pipeline](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/encode-chip-seq-pipeline-2/) can be used to obtain files in this format.\n\n* **ChIPseeker** works with human samples by default. However, this workflow can be run with mouse samples, in which case parameters **TxDb annotation package** and **Genome wide annotation** need to be set to mouse annotations. Also, a GMT file with mouse gene sets of interest has to be provided on the **Gene sets local** input of **GSEAPreranked**.\n\n* Output of downstream analysis of called peak sets is a merged HTML file with analysis report from **DiffBind** and **ChIPseeker**. **DiffBind** report contains a metadata data table with number of peaks in each set, the total number of unique peaks after merging, correlation heatmap and PCA plot, number of significant differential peaks, MA plots, volcano plots, PCA plots and boxplots, while **ChIPseeker** report contains the feature distribution plot and a plot with the distribution of transcription factor-binding loci relative to TSS.\n\n\n***Gene expression data downstream analysis***\n\n* The user does not choose the type of test that will be performed in **DESeq2** analysis. The appropriate test is chosen automatically:\n1.  if there are more than two values (levels) to a chosen **Covariate of interest** - LRT is used.\n2.  if the **Covariate of interest** has only two different values - Wald test is used to test for differential expression.\n\n* The **DESeq2** analysis report contains the list of input parameters, phenotype data table, a heatmap of input samples with cluster dendrogram, dispersion estimates plot and an MA plot showing the log2 fold changes attributable to a given gene over the mean of normalized counts and a short summary of results.\n\n### Common Issues and Important Notes\n\n***Downstream analysis of called peak sets***\n\n* **Input peak files** must have the **SampleID** metadata field set.\n\n***Gene expression data downstream analysis***\n\n* Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and it's field be populated in all the samples (**Expression data**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. Keep in mind that metadata keys are usually different from what is seen on the front-end. To match metadata keys to their corresponding values on the front-end please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). To learn how to add a custom metadata field to expression data files refer to the [following document](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#modifying-metadata-via-the-visual-interface).\n* Be careful when choosing covariates - generalized linear model fitting will fail if the model matrix is not full rank!\n* If your task fails with \"none of the transcripts in the quantification files are present in the first column of tx2gene\" message in the error log, and you are certain that you are using the proper GTF file - you can try rerunning the task with **ignoreTxVersion** option selected. This can happen if you, for example, download the transcriptome FASTA from the Ensembl website and use it to build aligner index - transcript version will then be included in transcript ID in the quantification output file, while in the GTF it's kept as a separate attribute so the transcript IDs will not match.\n\n### Performance benchmarking\n\n**Joint analysis of RNA-Seq and ATAC/ChIP-Seq** has been benchmarked for different input file sizes and quantities. Additional information about the number and sizes of **Input peak files**, **Input BAM files** and **Expression data** is listed in the table below. Testing was done using the default instance type. \n\n|Analysis type |File Sizes|Duration|Cost|Instance (AWS)|\n|:------:|:------:|:------:|:------:|:------:|\n|ChIP-Seq and RNA-Seq (human) |Input peak files (6) 1-1.9 MiB, Input BAM files (6) 4.3-6.3GB, Expression data (9) 9.7MiB|9min|$0.09|c4.2xlarge / r52.xlarge|\n|ATAC-Seq and RNA-Seq (human) |Input peak files (6) 5.7-6.4 MiB, Input BAM files (6) 4.3-6.3GB, Expression data (9) 9.7MiB|24min|$0.22|c4.2xlarge / r52.xlarge|\n|ATAC-Seq and RNA-Seq (mouse) |Input peak files(6) 1.6-2.9 MiB, Input BAM files (6) 770MiB-1GB, Expression data (6) 1.2MiB|11min|$0.10|c4.2xlarge / r52.xlarge|\n|ATAC-Seq and RNA-Seq (human) |Input peak files (24) 3.8-5.6 MiB, Input BAM files  (24) 2.2-4.9 GB, Expression data (19) 8.7 MiB|46min|$0.41|c4.2xlarge / r52.xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\u200b\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"in_peaks\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n\t\"in_bams\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n        \"in_abundances\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n\t\"in_gene_annotation\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n        \"in_metadata\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"analysis_name\": \"enter_name\",\n        \"factor\": \"enter_metadata_field\",\n        \"quantification tool\": \"enter_tool\",\n        \"gene_sets_db\": [\"enter_db_name\"]\n         }\n# Creates draft task\ntask = api.tasks.create(name=\"Joint analysis of RNA-Seq and ATAC/ChIP-Seq data - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\u200b\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\u200b\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References \n[1] [DiffBind v2.16.2](https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf)\n\n[2] [ChIPseeker v1.26.2](http://bioconductor.org/packages/devel/bioc/vignettes/ChIPseeker/inst/doc/ChIPseeker.html)\n\n[3] [DESeq2 v1.26.0](https://www.bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)\n\n[4] [GSEA v4.1.0](https://www.gsea-msigdb.org/gsea/index.jsp)\n\n[5] [Molecular Signatures Database v7.2](https://www.gsea-msigdb.org/gsea/msigdb/index.jsp)", "input": [{"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Input peak files"}, {"name": "Metadata sample sheet"}, {"name": "Input BAM files", "encodingFormat": "application/x-bam"}, {"name": "Analysis name"}, {"name": "Region Range of TSS"}, {"name": "TxDb annotation package"}, {"name": "Genome wide annotation"}, {"name": "Phenotype data"}, {"name": "Gene annotation", "encodingFormat": "application/x-gtf"}, {"name": "Expression data", "encodingFormat": "text/plain"}, {"name": "Gene sets local"}, {"name": "FDR cutoff"}, {"name": "Fit type"}, {"name": "Covariate of interest"}, {"name": "Turn off the independent filtering"}, {"name": "log2 fold change shrinkage"}, {"name": "Quantification tool"}, {"name": "Control variables"}, {"name": "Pre-filtering threhold"}, {"name": "ignoreTxVersion"}, {"name": "Grouping factor for collapsing technical replicates"}, {"name": "Factor level - test"}, {"name": "Factor level - reference"}, {"name": "Analysis type"}, {"name": "Gene sets database"}, {"name": "Min gene sets size"}, {"name": "Max gene sets size"}], "output": [{"name": "Annotated peaks"}, {"name": "Differential peaks analysis HTML report", "encodingFormat": "text/html"}, {"name": "DESeq2 HTML report", "encodingFormat": "text/html"}, {"name": "Differentially expressed genes"}, {"name": "Overlapped genes list"}, {"name": "GSEA ZIP report", "encodingFormat": "application/zip"}, {"name": "GSEA summary report", "encodingFormat": "text/html"}, {"name": "GSEA up-regulated gene sets report", "encodingFormat": "text/html"}, {"name": "GSEA down-regulated gene sets report", "encodingFormat": "text/html"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "ChIP-Seq", "ATAC-Seq"], "project": "SBG Public Data", "softwareVersion": ["v1.2", "v1.0"], "dateModified": 1648049433, "dateCreated": 1630425437, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-bustools-workflow/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-bustools-workflow/8", "applicationCategory": "Workflow", "name": "Kallisto BUStools Workflow", "description": "In the first step of the workflow, **Kallisto** performs pseudoalignment of the reads to a reference transcriptome and produces a BUS file format as an output. BUS is a binary representation of barcode and UMI sequences from single cell RNA-Seq reads, along with sets of equivalence classes of transcripts. In the second step, BUS files are processed with **BUStools**, a suite of tools that facilitates rapid quantification and analysis of single cell RNA-seq data [1,2].\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n - Provide a transcriptome reference to the **Transcriptome FASTA or Kallisto Index** input port to build the index archive using the **kallisto index** command. It is possible to skip this step in the workflow by providing a pre-build Kallisto index archive.\n - Provide raw sequencing reads to the **Input FASTQ files** input port to generate a BUS file with **kallisto bus** command.\n - If the whitelist file is provided on the **Barcode whitelist file** input port, the barcodes are corrected by processing the BUS file with the **\u200bbustools correct\u200b** command.\n - The BUS file is sorted with \u200b**bustools sort** commanda, where duplicate reads are collapsed into a single record and their abundance saved as a new metadata column in the BUS file named multiplicity.\n - If the capture file is provided on the **Capture list** input port and the Kallisto index file contains the intron sequences, bustools capture\u200b command can be used to produce spliced and unspliced matrices for RNA velocity.\n - Command **bustoools count\u200b** produces a matrix of gene counts per cell.\n\n\n### Changes Introduced by Seven Bridges\n\n - Included option **Create transcript to gene annotation** which enables translating Ensembl transcript IDs to gene IDs, required for processing single cell BUS file format. To use this parameter it is required to provide gene annotation file in GTF format.\n - We introduced an R script that outputs the cell-gene count matrix in .Rdata format if the option **Gene-level counts** (*--genecount*) is selected.\n - We set the default value for number of threads to 8 for **Kallsto BUS** and **BUStools sort**. This value can be changed by setting the **Number of threads** *(--threads)* option for both tools in the workflow.\n\n### Common Issues and Important Notes\n\n - To successfully execute the task, it is required to provide transcripts to genes mappings file to the **Transcripts to genes** input port. The alternative is to generate this file by providing an annotation file in GTF format to **GTF annotation** input port and selecting **Create transcript to gene annotation** optition.\n - BUStools suite of tools is created as a single tool in Common Workflow Language (CWL).\n - Bustools correct\u200b  is an optional step performed if \u200bwhitelist file is provided on **Barcode whitelist file** input port.\n - Bustools capture is an optional step performed if \u200bthe capture list file is provided on **Capture list** input port.\n - Cell-gene count file produced as Rdata object can be used in Seurat interactive analysis for identification of cell subpopulations and discovering the marker genes.\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the size of the Barcode and cDNA FASTQ files. Following table showcases the metrics for the task running on the c5.2xlarge on demand AWS instance. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n\nProtocol | Dataset | Fastq file 1 size(.gz) | Fastq file 2 size(.gz) | Duration | Cost | Instance type (AWS) |\n|--------|---------|---------------|-----------------|-----------|--------|-----|\n| 10x Chromium v3| 1k_v3 | 1.2 GB | 2.9 GB | 7 min. | $0.06 | c5.2xlarge |\n| 10x Chromium v2 | 4k_v2 | 7.6 GB | 22.4 GB | 25 min. | $0.21 | c5.2xlarge |\n| 10x Chromium v2| 8k_v2 | 12.9 GB | 49.8 GB | 50 min. |$0.43 | c5.2xlarge |\n| 10x Chromium v3| 10k_v3 | 11.4 GB | 27.3 GB | 41 min. | $0.35 | c5.2xlarge |\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n    \"in_reads\": list(api.files.query(project=project_id, names=['pbmc_1k_v3_S1_L001_R1_001.fastq.gz', \n                                                             \t'pbmc_1k_v3_S1_L001_R2_001.fastq.gz',\n                                                             \t'pbmc_1k_v3_S1_L002_R1_001.fastq.gz',\n                                                             \t'pbmc_1k_v3_S1_L002_R2_001.fastq.gz'])),\n    \"in_annotation\": api.files.query(project=project_id, names=['Homo_sapiens.GRCh38.97.chr_patch_hapl_scaff.gtf'])[0],\n    \"in_reference_or_index\": list(api.files.query(project=project_id, names=['Homo_sapiens.GRCh38.97.cdna.all.kallisto-0.46.0.index'])),\n    \"sequencing_technology\": \"10xv3\",\n    \"out_prefix\": \"pbmc_1k_v3\",\n    \"count_genecounts\": True,\n    \"tx_to_gene\": True\n\n}\ntask = api.tasks.create(name='Kallisto BUStools Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\n### References\n\n[1] [Modular and efficient pre-processing of single-cell RNA-seq](https://www.biorxiv.org/content/10.1101/673285v1)\n\n[2] [The barcode, UMI, set format and BUStools](https://www.biorxiv.org/content/10.1101/472571v2)", "input": [{"name": "Sequencing technology"}, {"name": "Number of threads"}, {"name": "Transcriptome FASTA or Kallisto Index", "encodingFormat": "application/x-fasta"}, {"name": "GTF annotation", "encodingFormat": "application/x-gtf"}, {"name": "K-mer length"}, {"name": "Make unique"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Create transcript to gene annotation"}, {"name": "Input FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Barcode whitelist file", "encodingFormat": "text/plain"}, {"name": "Capture list", "encodingFormat": "text/plain"}, {"name": "Number of threads"}, {"name": "Name of sorted BUS file"}, {"name": "Capture complement"}, {"name": "Capture transcripts"}, {"name": "Capture UMIs"}, {"name": "Capture barcodes"}, {"name": "Prefix name for count output"}, {"name": "Gene-level counts"}, {"name": "Count multimapping"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Output prefix"}, {"name": "Transcripts to genes"}], "output": [{"name": "Sorted BUS"}, {"name": "Compressed matrix"}, {"name": "Gene names", "encodingFormat": "text/plain"}, {"name": "Equivalence class", "encodingFormat": "text/plain"}, {"name": "Counts table"}, {"name": "Barcode names", "encodingFormat": "text/plain"}], "softwareRequirements": ["MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": [], "applicationSubCategory": ["Single Cell", "RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Pachter Lab", "softwareVersion": ["v1.0"], "dateModified": 1648468511, "dateCreated": 1575466039, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-pizzly-workflow/25", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-pizzly-workflow/25", "applicationCategory": "Workflow", "name": "Kallisto-Pizzly Workflow 0.43.1", "description": "The main purpose of the **Kallisto-Pizzly** workflow is to infer the maximum likelihood estimates of transcript abundances from RNA-Seq data using a process called **pseudoalignment** while also finding potential fusion genes as a direct result of the mentioned procedure. \n\n**Pseudoalignment** is the process of assigning reads to transcripts without doing the exact base-to-base alignment. The **Kallisto** tool implements a procedure, using a **transcriptome De Brujin graph** as the main data structure for the underlying algorithm, geared towards knowing the transcript from which a read originates rather than the actual mapping coordinates, since the former is crucial to estimating transcript abundances [1]. \n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilize the full likelihood model while obtaining near-optimal probabilistic RNA-seq quantification results. \n\nAs of version 0.43.1, **Kallisto** can output an additional file that contains information about reads that do not pseudoalign, which potentially means that they originate from fusion genes. This file can then further be processed by a tool called **Pizzly** (developed by some of the same authors that developed **Kallisto**), which reports candidate fusions by filtering false positives and assembling new transcripts from the fusion reads [2].\n \nThis workflow presents **Kallisto** and **Pizzly** tools combined, as the pseudoalignment process is quick enough to allow for searching for fusions candidates with **Pizzly**, with very little additional computation time, in addition to inferring transcript abundance estimates with **Kallisto**. \n\n*A list of **all inputs and parameters** with their corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Kallisto Index**, **Kallisto Quant** and **Pizzly**.\n- The main inputs to the workflow are **FASTQ read files** (single end or paired end). \n- A **Transcriptome FASTA file** also needs to be provided, in addition to a **GTF file** (which should be of the same annotations that were used in generating the **Transcriptome FASTA file**). \n- An already generated **Kallisto index file** can be provided to the **Kallisto Index** tool (**Transcriptome FASTA or Kallisto Index** input) in order to skip indexing and save some time. \n- Pizzly will not run by default (it will run in passthrough mode), as sometimes fusions are not of interest (if RNA-seq data is not cancer data, for example). To turn on **Pizzly** analysis, please specify the **Fusion** parameter (`--fusion`) inside the **Kallisto Quant** tool. \n- The workflow will generate transcript abundance estimates in plaintext (**Abundance plaintext**) and H5 (**Abundance H5**) formats (useful for processing by downstream tools for differential expression, like [Sleuth](https://pachterlab.github.io/sleuth/about)) as well as a file containing fusion candidates (**Fusions FASTA**), if specified. \n- A transcript **Expression matrix** will be generated if multiple samples are provided as an input. \n- The workflow is optimized to run in scatter mode. To run it successfully, just supply it with multiple samples (paired end or single end, with properly filled out **Sample ID** and **Paired End** metadata). \n\n### Changes Introduced by Seven Bridges\n\n- The plaintext abundance file will always be outputted by default (there is no need to specify the `--plaintext` parameter). \n- The `--insert-size` parameter value in **Pizzly** will automatically be filled in if not provided, as the **get_fragment_length.py** script provided by the tool author will in that case compute that value. \n- The `--kmer-length` parameter in **Pizzly** will automatically be filled in, as the information will be propagated from the **Kallisto Index** tool, through the use of metadata (again, if not specified by the user). \n- The options for specifying strandedness (`--fr-stranded` and `rf-stranded`) have been merged into a single options of enum type - **Strandedness**. \n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to properly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated kallisto index) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number**).\n- The GTF and FASTA files need to have compatible chromosome namings (i.e. >1, >2, ... or >chr1, >chr2, ...). \n- Outputting the **Pseudobam** file does not support multi-threading. So if you specify the **Pseudobam** (`--pseudobam`) option, only 1 core will be used (instead of the default multiple cores) and the overall speed of the execution might be slower.\n- If the location of the project where the workflow is running is set to Google US West, to run the task successfully, please reduce the default number of CPUs by setting the **Number of CPUs** parameter of the **SBG Pair FASTQs** to 32.\n\n### Performance Benchmarking\n\nThe main advantage of the Kallisto software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge (AWS) instance is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   11min   | $0.30| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   23min  | $0.50 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'gtf': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'transcriptome_fasta': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa'])),\n        'reference_fasta_file': list(api.files.query(project=project_id, names=['transcriptome_fasta_or_kallisto_index.ext']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Kallisto-Pizzly workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Kallisto paper](https://www.nature.com/articles/nbt.3519)   \n[2] [Pizzly paper](https://www.biorxiv.org/content/early/2017/07/20/166322)", "input": [{"name": "FASTQ read files", "encodingFormat": "text/fastq"}, {"name": "Transcriptome FASTA or kallisto index", "encodingFormat": "application/x-fasta"}, {"name": "Transcriptome FASTA for Pizzly", "encodingFormat": "application/x-fasta"}, {"name": "GTF file for Pizzly", "encodingFormat": "application/x-gtf"}, {"name": "K-mer length"}, {"name": "Output genetable"}, {"name": "Strandedness"}, {"name": "Pseudobam"}, {"name": "Output prefix"}, {"name": "Fusion"}, {"name": "Fragment length standard deviation"}, {"name": "Fragment length"}, {"name": "Bootstrap samples"}, {"name": "Bias"}, {"name": "Maximum number of parallel jobs"}], "output": [{"name": "Genetable", "encodingFormat": "text/plain"}, {"name": "Fusions FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Expression Matrix", "encodingFormat": "text/plain"}, {"name": "Run info json"}, {"name": "PseudoBAM or STDout", "encodingFormat": "application/x-sam"}, {"name": "Abundance plaintext"}, {"name": "Abundance H5"}], "codeRepository": ["https://github.com/pmelsted/pizzly"], "applicationSubCategory": ["RNA-Seq", "Gene Fusion", "Quantification"], "project": "SBG Public Data", "creator": "Nicolas L Bray, Harold Pimentel, P\u00e1ll Melsted and Lior Pachter", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468513, "dateCreated": 1514564523, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-pizzly-workflow-cwl-1-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-pizzly-workflow-cwl-1-0/11", "applicationCategory": "Workflow", "name": "Kallisto-Pizzly Workflow 0.46.0", "description": "The main purpose of the **Kallisto-Pizzly** workflow is to infer the maximum likelihood estimates of transcript abundances from RNA-Seq data using a process called **pseudoalignment** while also finding potential fusion genes as a direct result of the mentioned procedure. \n\n**Pseudoalignment** is the process of assigning reads to transcripts without doing the exact base-to-base alignment. The **Kallisto** tool implements a procedure, using a **transcriptome De Brujin graph** as the main data structure for the underlying algorithm, geared towards knowing the transcript from which a read originates rather than the actual mapping coordinates, since the former is crucial to estimating transcript abundances [1]. \n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilize the full likelihood model while obtaining near-optimal probabilistic RNA-seq quantification results. \n\n**Kallisto** can output an additional file that contains information about reads that do not pseudoalign, which potentially means that they originate from fusion genes. This file can then further be processed by a tool called **Pizzly** (developed by some of the same authors that developed **Kallisto**), which reports candidate fusions by filtering false positives and assembling new transcripts from the fusion reads [2].\n \nThis workflow presents **Kallisto** and **Pizzly** tools combined, as the pseudoalignment process is quick enough to allow for searching for fusions candidates with **Pizzly**, with very little additional computation time, in addition to inferring transcript abundance estimates with **Kallisto**. \n\n*A list of **all inputs and parameters** with their corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Kallisto Index**, **Kallisto Quant** and **Pizzly**.\n- The main inputs to the workflow are **FASTQ read files** (single end or paired end). \n- A **Transcriptome FASTA file** also needs to be provided, in addition to a **GTF file** (which should be of the same annotations that were used in generating the **Transcriptome FASTA file**). \n- An already generated **Kallisto index file** can be provided to the **Kallisto Index** tool (**Transcriptome FASTA or Kallisto Index** input) in order to skip indexing and save some time. \n- The workflow will generate transcript abundance estimates in plaintext (**Abundance plaintext**) and H5 (**Abundance H5**) formats (useful for processing by downstream tools for differential expression, like [Sleuth](https://pachterlab.github.io/sleuth/about)) as well as a file containing fusion candidates (**Fusions FASTA**), if specified. \n- A transcript **Expression matrix** will be generated if multiple samples are provided as an input. \n- The workflow is optimized to run in scatter mode. To run it successfully, just supply it with multiple samples (paired end or single end, with properly filled out **Sample ID** and **Paired End** metadata). \n\n### Changes Introduced by Seven Bridges\n\n- The plaintext abundance file will always be outputted by default (there is no need to specify the `--plaintext` parameter). \n- The `--insert-size` parameter value in **Pizzly** will automatically be filled in if not provided, as the **get_fragment_length.py** script provided by the tool author will in that case compute that value. \n- The `--kmer-length` parameter in **Pizzly** will automatically be filled in, as the information will be propagated from the **Kallisto Index** tool, through the use of metadata (again, if not specified by the user). \n- The options for specifying strandedness (`--fr-stranded` and `rf-stranded`) have been merged into a single options of enum type - **Strandedness**. \n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to properly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated kallisto index) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number**).\n- The GTF and FASTA files need to have compatible chromosome namings (i.e. >1, >2, ... or >chr1, >chr2, ...). \n- If the location of the project where the workflow is running is set to Google US West, to run the task successfully, please reduce the default number of CPUs by setting the **Number of CPUs** parameter of the **SBG Pair FASTQs** to 32.\n\n### Performance Benchmarking\n\nThe main advantage of the Kallisto software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge (AWS) instance is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   11min   | $0.30| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   23min  | $0.50 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'gtf': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'transcriptome_fasta': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa'])),\n        'reference_fasta_file': list(api.files.query(project=project_id, names=['transcriptome_fasta_or_kallisto_index.ext']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Kallisto-Pizzly workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Kallisto paper](https://www.nature.com/articles/nbt.3519)   \n[2] [Pizzly paper](https://www.biorxiv.org/content/early/2017/07/20/166322)", "input": [{"name": "FASTQ read files", "encodingFormat": "text/fastq"}, {"name": "Transcriptome FASTA or kallisto index", "encodingFormat": "application/x-fasta"}, {"name": "GTF file for Pizzly", "encodingFormat": "application/x-gtf"}, {"name": "Transcriptome FASTA for Pizzly", "encodingFormat": "application/x-fasta"}, {"name": "Column name"}, {"name": "Output file name"}, {"name": "Maximum number of parallel jobs"}, {"name": "Number of CPUs"}, {"name": "K-mer length"}, {"name": "Bias"}, {"name": "Bootstrap samples"}, {"name": "Fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Fusion"}, {"name": "Output prefix"}, {"name": "Pseudobam"}, {"name": "Strandedness"}, {"name": "Genomebam"}, {"name": "Output genetable"}], "output": [{"name": "Expression Matrix", "encodingFormat": "text/plain"}, {"name": "Abundance file in plaintext format"}, {"name": "Fusions FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Pseudoalignments", "encodingFormat": "application/x-bam"}, {"name": "HDF5 formatted abundance file"}, {"name": "Run info JSON"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/pmelsted/pizzly"], "applicationSubCategory": ["RNA-Seq", "Quantification", "Gene Fusion"], "project": "SBG Public Data", "creator": "Nicolas L Bray, Harold Pimentel, P\u00e1ll Melsted and Lior Pachter", "softwareVersion": ["v1.0"], "dateModified": 1648468512, "dateCreated": 1575463486, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-salmon-sleuth-0-30-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-salmon-sleuth-0-30-0/3", "applicationCategory": "Workflow", "name": "Kallisto/Salmon Sleuth Workflow 0.30.0", "description": "The main purpose of the **Kallisto/Salmon Sleuth 0.30.0** app is to provide an end-to-end workflow for differential expression analysis, with the option to choose the type of the pseudoaligner which will be used for the quantification step.\n\n**Pseudoalignment** is the process of assigning reads to transcripts without doing the exact base-to-base alignment. This workflow presents the **Sleuth** tool combined with the pseudoaligner of choice (**Kallisto** or **Salmon**). \n\nThe **Kallisto** tool implements a procedure, using a **transcriptome De Brujin graph** as the main data structure for the underlying algorithm, geared towards knowing the transcript from which a read originates rather than the actual mapping coordinates, since the former is crucial to estimating transcript abundances [1].\n\nThe **Salmon** tool infers transcript abundance estimates from RNA-seq data, using the **Selective Alignment (SA)** algorithm. **SA** is designed to remain as fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The **Salmon index** is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [2,3].\n\nRegardless of the pseudoaligner option chosen (**Kallisto** or **Salmon**), the result is a software running at speeds orders of magnitude faster than other tools which utilize the full likelihood model while obtaining near-optimal probabilistic RNA-seq quantification results.\n\nAfter transcript abundances are obtained, the normalization and test steps are performed by using the **sleuth** package.\n\n**Sleuth** is a program for analysis of RNA-Seq experiments for which transcript abundances have been quantified with **Kallisto** (or **Salmon**).\n\n**Sleuth** performs RNA-seq differential analysis of gene expression data that utilizes bootstrapping in conjunction with response error linear modeling to decouple biological variance from inferential variance. It relies on variance decomposition to identify biological differences in transcript or gene expression, while using a standard strategy of shrinkage to stabilize variance estimates from few samples.\n\n\n*A list of **all inputs and parameters** with their corresponding descriptions can be found at the bottom of the page.*\n\n\n### Common Use Cases\n\n- The workflow consists of the following steps: **SBG Choose Pseudoaligner**, **Kallisto Quant v0.46.0**, **Salmon Quant - Reads v1.0.0**, **SBG Create Expression Matrix CWL1.0** and **Sleuth**.\n- The main input to the workflow is the **list of FASTQ files** (single end or paired end).\n- **Kallisto** or **Salmon** index file also needs to be provided. If the index file of interest is not available, it can be generated from transcriptome reference file (**FASTA**) with the **Kallisto Index** or the **Salmon Index** tool, which are also available on the **Seven Bridges Platform**. If the **Salmon Index** tool is used, it is recommended to also provide the genome reference file to the tool so that it can create a hybrid index compatible with the improved mapping algorithm named **Selective Alignment (SA)**.\n- **Pseudoaligner** type is a required parameter. The two available options are **Kallisto** and **Salmon**.\n- The only required parameter of the **Sleuth** tool is the **Covariate of interest** (`--factor=`). The samples will be grouped according to the chosen variable of interest.\n- The parameter **Select level of DE analysis** (`--geneOrTranscriptLevel=`) allows the user to choose between **gene-level analysis** and **transcript-level analysis**. The default is **gene-level**.\n-  If gene-level analysis is selected, then the **Sleuth** analysis can be run either for human or mouse samples. Depending on the species the samples are coming from, the appropriate **BioMart** database needs to be used for adding the information about the genes transcripts are associated with. The parameter **Select species** (`--species=`) allows the user to choose between the two options (human and mouse) and **Sleuth** will use this input to collect the necessary information about genes from the corresponding **BioMart** database. The default value for this parameter is **human**. \n- The **Kallisto/Salmon Sleuth** workflow will generate a HTML report containing the results of the Sleuth analysis, normalized counts TXT file, workspace image (RData file), as well as the output CSV file of **Sleuth** analysis results. The workflow contains the **SBG Create Expression Matrix CWL1.0** tool which takes multiple abundance estimates files outputted by **Kallisto Quant** or **Salmon Quant - Reads** and creates a single expression counts matrix, based on the input column that the user specifies (the default is 'tpm', but any other string can be input here, like 'fpkm', 'counts' or similar), that can be used for further downstream analysis.\n- The workflow is optimized to run in scatter mode. To run it successfully, just supply it with multiple samples (paired end or single end, with properly filled out **Sample ID** and **Paired End** metadata). In case that input FASTQ files are paired-end, **SBG Choose Pseudoaligner** tool will group them into separate lists.\n\n\n### Changes Introduced by Seven Bridges\n\n* The analysis report contains the list of input parameters, the phenotype data table, plot projections of samples onto the principal components for a data set, a density plot of condition grouping, a mean-variance relationship of transcripts plot, a histogram of adjusted p-values and a short summary of the results.\n* As previously mentioned, **Salmon** results will automatically be preprocessed with the **Wassabi** package, as long as they are provided in the form of TAR archives (containing all outputs produced by **Salmon**). \n\n### Common Issues and Important Notes\n\n* Make sure that the transcriptome index file was generated either by the Kallisto Index tool or the Salmon Index tool. \n* Keep in mind that, regardless of the pseudoaligner option chosen, the number of bootstraps parameter (for **Kallisto Quant** - **Bootstrap samples** (`-b`) , for **Salmon Quant - Reads** - **Number of bootstraps** (`--numBootstraps`)) for the chosen pseudoaligner should be set to the desired value.\n* In the case of single-end reads, it is important to provide the information about the estimated average fragment length and the estimated standard deviation of fragment length to the selected quantification tool. The **Kallisto Quant** tool has parameters **Fragment length** (`-l`) and **Fragment length standard deviation** (`-s`) which are required for single-end reads, while the **Salmon Quant - Reads** tool's parameters **Maximum fragment length** (`--fldMax`) and **Mean fragment length** (`--fldMean`) are optional (will be set to default values if not provided), but it is recommended to set these parameters in accordance with the provided reads.\n* If phenotype data is read from the metadata, any metadata key entered in the **Covariate of interest** or **Control variables** field needs to exist and its field needs to be populated in all the samples provided on the **Expression data** input. \n* If phenotype data is read from the provided CSV file - the **Covariate of interest** keys need to match the column names in the CSV\u2019s header. \n* Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform).\n* To learn how to add a custom metadata field to expression data files refer to the [following document](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#section-modifying-metadata-via-the-visual-interface).\n* Be careful when choosing covariates - generalized linear model fitting will fail if the model matrix is not a full rank matrix!\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. The file names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_index': list(api.files.query(project=project_id, names=['GRCh38ERCC.ensembl97.transcriptome.kallisto-0.46.0.index'])),\n        'title': 'Test_Kallisto_Branch_API',\n        'factor': 'sample_type',\n        'geneOrTranscriptLevel': 'gene',\n        'species': 'human',\n        'pseudoaligner': 'Kallisto',\n        'bootstrap_samples': 30\n        }\n\n# Run the task\ntask = api.tasks.create(name='Kallisto - Salmon Sleuth workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### Performance Benchmarking\n\nThe main advantage of the pseudoalignment approach is that it is not computationally challenging, as alignment in the traditional sense is not performed. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| # of samples | File sizes | Paired-end | # of reads | Read length | Pseudoaligner | Duration | Cost | Instance (AWS) |\n|:---------------:|:-----------:|:--------------:|:----------:|:----------------:|:-----------------:|:-------------:|:-----:|:---------------------:|\n|     6    |  24.0MB - 34.2MB  |     Yes    |     0.3M - 0.4M     |     151     |   Kallisto v0.46.0    |   6min   | $0.17 |  c4.8xlarge (2048 GB)  |\n|     6    |  24.0MB - 34.2MB  |     Yes    |     0.3M - 0.4M     |     151     |   Salmon v1.0.0 (SA)    |   31min   | $0.83 |  c4.8xlarge (2048 GB)  |\n|     6    |  2.2GB - 2.4GB  |     No    |     28.0M - 29.8M      |     100     |   Kallisto v0.46.0    |   17min   | $0.46 |  c4.8xlarge (2048 GB)  |\n|     6    |  2.2GB - 2.4GB  |     No    |     28.0M - 29.8M      |     100     |   Salmon v1.0.0 (SA)    |   46min   | $1.23 |  c4.8xlarge (2048 GB)  |\n|     10    |  20.0GB - 41.3GB  |     Yes    |     60.7M - 125.4M     |     101     |   Kallisto v0.46.0    |   4h 29min   | $7.14 |  c4.8xlarge (2048 GB)  |\n|     10    |  20.0GB - 41.3GB  |     Yes    |     60.7M - 125.4M     |     101     |   Salmon v1.0.0 (SA)    |   6h 37min   | $10.53 |  c4.8xlarge (2048 GB)  |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n\n### References\n\n[1] [Kallisto paper](https://www.nature.com/articles/nbt.3519)   \n[2] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[3] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)", "input": [{"name": "List of FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Kallisto/Salmon index file", "encodingFormat": "application/x-tar"}, {"name": "Bootstrap samples"}, {"name": "Fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Number of bootstraps"}, {"name": "Phenotype data"}, {"name": "Column name"}, {"name": "Output file name"}, {"name": "Pseudoaligner"}, {"name": "Analysis title"}, {"name": "Covariate of interest"}, {"name": "FDR cutoff"}, {"name": "Number of CPUs"}, {"name": "Control variables"}, {"name": "Skip Sleuth execution"}, {"name": "Select species"}, {"name": "Select level of DE analysis"}], "output": [{"name": "RData files"}, {"name": "Sleuth analysis results"}, {"name": "Normalized counts", "encodingFormat": "text/plain"}, {"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Expression matrix", "encodingFormat": "text/plain"}], "softwareRequirements": ["ScatterFeatureRequirement", "MultipleInputFeatureRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/kallisto-salmon-sleuth-0-30-0/3.png", "codeRepository": [], "applicationSubCategory": ["Transcriptomics", "Quantification", "Differential Expression", "CWL1.0"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1584705545, "dateCreated": 1582907054, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/lobstr-pcr-noise-model-training/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/lobstr-pcr-noise-model-training/7", "applicationCategory": "Workflow", "name": "lobSTR PCR noise model training", "description": "The **lobSTR PCR noise model training** workflow is used for training PCR stutter and step models that will be later used in the **Microsatellite instability profiling workflow**. It contains tools from **lobSTR 4.0.6** toolkit [1].\n\nThe **lobSTR PCR noise model training** workflow requires the following input files and parameters:\n\n* **Input reads** (`--p1` and `--p2`) - Reads in FASTQ format (gzipped input accepted). If providing paired-end reads, then in two separate FASTQ files. Please make sure to set the appropriate metadata fields accordingly.\n\n* **lobSTR index bundle** - A **lobSTR** reference and index TAR bundle, pre-built and distributed with the **lobSTR** package. The chosen bundle must match the reference genome assembly used for read alignment. For the UCSC HG19 genome build we recommend \"hg19.lobSTR_v3.0.2.ref.tar\" as an index, which can be found on the Seven Bridges platform in the Public Reference Files section.\n\n* **Chromosomes for homozygous calls** (`--haploid`) - A list of chromosomes that are haploid in string format. For example, in case of human male samples, this parameter would be set to \"chrX,chrY\". This list must be nonempty, because **lobSTR** trains it's models only on haploid chromosomes.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nThe **lobSTR PCR noise model training** workflow can be used to train a custom noise model, which specifies the probability of observing PCR stutter at a given locus based on sequence properties (motif length, total STR length, GC content, and STR purity) and the expected step size distribution. The model needs to be trained using data from haploid chromosomes. As for these, there should be a single allele present and reads supporting an allele other than the model allele are likely due to stutter noise. Essentially, model training requires a deep coverage human male genome, or a genome with haploid chromosomes if a different species are concerned.\n\nTherefore, the workflow should be run only using the largest male normal sample from a particular data submitting center and sequencing platform. These models should later be used for all samples that are from the same data submitting center and sequencing platform. Model files for illumina PCR-free libraries can be found in the Seven Bridges Public Reference Files section as \"illumina_v3.pcrfree.stepmodel\" and \"illumina_v3.pcrfree.stuttermodel\".\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n* Since the models are trained only on haploid chromosomes, if all available reads are human female samples (which can happen in case of endometrial cancer, for example), then this model cannot be trained. In case of illumina PCR-free libraries, as mentioned above, stepmodel and stuttermodel files can be found among Public Reference Files as \"illumina_v3.pcrfree.stepmodel\" and \"illumina_v3.pcrfree.stuttermodel\". But in any other case, male samples would need to be obtained so the models can be trained.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\n\n### Performance Benchmarking\n\nThe instance is set to c4.8xlarge (AWS). The reason for this is that the workflow often fails when using smaller instances on data that is even as small as 10GB per FASTQ paired end. Additionally, due to the fact that tasks on this instance last shorter than on smaller instances, c4.8xlarge (AWS) has proved to be optimal for this use case.\n\nIn the following table you can find estimates of running times and costs. All samples are aligned onto the hg19 human reference genome, and using the appropriate **lobSTR** index.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*    \n\n#####Example run:\n\n| Experiment type | Input size | Paired-end | Read length | Duration | Cost | Instance (AWS) | Instance type |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|------------|\n| WES         | 2 x 7 GB     | Yes        | 100         | 15min.   | $0.40            | c4.8xlarge      | On-demand        |\n| WES         | 2 x 65 GB     | Yes        | 100         | 1h 44min.   | $2.91            | c4.8xlarge      | On-demand        |\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    'index': api.files.query(project=project_id, names=['hg19.lobSTR_v3.0.2.ref.tar'])[0], \n    'reads': list(api.files.query(project=project_id, names=['C835.HCC1143_BL.4.converted.pe_1.fastq', 'C835.HCC1143_BL.4.converted.pe_2.fastq']))\n    'haploid' = 'chrX,chrY'\n}\ntask = api.tasks.create(name='WES Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [lobSTR: A short tandem repeat profiler for personal genomes. Gymrek et al.](http://genome.cshlp.org/content/22/6/1154.long)", "input": [{"name": "lobSTR index bundle", "encodingFormat": "application/x-tar"}, {"name": "Input reads", "encodingFormat": "application/x-bam"}, {"name": "Chromosomes for homozygous calls"}], "output": [{"name": "Step model"}, {"name": "Stutter model"}], "codeRepository": [], "applicationSubCategory": ["Microsatellites"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155967, "dateCreated": 1512152024, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/mbased-workflow/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/mbased-workflow/4", "applicationCategory": "Workflow", "name": "MBASED Workflow", "description": "The **MBASED** (Meta-analysis Based Allele-Specific Expression Detection) **workflow** detects allele-specific gene expression from RNA count data. It offers VEP annotation of variants from a normal VCF file, as well as phasing them in order to get more specific results of the **MBASED tool**, which is the basis of this workflow. \n\nTo robustly estimate gene-level ASE from SNV-level RNA-Seq read counts, **MBASED Workflow**  employs a meta-analytic approach that can be used in both one-sample and two-sample analyses, making **MBASED** a versatile tool for investigating allele-specific expression, both within an individual sample and in the context of differential ASE [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases ###\n\n\n* This workflow takes an annotation file (on the **Input GTF File** input) and uses the **SBG GTF VEP Preprocess** tool to format a GTF into a proper GTF.GZ input for **Variant Effect Predictor** (VEP) tool. Then **VEP** takes the input VCF and the resulting GTF.GZ to annotate variants with the genes and regulatory features they hit. **VEP** requires two more inputs - cache file for the chosen species (**Species cache file**) and a FASTA file or a directory containing FASTA files to use to look up reference sequence..The **VEP** annotated VCF file is then passed to the **Beagle 4.1** where phasing takes place. The phased VCF is then passed to the **MBASED tool** that performs ASE (Allele-Specific Expression) analysis.\n* The **MBASED** tool produces two TSV outputs - one containing the Major Allele Frequency value, P-value and P-value for heterogeneity of ASE (the **ASE per gene** output), and the other containing additional information about each mutation detected (the **SNPs** output).\n* **Execution mode** parameter has to be set before the execution. Use **gt** if input VCF file contains a GT (genotype) format field for each marker. Use **gl** if your input VCF file contains a GL or PL (genotype likelihood) format field for each marker. Any data in the GT format field will be ignored. Use **gtgl** if input VCF file contains a GT, GL or PL format field for each marker. If a genotype is non-missing, **Beagle 4.1** will ignore the genotype likelihood. If both GL and PL format fields are present for a marker, the GL field will be used.\n\n\n### Changes Introduced by Seven Bridges ###\nNone\n\n\n### Common Issues and Important Notes ###\n- Each **MBASED Workflow**'s input file is required.\n- The **Execution mode** parameter is required and should be set appropriately, according to the VCF file at the input of the workflow, the way described in the Pipeline Design section.\n- There is no metadata field whose absence would cause the task failure.\n- It is also not necessary to provide a FAI file in order to run **MBASED Workflow** correctly.\n\n\n### Performance Benchmarking ###\n\n\nBelow is a table describing the runtimes and task costs for different file sizes:\n\n | VCF File Size | Duration | Cost   | Instance (AWS) |\n|---------------|----------|--------|----------------|\n| 6.8 MB         | 14m 1s | $0.08| c4.2xlarge     |\n| 10.7 MB | 14m 42s | $0.09  | c4.2xlarge     |\n| 15.7 MB          | 17m 16s  | $0.10\t  | c4.2xlarge     |\n| 6.8 MB        | 9m 26s  | $0.13 | c4.8xlarge     |\n| 6.8 MB        | 15m 51s  |$0.16 | m4.2xlarge     |\n\n Changing instances (increasing CPUs used) speeds up the execution of MBASED workflow while increasing RAM does not improve performance. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the Knowledge Center for more details.*\n\n\n### API Python Implementation ###\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"cache_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_references\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])), \n\t\"in_variants\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_gtf\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"execution_mode\": \"gt\"}\n# Creates draft task\ntask = api.tasks.create(name=\"MBASED Workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References ###\n\n[1] Mayba, O., Gilbert, H.N., Liu, J. et al. MBASED: allele-specific expression detection \\ \nin cancer tissues and cell lines. Genome Biol 15, 405 (2014). \\\n[https://doi.org/10.1186/s13059-014-0405-3](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0405-3)", "input": [{"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Species cache file", "encodingFormat": "application/x-tar"}, {"name": "Fasta file", "encodingFormat": "application/x-fasta"}, {"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Output html report"}, {"name": "Input GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Execution mode"}], "output": [{"name": "SNPs"}, {"name": "HTML report", "encodingFormat": "text/html"}, {"name": "ASE per gene"}, {"name": "Phased variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-vep"], "applicationSubCategory": ["RNA-Seq", "Allele Specific Expression"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648468511, "dateCreated": 1612264152, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/metagenomics-profiling-kraken2/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/metagenomics-profiling-kraken2/8", "applicationCategory": "Workflow", "name": "Metagenomics Profiling - Kraken2", "description": "**Metagenomics Profiling - Kraken2** workflow is used for metagenomic classification, abundance estimation and visualisation. The workflow assigns taxonomic labels to DNA sequences, computes the abundance of species in metagenomics samples and visualises relative abundances and confidences within the complex hierarchies of metagenomic classifications. It allows users to assign reads from their samples to a likely species of origin and quantify each species' abundance in analyzed sample. Classification of sequences is performed by **Kraken2** [1]. The reference **Kraken2 database** is required for classification, and it has to be generated using the **Kraken2 Build** tool, available in the Public Apps gallery. **Kraken2 Build** can generate a database, including library and taxonomy files, for different reference genomes. Depending on the users' needs, the *standard* build can generate a database with most commonly used reference genomes, but can also build following databases: one of the *special* databases (16S databases), database for pre-specified organisms from NCBI or build a database from specific libraries provided on the input. After sequence classification, the species' abundance is estimated by **Bracken** using the generated **Kraken2** report with aggregate counts/clade and **Bracken database**, built using the **Bracken build** tool, available in the Public Apps gallery. Building of a **Bracken database** requires the specific **Kraken2 database** to be provided as an input to the **Bracken build** tool. **Kraken2 output file** is forwarded to **Krona** for visualisation of relative abundances and confidences within the complex hierarchies of metagenomic classifications. The workflow output is provided as text files that contain aggregate counts/clade (**Kraken2 report**), abundance estimation (**Bracken output**) and each sequence (or sequence pair, in the case of paired reads) classified by Kraken 2 (**Kraken2 output file**). There is also a Krona chart report, an HTML file with circular **Krona** [6] interactive graphs. Optionally, the workflow can output files with classified and unclassified sequences (**Kraken2 unclassified sequences** and **Kraken2 classified sequences**).\n\nThe workflow takes three inputs:\n\n- Metagenomic samples, a list of files in FASTQ (FASTQ, FASTQ.GZ, FQ, FQ.GZ) format, with their paired-end and sample ID metadata set. This workflow will produce a report for each sample provided and one **Krona** interactive graph with all samples;\n- A reference **Kraken2 database** in TAR.GZ format; generated by **Kraken2 Build**.\n- A **Bracken database** in TAR.GZ format; generated by **Bracken Build** using reference **Kraken2 database**.\n\\\n\\\nThe tool versions used in the workflow are: Kraken2 2.0.9, Bracken 2.5 and Krona 2.7.1.\n\\\n\\\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\\\n\\\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n \n**Metagenomics Profiling - Kraken2** workflow can be used to classify input sequences in FASTQ (FASTQ, FASTQ.GZ, FQ, FQ.GZ) format using a database generated by **Kraken2 Build**, compute the abundance of species in metagenomics samples using the **Bracken database** generated by **Bracken build** and report from **Kraken2** and visualize the metagenomics profiles from **Krona** results. Optionally, the workflow can output files with classified and unclassified sequences, if defined on the `--classified_out` and `--unclassified_out` inputs.\\\n\\\n*For running the workflow the following inputs are required:*\n- **Kraken2 database** - built using the **Kraken2 Build** tool.\n- **List of FASTQ files**  - Input reads data in FASTQ format containing the sequences to be classified, with **Sample ID** metadata fields set (required).\n- **Bracken database** - built using the **Bracken Build** tool.\n- **Kraken2 database name** - name of the database to be used for classifying, which has to be present in the provided Kraken2 database archive.\n- **File name for the report with aggregate counts/clade** has to be provided on the input.\\\n*Optional inputs:*\n- **File name for classified sequences** and **File name for unclassified sequences** must be defined on the input in order for the workflow to generate that output, otherwise the workflow will not generate those files. The default extension for those files is `.fastq`. The extension for those files can be changed by providing the specific extension on the **Extension for classified output file** and **Extension for unclassified output file** inputs.\\\n\n### Changes Introduced by Seven Bridges\n\n- **Kraken2 database** and **Bracken database** inputs,  containing the database top-level folder, are expected to be provided as TAR.GZ archives.\n- For **Bracken**, it is not allowed to specify output name as it is built from the **Kraken report**  input name, with the addition of the .bracken extension.\n- A taxonomy database for Krona is built at the beginning of the analysis.\n- For **Krona**, Column of taxonomy ID (`-t`) and Column of query ID (`-q`) parameters are set to 3 and 2 by default, respectively.\n\n### Common Issues and Important Notes\n\n- FASTQ files must have **Sample ID**  and **Paired-end** (if applicable) fields set correctly in their metadata for scatter and processing to work as intended.\n- When provided with a number of files with different sample IDs on the **List of FASTQ files** input, this will result in one Krona chart `krona_report.html` on the output with charts for each sample that can be chosen from the drop-down menu with sample IDs.\\\n- **Memory per job [MB]** must be set accordingly depending on the size of input **Kraken2 database**. Kraken2 stores the whole database in RAM memory, so this input must be set with enough free memory to hold the database. The default value which **Kraken2** will use is 500.\n- **Use smart resource allocation** input when set to `True` will automatically set **Memory per job [MB]** equal to the size of the Kraken2 database. **IMPORTANT**: use this option carefully as it might allocate a bigger instance, which in turn could lead to increased costs.\n- Default instance hint for the **Kraken2** tool is r5.2xlarge with 64GB RAM, which is optimal for running the workflow with a 61.3GB **Kraken2 database**.\n- The workflow is set up to process files in parallel (scatter mode) when provided with a number of files with different sample IDs on the **List of FASTQ files** input. In order to run more than one sample in parallel, it is important to set the appropriate instance hint for the **Kraken2** tool. Depending on the size of the **Kraken2 database**, each parallel job would need **Memory per job [MB]** equal to the database size or more (by default, the workflow will set it to 500), and depending on the total RAM on the specified instance it will scatter one or more jobs in parallel and the rest would execute consecutively after the previous one is completed (for example: if a **Kraken2 database** is 60GB, with four FASTQ files on the input, **Memory per job [MB]** is set to 62000 and instance hint is set to r5.4xlarge with 124GB of RAM, the task will run with two jobs in parallel and after they complete it will run the other two). Number of parallel jobs also depends on the number of available cores on the instance. So, even if there is enough RAM for more jobs, the maximum number of parallel single-thread jobs can't be greater than the number of available CPU cores.\n\n\n\n### Performance Benchmarking\n\n* Performance of the workflow greatly depends on the **Kraken2 database** size and the selected resources.\n\nThe sample sets mentioned in the table are:\n\n1. Big files and standard DB:\n   * Paired-end FASTQ files SRS013948 (3.8GB each) from Human Microbiome Project with standard Kraken2 database (size: 61.3GB)\n2. Big files and viral DB:\n   * Paired-end FASTQ files SRS013948 (3.8GB each) from Human Microbiome Project with viral Kraken2 database (size: 5.5GB)\n3. Small files and bacterial DB:\n   * Paired-end FASTQ files SRS050007 (131.3 MB each) from Human Microbiome Project with bacterial Kraken2 database (size: 61.3GB)\n4. 34 files and standard DB:\n   * 34 paired-end FASTQ files for 17 samples from Human Microbiome Project with standard Kraken2 database (size: 61.3GB)\n5. 20 16S files and special DB:\n   * 20 FASTQ files of 16S sequences (13.3 to 28 MB per file) with special Greengenes Kraken2 database (size: 338.6MB)\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Big files and standard DB | 1 h 38 min |$0.72 + $0.23 | c5.2xlarge and c4.2xlarge - 1024 GB EBS | \n| Big files and viral DB | 26 min | $0.19 + $0.06 | c4.2xlarge and r5.2xlarge - 1024 GB EBS |  \n| Small files and bacterial DB | 1 h 16 min | $0.74 + $0.18 | c4.2xlarge and r5.4xlarge - 1024 GB EBS |  \n| 34 files and standard DB | 8 h 37 min | $7.71 + $1.20 | c4.2xlarge and r5.4xlarge - 1024 GB EBS |  \n| 20 16S files and special DB: | 10 min | $0.08 + $0.02 | c4.2xlarge and r5.4xlarge - 1024 GB EBS |  \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project.\ninputs = {\n    \"fastq_list\": [\"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\", ... ],\n    \"kraken2_database\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"in_bracken_db\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"database_name\": \"name_of_database\",\n    \"report\": \"report_name\",\n    \"mem_per_job\": 60000,\n    \"classified_out\": \"classified_sequences_name\",  \n    \"unclassified_out\": \"unclassified_sequences_name\"\n}\ntask = api.tasks.create(name='Metagenomics Profiling - Kraken2 workflow - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Kraken2 manual](https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown)\\\n[2] [Kraken2 publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0)\\\n[3] [Bracken git page](https://github.com/jenniferlu717/Bracken/)\\\n[4] [Bracken webpage](https://ccb.jhu.edu/software/bracken/)\\\n[5] [Bracken publication](https://peerj.com/articles/cs-104/)\\\n[6] [Krona publication](https://pubmed.ncbi.nlm.nih.gov/21961884/)", "input": [{"name": "Bracken database", "encodingFormat": "application/x-tar"}, {"name": "File name for the report with aggregate counts/clade"}, {"name": "List of FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Kraken2 database", "encodingFormat": "application/x-tar"}, {"name": "Kraken2 database name"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "File name for unclassified sequences"}, {"name": "File name for classified sequences"}, {"name": "Extension for classified output file"}, {"name": "Extension for unclassified output file"}, {"name": "Use smart resource allocation"}], "output": [{"name": "Output abundance estimation"}, {"name": "Unclassified sequences", "encodingFormat": "application/x-fasta"}, {"name": "Report with aggregate counts/clade"}, {"name": "Kraken2 output file"}, {"name": "Classified sequences", "encodingFormat": "application/x-fasta"}, {"name": "Krona chart", "encodingFormat": "text/html"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/DerrickWood/kraken2", "https://github.com/jenniferlu717/Bracken/", "https://github.com/marbl/Krona"], "applicationSubCategory": ["Metagenomics", "Taxonomic Profiling"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648561064, "dateCreated": 1612269282, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/metagenomics-wgs-analysis-centrifuge-1-0-3/20", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/metagenomics-wgs-analysis-centrifuge-1-0-3/20", "applicationCategory": "Workflow", "name": "Metagenomics WGS analysis -  Centrifuge 1.0.3", "description": "**Metagenomics WGS analysis -  Centrifuge 1.0.3** is a workflow for analyzing metagenomic samples against a custom reference, allowing researchers to assign reads in their samples to a likely species of origin and quantify each species\u2019 abundance in the sample. Alignment and classification are performed by **Centrifuge classifier** [1]. You can use our default, publicly available references which can be found on the Seven Bridges platform under Public Reference Files, or build your own reference from NCBI using the **Reference Index Creation** workflow or **Centrifuge Build** tool.  Publicly available references are *bacteria_archaea.tar* (which consists of all the complete genomes for bacteria and archaea), *bacteria_archaea_viruses.tar* (which consists of all the complete genomes for bacteria, viruses, and archaea) and *nt.tar.gz* (that consists of all genomes available in NCBI's nt database). Alignment and classification are then performed by **Centrifuge Classifier**. The output is provided as circular trees and bar charts (**HTML report for all samples**), as well as text files that contain both summary metrics per species (**Centrifuge report**) and the classification of individual reads (**Classification results**). There is also **Krona chart report**, an HTML file with circular Krona [2] interactive graphs.\n\nOutput files containing estimated read counts (**Counts table**) and taxonomy information (**Taxonomy table**) are meant to be used in **Microbiome Differential Abundance Analysis**, which analyzes differential abundance of species between samples using **metagenomeSeq** R package [3]. This analysis could be copied from [Data Cruncher Interactive Analyses](https://igor.sbgenomics.com/u/sevenbridges/data-cruncher-interactive-analyses/analysis/cruncher) public project and executed within Jupyter Notebook using our [Data Cruncher](https://docs.sevenbridges.com/docs/run-an-analysis-using-data-cruncher).\n\nThe workflow takes two inputs:\n\n- **Metagenomic samples**, a list of FASTQ or FASTQ.GZ files, with their paired-end and sample ID metadata set. This workflow will produce a report for each sample provided, and it is more efficient to run this with all your samples than to create a batch task (see **Common Issuses** below);\n- A **Centrifuge Reference Index** in TAR format; for large indexes the TAR.GZ format can be used, however, in that case we suggest the user manually set the required memory for Centrifuge classifier job by changing the value for the **Memory per job** parameter (see **Common Issuses** below).\n\nA list of all inputs and parameters with corresponding descriptions can be found at the end of the page.\n\n### Common Use Cases\n\nThis workflow is designed to analyze several metagenomics samples in parallel (scattered by **Sample_ID**). \nThe user should provide FASTQ files for a desired number of samples and one Centrifuge Reference Index file (in TAR or TAR.GZ format). If the samples are host-associated, it is presumed that the reads are already cleaned from the host sequences (as done in [Human Microbiome Project](https://hmpdacc.org/hmp/doc/HumanSequenceRemoval_SOP.pdf)).\n\nA user can upload their own index, or they can use the **Reference Index Creation workflow** from the SBG platform for creating one by downloading reference sequences from NCBI directly. Providing an index with a smaller number of organisms (for example, in cases when the user is just interested in detecting one particular species) can result in mis-calculated abundance of organisms within the sample.\n\nAfter the workflow has successfully completed, it outputs Ccentrifuge results and reports, Kraken-like reports, alignment metrics, circular graphs and bar charts for each sample, and one final HTML report for the whole analysis.\n\n\n### Changes Introduced by Seven Bridges\n\n* **Centrifuge Classifier** option `--met-file` does not work properly, so the basic alignment (classification) metrics are calculated with an in-house script called **SBG Centrifuge-Alignment Metrics**. It outputs the total number of reads, the number of classified and unclassified reads, and those classified to exactly one category.\n\n\n### Common Issues and Important Notes\n\n* The Metadata field **Sample_ID** must be set for all files found on the **Metagenomic samples** input node. This info is important for pairing files and scattering the whole pipeline, as well as for creating reports.\n* The maximum number of samples that can be analyzed with this workflow is limited by the available disk space. Based on our experience, the workflow uses approximately  2.8 times more disk space than the size of all the input files (**Metagenomic samples** and **Centrifuge Reference Index** ). By default, the storage of 1 TB is available for this workflow, which means that maximum overall size of input files should not exceed 350 GB.\n* If the index is in TAR.GZ format, the memory for Centrifuge Classifier should be set manually by changing the default value for the **Memory per job** parameter (in MB). Based on our experience, it would be enough to use twice as much memory as the size of the index file and with an additional 4GB overhead. For example, if the size of the index file is 8GB, the user should use 8 x 2 + 4 = 20GB, that is 20 x 1024 = 20480MB.\n\n### Performance Benchmarking\n\n**Centrifuge Classifier** needs enough memory (based on our experience 4GB more than the size of the index files is suggested) in order to work properly. If the index is in TAR format, the tool will automatically allocate the required memory size. However, if the index is in TAR.GZ, the compression ratio is not always the same, so we suggest in that case the user manually set the required amount of memory necessary for the Centrifuge classifier job by changing the default value for the **Memory per job** parameter. This way, using instances with more memory than needed or task failures would be avoided.\n\nBased on our experience, we chose to manually set the **r4.4xlarge** instance for running the Metagenomics WGS analysis workflow, because it has the most appropriate ratio of CPU cores to RAM for Centrifuge Classifier and a good cost-to-value ratio. The instance could be changed by the user if necessary. To find out how to change the instance, please refer to the [documentation](https://docs.sevenbridges.com/docs/set-computation-instances#section-set-the-instance-type-for-a-workflow). \n\nIn the following table you can find estimates of **Metagenomics WGS analysis - Centrifuge 1.0.3** running time and cost.  *Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Experiment type | Input size | Duration | Cost | Instance (AWS)\n| --- | --- | --- | --- | --- |\n|Bacteria index, one throat sample|Index 13 GB, reads SRS013948 throat sample 2 x 3.8 GB| 26m| $ 0.59 | r4.4xlarge|\n|p_h_v.tar index, 17 samples (34 fastq files)|Index 6.9 GB, FASTQ files ranging from 130 MB up to 3.8 GB| 1h 1m| $ 1.25 | r4.4xlarge|\n|Large index file - nt.tar.gz, one throat sample|Index 63.9 GB zipped, reads SRS013948 throat sample 2 x 3.8 GB | 1h 32m | $ 3.4 | r4.8xlarge|\n\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n    \"centrifuge_index_archive \": api.files.query(project=project_id, names=['bacteria.tar'])[0],\n    \"fastq_list\": list(api.files.query(project=project_id, names=['sample1.pe_1.fastq', \n                                                             'sample1.pe_2.fastq']))\n}\ntask = api.tasks.create(name='Metagenomics WGS Analysis - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Centrifuge home page - manual](http://www.ccb.jhu.edu/software/centrifuge/manual.shtml)\n[2] [Krona hierarchical graphs](https://www.google.com/url?q=https://github.com/marbl/Krona/wiki&sa=D&source=hangouts&ust=1527254314760000&usg=AFQjCNGqbHlwzXvtqwi96JJfVSltx2fXrw)  \n[3] [MetagenomeSeq home page](https://bioconductor.org/packages/release/bioc/html/metagenomeSeq.html)", "input": [{"name": "Centrifuge Reference Index", "encodingFormat": "application/x-tar"}, {"name": "Metagenomic samples", "encodingFormat": "text/fastq"}, {"name": "Marker factor"}, {"name": "Lower annotated category"}, {"name": "Abundance limit"}, {"name": "Higher annotated category"}, {"name": "Clade label abundance"}, {"name": "Image size"}, {"name": "Pad in"}, {"name": "Image format"}, {"name": "Image dpi"}, {"name": "Unpaired reads that didn't align"}, {"name": "Paired reads not aligned concordantly"}, {"name": "Trim from 5'"}, {"name": "Trim from 3'"}, {"name": "Time"}, {"name": "Columns in tabular format"}, {"name": "Skip the first n reads"}, {"name": "Quiet"}, {"name": "Query input files"}, {"name": "Quality scale"}, {"name": "QC filter"}, {"name": "Parallel threads"}, {"name": "Output format"}, {"name": "No reverse complement"}, {"name": "No forward version"}, {"name": "Minimum summed length"}, {"name": "Minimum length of partial hits"}, {"name": "Metrics standard error"}, {"name": "Metrics"}, {"name": "Memory per job"}, {"name": "Memory mapping"}, {"name": "Ignore qualities"}, {"name": "Host taxids"}, {"name": "Exclude taxids"}, {"name": "Unpaired reads that aligned at least once"}, {"name": "Align first n reads"}, {"name": "Paired reads aligned concordantly"}, {"name": "SRA accession number"}], "output": [{"name": "Centrifuge report"}, {"name": "Classification result", "encodingFormat": "text/plain"}, {"name": "Centrifuge Krakenlike report", "encodingFormat": "text/plain"}, {"name": "Alignment metrics", "encodingFormat": "text/plain"}, {"name": "Zipped report", "encodingFormat": "application/zip"}, {"name": "Krona chart report", "encodingFormat": "text/html"}, {"name": "HTML report for all samples", "encodingFormat": "text/html"}, {"name": "Counts table"}, {"name": "Taxonomy table"}, {"name": "Pairs not aligned concordantly", "encodingFormat": "text/plain"}, {"name": "Pairs aligned concordantly", "encodingFormat": "text/plain"}, {"name": "Aligned unpaired reads", "encodingFormat": "text/plain"}, {"name": "Unaligned unpaired reads", "encodingFormat": "text/plain"}], "codeRepository": [], "applicationSubCategory": ["Metagenomics", "Taxonomic Profiling"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561063, "dateCreated": 1509720865, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/metaphlan-wf/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/metaphlan-wf/6", "applicationCategory": "Workflow", "name": "Metagenomics WGS analysis - MetaPhlAn 2.0", "description": "**Metagenomics WGS analysis - MetaPhlAn 2.0** is a metagenomics workflow for profiling the composition of microbial communities (Bacteria, Archaea, Eukaryotes and Viruses) from metagenomic shotgun sequencing data with species-level, built around the **MetaPhlAn 2** tool.\n\nThe workflow relies on ~1M unique clade-specific marker genes identified from ~17,000 reference genomes (~13,500 bacterial and archaeal, ~3,500 viral, and ~110 eukaryotic) for accurate profiling, although the user can upload their own version of marker genes database.\n\n\nThe workflow takes three inputs:\n\n- FASTA/FASTQ files as input (alternatively SAM from a previous bowtie2 run), be it single-end, paired end or multi-FASTQ (multiple FASTQ files for a single sample) for each sample, with **sample_ID** metadata set. \n\n- **Metaphlan database** in TAR.GZ format, which will be available on the platform as a suggested input. In case the users have their own marker database file, it can be uploaded instead. The default database file will be available on the platform as a suggested input and regularly updated from the metaphlan [repository](https://bitbucket.org/biobakery/metaphlan2/downloads/). \n\n- The **file type** being analyzed (FASTA/FASTQ/SAM). Only one type of files can be used, therefore you cannot combine FASTA/FASTQ/SAM files with each other.\n\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\n\nThe workflow is set to use the maximum of instance's resources to analyze multiple metagenomics samples in parallel, scattering the samples by **Sample_ID**. Workflow will automatically assign job resources and set the optimal (`--nproc`) parameter that will choose the most suitable parallelization level for the number of input files. Quantitative comparisons of species abundance can be done using other tools, outside of this workflow. For example, one such tool is **MetagenomeSeq** (available as a separate interactive analysis), an R package designed to determine features that are differentially abundant between two or more groups of multiple samples.\n\nThe converter for **MetagenomeSeq** is included in the workflow as well, with the outputs being **taxonomy output** and **counts output**.\n\nAfter the workflow has successfully completed, the main output is **abundance matrix** (as a TXT) containing merged abundances for all samples together with one HTML report for the whole analysis containing circular phylogenetic tree and bar charts of 15 most abundant species for each sample. Optionally **BIOM matrix** and **SAMOUT matrix** file can be separately obtained for each sample.\n\nWhen running the analysis with a SAM file input, the SAM file has to come from a previous **MetaPhlAn 2.0** run (where the SAMOUT was set to TRUE) or from a bowtie2 run. It is only used when we want to redo an abundance analysis without classification. Downstream from **MetaPhlAn 2.0** the workflow will continue in the same manner and all the outputs types will be the same as with FASTA/FASTQ files. When using SAM as input, the workflow runs much faster than when using FASTQ/FASTA files. \n\n### Changes Introduced by Seven Bridges\n\n- Custom scripts were introduced for Export2graphlan and export to Krona (inside the Krona WF) to conform to the platform standards.\n- Custom script for producing files for Interactive Differential Abundance Analysis.\n\n\n### Common Issues and Important Notes\n\n- Only one type of files can be used, therefore you cannot combine FASTA/FASTQ/SAM files with each other.\n\n- The file name and extension do not matter, but the type of the file being analyzed (FASTA/FASTQ/SAM) needs to be specified, otherwise the tool will return an error.\n\n- When using SAM files, user needs to make sure that each SAM file has different and properly set metadata **Sample_ID** field, as **MetaPhlAn 2** cannot handle multiple SAM files with the same metadata fields at once.\n\n- If supplying the tool with files missing the metadata **Sample_ID** field, the workflow will treat all those files as a single MULTIFASTA/MULTIFASTQ sample downstream.\n\n\n### Performance Benchmarking\n\n- **MetaPhlAn 2** is using ~2GB per sample, regardless of the number of processors used. The speed of the workflow grows with the number of processors used, although the most significant gain is at 4 processors per sample (--n_proc 4). therefore we recommend that the user does not alter the default parallelization settings, while a larger instance can be chosen for a large pool of files.\n\n- The workflow creates temporary files that together take around 15% of all input files size, therefore the maximum combined input files size should not go over 850 GB. Output options do not affect memory since report and BIOM files are rather small.\n                   \n| Input size | Duration | Cost | Instance (AWS) | Files | Samples |\n|---------------|-----------------|-----------|--------|-----|----|\n| 19 GB | 36 min. | $0.23 | c4.2xlarge | 18 | 9 |\n| 42 GB | 1h 13 min. | $0.48 | c4.2xlarge | 34 | 17\n| 49 GB | 1h 03 min. | $0.40 | c4.2xlarge | 4 | 1 |\n| 125 GB | 3h 58 min. | $1.59 | c4.2xlarge | 50 | 24 |\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n\nimport sevenbridges\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"your_token\", \"https://api.sbgenomics.com/v2\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\n\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\n\ninputs = {\n   \"metaphlan_db\": api.files.query(project=project_id, names=[\"metaphlan_db.tar.gz\"])[0],\n   \"fastq_list\": list(api.files.query(project=project_id, names=[\"SRS014470-Tongue_dorsum.fasta\", \"SRS014476-Supragingival_plaque.fasta\"])),\n   \"input_type\": \"fasta\" #or alternativelly SAM/FASTQ\n}\n\n\ntask = api.tasks.create(name=\"TASK_NAME\", project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [MetaPhlAn 2 home page](https://bitbucket.org/biobakery/metaphlan2/overview)", "input": [{"name": "Metaphlan database", "encodingFormat": "application/x-tar"}, {"name": "Input array", "encodingFormat": "text/fastq"}, {"name": "Input file type"}], "output": [{"name": "SAMOUT matrix"}, {"name": "BIOM matrix"}, {"name": "b64html", "encodingFormat": "text/html"}, {"name": "Merged Abundances", "encodingFormat": "text/plain"}, {"name": "Zipped HTML report", "encodingFormat": "application/zip"}, {"name": "Taxonomy output"}, {"name": "Counts output"}, {"name": "Krona sunburst plot", "encodingFormat": "text/html"}, {"name": "Abundance table"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/metaphlan-wf/6.png", "codeRepository": [], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "creator": "The Huttenhower Lab / Duy Tin Truong, Eric A Franzosa, Timothy L Tickle, Matthias Scholz, George Weingart, Edoardo Pasolli, Adrian Tett, Curtis Huttenhower & Nicola Segata", "softwareVersion": ["sbg:draft-2"], "dateModified": 1548263911, "dateCreated": 1526055358, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/metagenomics-wgs-functional-profiling-humann2/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/metagenomics-wgs-functional-profiling-humann2/11", "applicationCategory": "Workflow", "name": "Metagenomics WGS Functional Profiling - HUMAnN2", "description": "**Metagenomic WGS Functional Profiling - HUMAnN2** (the HMP Unified Metabolic Analysis Network)   workflow provides a complete functional profiling analysis of input samples by determining the presence/absence and abundance of metabolic pathways in a microbial community from metagenomic sequencing data. The main tool of this workflow is **HUMAnN2**, which introduces a novel tiered search algorithm that provides highly accurate profiles for characterized members of microbial communities. Tiered search provides taxonomic stratification of microbial functions at the species level, thus quantifying the community abundance of functions while assigning them to specific contributors. It is appropriate for any type of microbial community shotgun sequence profiling (i.e. not just the human microbiome) [1].\n\nThe **Metagenomic WGS Functional Profiling - HUMAnN2** tool requires the following input files:\n- **MetaPhlAn database** file in TAR.GZ format, a database containing all clade-specific marker gene sequences used by **MetaPhlan2**,\n- **ChocoPhlAn database** file in TAR.GZ format, a pangenome database created by clustering the NCBI coding sequences,\n- **UniRef database** file in TAR.GZ format, with gene family definitions which may be clustered at different levels,\n- **FASTQ list**, an array of files containing metagenomic reads in FASTQ format, and\n- **Metadata file** in TSV format, containing information about the input files.  \n\nA list of all inputs and parameters with corresponding descriptions can be found at the end of the page.\n\n\n### Common Use Cases\n\nThis workflow is designed to analyze several metagenomics samples in parallel (scattered by Sample_ID). The user should provide FASTQ files for a desired number of samples and all needed databases listed above. **Metagenomic WGS Functional Profiling - HUMAnN2** takes files from the **FASTQ list** input node with raw reads. According to [Human Microbiome Project](https://hmpdacc.org/), samples should be cleaned from host genomic sequences. The first step of the workflow is **KneadData** tool, which performs quality control on metagenomic sequencing data that contain a high ratio of host to microbial sequencing reads. This tool aims to perform separation of microbial reads from \"contaminant\" reads, originating from the host, bacterial 16S sequences, or other sources. It can also perform filtering of low-quality bases from the 3' end of the sequence and the removal of adapter sequences [2]. **KneadData** is also used to concatenate paired-end reads, as the following **HUMAnN2** tool requires the paired-end reads to be concatenated first.\n**HUMAnN2** takes the output from **KneadData**, along with three databases provided at the workflow input, **MetaPhlAn database**, **ChocoPhlAn database** and **UniRef database** (in TAR.GZ format) needed for this functional metagenomic analysis. At first, known microbial species are identified in a sample by screening input reads with **MetaPhlAn2** (part of **HUMAnN2**). Then, a sample-specific database is preconstructed, containing functionally annotated pangenomes of the identified species. After that **HUMAnN2** performs nucleotide-level mapping of all sample reads against the sample\u2019s pangenome database. Relative to comprehensive translated search, nucleotide-level mapping against relevant pangenomes quickly explains a large fraction of reads with fewer opportunities for spurious alignment. Reads that do not align to identified species\u2019 pangenomes are subjected to accelerated translated search against a comprehensive protein database, **UniRef90** or **UniRef50** databases. **UniRef90** is built by clustering **UniRef100** sequences with 11 or more residues, with each cluster being composed of sequences that have at least 90% sequence identity and 80% overlap with the longest sequence (a.k.a. seed sequence) of the cluster. Similarly, **UniRef50** is built by clustering **UniRef90** seed sequences that have at least 50% sequence identity and 80% overlap with the longest sequence in the cluster.\nThis tiered search generates mappings of input reads to gene sequences with known or ambiguous taxonomy. These mappings are weighted by quality and sequence length to estimate per-organism and community-total gene family abundance. Finally, gene families annotated to metabolic enzymes are further analyzed to reconstruct and quantify complete metabolic pathways (by default, **MetaCyc** is used) in the community and per organism.\nThe results of the functional profiling are presented in three main output files:\n- **Gene families**, a tab delimited file that details the abundance of each gene family in the community. Gene families are groups of evolutionarily-related protein-coding sequences that often perform similar functions. Gene family abundance at the community level is stratified to show the contributions from known and unknown species. Individual species' abundance contributions sum to the community total abundance.\n**HUMAnN2** uses the **MetaPhlAn2** software along with the **ChocoPhlAn** database and translated search database for this computation.\nGene family abundance is reported in RPK (reads per kilobase) units to normalize for gene length; RPK units reflect relative gene (or transcript) copy number in the community. The resulting line in this file  looks like the following one:    \n    *UniRef50_O83668: Fructose-bisphosphate aldolase|g__Bacteroides.s__Bacteroides_vulgatus  31.0*\n\n- **Pathway abundance**, a tab delimited file which details the abundance of each pathway in the community as a function of the abundances of the pathway's component reactions, with each reaction's abundance being computed as the sum over abundances of genes catalyzing the reaction. By default, **HUMAnN2** uses **MetaCyc** pathway definitions and **MinPath** to identify a parsimonious set of pathways which explain observed reactions in the community. The resulting line in this file looks like the following one:   \n    *PWY-5484: glycolysis II (from fructose-6P)|g__Bacteroides.s__Bacteroides_caccae 16.7*\n\n- **Pathway coverage**, a tab delimited file, which provides an alternative description of the presence (1) and absence (0) of pathways in a community, independent of their quantitative abundance. **HUMAnN2** assigns a confidence score to each reaction detected in the community. Reactions with abundance greater than the median reaction abundance are considered to be more confidently detected than those below the median abundance. **HUMAnN2** then computes pathway coverage using the same algorithms described above in the context of pathway abundance, but substituting reaction confidence for reaction abundance. The resulting line in this file looks like the following one:\n    *PWY0-1301: melibiose degradation|g__Bacteroides.s__Bacteroides_caccae   1.0*\n\nBesides these main output files, there are two more outputs from **HUMAnN2** tool:\n- **Output temp files**, which contains a file with the bugs list output from **MetaPhlAn2**, a file with the reduced alignment results from **Bowtie2**, a file with results from **DIAMOND**, and all other reports and output files from tools within **HUMAnN2** tool.\n\n**HUMAnN2-JoinTables** and **HUMAnN2-RenormTable** are used to merge multiple single-sample output files into a single table with multiple samples and then to normalize the joint table to relative abundance or copies per million (CPM) units.\nThe final part of the workflow is the **HUMAnN2-BarPlotComplete** tool, which takes the input pathway abundance table and the metadata TSV file, and for each pathway in the input table, except for the unmapped ones, produces an output figure displaying relative abundance for each of the input samples in PNG format. These figures are used as an input for the **HUMAnN2-CreateReport** tool, which integrates them into an RMarkdown report and converts it to an HTML file, where relative abundance figure for a specific pathway is displayed by selecting a pathway of interest.\n \n### Changes Introduced by Seven Bridges\n\nNo changes were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n* End-pairing relationships are currently not taken into account during **HUMAnN2**'s alignment steps. This is due to the fact that **HUMAnN2** strictly aligns reads to isolated coding sequences: either at the nucleotide level or through translated search. The best way to use paired-end sequencing data with **HUMAnN2** is simply to concatenate all reads into a single FASTA or FASTQ file.\n\n### Performance Benchmarking\n\n**HUMAnN2** requires at least 16GB of memory (according to the tool specifications) in order to work properly. Based on the light load testing, setting the **c4.8xlarge** AWS instance will be sufficient when running the **Metagenomic WGS Functional Profiling - HUMAnN2** workflow with just a few samples, in which case it is recommended to set the number of threads to 16. \nBased on our experience, we chose to manually set the **c5.18xlarge** AWS instance for running the **HUMAnN2** and **KneadData** tools within the workflow, because it a good cost-to-value ratio when running the **Metagenomic WGS Functional Profiling - HUMAnN2** workflow with more than just a few input samples. The instance could be changed by the user if necessary.\n\nIn the following table you can find estimates of **Metagenomic WGS Functional Profiling - HUMAnN2** workflow running time and cost. Benchmarking is performed on a different number of samples (which varies in input FASTQ files sizes) with full **MetaPhlAn2** and **ChocoPhlAn** databases, and both **Uniref50** and **Uniref90** databases. **KneadData** is used only for trimming.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input files sizes | Duration | Cost | Instance (AWS) for **HUMAnN2**\n| --- | --- | --- | --- |\n|Uniref50 database 2.5 GB, 2 samples (reads ranging from 130 MB to 170 MB)| 37m| $0.97 | c4.8xlarge|\n|Uniref50 database 2.5 GB, 5 samples (reads ranging from 170 MB to 375 MB)| 1h 30m| $2.18 | c4.8xlarge|\n|Uniref50 database 2.5 GB, 5 samples (reads ranging from 170 MB to 375 MB)| 1h 1m| $2.26 | c5.18xlarge|\n|Uniref90 database 5.9 GB, 17 samples (reads ranging from 130 MB to 3.8 GB)| 3h 37m| $9.89 | c5.18xlarge|\n|Uniref90 database 5.9 GB, 17 samples (reads ranging from 130 MB to 3.8 GB)| 3h 27m| $14.30 | r4.16xlarge|\n\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n    \"fastq_list\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n    \"chocophlan_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"uniref_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"metaphlan_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"metadata_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"Metagenomics WGS Functional Profiling - HUMAnN2 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] Franzosa EA, McIver LJ*, Rahnavard G, Thompson LR, Schirmer M, Weingart G, Schwarzberg Lipson K, Knight R, Caporaso JG, Segata N, Huttenhower C. *Species-level functional profiling of metagenomes and metatranscriptomes*. Nat Methods 15: 962-968 (2018).\n[2] [KneadData Wiki](https://bitbucket.org/biobakery/kneaddata/wiki/Home).", "input": [{"name": "FASTQ list", "encodingFormat": "text/fastq"}, {"name": "Reference database", "encodingFormat": "application/x-tar"}, {"name": "ChocoPhlAn database", "encodingFormat": "application/x-tar"}, {"name": "UniRef database", "encodingFormat": "application/x-tar"}, {"name": "MetaPhlAn database", "encodingFormat": "application/x-tar"}, {"name": "Metadata file"}, {"name": "Options for trimmomatic"}, {"name": "Max amount of memory [ Trimmomatic option ]"}, {"name": "Match probability [ TRF option ]"}, {"name": "Indel probability [ TRF option ]"}, {"name": "Mismatching penalty [ TRF option ]"}, {"name": "Minimum alignment score to report [ TRF option ]"}, {"name": "Maximum period size to report [ TRF option ]"}, {"name": "Matching weight [ TRF option ]"}, {"name": "Indel penalty [ TRF option ]"}, {"name": "Number of threads"}, {"name": "Store temp output files"}, {"name": "Run TRF"}, {"name": "Run FastQC at the end"}, {"name": "Run FastQC at the beginning"}, {"name": "Run BMTagger"}, {"name": "Remove intermediate output files"}, {"name": "Quality scores"}, {"name": "Number of processes"}, {"name": "Output prefix"}, {"name": "Output directory"}, {"name": "Level of log messages"}, {"name": "Log file"}, {"name": "Bypass trim"}, {"name": "Filter the input in serial for multiple databases [ Bowtie2 option ]"}, {"name": "Reorder sequences [ Bowtie2 option ]"}, {"name": "Options for bowtie2"}, {"name": "Do not include discordant alignments [ Bowtie2 option ]"}, {"name": "Concatenate pair files [ Bowtie2 option ]"}, {"name": "Remove adapter sequences"}, {"name": "Update RPK in sample names"}, {"name": "Units"}, {"name": "Mode"}, {"name": "Update RPK in sample names"}, {"name": "Units"}, {"name": "Mode"}, {"name": "Xipe"}, {"name": "Translated subject coverage threshold"}, {"name": "Translated query coverage threshold"}, {"name": "Translated alignment"}, {"name": "Remove temp output"}, {"name": "Remove stratified output"}, {"name": "Remove column description output"}, {"name": "Prescreen threshold"}, {"name": "Pick frames"}, {"name": "Pathways"}, {"name": "Output max decimals"}, {"name": "Output format"}, {"name": "Output basename"}, {"name": "Number of threads"}, {"name": "Minpath"}, {"name": "Metaphlan options"}, {"name": "Input format"}, {"name": "Identity threshold"}, {"name": "Gap fill"}, {"name": "Evalue treshold"}, {"name": "Annotation gene index"}], "output": [{"name": "KneadData output fastqc files"}, {"name": "Pathway coverage"}, {"name": "Gene families"}, {"name": "Pathway abundance renornmed table"}, {"name": "Gene families renormed table"}, {"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Compressed HUMAnN2 temp folder", "encodingFormat": "application/zip"}, {"name": "KneadData output files archives", "encodingFormat": "application/zip"}], "codeRepository": [], "applicationSubCategory": ["Metagenomics", "Functional Characterization"], "project": "SBG Public Data", "creator": "The Huttenhower Lab", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648560771, "dateCreated": 1551887473, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/microsatellite-instability-profiling-wf/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/microsatellite-instability-profiling-wf/13", "applicationCategory": "Workflow", "name": "Microsatellite instability profiling workflow", "description": "The **Microsatellite instability profiling workflow** enables profiling microsatellite instability using WXS or WGS data. It requires FASTQ read input files, and uses **BWA-MEM** for read alignment, **lobSTR** for calling microsatellite variants, and **MSIsensor** for quantifying microsatellite instability. It works in batch mode (even though all files are provided together as an input bundle, separate tasks are created for each input file), please take note that the samples that are batched together must be from same sequencing center, same data submitting center and of the same gender.\n\nThe **Microsatellite instability profiling workflow** requires the following input files and parameters: \n\n*  **Reference**: A Reference genome assembly in FASTA format, or a TAR bundle produced by **BWA-MEM Index**, which will be used during read alignment. If a TAR bundle is provided, the reference FASTA does not require indexing, resulting in shorter execution time.\n \n* **lobSTR index bundle**: A **lobSTR** reference and index TAR bundle, pre-built and distributed with the **lobSTR** package. The chosen bundle must match the reference genome assembly used for read alignment. For UCSC HG19 genome build we recommend \"hg19.lobSTR\\_v3.0.2.ref.tar\" as an index, which can be found on the Seven Bridges platform under Public Reference Files.\n\n*  **Stutter model** and **Step model**: Stutter and step PCR noise model files, appropriate for the particular library prep protocol used for samples under analysis. Model files for illumina PCR-free libraries can be found among Public Reference Files as \"illumina\\_v3.pcrfree.stepmodel\" and \"illumina\\_v3.pcrfree.stuttermodel\". These files can be built for any specific sequencer, library-prep and/or data submitting center by running the **lobSTR PCR noise model training** workflow (available under Public Apps Gallery) on the largest normal male sample.\n\n*  **Input reads**: Reads in FASTQ format (gzipped input accepted). If providing paired-end reads, then in two separate FASTQ files per sample. Please make sure to set the appropriate metadata fields accordingly. At least tumor reads are required, but in that case only **lobSTR** will output the results. If normal reads are inputted as well, then both **MSIsensor** and **lobSTR** will provide output files.\n\n* **Microsatellites list**: A list of microsatellites that **MSIsensor** will use needs to be provided. A pre-built list for the hg19 reference is available on the platform in Public Reference Files under the name \u201cucsc.hg19.msisensor\\_microsates_list.txt\u201d. Lists for other genome builds can be generated using the **MSIsensor scan** tool.\n\n* **Chromosomes for homozygous calls** (`--haploid`) parameter: It is necessary to explicitly list chromosome names that should be treated as haploid by **lobSTR Allelotype**. For example, if processing reads from a human male sample, aligned to hg19, 'chrX,chrY' should be inputted in the **Chromosomes for homozygous calls** (`--haploid`) field of **lobSTR Allelotype** under the App Settings tab.\n\n* **Metadata criteria**: There needs to be a way to separate tumor and normal samples in order for **MSIsensor** to work. That is done by using the **SBG Split Pair by Metadata** tool inside the workflow. Tool accepts all BAMs on the input and **output_1** of the tool should contain all normal samples while **output_2** should contain all tumor samples. This should be set by using **Metadata criteria** parameter field. Format of setting this is \"metadata_fileld:metadata_value_for_normal/tumor\". For example, one way of setting up your task properly is to have **Sample type** metadata field of all **Input reads** set to either \u201cNormal tissue\u201d or \u201cTumor tissue\u201d, and then have **Metadata criteria** set to \"sample_type:Normal tissue\" for output_1 and \"sample_type:Tumor tissue\" for output_2.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nMicrosatelllites or short tandem repeats (STRs) are stretches of repetitive DNA, usually considered to have core repeat units of 1 - 6bp. [1] They can be found anywhere in genome and they generally occupy 3% of it. Approximately 99% of them are shorter than 40bp.\n\nMicrosatellite instability (MSI) refers to widespread somatic changes in the number of repeats at microsatellite loci in the genome. MSI typically reflects defects in the DNA mismatch repair machinery, which are a hallmark of certain types of cancers. This workflow enables profiling MSI using WES or WGS data. It can be run in two modes:\n\n1. **Paired tumor normal sample mode** - that enables differentiating somatic variants from germline variants at  microsatellite loci. This mode leverages **MSIsensor** to obtain a measure of microsatellite instability. [2] VCFs containing results of **lobSTR Allelotyping**, [3] at a set of reference microsatellite loci, for both the normal and tumor samples is also outputted.\n\n2. **Tumor sample only mode** - in which case only the output of **lobSTR Allelotyping** will be obtained.\n\nThe output VCFs will contain a list of all microsatellite loci that were allelotyped by **lobSTR**, with the ALT allele field left blank ('.') if no microsatellite variations were reported.\n### Changes Introduced by Seven Bridges\n\n* **MSIsensor** has pass through mode integrated - in case of tumor only files, **MSIsensor** will just go to the completed stage instead of causing the task to fail.\n\n### Common Issues and Important Notes\n\n* **MSIsensor msi fails with exit code 139**: This issue occurs when contigs from the reference genome, represented in the microsatellites list, are absent from the BAM/BAI supplied as input. Most often, this is caused by different chromosome naming conventions (e.g. 'chr1' vs '1')  and indicates that different reference genomes were mistakenly used for aligning reads and generating the microsatellites list. However, this is also known to happen when UCSC hg19 is used as a reference (consistently). Namely, 'chrM' and various 'chrUn' are present in the reference sequence, but have no reads aligned to them in the BAM/BAI files. This issue is a limitation of the tool itself and can be circumvented by manually editing the microsatellites list to include only the 22 autosomal and sex chromosomes.\n\n* The **Chromosomes for homozygous calls** (`--haploid`) parameter needs to be explicitly filled with chromosome names that should be treated as haploid by **lobSTR Allelotype**. For example, if processing reads from a human male sample aligned to hg19, 'chrX,chrY' is the value that should be filled for that parameter. In case all available reads are human female samples (which can happen in case of endometrial cancer, for example), then stutter and step models cannot be trained. In case of illumina PCR-free libraries, as mentioned above, stepmodel and stuttermodel files can be found among Public Reference Files as \"illumina_v3.pcrfree.stepmodel\" and \"illumina_v3.pcrfree.stuttermodel\". But in any other case, male samples would need to be obtained so the models can be trained.\n\n* There must be a way to separate tumor and normal samples. One of the ways to do this is to have the metadata field **Sample type** on all files on **Input reads** input filled as either 'Normal tissue' or 'Tumor tissue'. More information regarding this can be found on the beginning of the page in the part where required inputs and parameters are listed.\n\n* The **Case ID** metadata field needs to be filled uniquely in case multiple cases are run for all files presented as **Input reads** input.\n \n* The **Sample ID** metadata field needs to be filled for all files presented as **Input reads** inputs. Values for **Sample ID** metadata field need to be unique for each sample - they need to be different for tumor and normal samples, and different for samples from different cases, if more than one case is run in a batch.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Performance Benchmarking\n\nThe instance is set to c4.8xlarge (AWS). The reason for this is that the workflow often fails when using smaller instances on data that is even as small as 10GB per FASTQ paired end. Additionally, due to the fact that tasks on this instance last shorter than on smaller instances, c4.8xlarge (AWS) has proved to be optimal for this use case.\n\nIn the following table you can find estimates of running times and costs. All samples are aligned onto the hg19 human reference genome, and using appropriate lobSTR indexes.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n#####Example runs:\n\n| Experiment type | Input size (T+N) | Paired-end | Read length | Duration | Cost | Instance (AWS)| Instance type |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|------------|\n| WES         | 2 x 7 GB + 2 x 1.8 GB     | Yes        | 100         | 42m   | $1.11            | c4.8xlarge      | On-demand        |\n| WES         | 2 x 40 GB + 2 x 52 GB     | Yes        | 100         | 3h 24m   | $5.41            | c4.8xlarge      | On-demand        |\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\n#Additionally, haploid is a field for the list of haploid chromosomes, but, in case of female human samples, it doesn't need to be set because there are no haploid chromosomes, or it can be set to empty string.\n#If these samples were male, inputs would require additional line: \"'haploid: 'chrX,chrY'\"\ninputs = {\n    'index': api.files.query(project=project_id, names=['hg19.lobSTR_v3.0.2.ref.tar'])[0], \n    'reference': api.files.query(project=project_id, names=['ucsc.hg19.fasta.tar'])[0],\n    'microsats_list': api.files.query(project=project_id, names=['ucsc.hg19.msisensor_microsats_list.txt'])[0],\n    'step_model': api.files.query(project=project_id, names=['illumina_v3.pcrfree.stepmodel'])[0],\n    'stutter_model': api.files.query(project=project_id, names=['illumina_v3.pcrfree.stuttermodel'])[0],\n    'input_reads': list(api.files.query(project=project_id, names=['CCLE-HCC1143-DNA-10_Illumina.converted.pe_1.fastq', 'CCLE-HCC1143-DNA-10_Illumina.converted.pe_2.fastq', CCLE-HCC1143BL-DNA-10_Illumina.converted.pe_1.fastq, CCLE-HCC1143BL-DNA-10_Illumina.converted.pe_2.fastq])),\n    'metadata_criteria': {'output_1': 'sample_type:Normal tissue', 'output_2': 'sample_type:Tumor tissue'}\n}\ntask = api.tasks.create(name='WES Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [A Brief Review of Short Tandem Repeat Mutation, Fan et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5054066/)\n\n[2] [MSIsensor: microsatellite instability detection using paired tumor-normal sequence data, Niu et al.](https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt755)\n\n[3] [lobSTR: A short tandem repeat profiler for personal genomes. Gymrek et al.](http://genome.cshlp.org/content/22/6/1154.long)", "input": [{"name": "Stutter model"}, {"name": "Step model"}, {"name": "lobSTR index bundle", "encodingFormat": "application/x-tar"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Microsatellites list", "encodingFormat": "text/plain"}, {"name": "Reference", "encodingFormat": "application/x-tar"}, {"name": "Chromosomes for homozygous calls"}, {"name": "Metadata criteria"}], "output": [{"name": "Report zip", "encodingFormat": "application/zip"}, {"name": "FastQC charts", "encodingFormat": "text/html"}, {"name": "Filtered VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Filtering report"}, {"name": "Run statistics"}, {"name": "Germline loci", "encodingFormat": "text/plain"}, {"name": "MSI score", "encodingFormat": "text/plain"}, {"name": "Somatic loci", "encodingFormat": "text/plain"}], "codeRepository": [], "applicationSubCategory": ["Microsatellites"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155966, "dateCreated": 1512152591, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/mutational-signatures-deconstructsigs-1-8-0/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/mutational-signatures-deconstructsigs-1-8-0/12", "applicationCategory": "Workflow", "name": "Mutational Signatures - deconstructSigs 1.8.0", "description": "The **Mutational Signatures  - deconstructSigs 1.8.0** is a workflow based on **deconstructSigs**, creating a mutational profile for somatic tumor samples and comparing it with predefined signatures giving the weights and signatures composition of a given profile. \n\nMutational signatures are markers based on somatic variant patterns which can be used to categorize cancer. These patterns are based on trinucleotides, each of which corresponds to six substitutions, such as C>A, C>G, C>T, T>A, T>C, and T>G, which are flanked by the 4 types of nucleotides upstream and downstream, totaling to 96 possibilities. The frequencies of the different patterns identified in a sample can be compared to previously identified patterns, assembled into validated signatures such as Cosmic, thus inferring the mutational processes and cancer type [1].\n\nThe **Mutational Signatures  - deconstructSigs 1.8.0** workflow is built around the R package **deconstructSigs 1.8.0** using VCF to calculate the pattern frequencies and infer the related mutational signatures based on the 30 Cosmic curated ones, adding a weight to each identified signature and producing graphics for representation [2]. The input VCF is filtered prior the analysis by **deconstructSigs**, using **VCFtools** to keep only SNP variants. The output is composed of a set of two graphics representing: (1) the sample tumor profile, the calculated signature profile, and the difference between the two; and (2) a pie chart representing the weights of the different signatures associated with the sample tumor profile. The workflow also outputs text files containing the numerical values of the different plots.\n\n###Common Use Cases\n\nThe **Mutational Signatures** workflow takes VCF as an input. Be sure to have a genotype field (GT) set on [VCF standard format](http://www.internationalgenome.org/wiki/Analysis/vcf4.0/) (e.g. 0/1 or 0|1). It is also possible to change values for the following parameters: **Reference genome**, **Normalization method**, and **Reference signature matrix**, which will be used to infer the signatures. \n\nFor a given VCF file with multiple samples, the output files will be compressed together for each sample separately by default (the **ZIP file per sample** parameter set to *yes*) each one named using the sample names provided in the VCF file header. However the output for all samples from the VCF file can be zipped together by setting the **ZIP file per sample** parameter to *no* and named by **Case ID** metadata field or VCF file name if **Case ID** is not provided.      \n\n* **Reference genome** parameter possible values: hg19 or hg38; hg19 is selected by default\n* **Reference signatures matrix** parameter possible values:  \n  - signature.nature2013: the first matrix developed\n  - signature.cosmic: an enriched version of the first matrix; signature.cosmic matrix is selected by default\n* **Normalization method**: \n  * default: only normalize the trinucleotide count\n  * exome2genome: multiplied by a ratio of that trinucleotide's occurence in the genome to the trinucleotide's occurence in the exome\n\nThe method of normalization chosen should match how the reference signatures were normalized. For exome data, the 'exome2genome' method is appropriate for the signatures included with the tool. For whole genome data, use the 'default' method to obtain consistent results.\n\n###Change Introduced by Seven Bridges\n\n* Some improvements, not present in the 1.8.0 version of **deconstructSig** but developed later and present in the [github repository](https://github.com/raerose01/deconstructSigs) have been added.\n\n###Common Issues and Important Notes\n\n* The VCFs produced by the caller such as **Strelka**, which don\u2019t contain a genotype field formatted as described perviously (i.e. 0/1 or 0|1) are not usable with **deconstructSig**.\n* The workflow will fail if the VCF provided contains only indels. \n* If the **Case ID** metadata field is not provided, the output zip file will be named according to the VCF file name.\n\n###Performance Benchmarking\nThe **Mutational Signature** workflow is cheap and fast for analyzing somatic tumor signatures. The use of the c4.xlarge (AWS) instance has been selected, as it is sufficient to treat VCF files and scatter multiple files on four threads for a minimum cost. For example a multiple sample single file VCF runs in few minutes for the minimum cost ($0.01). The workflow, tested with 307 files from whole exome sequencing as input, counting a total of 395284 variants, completed running in 1h 36min for the cost of $0.11.\n\n###API Python implementation\n\nThe workflow's draft task can also be submitted via the API. Learn how to get your Authentication token and supply your API endpoint for the environment you\u2019re using from our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n    from sevenbridges import Api\n    \n    authentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\n    api = Api(token=authentication_token, url=api_endpoint)\n    # Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\n    project_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n    # Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\n    inputs = {\n        \"input_file\": api.files.query(project=project_id, names=['Sample1.vcf'])[0],\n        \"ref\": 'signatures.cosmic',\n        \"normeth\": 'default',\n        \"group\": 'no',\n        \"genome\": 'hg19',\n    }\n    task = api.tasks.create(name='Mutational Signatures Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. Learn more about using these API clients from the [API R client documentation](https://sbg.github.io/sevenbridges-r/) and the [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n[1] [Cosmic](http://cancer.sanger.ac.uk/cosmic/signatures)  \n[2] [Deconstructsig](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0893-4)", "input": [{"name": "Input VCFs", "encodingFormat": "application/x-vcf"}, {"name": "Reference signature matrix"}, {"name": "Normalization method"}, {"name": "Reference genome"}], "output": [{"name": "Zipped outputs", "encodingFormat": "application/zip"}, {"name": "Output in txt format", "encodingFormat": "text/plain"}, {"name": "Signatures plots"}], "codeRepository": ["https://github.com/raerose01/deconstructSigs"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033583, "dateCreated": 1513097322, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/novobreak-analysis/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/novobreak-analysis/5", "applicationCategory": "Workflow", "name": "novoBreak Analysis", "description": "Workflow developed under Zechen Chong at University of Texas MD Anderson Cancer Center and UAB Informatics Institute.\n\nnovoBreak Analysis workflow is an extended methodology for detection of structural variants' breakpoints using novoBreak. Workflow is divided into two steps represented by two nested workflows.\n\nFirst step consists of running the novoBreak tool in order to create somatic k-mers, converting the BAM output (containing the mentioned k-mers) of novoBreak into FastQs, and grouping the k-mers into clusters. Output of the first step is a TXT containing these clusters. Following tools are used during the first step:\n\n* novoBreak\n* SAMTools BAM2FQ\n* Group Breakpoint reads\n\nThe output of the first step is passed on to the second step where the produced k-mers get assembled and aligned to the provided reference producing a BAM file. Next, the variant calling on this BAM file is done followed by filtering of the resulting VCF file, and VCF file sorting and reordering. Following tools are used in the second step:\n\n* SSAKE Assembly\n* BWA MEM\n* Infer Structural Variants - Step 1\n* Infer Structural Variants - Step 2\n* VCFTools Sort\n* SBG VCF Reorder\n\nAuthor can be reached at [zchong@mdanderson.org](mailto:zchong@mdanderson.org).\n##**Inputs and outputs of the novoBreak Analysis workflow**\n\n###**Inputs**:\n\n* **Normal BAM** - Indexed Normal sample BAM\n* **Tumor BAM** - Indexed Tumor sample BAM\n* **Reference index TAR** - Reference FASTA TAR bundle containing the BWA indexed reference\n\n###**Outputs**:\n\n* **novoBreak passed VCF** - A VCF file containing the found structural variants\n\n###**Common issues**\n\n* The reference FASTA TAR bundle should contain the same reference as the one used for mapping the input Tumor/Normal BAM files\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n        \"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n        \"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n        \"normal_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"novoBreak Analysis - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart). \n\n##Cost optimization using Seven Bridges platform features\n\nInitial novoBreak Analysis workflow was divided into two steps so that different approaches to optimization could be applied on each of them, resulting in the most optimal performance.\n\nFirst step of the analysis is running on the AWS instance best suited for the requirements of the tools used in it.\n\n**Second step of the analysis is utilizing the multi instance and scatter features of the Seven Bridges platform.**  In order to shorten the long execution time of Infer Structural Variants - Step 2 (this tool does the post-processing and filtering on the VCF file) the processed VCF file was split into multiple smaller VCF files, each being processed using a single CPU out of two 36CPU instances, effectively running **72 processes in parallel**. \n\n**The cost of a single run was reduced by 40% without increase in the running time by implementation of above mentioned optimization features.** \n\nIn order to achieve this two additional tools were added to the second step:\n\n* SBG Array splitter\n* VCFTools Concat\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference index** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\nNote that execution time does not exclusively depend on sample size, but sample complexity as well. Samples with lower read quality might take longer.\n\n| Tumor + Normal Size (GB) | Duration (min) | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 61                     | 245             | 16    | 3.26     | c4.2xlarge + m4.4xlarge + c4.8xlarge |\n| 116                     | 177             | 16    | 2.36     | c4.2xlarge + m4.4xlarge + c4.8xlarge |\n| 195                    | 579            | 16     | 7.72     | c4.2xlarge + m4.4xlarge + c4.8xlarge |\n| 297                    | 387            | 16     | 5.29     | c4.2xlarge + m4.4xlarge + c4.8xlarge |", "input": [{"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Reference index TAR", "encodingFormat": "application/x-tar"}, {"name": "Normal BAM", "encodingFormat": "application/x-bam"}, {"name": "Kmer size"}], "output": [{"name": "novoBreak passed VCF", "encodingFormat": "application/x-vcf"}], "codeRepository": [], "applicationSubCategory": ["WGS", "Variant Calling"], "project": "SBG Public Data", "creator": "University of Texas MD Anderson Cancer Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649152604, "dateCreated": 1534854455, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/multi-instance-whole-genome-sequencing-gatk4-0-workflow/32", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/multi-instance-whole-genome-sequencing-gatk4-0-workflow/32", "applicationCategory": "Workflow", "name": "[obsolete] Multi-instance Whole Genome Sequencing GATK4.0 workflow", "description": "Configurable multi-instance workflow that processes whole genome reads in 2-3 hours. This workflow is based on several assumptions regarding the intended use-cases, and in line with that implements multiple trade-offs to optimise for speed over accuracy. As there are multiple caveats associated with the workflow it shouldn't be used without previously carefully considering the trade-offs and whether those are suitable for the intended purpose. \n\nTo meet the needs for fast genomic data analysis, Seven Bridges has developed a configurable workflow that processes whole genome data 2.78 times faster than before. SB wrapped/published the Broad Institute GATK4 beta on August 31, 2017. After publishing the final version of the GATK 4.0 Whole Genome Sequencing workflow (January 11, 2018), SB moved another step forward and optimized its execution using a configurable number of parallel instances. Conceptually, sequencer reads are split into an optimal number of portions and each portion is processed independently on a different instance, at the end, all results are merged. The quality of the variant calls is equivalent between Multi-instance and Single-Instance GATK 4.0 Whole Genome Sequencing workflow.\n\n### Common use cases\nThis Whole Genome Sequencing (WGS) workflow identifies variants from a human whole-genome resequencing experiment. This workflow performs optimally on data from experiments which utilize a PCR-free library preparation protocol (for PCR-free library Pcr Indel Model parameter should be set to NONE) and targets 30x mean coverage across the genome. However, it is also suitable for a range of coverages (verified up to 150x). Although WGS generally has lower coverage than Whole Exome Sequencing (WES), this method can detect variants outside of protein-coding areas and can detect changes affecting regulatory regions and various controlling mechanisms. These characteristics allow for a wider application of the workflow, especially in cases when novel variants are expected. For example, WGS can be used if the phenotype or family history strongly implicates genetic etiology but the phenotype does not correspond to a specific disorder for which a test targeting a particular gene is clinically available.\n\n### Workflow structure\nThe workflow utilizes the Broad Institute's GATK4 tools. Sequenced reads are first split into chunks depending on the number of instances (machines) selected. Each chunk is aligned on a different instance with BWA-MEM. BWA-MEM is set to use 18 threads allowing two of its jobs per default c5.9xlarge instance. That configuration has proven to give the best performance. The next step uses algorithms developed by the Broad Institute to do a re-evaluation of the qualities of sequenced bases (BaseRecalibrator). Adding new qualities is done using the GATK ApplyBQSR tool by splitting the processing into intervals given in the input BED file. In case of scattered BAM files, removing duplicates is performed using the Sambamba MarkDuplicates tool. Generated BAM files are passed to variant calling (GATK HaplotypCaller). For more information on how variant calling is performed, please refer to the [Broad Institute's web site](https://software.broadinstitute.org/gatk/best-practices/).\n\n### Changes introduced by Seven Bridges\n\nTo achieve optimal usage of a computational instance\u2019s resources, analysis is divided into a number of jobs which correspond to the number of interval regions in the input BED file plus one additional job for much smaller, mitochondrial, and global contigs. The SBG PrepareIntervals tool splits the input BED file (Target BED) into several smaller BED files. GATK BaseRecalibrator, set to use only reads from chromosome 20, collects all the BAM files and uses only those covered with the BQSR intervals string input to create the model for base quality score recalibration (BQSR). This lowers execution time while preserving a similar quality of recalibration. GATK ApplyBQSR applies the quality mapping table received from GATK BaseRecalibrator to the BAMs received from BWA-MEM. GATK ApplyBQSR also works in scatter mode set on the intervals input (one job per BED file) received from the SBG PrepareIntervals tool. GATK HaplotypeCaller is scattered by BAM file received from GATK ApplyBQSR and performs variant calling on each of the BAMs, outputting raw variant calling format files (VCFs). After variant calling [Smart Variant Filtering](https://www.sevenbridges.com/smart-variant-filtering/) is performed also in parallel. In final step all VCF parts are merged into one VCF.\n\n\n### Common issues and Important notes\n\n- In order to complete the execution of the workflow, the following fields in the metadata of FASTQ files must be set: **Paired-end, Sample ID and Platform**.\n- All reference files must correspond to the same reference genome (HG19, GRCh 37, HG38,...). If some of the reference files has contigs not listed in reference genome the pipeline cannot be executed.\n- BWA-MEM index files are packed together with the reference genome in the TAR files which are available on SBG Public files.\n- If HG38 is used, it is recommended to increase the memory per job for HaplotypeCaller from 2048 MB to at least 4096 MB depending on the type of sample and coverage. If memory_per_job for HaplotypeCaller is not set and HG38 is used the memory for HaplotypeCaller is set 4096 MB.\n- This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Performance Benchmarking\n\nThe Multi-Instance Whole Genome Sequencing GATK 4.0 workflow keeps a similar price with up to a 3.0 times improvement in the total execution time compared to a Single-Instance implementation:\n\n|              | NA12878_Robot_30x | afr_mother_30x | afr_father_30x | HG001-30x PCR-FREE | HG001-50x | HG002-50x |\n|--------------|-------------------|----------------|----------------|--------------------|-----------|-----------|\n| cost [$]     | 7.35              | 7.66           | 8.28           | 5.63               | 8.58      | 8.87      |\n| time [h] | 3.867             | 3.933          | 4.1667         | 3.07               | 4.68      | 4.73      |\n\n The execution time for single instance 30x samples (n=4) varies between 8.5 and 13 hours (11.28 hours on average) with tested samples, while for multi-instance it is between 3 and 4 hours (3.75 hours average) achieving the speed improvement factor of 3.0. With multi-instance, the cost increased from $6.2 to $7.2 on average. For 50x samples (n=2), the speed improvement factor is 2.76, from 13 hours to 4.7 hours, with the cost increasing from $7.69 to $8.72 on average. The Multi-Instance Whole Genome Sequencing workflow can process a 30x whole genome sample in 3 hours without any additional computational resource such as GPU or FPGA. In order to confirm the same quality of variant calling between the Single-Instance and Multi-Instance GATK 4.0 Whole Genome Sequencing workflows, testing has been performed using Genome In A Bottle samples. The differences in precision, recall and f-score are lower than 0.001%, which is expected due to the random component.\n\n*All costs were obtained by using **aws spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "dbsnp", "encodingFormat": "text/x-bed"}, {"name": "Mills", "encodingFormat": "text/x-bed"}, {"name": "Fastq", "encodingFormat": "text/fastq"}, {"name": "Reference or TAR with BWA reference indices", "encodingFormat": "application/x-fasta"}, {"name": "Known indels 1000g bqsr", "encodingFormat": "application/x-vcf"}, {"name": "indel_model"}, {"name": "snv_model"}, {"name": "Number of instances"}, {"name": "Memory Per Job"}, {"name": "Pcr Indel Model"}], "output": [{"name": "Raw VCF", "encodingFormat": "application/x-vcf"}, {"name": "Filtered VCF", "encodingFormat": "application/x-vcf"}], "applicationSubCategory": ["WGS"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035754, "dateCreated": 1526403262, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/ont-wgs-data-processing-1-cwl-1-1/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/ont-wgs-data-processing-1-cwl-1-1/9", "applicationCategory": "Workflow", "name": "ONT WGS Data Processing", "description": "**ONT WGS Data Processing** is a basic workflow for processing Oxford Nanopore WGS data.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**ONT WGS Data Processing** can be used to align ONT WGS data and call structural variants. The workflow is based on one of the Broad Institute ONT-processing workflows [1]. Starting  inputs for the workflow are ONT data in FASTQ format (**Input reads**) and the reference genome files (**Reference sequence**, **Input DICT file** and **Reference minimap2 index**). The **Reference minimap2 index** input will also accept a reference FASTA file (same as the one provided as **Reference sequence**). The workflow consists of the following steps:\n1. Alignment with **Minimap2** (v2.17). The native output of **Minimap2** is a SAM stream, which is piped into **Sambamba View** (v0.80) for conversion into a BAM file. Input FASTQ files are QC-ed with **NanoPlot** (v1.33.0).\n2. The aligned BAM file is sorted with **Samtools Sort** (v1.9) and indexed with **Samtools Index** (v1.9). \n3. If more than one FASTQ files are provided on input, they will be processed independently and finally merged with **Sambamba Merge** (v0.7.1). If only one FASTQ file is provided, the merging step is skipped.\n4. The aligned, sorted and potentially merged BAM file is quality checked using **Samtools Flagstat** (v1.9), **mosdepth** (v0.3.1), and a Samtools (v1.9)-based utility script for summarizing reads and their lengths. **Mosdepth** is invoked three times with different parameters and scattered over chromosomes derived from the **Input DICT file**. **Mosdepth** outputs are then collated together with utility scripts (summarize-depth and postprocess-mosdepth).\n5. Structural variants are called with three SV callers: **cuteSV** (v1.0.9), **Sniffles**(v1.0.12b) and **SVIM** (v1.4.2).\n\n### Changes Introduced by Seven Bridges\n\nThe overall architecture of the Broad ONT WF was retained for this workflow (as were the command line parameters), with the following key differences:\n  * **NanoPlot** is executed on the FASTQ inputs, not on sequencing summary files.\n  * **Minimap2** will create the RG entry from the metadata fields of the input FASTQ files if the RG string is not provided on input (field **Sample ID** is required, whereas **Platform**, **Platform unit ID**, and **Library ID** are parsed if present and are recommended).\n  * The original WF writes out a SAM file to disk, as a safe-guard for rare issues with truncated outputs. This workflow pipes the SAM stream into **Sambamba View** and writes out a BAM file, with a bash piping safeguard in place.\n  * Only one round of merging is implemented in the WF and QC steps are only done on the final merged product.\n  * Merging is done with **Sambamba Merge** instead of **Samtools Merge** - and the tool is set to be optional (will only execute if it receives more than one input file) in order to allow users to process a single input FASTQ file with the workflow.\n  * Parameter --no-PG was omitted from **Samtools Sort** as it was not available in v1.9. If requested, the toolkit can be bumped to 1.10, where the flag is available.\n  * **GATK ComputeLongReadMetrics** was omitted from the workflow as the tool is currently not a part of the official GATK release.\n  * **Picard CollectRnaSeqMetrics** was omitted and can be added on request.\n  * To reduce the number of **Mosdepth** outputs, a postprocessing utility tool was added (postprocess-mosdepth) which tries to concatenate outputs from scattered **Mosdepth** jobs.\n  * Postprocessing clean-up of VCFs produced by SV callers was omitted from the WF, variant calls are output as produced by the wrapped SV callers.\n  * Calling small variants with **Longshot** was omitted from the workflow.\n  * As we ran into issues with **PBSV Call** during testing (crashes on some inputs), calling SVs with **PBSV** was omitted from the WF.\n  * One of the steps in the Broad Institute ONT WF is filtering the merged BAM file to obtain a BAM file without MQ0 reads. This filtering step was omitted from this workflow, to avoid creating two large BAM outputs with similar contents. If desired, the BAM file with no MQ0 reads can be easily obtained by running **Samtools View** and **Samtools Index** with the appropriate flags on the merged BAM file produced by this workflow.\n\n### Common Issues and Important Notes\n\n* The following inputs are required:\n  - **Input reads** - ONT reads to process, in FASTQ format (FASTQ.GZ preferred).\n  - **Reference sequence** - Reference genome FASTA sequence\n  - **Reference minimap2 index**  - Either a minimap2 index file or a reference FASTA sequence should be provided here\n  - **Input DICT file** - DICT index matching the reference genome FASTA used. This file is used to generate the intervals for scattering mosdepth.\n\n* **Input reads** input should have the metadata field **Sample ID** set (fields **Platform**, **Platform unit ID**, and **Library ID** are also recommended).\n\n* This is a basic, first-pass workflow for processing WGS ONT data. Please consider also analyzing your ONT data through individual tool executions, as they may offer more flexibility, given the input sizes. All tools included in this workflow are available on request from Seven Bridges and may also be offered separately in the Public Apps gallery in the future.\n\n* Please note that this workflow has instance hints set by default (`c5.9xlarge;ebs-gp2;1024` and `c2-standard-30;pd-ssd;1000` for AWS and Google Cloud environments, respectively). To use different instances, either override the requested instances at task runtime or unset the hints in the workflow editor and use the exposed performance parameters to request different resources.\n\n* If performance is critical, please consider deleting the **cuteSV** node from the workflow before use, as during testing, for some samples, executing this tool took >20 hours on 15 CPU threads.\n\n* The version of **NanoPlot** used in this workflow (1.33.0) requires network access for successful execution. The tool uses the network connection to download a plotting library, it does not transmit any analysis data.\n\n### Performance Benchmarking\n\nDatasets analyzed with GRCh38: \n * [CHM13 ONT data](https://pubmed.ncbi.nlm.nih.gov/29431738/) (50\u00d7 coverage whole-genome sequencing dataset of the CHM13hTERT human cell line on the Oxford Nanopore GridION) - 140 GB FQ.GZ file, downloaded from: https://s3.amazonaws.com/nanopore-human-wgs/chm13/nanopore/rel2/rel2.fastq.gz\n\n * [Cliveome2](https://github.com/nanoporetech/ONT-HG1/blob/master/ver2/CONTENTS.md) ONT data (merged BAM file, 130 GB), converted to FASTQ.GZ with **Biobambam2 bamtofastq**.\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| CHM13 | 22 h 44 min |$34.8 + $3.06 | c5.9xlarge 1000 GB EBS | \n| Cliveome 2 | 26 h 52 min |$41.1 + $3.61 | c5.9xlarge 1000 GB EBS | \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [Broad Institute ONT WGS WF](https://github.com/broadinstitute/long-read-pipelines/blob/main/wdl/ONTWholeGenomeSingleFlowcell.wdl)", "input": [{"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Reference minimap2 index", "encodingFormat": "application/x-fasta"}, {"name": "Read group line"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Number of threads to use"}, {"name": "Number of CPUs per job"}, {"name": "Input DICT file"}, {"name": "Number of threads"}, {"name": "Threads"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Number of threads"}, {"name": "CPUs per job"}, {"name": "Memory per job"}, {"name": "CPUs per job"}, {"name": "Number of threads"}, {"name": "Memory per thread"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Memory per job [MB]"}, {"name": "Memory per job"}], "output": [{"name": "Flagstat statistics"}, {"name": "SVIM SV calls", "encodingFormat": "application/x-vcf"}, {"name": "cuteSV SV calls", "encodingFormat": "application/x-vcf"}, {"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Summarized depth", "encodingFormat": "text/plain"}, {"name": "Merged BAM file", "encodingFormat": "application/x-bam"}, {"name": "Read names and lengths"}, {"name": "Sniffles filtered SV calls", "encodingFormat": "text/x-bed"}, {"name": "Merged mosdepth outputs", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "applicationSubCategory": ["Long Reads", "WGS"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["v1.1", "v1.0"], "dateModified": 1648047365, "dateCreated": 1612365238, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/pacbio-ccs-clr-wgs-variant-calling-1-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/pacbio-ccs-clr-wgs-variant-calling-1-0/4", "applicationCategory": "Workflow", "name": "PacBio CCS or CLR WGS Variant Calling", "description": "**PacBio CCS or CLR WGS Variant Calling** calls variants from PacBio HiFi or CLR reads [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**PacBio CCS or CLR WGS Variant Calling**  workflow can be used to call structural variants in PacBio CCS or CLR data. The workflow can also call small variants in CCS data using **Clair3**. The **CCS mode** input parameter should be used to set the type of analysis to execute.\n\n### Changes Introduced by Seven Bridges\n\n* This workflow implements Broad Institute WDL workflows  [PBCCSWholeGenome.wdl ](https://github.com/broadinstitute/long-read-pipelines/blob/53846066e2fa6612c128a98eac2b9beef793b8dc/wdl/PBCCSWholeGenome.wdl) and [PBCLRWholeGenome.wdl](https://github.com/broadinstitute/long-read-pipelines/blob/53846066e2fa6612c128a98eac2b9beef793b8dc/wdl/PBCLRWholeGenome.wdl) (commit: 53846066e2fa6612c128a98eac2b9beef793b8dc) in CWL1.2.\n* Original Docker images are used for most steps (for **Mosdepth** invocations, a pre-existing Seven Bridges Docker image with the same tool version is used).\n* Most WDL tasks were wrapped as-is, even if they could be split into several tools and minor scripts. For most tools/steps, WDL command lines are slotted into bash scripts in the wrapper.\n* Default WDL parameter values were implemented, unless stated otherwise.\n* Google-specific segments (providing Google credentials, copying outputs with gsutil to specific destinations, etc.) were omitted from the step scripts in which they occur.\n* This workflow combines CCS and CLR variant calling analysis flows (as many of the steps are shared between the two).\n* The type of the analysis (user input **CCS mode**) determines which steps are executed. This input also controls the CCS-specific parameter in **PBSV**.\n* By default, the two WDL workflows are both run in fast/less sensitive mode, which scatters **PBSV** and **Sniffles** SV calling. In this implementation, the slower (more sensitive) approach is used and the callers are not scattered.\n* Mitochondrial variant calling is not supported (this matches the WDL default value for this parameter).\n* **MosDepthOverBed** and **SummarizeDepthOverWholeBed** steps were combined into a single CWL tool: **MosDepthOverBedWithSummarize**.\n* **NanoplotFromBam** was linked to the **ComputeGenomeLength** tool and the required coverage value calculated by parsing Nanoplot textual output within the wrapper (OutputEval JS). The remaining parsed outputs from WDL were not recreated separately - instead, the textual file with all values is provided as an output.\n* **Sniffles** postprocessing script was altered to properly parse the tool output - the original script is set to parse a file which is currently not generated by the tool.\n* **Tabix BGZIP** and **Tabix Index** v1.14 tools were used, as were **Samtools View** and **Samtools Index** v1.11.\n* A newer version of **Clair3** (v0.1-r10) is used. The original WDL file references v0.1-r6.\n* The **DVPepper** variant calling WDL workflow branch was omitted from the CWL implementation as the procedure is still experimental and may not be suitable for production runs.\n\n### Common Issues and Important Notes\n\n* **Input alignments**, **Reference sequence**, **Reference sequence DICT file**, **Reference sequence FAI index** and **CCS mode**  inputs are required.\n* **Input alignments** BAM file should contain the MD tag (required for **Sniffles**). It is recommended to prepare this file using the **PacBio Flowcell Data Processing** workflow, which is a companion workflow to this one.\n\n### Limitations\n\n* This workflow was tested only on human data, using the GRCh38 reference genome assembly.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| CCS 30 GB BAM | 528 min | $10.63 + $1.23 | c5.9xlarge - 1024GB EBS; c4.2xlarge - 1024GB EBS | \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*      \n\n\n### Portability\n\n**PacBio CCS or CLR WGS Variant Calling** workflow was not tested with cwltool.\n\n### References\n\n[1] [PacBio WDL workflows - Broad Institute GitHub repository](https://github.com/broadinstitute/long-read-pipelines/tree/53846066e2fa6612c128a98eac2b9beef793b8dc)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Mosdepth regions BED file", "encodingFormat": "text/x-bed"}, {"name": "Tandem repeat intervals for indel clustering", "encodingFormat": "text/x-bed"}, {"name": "Reference sequence DICT file"}, {"name": "Candidate sites VCF file"}, {"name": "Reference sequence FAI index"}, {"name": "CCS mode"}, {"name": "Memory per job [MB]"}, {"name": "Maximum number of threads"}, {"name": "Memory per job [MB]"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Summarized depth", "encodingFormat": "text/plain"}, {"name": "PBSV variants", "encodingFormat": "application/x-vcf"}, {"name": "Clair VCF variants", "encodingFormat": "application/x-vcf"}, {"name": "Clair GVCF variants", "encodingFormat": "application/x-vcf"}, {"name": "Aligned estimated fold coverage"}, {"name": "Nanoplot stats", "encodingFormat": "text/plain"}, {"name": "Sniffles variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/pacbio-ccs-clr-wgs-variant-calling-1-0/4.png", "codeRepository": ["https://github.com/broadinstitute/long-read-pipelines", "https://github.com/broadinstitute/long-read-pipelines/tree/53846066e2fa6612c128a98eac2b9beef793b8dc", "https://github.com/broadinstitute/long-read-pipelines/releases/tag/3.0.34"], "applicationSubCategory": ["Long Reads", "Structural Variant Calling", "Variant Calling"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1649411029, "dateCreated": 1649411028, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/pacbio-flowcell-data-processing-1-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/pacbio-flowcell-data-processing-1-0/3", "applicationCategory": "Workflow", "name": "PacBio Flowcell Data Processing", "description": "**PacBio Flowcell Data Processing** processes PacBio long reads data [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**PacBio Flowcell Data Processing**  workflow can be used to process PacBio CCS or CLR data in preparation for variant calling. The **Experiment type** input parameter should be used to set the type of analysis to execute.\n\n### Changes Introduced by Seven Bridges\n\n* This workflow implements Broad Institute WDL workflow [PBFlowcell.wdl ](https://github.com/broadinstitute/long-read-pipelines/blob/53846066e2fa6612c128a98eac2b9beef793b8dc/wdl/PBFlowcell.wdl) (commit: 53846066e2fa6612c128a98eac2b9beef793b8dc) in CWL1.2.\n* Original Docker images are used for most steps (for **Mosdepth** invocations, a pre-existing Seven Bridges Docker image with the same tool version is used).\n* Most WDL tasks were wrapped as-is, even if they could be split into several tools and minor scripts. For most tools/steps, WDL command lines are slotted into bash scripts in the wrapper.\n* Default WDL parameter values were implemented, unless stated otherwise.\n* Google-specific segments (providing Google credentials, copying outputs with gsutil to specific destinations, etc.) were omitted from the step scripts in which they occur.\n* **PBIndex** node was added to the beginning of the workflow to support processing inputs which lack the appropriate indices (PBI).\n* **GetRunInfo** node - the Python script used in the WDL task has been skipped as it requires the user to provide valid Google credentials. As the only output value from this script used in the WF is the platform unit string (PU), a workaround bash snippet was used to produce it and pass it on:\n    `samtools view -H $1 | grep '^@RG' -m 1 | awk '{for(i=1;i<=NF;i++){if($i~/^PU/){print $i}}}' | cut -d: -f2 > run_info_pu.txt`\n* Multiple steps were altered to produce dynamically named outputs (based on sample ID or input file names): **Align**, **SummarizePBI**, **ExtractHifiReads**, **MergeFastqs**, **MergeBams**\n* A utility node (**file-junction**) was added to facilitate optional executions of CWL code.\n* A utility step **postprocess-mosdepth** was added to collect and collate the outputs of scattered Mosdepth processes, to prevent the cluttering of the output file list.\n* **NanoplotFromBam** and **SummarizePBI** nodes were linked to the **ComputeGenomeLength** tool and the required coverage values calculated by parsing their textual outputs within the wrapper (OutputEval JS). The remaining parsed outputs from the WDL were not recreated separately - instead, the textual files with all values are provided as outputs.\n* **MergeCCSReports** was set up to use a modified Python script provided in the wrapper itself as the CCS output file no longer matches the parser used in the original Python code. A line of the script was modified, requesting that a line must contain a colon to be parsed (as opposed to containing a specific string, which has changed in CCS). The script also now supports the parsing of mock input files (does not attempt division by zero).\n* **SummarizeCCSReport** was omitted from the workflow to declutter the list of outputs. All values this tool outputs are present in the **Merged CCS report** output file generated by **MergeCCSReports**.\n\n### Common Issues and Important Notes\n\n* **Unaligned BAM file**, **Reference sequence** and **Reference sequence DICT** input files are required.\n* **Validate shards**, **Experiment type** and **Library** input parameters are required.\n*  **Number of shards** and **Quality threshold** input parameters are required, but suitable values for these parameters have been suggested.\n\n### Limitations\n\n* This workflow was tested only on human data, using the GRCh38 reference genome assembly.\n\n### Performance Benchmarking\n\n**PacBio Flowcell Data Processing** performance depends on the size of the inputs and the selected instance. Alignment is the most demanding analysis step and dictates the duration of tasks.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| CCS 50 GB BAM | 213 min | $5.43 + $0.49 | c5.9xlarge - 1024GB EBS | \n| CCS 50 GB BAM | 910 min | $4.46 + $2.11 | c4.2xlarge - 1024GB EBS; r5.xlarge - 1024GB EBS |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*      \n\n\n### Portability\n\n**PacBio Flowcell Data Processing** workflow was not tested with cwltool.\n\n### References\n\n[1] [PacBio WDL workflows - Broad Institute GitHub repository](https://github.com/broadinstitute/long-read-pipelines/tree/53846066e2fa6612c128a98eac2b9beef793b8dc)", "input": [{"name": "Sample ID"}, {"name": "Number of shards"}, {"name": "Validate shards"}, {"name": "Experiment type"}, {"name": "Library"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Reference sequence DICT"}, {"name": "Unaligned BAM file", "encodingFormat": "application/x-bam"}, {"name": "Quality threshold"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Indexed HiFi BAM file", "encodingFormat": "application/x-bam"}, {"name": "Merged reads"}, {"name": "Indexed alignments", "encodingFormat": "application/x-bam"}, {"name": "Summarized depth", "encodingFormat": "text/plain"}, {"name": "Merged mosdepth outputs", "encodingFormat": "text/x-bed"}, {"name": "FlagStat stats", "encodingFormat": "text/plain"}, {"name": "Map file - raw reads", "encodingFormat": "text/plain"}, {"name": "Map file", "encodingFormat": "text/plain"}, {"name": "Read metrics"}, {"name": "Nanoplot stats", "encodingFormat": "text/plain"}, {"name": "Aligned estimated fold coverage"}, {"name": "Merged CCS report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/pacbio-flowcell-data-processing-1-0/3.png", "codeRepository": ["https://github.com/broadinstitute/long-read-pipelines", "https://github.com/broadinstitute/long-read-pipelines/tree/53846066e2fa6612c128a98eac2b9beef793b8dc", "https://github.com/broadinstitute/long-read-pipelines/releases/tag/3.0.34"], "applicationSubCategory": ["Long Reads"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1649411030, "dateCreated": 1649411029, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/phewas-r-0-99-5-4-cwl1-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/phewas-r-0-99-5-4-cwl1-1/6", "applicationCategory": "Workflow", "name": "PheWAS R CWL1.1", "description": "**PheWAS R CWL1.1** can be used to conduct PheWAS analyses [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**PheWAS R CWL1.1** workflow wraps a subset of functions from the PheWAS R package [1,2] and can be used to run a PheWAS analysis. As the PheWAS R package is designed to be used interactively, please consider running a Data Cruncher RStudio session if a higher degree of flexibility and customization is required. The workflow produces RDATA outputs which can be loaded into an R (>3.5) session.\n\nThe workflow consists of three tools: **PheWAS R Prepare Phenotypes CWL1.1**, **PheWAS R PheWAS CWL1.1** and **PheWAS R Plotting**.\n\n**PheWAS R Prepare Phenotypes CWL1.1** includes wrappers for PheWAS R functions `createPhenotypes()` and `plinkPhewasExport()` and can be used to prepare phenotype data for PheWAS analysis with **PheWAS R PheWAS CWL1.1** tool. The tool outputs an RDATA file with the phenotypes (**Prepared phenotypes**) and optionally, produces a file with phenotypes in Plink format **Phenotypes for Plink**) if **Export phenotypes to Plink** input parameter is set, triggering `plinkPhewasExport()`. The tool expects a CSV file as an input, which should contain columns for id, vocabulary_id, code, and index. The column types must be explicitly specified to ensure ICD9 codes are properly parsed.\n\n**PheWAS R PheWAS CWL1.1** tool is a wrapper for two functions from the PheWAS R package: `phewas()` and `phewasDT()`. The tool requires separate inputs for genotypes (in Plink RAW format), phenotypes (as generated by PheWAS `createPhenotypes()` function - please see the **PheWAS R Prepare Phenotypes CWL1.1** tool), covariates and adjustments if needed. The tool outputs a data frame with PheWAS results (and models if requested) (**PheWAS results**) and a browsable table of the results (**PheWAS results DT**) in HTML format.\n\n**PheWAS R Plotting CWL1.1** is a wrapper for the `phewasManhattan()` function from the PheWAS R package and its nested `phenotypeManhattan()` and `phenotypePlot()` functions. The tool takes the results of the `phewas()` function as input, provided as an RDATA file (as output by the **PheWAS R PheWAS CWL1.1** tool) and produces a PNG or PDF plot of the results. Please note that, even though the tool offers parameters for customizing the generated plot, it is still advisable to generate the plot interactively, for example, by loading the RDATA file with PheWAS results into an RStudio (R > 3.5) Data Cruncher session, as the appearance of the final plot will greatly depend on the input data and iterative plotting is expected. For convenience, the RDATA file containing the ggplot2 plot is also created.\n\n\n\n### Changes Introduced by Seven Bridges\n\n* `plinkPhewasExport()` is triggered on request, via the input parameter **Export phenotypes to Plink**.\n* The RDATA output files can be passed on to downstream tools. Please note that the version of R used by the tools is 3.6.2 and that the files will not be readable by sessions with R < 3.5.\n* The wrapper for `phewasDT()` function only supports parameters specific to this function (generic data table parameters were omitted from the wrapper).\n\n### Common Issues and Important Notes\n\n* Inputs **File with ICD codes**, **ICD file R column types**, **Export phenotypes to Plink**, **Plink RAW genotypes**, **Models present in results**, and **Plot type** are required.\n* Input **File with ICD codes** should be a CSV file with columns for id, vocabulary_id, code, and index. The column types must be explicitly specified (**ICD file R column types**) to ensure ICD9 codes are properly parsed.\n* Please note that the workflow has not been tested with custom vocabulary maps.\n* For continuous code count phenotypes, set **Minimum code count** to 'NA'.\n* **Plink RAW genotypes** file can be prepared with Plink:\n`plink --recodeA --bfile starting_data --extract snps_to_analyze.txt --out genotypes`\n\n### Performance Benchmarking\n\nPerformance depends on the size and complexity (number of samples, genotypes and phenotypes) of the input datasets and input parameters. One way to reduce costs is to reduce the amount of allocated storage.\n\nDataset 1: simulated, 20000 samples, 1 genotype, 20 phenotypes\n\nDataset 2: simulated, 20000 samples, 1 genotype, 50 phenotypes\n\nDataset 3: simulated, 20000 samples, 500 genotypes, 100 phenotypes\n\nDataset 4: 100000 samples, 1 genotype, 20 phenotypes\n\nDataset 5: 100000 samples, 500 genotypes, 20 phenotypes\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1 | 4 min | $0.03 + $0.01 | c4.2xlarge - 1000 GB EBS | \n| Dataset 2 | 4 min | $0.03 + $0.01 | c4.2xlarge - 1000 GB EBS | \n| Dataset 3 | 58 min | $0.66 + $0.13 |  c5.4xlarge - 1000 GB EBS |\n| Dataset 3 | 27 min | $0.69 + $0.01 |  c5.9xlarge - 100 GB EBS |\n| Dataset 4 | 10 min | $0.07 + $0.02 |  c4.2xlarge - 1000 GB EBS |\n| Dataset 5 | 51 min | $0.58 + $0.12 |  c5.4xlarge - 1000 GB EBS |\n| Dataset 5 | 34 min | $0.87 + $0.01 |  c5.9xlarge - 100 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**PheWAS R CWL1.1** was tested with cwltool version 3.1.20211107152837. The `icd_codes`, `in_genotypes`, `column_types`, `export_pheno_to_plink`, `in_has_models` and `plot_type` inputs were provided in the job.yaml/job.json file and used for testing.\n\n\n### References\n\n[1] [PheWAS R publication](https://academic.oup.com/bioinformatics/article/30/16/2375/2748157)\n\n[2] [PheWAS R documentation](https://rdrr.io/github/PheWAS/PheWAS/man/phewas-package.html)", "input": [{"name": "File with ICD codes"}, {"name": "ID sex file for restricting phecodes"}, {"name": "Full population IDs file"}, {"name": "Plink RAW genotypes"}, {"name": "Covariates"}, {"name": "Memory per job [MB]"}, {"name": "ICD file R column types"}, {"name": "Minimum code count"}, {"name": "Apply PheWAS exclusions to phecodes"}, {"name": "Translate input to phecodes"}, {"name": "Export phenotypes to Plink"}, {"name": "Output file name prefix"}, {"name": "Plink output file name prefix"}, {"name": "Translate IDs for Plink"}, {"name": "Memory per job [MB]"}, {"name": "Number of CPUs per job"}, {"name": "Adjustments"}, {"name": "Number of cores"}, {"name": "Additive genotypes"}, {"name": "Significance thresholds"}, {"name": "Alpha"}, {"name": "Unadjusted implementation"}, {"name": "Return models"}, {"name": "Minimum number of records"}, {"name": "MASS confidence interval level"}, {"name": "Quick confidence interval level"}, {"name": "Clean phecode predictors"}, {"name": "Add phecode info to DT table"}, {"name": "OR digits for DT"}, {"name": "P digits for DT"}, {"name": "Output file name prefix"}, {"name": "Memory per job [MB]"}, {"name": "Models present in results"}, {"name": "OR direction"}, {"name": "Annotate level"}, {"name": "Y axis interval"}, {"name": "X axis label"}, {"name": "Y axis label"}, {"name": "Max y value"}, {"name": "Max x value"}, {"name": "Annotate phenotype description"}, {"name": "Size of annotation text"}, {"name": "Include the phenotype name in the annotation"}, {"name": "Plot title"}, {"name": "Output file name prefix"}, {"name": "Plot type"}], "output": [{"name": "PheWAS results DT", "encodingFormat": "text/html"}, {"name": "PheWAS results"}, {"name": "PheWAS plot"}, {"name": "Phenotypes for Plink"}, {"name": "Prepared phenotypes"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/PheWAS/PheWAS", "https://github.com/PheWAS/PheWAS/tree/master/R", "https://github.com/PheWAS/PheWAS/archive/3cf3302f5fa3d83c9c33aea83505647ce11c633f.zip"], "applicationSubCategory": ["PheWAS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Robert Carroll", "softwareVersion": ["v1.1"], "dateModified": 1648047366, "dateCreated": 1612365013, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/purple-2-51-cnv-calling-workflow/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/purple-2-51-cnv-calling-workflow/5", "applicationCategory": "Workflow", "name": "PURPLE CNV Calling Workflow", "description": "**PURPLE CNV Calling Workflow** is used for somatic CNV calling and purity and ploidy estimation on WGS data. \n\nIt is based on PURPLE 2.51, and consists of two additional tools - AMBER and COBALT.\n\nThe workflow first calculates B-allele frequency (BAF) with AMBER and read depth ratios with COBALT, which is then used by PURPLE to estimate the purity, ploidy and copy number profile of a tumor sample.\n\n**PURPLE CNV Calling Workflow** supports both GRCh 37 and 38 reference assemblies.\n\nWhole exome sequenced (WES) data is not supported [1]. \n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n\nTo run the workflow, the following input parameters are required:\n\n- **Tumor BAM file** - Tumor BAM file with BAI index file is required.\n- **Matched normal BAM file** - Matched normal BAM file with BAI index file is required.\n- **Reference FASTA** - Reference genome in FASTA format along with FAI and DICT secondary files.\n- **GC profile** - GC profile file, used by both PURPLE and COBALT tools.\n- **VCF file containing likely heterozygous sites** - BAF loci VCF file containing likely heterozygous sites, used by AMBER tool.\n- **Sample name** - Name of the sample being processed.\n\nAlthough not required, the following input parameters are strongly recommended:\n\n- **Structural variant VCF**\n\n    - Providing a high quality set of structural variant calls to PURPLE allows exact base resolution of copy number changes. An accurate estimation of VAF at each breakend also allows PURPLE to infer copy number \n       changes even across very short segments of the genome where a depth based estimation is inaccurate or impractical. For these purposes, PURPLE provides full support and integration with the structural variant \n       caller GRIDSS [1]. \n\n\n- **Somatic variant VCF**\n\n    - A high quality set of somatic SNV and INDEL calls can also improve the accuracy and utility of PURPLE. If provided, passing (or unfiltered) variants enhance the purity and ploidy fit in 2 ways. Firstly, each solution \n       receives a penalty for the proportion of somatic variants which have implied variant copy numbers that are inconsistent with the minor and major allele copy number. Secondly, for highly diploid samples, the VAFs \n       of the somatic variants are used directly to calculate a somatic variant implied purity. For both purposes, accurate VAF estimation is essential thus PURPLE requires the \u2018AD\u2019 (Allelic Depth) field in the VCF. High \n       quality filtering of artifacts and false positive calls is also critical to achieving an accurate fit [1]. \n\n\nMain outputs of the workflow are:\n\n- **Purple purity TSV** - The purity file contains a single row with a summary of the purity fit.\n- **Purple purity range TSV** - The purity range file summarises the best fit per purity sorted by score.\n- **Purple CNV somatic TSV** - The copy number file contains the copy number profile of all (contiguous) segments of the tumor sample. \n\n \n\n### Changes Introduced by Seven Bridges\n\n- PURPLE can optionally persist its output to a SQL database, however this option is not available on the Platform.\n- AMBER Multiple Reference / Donor mode (using multiple reference BAM files) is not supported on the Seven Bridges Platform.\n- AMBER instance hint was changed to c4.8xlarge, due to failing of some tasks during testing caused by lack of CPU memory requirements.\n\n\n\n### Common Issues and Important Notes\n\n1. If structural or somatic VCF files have been supplied then corresponding VCFs will be written to the output enriched with purity information.\n\n2. If **Somatic VCF** is supplied, sample IDs in the VCF header must match normal and tumor sample names used in the rest of the workflow. These names will be either taken from **Matched normal sample name** and **Tumor sample name** input parameters or generated by default by appending '_N' and '_T' to the **Sample name** value for normal and tumor sample name, respectively. \n\n3. Input files (BAM files, reference genome, GC content and VCF files) must use the same chromosome notation (e.g. 1 vs chr1).\n\n4. If **Driver catalog** parameter is set to True, **Driver gene panel** and **Hotspots** VCF file must be provided. Note that generating the driver catalog for SNVs assumes that the VCF has been annotated with SNPEFF [1]. \n\n5. Smoothing, somatic fit and fitting arguments are optional and changing these values without a thorough understanding of the system is not recommended [1]. \n\n6. Current version of the workflow does not support Tumor-Only mode.\n\n\n\n### Performance Benchmarking\n\n\n| Experimental strategy |  Input size | Duration |  Cost |  Instance (AWS on-demand) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|\n|     WGS     |  2 x 110GB |   45min   | $1.13 | **NOTE** |\n|     WGS     | 2 x 220GB |   1h 16min  | $2.03 | **NOTE** |\n\n**NOTE**: The workflow uses multiple types of instances, since the memory resources are allocated dynamically, as predicted via CWL code.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\nDue to high memory consumption, the workflow could not be tested locally with cwltool. Instead, cwltool --validate option is used for ensuring that workflow's CWL code satisfies the CWL 1.1 specification.\n\n### References\n\n[1] [PURPLE Github repository](https://github.com/hartwigmedical/hmftools/tree/master/purple)", "input": [{"name": "Sample name"}, {"name": "Tumor BAM file", "encodingFormat": "application/x-bam"}, {"name": "Matched normal BAM file", "encodingFormat": "application/x-bam"}, {"name": "Somatic variant VCF", "encodingFormat": "application/x-vcf"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "GC profile"}, {"name": "Validation stringency"}, {"name": "Driver gene panel", "encodingFormat": "text/plain"}, {"name": "Matched normal sample name"}, {"name": "Tumor sample name"}, {"name": "VCF file containing likely heterozygous sites", "encodingFormat": "application/x-vcf"}, {"name": "Hotspots", "encodingFormat": "application/x-vcf"}, {"name": "Threads - Cobalt"}, {"name": "Threads - Amber"}, {"name": "Somatic minimum peak"}, {"name": "Somatic minimum variants"}, {"name": "Somatic minimum purity spread"}, {"name": "Somatic minimum purity"}, {"name": "Somatic penalty weight"}, {"name": "Highly diploid percentage"}, {"name": "Minimum diploid tumor ratio count"}, {"name": "Minimum diploid tumor ratio count centromere"}, {"name": "Minimum purity"}, {"name": "Maximum purity"}, {"name": "Purity increment"}, {"name": "Maximum norm factor"}, {"name": "Minimum norm factor"}, {"name": "Norm factor increments"}, {"name": "Ploidy penalty factor"}, {"name": "Minimum ploidy penalty"}, {"name": "Minimum ploidy penalty standard deviation"}, {"name": "Ploidy penalty standard deviation"}, {"name": "No charts"}, {"name": "Maximum ploidy"}, {"name": "Minimum ploidy"}, {"name": "Threads - Purple"}, {"name": "Minimum quality"}, {"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Minimum percentage of median depth"}, {"name": "Maximum percentage of median depth"}, {"name": "Minimum heterozygous allelic frequency percentage"}, {"name": "Maximum heterozygous allelic frequency percentage"}, {"name": "Driver catalog"}, {"name": "Structural variant VCF", "encodingFormat": "application/x-vcf"}, {"name": "Structural variant recovery VCF", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Amber output directory"}, {"name": "Cobalt output directory"}, {"name": "Purple output directory"}, {"name": "Structural VCF", "encodingFormat": "application/x-vcf"}, {"name": "Somatic VCF", "encodingFormat": "application/x-vcf"}, {"name": "Purple purity range TSV"}, {"name": "Purple purity TSV"}, {"name": "Purple CNV gene TSV"}, {"name": "Driver catalog TSV"}, {"name": "Purple CNV somatic TSV"}], "softwareRequirements": ["LoadListingRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/hartwigmedical/hmftools/tree/773c1beccea0f7bb8db2ad10b1da46782ba03531/purple", "https://github.com/hartwigmedical/hmftools/releases/tag/purple-v2.51", "https://github.com/hartwigmedical/hmftools/archive/refs/tags/purple-v2.51.tar.gz", "https://github.com/hartwigmedical/hmftools/tree/773c1beccea0f7bb8db2ad10b1da46782ba03531/purple"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Hartwig Medical Foundation Tools", "softwareVersion": ["v1.1"], "dateModified": 1648049434, "dateCreated": 1639153219, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/qiime2-16s-rrna-feature-classifier-training/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/qiime2-16s-rrna-feature-classifier-training/10", "applicationCategory": "Workflow", "name": "QIIME2 16s rRNA Feature Classifier Training", "description": "**QIIME2 16s rRNA Feature Classifier Training** is a workflow for generating and training a feature classifier for a particular dataset, which can then be used in the **QIIME2 16s rRNA Metagenomic Profiling** workflow.\n\n\nThe workflow takes two inputs:\n\n* **Input reference sequences**, files in FASTA format that contain the representative sequences of a database of choice;\n* **Input reference taxonomy**, a TXT file that contains the taxonomic classifications of a database of choice. \n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\nThis workflow is used for training a feature classifier for a given reference database (and corresponding taxonomy), while taking into account the particular dataset the classifier will be used on. \n\nThe user should provide the reference sequences and the corresponding taxonomic classification in FASTA format and TXT format, respectively. \n\nDepending on the type of analysis to be performed in the **QIIME2 16s rRNA Metagenomic Profiling** workflow, the user can choose one of the following databases available in the **Public Reference Files**: \n* **SILVA 128** (for 16S/Archaea analysis), \n* **GreenGenes 13-08** (for 16S/Archaea analysis), and\n* **CORE** (oral microbiome database; only for bacterial, 16S analysis).\n\nAs the taxonomic classification accuracy improves when a Naive Bayes classifier is trained only on the region of the target sequences [1], the user should input the dataset sequence length (**Truncate length**), the forward and the reverse primer sequences (**Forward primer** and **Reverse primer**), that are specific for a given dataset. For example, in the [QIIME2 Training Feature Classifiers Tutorial](https://docs.qiime2.org/2018.2/tutorials/feature-classifier/), the sequence reads that are being classified are 120-base single-end reads that were amplified with the 515F/806R primer pair. The extracted reads from the reference database are based on matches to this primer pair, and then sliced to 120 bases.\nAfter the reference taxonomy and reference reads are imported into QIIME2 artifact and the reads are extracted, the [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) classifier now is trained using the reference reads and taxonomy that were previously created.\n\nAfter the workflow has successfully completed, it outputs a **Trained Feature Classifier** artifact.\n\n### Changes Introduced by Seven Bridges\nNo modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\nNo common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\nThe execution time takes approximately one hour on the default instance. Unless specified otherwise, the default instance used to run the **QIIME2 16s rRNA Feature Classifier Training** workflow will be c4.4xlarge (AWS), although the c4.2xlarge instance (AWS) can also be used for small databases. In the following table you can find estimates of **QIIME2 16s rRNA Feature Classifier Training** running time and cost.  *Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n| Experiment type | Input size | Duration | Cost | Instance (AWS)\n| --- | --- | --- | --- | --- |\n|Training CORE classifier|Reference sequences for CORE 99OTUs database (1.7MB FASTQ file) and corresponding reference taxonomy (185KB TXT file)| 4m| $0.13 | c4.4xlarge|\n|Training SILVA 128 classifier|Reference sequences for SILVA 16S 99OTUs database (469.5MB FASTQ file) and corresponding reference taxonomy (53.1MB TXT file)| 1h 14m| $2.23 | c4.4xlarge|\n|Training UNITE classifier|Reference sequences for UNITE 99OTUs database (16.5MB FASTQ file) and corresponding reference taxonomy (4MB TXT file)|15m| $0.49 | c4.4xlarge|\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n    \"input_taxonomy\": list(api.files.query(project=project_id, names=['taxonomy_qiime2.txt'])),\n    \"input_sequences\": list(api.files.query(project=project_id, names=['sequences_qiime2.fna'])),\n    \"reverse_primer\": \"GGACTACHVGGGTWTCTAAT\",\n    \"forward_primer\": \"GTGCCAGCMGCCGCGGTAA\",\n    \"type\": \"'FeatureData[Taxonomy]'\",\n    \"source_format\": \"HeaderlessTSVTaxonomyFormat\",\n    \"type_1\": \"'FeatureData[Sequence]'\",\n    \"trim_left\": 120\n}\ntask = api.tasks.create(name='Feature Classifier Training - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n[1] [Werner et al., 2012](https://www.ncbi.nlm.nih.gov/pubmed/21716311)", "input": [{"name": "Input reference taxonomy", "encodingFormat": "application/x-fasta"}, {"name": "Input reference sequences", "encodingFormat": "application/x-fasta"}, {"name": "Classify alpha"}, {"name": "Output classifier name"}, {"name": "Input class weight"}, {"name": "Feat ext n features"}, {"name": "Feat ext lowercase"}, {"name": "Feat ext input"}, {"name": "Feat ext binary"}, {"name": "Feat ext analyzer"}, {"name": "Feat ext alternate sign"}, {"name": "Classify fit prior"}, {"name": "Classify class prior"}, {"name": "Classify chunk size"}, {"name": "Truncate length"}, {"name": "Trim left"}, {"name": "Reverse primer"}, {"name": "Identity"}, {"name": "Forward primer"}, {"name": "Type"}, {"name": "Source format"}, {"name": "Type"}, {"name": "Source format"}], "output": [{"name": "Trained Feature Classifier"}], "codeRepository": [], "applicationSubCategory": ["Metagenomics", "Indexing"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561064, "dateCreated": 1526055469, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/qiime2-16s-rrna-metagenomic-profiling/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/qiime2-16s-rrna-metagenomic-profiling/13", "applicationCategory": "Workflow", "name": "QIIME2 16s rRNA Metagenomic Profiling", "description": "**QIIME2 16S rRNA Metagenomic Profiling** is a workflow based on the QIIME2 toolkit, used to perform an analysis of microbiome samples containing 16s rRNA gene sequences. The analysis is based on [QIIME2 tutorial](https://docs.qiime2.org/2018.2/tutorials/moving-pictures/) and includes demultiplexing and quality filtering, OTU picking, taxonomic assignment, and phylogenetic reconstruction of the given samples.\n\nThe workflow takes three inputs:\n\n* **Input files**, FASTQ or FASTQ.GZ files containing metagenomic sequences; SBG sample ID metadata must be filled according to sample IDs in the **Metadata file**;\n* **Classifier**, a pre-trained Naive Bayes classifier (the output of the **QIIME2 16s rRNA Feature Classifier Training** workflow) in QZA format;\n* **Metadata file**, a TSV file with metadata about samples used in analysis.\n\nThe output from each step of the analysis is given in QIIME2 artifact format, in case a user wants to analyze it or view it on the QIIME2 website. Finally, taxonomic profiles are given in the form of Krona charts for visualizing, and in TSV format for further analysis.\n\nBecause of the large number of output files produced in the WF, all of which related to a group of samples, the **Analysis name** parameter is used for naming those files.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page*.\n\n### Common Use Cases\n\nThe first step in the workflow is importing the FASTQ sequences into appropriate QIIME artifact format. The files with data that need to be transformed are passed to the **QIIME - Import** on **Input files** port. They need to match the [semantic type](https://docs.qiime2.org/2018.2/semantic-types/) specified with the **Type** argument (`--type`). The table below shows possible input files with corresponding semantic types and source formats, which can be used with the **QIIME2 16S rRNA metagenomic profiling** workflow. For manifest-based formats, a manifest file will be created automatically according to the input files. \n\n| Semantic Type | Source format | Input files |\n| --- | --- | --- |\n| EMPSingleEndSequences | None | two FASTQ.GZ files, one containing sequence reads and one containing the associated barcode reads, with the sequence data still multiplexed |\n| EMPPairedEndSequences | None | three FASTQ.GZ files, one containing forward sequence reads, one containing reverse sequence reads, and one containing the associated barcode reads, with the sequence data still multiplexed |\n| SampleData[SequencesWithQuality] | SingleEndFastqManifestPhred33, SingleEndFastqManifestPhred64 | single read FASTQ.GZ / FASTQ files for each sample id |\n| SampleData[PairedEndSequencesWithQuality] | PairedEndFastqManifestPhred33, PairedEndFastqManifestPhred64 | forward and reverse read FASTQ.GZ / FASTQ files for each sample id |\n| SampleData[SequencesWithQuality] | CasavaOneEightSingleLanePerSampleDirFmt | one FASTQ.GZ file for each sample in the study, and the file name includes the sample identifier |\n| SampleData[PairedEndSequencesWithQuality] | CasavaOneEightSingleLanePerSampleDirFmt | two FASTQ.GZ files for each sample in the study, and the file name includes the sample identifier |\n\nNext, the sequences are demultiplexed using the **QIIME - Demultiplex** tool, for which the **Metadata file** in TSV format needs to be provided The file should contain the information about which barcode sequence is associated with each sample. Also, if the sequences provided to the input are already demultiplexed (in that case there is no need to provide barcode sequences to the workflow input), they are just forwarded to the **QIIME - Demultiplex** output. One of the outputs of the **QIIME - Demultiplex** tool is **Demultiplex visualization** in QZV format, which is a summary of the demultiplexed results and can be viewed on the [QIIME2 website](http://view.qiime2.org). Demultiplexed sequences (in QZA format) are then filtered and trimmed based on PHRED scores and ambiguous nucleotide characters with the **QIIME - Quality Filter** tool.\n\n**QIIME - Deblur Denoise** performs sequence quality control and trimming for Illumina data using the Deblur workflow with a 16S reference as a positive filter, and produces features (OTUs) with representative sequences. The specific reference used is the 88% OTUs from the **Greengenes 13_8 database**. This mode of operation should only be used when data were generated from a 16S amplicon protocol on an Illumina platform. The reference is only used to assess whether each sequence is likely to be 16S by a local alignment using SortMeRNA with a permissive e-value; the reference is not used to characterize the sequences. This step requires one parameter that is used in quality filtering, **Trim length** (`--p-trim-length n`), which truncates the sequences at position `n`. In general, Deblur developers recommend setting this value to a length where the median quality score begins to drop too low. The quality plots (prior to quality filtering) should suggest a reasonable choice in the 115 to 130 sequence position range (this is a subjective assessment). Filtering statistics can be viewed in one of the outputs (**Deblur stats visualization**), which is in QZV format, and can be viewed on the [QIIME2 website](http://view.qiime2.org).\n\nAdditionally, **QIIME - Deblur Denoise** creates two feature table artifacts, a FeatureTable[Frequency] QIIME 2 artifact, which contains counts (frequencies) of each unique sequence in each sample in the dataset (**Feature table artifact**), and a FeatureData[Sequence] artifact, which maps feature identifiers in the FeatureTable to the sequences they represent (**Representative sequences artifact**). **Feature table artifact** is further explored with **QIIME - Feature Table Summarize**, which creates visual summaries of the filtered data (giving information on how many sequences are associated with each sample and with each feature, histograms of those distributions, and some related summary statistics). \n\nUsing the outputs from **QIIME - Deblur Denoise** (**Feature table artifact** and **Representative sequences artifact**), **QIIME2 Taxonomy WF** is then used for taxonomic profiling of the samples, relating to sample metadata. This mini workflow outputs **Taxonomy visualization** and **Taxa barplot visualization** files as QIIME2 visualization artifacts, and the classification result, **Classification result**, in QZA format.\n\nTaking **Representative sequences artifact** from the **QIIME - Deblur Denoise** tool, **QIIME2 Phylogeny WF** generates both an unrooted and a rooted phylogenetic tree that relates the features in the input representative sequences to one another. This mini workflow outputs three QIIME2 artifacts: **Unrooted tree**, **Rooted tree** and **Masked alignment** (with masked and aligned representative sequences).\n\nFinally, **SBG QIIME2 reporting** takes the **Taxonomy artifact** and **Feature table artifact** to produce an HTML report with Krona charts and several TSV files with feature data, that can be further analyzed outside the QIIME framework. It has several outputs:\n\n* **Feature table**, a file representing a feature table with taxonomic assignments;\n* **Feature count** file with features counts and **Feature taxonomy** file with feature to taxonomy classification data, both needed for Interactive Differential Abundance Analysis on [Data Cruncher](https://www.sevenbridges.com/data-cruncher-interactive-analysis/);\n* **HTML report with Krona charts**, per sample.\n\n### Changes Introduced by Seven Bridges\n\nNo modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\nMetadata file is specific to a given microbiome study, and it is up to the user to decide what information is collected and tracked as metadata. QIIME 2 metadata is most commonly stored in a TSV (i.e. tab-separated values) file. These files typically have a TSV or TXT file extension.\nWhen creating a metadata file, one should have in mind the following:\n\n* The ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\n* The ID column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. \n* The ID column name (i.e. ID header) must be one of the following values (case-insensitive): *sampleid*, *sample id*, *sample-id*, *featureid*, *feature id*, *feature-id*.\n* IDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored. See the section Recommendations for Identifiers for recommendations on choosing identifiers in your study.\n* IDs cannot be empty (i.e. they must consist of at least one character).\n* IDs must be unique (exact string matching is performed to detect duplicates).\n* At least one ID must be present in the file.\n* IDs cannot use any of the reserved ID column names listed above.\n\n\nMore information about creating the metadata file can be found in the [Metadata in QIIME 2 Tutorial](https://docs.qiime2.org/2018.4/tutorials/metadata/).\n\n\nThis workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Performance Benchmarking\nThis workflow is not memory demanding. Its run time depends on the number of samples. In the following table you can find estimates of **QIIME2 16s rRNA Metagenomic Profiling** running time and cost.  *Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n| Inputs | Duration | Cost | Instance (AWS)\n| --- | --- | --- | --- |\n|10 samples, ranging from 32MB to 73MB in size; metadata file and GreenGenes 13-08-99OTUs classifier (28.5MB) | 19m| $ 0.47 | c4.2xlarge |\n|100 samples, ranging from 13.3MB to 89MB in size; metadata file and GreenGenes 13-08-99OTUs classifier (28.5MB)| 1h 55m | $1.68 | c4.2xlarge |\n|525 samples, ranging from 13.3MB to 103.3MB in size; metadata file and GreenGenes 13-08-99OTUs classifier (28.5MB)| 9h 16m | $4.10 | c4.2xlarge |\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n    \"classifier\": api.files.query(project=project_id, names=['classifier.qza'])[0],\n    \"input_files\": list(api.files.query(project=project_id, names=['sample1.fastq', 'sample12.fastq'])),\n    \"metadata_file\": api.files.query(project=project_id, names=['metadata.tsv'])[0],\n   \"analysis_name\" : \"sample_analysis\",\n   \"type\" : \"SampleData[SequencesWithQuality]\",\n    \"p_sample_stats\" : True,\n    \"trim_length\" : 120\n}\ntask = api.tasks.create(name='QIIME2 Metagenomic Profiling', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\nKuczynski, J., Stombaugh, J., Walters, W. A., Gonz\u00e1lez, A., Caporaso, J. G., & Knight, R. (2011). [Using QIIME to analyze 16S rRNA gene sequences from Microbial Communities](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3249058/). *Current Protocols in Bioinformatics*. http://doi.org/10.1002/0471250953.bi1007s36", "input": [{"name": "Analysis name"}, {"name": "Metadata file"}, {"name": "Input files", "encodingFormat": "application/x-fasta"}, {"name": "Classifier"}, {"name": "Demultiplexed sequences"}, {"name": "Barcode column"}, {"name": "Output visualization file name"}, {"name": "Trim length"}, {"name": "P sample statistics"}, {"name": "threshold of features to be discarded"}, {"name": "Features to retain"}, {"name": "P mean error"}, {"name": "Number of jobs to start"}, {"name": "P indel probability"}, {"name": "Maximum number of indels"}, {"name": "Hash the feature IDs"}, {"name": "Output table name"}, {"name": "Output statistics"}, {"name": "Name of the output representative sequences"}, {"name": "P quality window"}, {"name": "P minimum quality"}, {"name": "P minimum length fraction"}, {"name": "Maximum number of ambiguous base calls"}, {"name": "Output filtered sequences name"}, {"name": "Output filter stats"}, {"name": "Type"}, {"name": "Source format"}], "output": [{"name": "Krona chart", "encodingFormat": "text/html"}, {"name": "Final feature table", "encodingFormat": "text/plain"}, {"name": "Feature taxonomy"}, {"name": "Feature count"}, {"name": "Taxa barplot visualization"}, {"name": "Representative sequences artifact"}, {"name": "Demultiplex visualization"}, {"name": "Filter statistics"}, {"name": "Feature table artifact"}, {"name": "Deblur stats visualization"}, {"name": "Feature table visualization"}, {"name": "Classification result"}, {"name": "Unrooted tree"}, {"name": "Rooted tree"}, {"name": "Masked alignment"}, {"name": "Taxonomy visualization"}], "codeRepository": [], "applicationSubCategory": ["Metagenomics", "Taxonomic Profiling"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561064, "dateCreated": 1526055357, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/qiime2-core-diversity-report/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/qiime2-core-diversity-report/5", "applicationCategory": "Workflow", "name": "QIIME2 Core Diversity Report", "description": "**QIIME2 Core Diversity Report** is a workflow for analyzing samples' diversity and producing reports suitable for viewing across all Seven Bridges platforms.  It is based on `qiime core-diversity` plug-in (**QIIME - Diversity Core-Metrics-Phylogenetic** app), which rarefies data in **Feature table artifact** to a user-specified depth, computes several alpha and beta diversity metrics, and generates principal coordinates analysis (PCoA) plots for each of the beta diversity metrics.\n \nThe workflow takes three inputs:\n\n* **Feature table artifact** - a QIIME2 artifact in QZA format, which contains feature data determined with **QIIME - Deblur** or **QIIME - Dada2** apps;\n* **Phylogeny tree** - a QZA file with phylogeny tree; and\n* **Sample metadata** - a TSV file with metadata on samples used in the analysis.\n\nAll artifacts commonly generated with `qiime core-diversity` are output in one TAR file (**Core diversity artifacts**), and they can be viewed on [QIIME2 View website](http://view.qiime2.org). Additionally, each of the artifacts is transformed into common file formats, such as TSV or TAR.GZ, so they can be downloaded and viewed locally or directly on the Seven Bridges Platform.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page*.\n\n### Common Use Cases\nThis workflow is used in metagenomics analysis of 16s rRNA sequences, for alpha and beta diversity metrics. The metrics computed by default are:\n\n* Alpha diversity\n    * Shannon\u2019s diversity index (a quantitative measure of community richness)\n    * Observed OTUs (a qualitative measure of community richness)\n    * Faith\u2019s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\n    * Evenness (or Pielou\u2019s Evenness; a measure of community evenness)\n* Beta diversity\n    * Jaccard distance (a qualitative measure of community dissimilarity)\n    * Bray-Curtis distance (a quantitative measure of community dissimilarity)\n    * Unweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n    * Weighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features.\n\nIt should be run after the **QIIME2 16s rRNA Metagenomic Profiling** workflow, with parameter **P sampling depth** carefully chosen, depending on the results of the profiling analysis. This parameter is used as the total frequency that each sample should be rarefied to, prior to computing diversity metrics. It is recommended to make your choice by reviewing the information presented in the **Feature Table Visualization** file that was created in  **QIIME2 16s rRNA Metagenomic Profiling** and choosing a value that is as high as possible (so you retain more sequences per sample), while excluding as few samples as possible.\n\n### Changes Introduced by Seven Bridges\n\n* The output files are transformed into TSV or TAR.GZ formats. Original QIIME artifacts are provided within the **Core diversity artifacts** output.\n \n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\nThe execution time takes several minutes on the default instance. Unless specified otherwise, the default instance used to run the **QIIME2 Core Diversity Report** will be c4.2xlarge (AWS).\n\n### API Python Implementation\n\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n    \"input_table\": api.files.query(project=project_id, names=['table.qza'])[0],\n    \"input_phylogeny\": api.files.query(project=project_id, names=['rooted_tree.qza'])[0],\n    \"input_tmetadata\": api.files.query(project=project_id, names=['sample_metadata.tsv'])[0]\n  }\ntask = api.tasks.create(name='QIIME2 Core Diversity Report Test Run', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).", "input": [{"name": "Feature table artifact"}, {"name": "Phylogeny tree"}, {"name": "Sample metadata"}, {"name": "P sampling depth"}], "output": [{"name": "Core diversity artifacts", "encodingFormat": "application/x-tar"}, {"name": "Core diversity results", "encodingFormat": "application/x-tar"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/qiime2-core-diversity-report/5.png", "codeRepository": [], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1527180394, "dateCreated": 1526055469, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/reference-index-creation-centrifuge-1-0-3/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/reference-index-creation-centrifuge-1-0-3/8", "applicationCategory": "Workflow", "name": "Reference Index Creation - Centrifuge 1.0.3", "description": "**The Reference Index Creation workflow** is designed for creating a Centrifuge index for reference sequences from NCBI. It downloads the taxonomy and sequences (using **Centrifuge Download**) and creates an index for them with **Centrifuge Build**. The index can be used in further analyses using Centrifuge tools.\n\nThe pipeline has the following required inputs:\n\n- **Domain** - list of phylogenic domains a user wishes to make the reference for;\n- **Basename** - a base for naming the index files (index files will be named *basename.1.cf*, *basename.2.cf* and *basename.3.cf*).\n\nIf not specified otherwise, the index will be created out of all of the sequences from the phylogenic domains found in the input. However, a user can create an index for only a subset of sequences, in which case the following inputs should be set:\n\n- **Taxids** (`-t`) - list of IDs for sequences a user wishes to make the reference for;\n- **Assembly level** (`-a`) - e.g.\"Chromosome\";\n- **Refseq category** (`-c`) - e.g. \"reference genome\".\n\nThe pipeline has one output:\n\n- **Archived index** - the file will be named *basename.tar* and will contain all three index files. The **Reference genome** metadata field of this archive file will be set to *basename* - this value will be used in the underlying analysis steps.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\nIn metagenomics analysis, one of the most common tasks is to classify reads from a sample into taxonomic or functional categories. Centrifuge toolkit (and its tool called Centrifuge Classifier) is one of the software based solutions available for this task. The classifier will classify each read into one or more taxonomic categories found in an index file.\n\nThe **Reference Index Creation** workflow can be used for creating such an index. Centrifuge indexes can be built with arbitrary sequences. Standard sequences to use for building the index include all of the complete bacterial and viral genomes available, or sequences that are part of the BLAST nt database. The **Reference Index Creation** workflow will first download current taxonomy information from NCBI (`--database=\"taxonomy\"`), which will be stored in nodes.dmp and names.dmp files. Afterwards, it will download reference sequences either from whole phylogenic domains (bacterial, archeae, viral etc.) or from a list of taxa IDs (`--database=\"refseq\"`).\n\nTo run the pipeline, the following parameters should be set:\n\n- **Domain** and optionally **Taxids** - depending on the set of sequences a user wants to create an index for, one or both of these parameters should be set. Multiple values should be comma separated. In case of domains, the tool will automatically add commas between selected domains. In case of taxids, the user should do this manually e.g. \"9031,9606,10090\". If taxids are not specified, whole phylogenic domains will be downloaded.\n- **Basename** - this string parameter refers to the names of the index files that will be created. The value of this parameter is also written to the corresponding output's tar metadata field labeled **Reference genome**.\n\n### Changes Introduced by Seven Bridges\n\n* **Reference Index Creation** is designed to build an index only by downloading sequences from NCBI. If a user has a custom reference sequence file, they should use **Centrifuge Build** instead.\n* The value of the basename input will be automatically written to the metadata field **Reference genome** of the corresponding output file.\n\n### Common Issues and Important Notes\n\n* **Centrifuge Build** has memory demands that depend on the size of the reference sequences. That is why its memory requirement is automatically set to a value higher than seven times the size of the sequences (this value is based on our  experience). This should be changed only with good reason. \n* **Domain** and **Basename** are required inputs. In cases when **taxids** are provided, **Assembly level** and **Refseq category** are required, too.\n\n###  Performance Benchmarking\n\nBased on our experience, Centrifuge Build needs approximately seven times more RAM memory than the size of the sequences used for creating the index. The largest file we tested had 30GB of input sequences, and it used around 190GB of RAM. Because of the extensive range of memory requirements, we chose not to manually set instances for the workflow configuration level. Instead, the memory is set automatically for **Centrifuge Build** on the tool configuration level. Alternatively, parameter **Memory per job** will determine which instance will be used. Otherwise, if the user wants to use a particular instance, we suggest manually setting one of the memory optimized instances, such as r4 instances on AWS.\n \nIn the following table you can find estimates of **Reference Index Creation** running time and cost.  \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n \n\n| Experiment type | Input size | Duration | Cost | Instance (AWS)\n| --- | --- | --- | --- | --- |\n| Creating viral index | 246 MB input sequences, 121 MB output tar index | 10m | $0.21| c4.4xlarge |\n| Creating bacteria index | 36 GB input sequences, 16.4 GB output tar index | 7h 58m | $ 26.6 | m4.16xlarge|\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n    'taxids': '9606,9940,10090',\n    'basename': 'testing_mammals',\n    'refseq_category': 'reference genome',\n    'assembly_level': 'Chromosome',\n    'domain': ['vertebrate_mammalian]\n}\ntask = api.tasks.create(name='Reference Index Creation- API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Centrifuge home page - manual](http://www.ccb.jhu.edu/software/centrifuge/manual.shtml)", "input": [{"name": "Taxids"}, {"name": "Domain"}, {"name": "Basename"}, {"name": "Refseq category"}, {"name": "Assembly level"}, {"name": "Memory per job"}], "output": [{"name": "index_tar", "encodingFormat": "application/x-tar"}], "codeRepository": [], "applicationSubCategory": ["Metagenomics", "Indexing"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561064, "dateCreated": 1509720866, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-alignment-hisat2-2-0-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-alignment-hisat2-2-0-1/6", "applicationCategory": "Workflow", "name": "RNA-Seq Alignment - HISAT2", "description": "This pipeline performs the first step of RNA-Seq analysis - alignment to a reference genome and transcriptome. HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (whole-genome, transcriptome, and exome sequencing data) against the general human population (as well as against a single reference genome). Based on GCSA (an extension of BWT for a graph), we designed and implemented a graph FM index (GFM), an original approach and its first implementation to the best of our knowledge. In addition to using one global GFM index that represents general population, HISAT2 uses a large set of small GFM indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp, with 55,000 indexes needed to cover human population). These small indexes (called local indexes) combined with several alignment strategies enable effective alignment of sequencing reads. This new indexing scheme is called Hierarchical Graph FM index (HGFM). We have developed HISAT 2 based on the HISAT and Bowtie2 implementations. HISAT2 outputs alignments in SAM format, enabling interoperation with a large number of other tools (e.g. SAMtools, GATK) that use SAM.\n\n__Required inputs__\n\nThis workflow has two required inputs:\n\nInput fastq reads (ID: reads) - HISAT2 accepts one fastq file per sample for single-end data, or two files per sample for paired-end data. NOTE: For paired-end reads it is crucial to set the metadata 'paired-end' field as 1 for one input file, as 2 for the other input file.\n\nGenome fasta files (ID: reference_files) - reference sequence to which to align the reads.\n\n__Optional inputs__\n\nSplice sites file (generated by HISAT2 ExtractSpliceSites),\nExons file (generated by HISAT2 ExtractExons),\nSNP file (generated by HISAT2 ExtractSNPs).\n\n__Outputs__\n\nThis workflow generates six output files:\n\nIndex files (ID: output_indexed),\nAligned reads in SAM format (ID: alignment_file),\nAligned reads in sorted BAM format (ID: sorted_bam),\nNovel splice sited detected by HISAT2 (ID: novel_ss_out),\nHISAT2 metrics file (ID: metrics_file), \nAlignment summary metrics reported by Picard CollectAlignmentSummaryMetrics (ID: summary_metrics).\n\n__Common issues__\n\nFor paired-end alignments it is crucial to set the metadata 'paired-end' field as 1 and 2 respectively for the two input fastq files, otherwise the task will fail.", "input": [{"name": "HISAT2 reference or TAR"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "ss", "encodingFormat": "text/plain"}, {"name": "snp"}, {"name": "exon", "encodingFormat": "text/plain"}, {"name": "reads"}], "output": [{"name": "summary_metrics"}, {"name": "alignment_file"}, {"name": "novel_ss_out"}, {"name": "metrics_file"}, {"name": "sorted_bam"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/rna-seq-alignment-hisat2-2-0-1/6.png", "applicationSubCategory": ["Indexing", "Alignment"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1478271135, "dateCreated": 1478268628, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-alignment-star-2-5-4b/24", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-alignment-star-2-5-4b/24", "applicationCategory": "Workflow", "name": "RNA-seq alignment - STAR 2.5.4b", "description": "This workflow performs the first step of RNA-seq analysis - alignment to a reference genome and transcriptome. \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is used in this workflow. **STAR** is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main idea behind this workflow is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nThe important notes about the workflow are: \n\n- The workflow consists of two steps: **STAR Genome Generate**, which generates the necessary index files for the alignment procedure, and **STAR** itself, which does the alignment.\n- The main input to the workflow are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- A **Reference or index** file also needs to be provided. If the index archive is already generated for the desired reference, it can be provided instead of the reference in order to skip indexing and save computation time. Otherwise, a reference FASTA file is required here.\n- If a FASTA file is provided as an input, a **Splice junctions GTF file** (`--sjdbGTFfile`) should also be provided. It\u2019s generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this workflow is the **Aligned reads** output in coordinate-sorted BAM format. The **Transcriptome aligned reads** BAM file is also produced by default. \n- Gene counts are also outputted by default on the **Reads per gene** output. \n- STAR can detect chimeric transcripts, but the parameter **Min segment length** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki, which is set by default). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis (using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads are outputted by default, both in FASTQ format on the **Unmapped reads** output, and within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs. \n- A basic **Two-pass mode** is also turned on during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass. \n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2 or more then one single-end FASTQ file), the Paired-end metadata field must be set. Read Groups will be formed using metadata fields following this hierarchy: Sample ID/Library ID/Platform Unit ID/File Segment Number. If No read groups parameter is set to True, read groups won\u2019t be formed.\n- The GTF and FASTA files need to have compatible transcript IDs and chromosome names.\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in a subsequent analysis (i.e. downstream alignment). This is changed in this workflow - unmapped reads are by default sorted by read ID. This option can be turned off to decrease workflow runtime by a small margin, if unmapped reads are not of further interest. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The workflow can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will be inferred automatically using a built-in **Samtools** script. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the following workflow options in mind - indexing is not performed, unmapped reads are sorted by read id, output BAM is sorted by coordinate and basic two pass mode is turned on:\n\n\n\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 230 MB |     Yes    |     1M     |     101     |   18min   | $0.40 | c4.8xlarge |\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   30min   | $0.60 | c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M    |     101     |   64min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'sjdbGTFfile': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'reference_or_index': list(api.files.query(project=project_id, names=['reference_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='RNA-seq alignment - STAR 2.5.4b - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)", "input": [{"name": "Splice junctions GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Reference or index", "encodingFormat": "application/x-tar"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "Bins size"}, {"name": "Exon parent name"}, {"name": "Gene name"}, {"name": "\"Overhang\" length"}, {"name": "Limit Genome Generate RAM"}, {"name": "Output unmapped reads"}, {"name": "Output format"}, {"name": "Output sorting type"}, {"name": "Strand field flag"}, {"name": "Write unmapped in SAM"}, {"name": "Sorting in SAM"}, {"name": "Min segment length"}, {"name": "Quantification mode"}, {"name": "Reads to process in 1st step"}, {"name": "Two-pass mode"}, {"name": "\"Overhang\" length"}, {"name": "Chimeric output type"}, {"name": "Prohibit alignment type"}, {"name": "Limit BAM sorting memory"}, {"name": "Chimeric filter"}, {"name": "Unmapped output file names"}, {"name": "No read groups"}, {"name": "Sort unmapped reads"}, {"name": "Number of threads"}, {"name": "Output file name prefix"}, {"name": "BAM remove duplicates type"}, {"name": "BAM remove duplicates mate2 bases"}], "output": [{"name": "Unmapped reads", "encodingFormat": "text/fastq"}, {"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Transcriptome aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Chimeric alignments", "encodingFormat": "application/x-sam"}, {"name": "Chimeric junctions"}, {"name": "Splice junctions"}, {"name": "Reads per gene"}, {"name": "Log files"}], "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/archive/2.5.4b.tar.gz"], "applicationSubCategory": ["Alignment", "RNA"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1630488106, "dateCreated": 1522949980, "contributor": ["admin", "marko-division-test/ruzica_gagic"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-alignment-star-2-5-4b-cwl-1-0/18", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-alignment-star-2-5-4b-cwl-1-0/18", "applicationCategory": "Workflow", "name": "RNA-seq alignment - STAR 2.5.4b CWL1.0", "description": "This workflow performs the first step of RNA-seq analysis - alignment to a reference genome and transcriptome. \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is used in this workflow. **STAR** is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main idea behind this workflow is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nThe important notes about the workflow are: \n\n- The workflow consists of two steps: **STAR Genome Generate**, which generates the necessary index files for the alignment procedure, and **STAR** itself, which does the alignment.\n- The main input to the workflow are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- A **Reference or index** file also needs to be provided. If the index archive is already generated for the desired reference, it can be provided instead of the reference in order to skip indexing and save computation time. Otherwise, a reference FASTA file is required here.\n- If a FASTA file is provided as an input, a **Splice junctions GTF file** (`--sjdbGTFfile`) should also be provided. It\u2019s generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this workflow is the **Aligned reads** output in coordinate-sorted BAM format. The **Transcriptome aligned reads** BAM file is also produced by default. \n- Gene counts are also outputted by default on the **Reads per gene** output. \n- STAR can detect chimeric transcripts, but the parameter **Min segment length** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki, which is set by default). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis (using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads are outputted by default, both in FASTQ format on the **Unmapped reads** output, and within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs. \n- A basic **Two-pass mode** is also turned on during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass. \n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number**).\n- The GTF and FASTA files need to have compatible transcript IDs and chromosome names.\n- This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in a subsequent analysis (i.e. downstream alignment). This is changed in this workflow - unmapped reads are by default sorted by read ID. This option can be turned off to decrease workflow runtime by a small margin, if unmapped reads are not of further interest. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The workflow can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will be inferred automatically using a built-in **Samtools** script. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the following workflow options in mind - indexing is not performed, unmapped reads are sorted by read id, output BAM is sorted by coordinate and basic two pass mode is turned on:\n\n\n\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 230 MB |     Yes    |     1M     |     101     |   18min   | $0.40 | c4.8xlarge |\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   30min   | $0.60 | c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M    |     101     |   64min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'sjdbGTFfile': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'reference_or_index': list(api.files.query(project=project_id, names=['reference_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='RNA-seq alignment - STAR 2.5.4b - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)", "input": [{"name": "Splice junctions GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Reference or index", "encodingFormat": "application/x-tar"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "\"Overhang\" length"}, {"name": "Exons' parents name"}, {"name": "Gene name"}, {"name": "Limit Genome Generate RAM"}, {"name": "Bins size"}, {"name": "Unmapped output file names"}, {"name": "Two-pass mode"}, {"name": "Reads to process in 1st step"}, {"name": "Sort unmapped reads"}, {"name": "\"Overhang\" length"}, {"name": "Number of threads"}, {"name": "Prohibit alignment type"}, {"name": "Quantification mode"}, {"name": "Output sorting type"}, {"name": "Write unmapped in SAM"}, {"name": "Output format"}, {"name": "Strand field flag"}, {"name": "Sorting in SAM"}, {"name": "Output unmapped reads"}, {"name": "Output file name prefix"}, {"name": "No read groups"}, {"name": "Limit BAM sorting memory"}, {"name": "Min segment length"}, {"name": "Chimeric output type"}, {"name": "Chimeric filter"}, {"name": "BAM remove duplicates type"}, {"name": "BAM remove duplicates mate2 bases"}], "output": [{"name": "Unmapped reads", "encodingFormat": "text/fastq"}, {"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Transcriptome aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Chimeric alignments", "encodingFormat": "application/x-sam"}, {"name": "Chimeric junctions"}, {"name": "Splice junctions"}, {"name": "Reads per gene"}, {"name": "Log files"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/rna-seq-alignment-star-2-5-4b-cwl-1-0/18.png", "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/archive/2.5.4b.tar.gz"], "applicationSubCategory": ["Alignment", "RNA", "CWL1.0"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["v1.0"], "dateModified": 1584566925, "dateCreated": 1523623643, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-alignment-star-2-7-0e/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-alignment-star-2-7-0e/4", "applicationCategory": "Workflow", "name": "RNA-seq alignment - STAR 2.7.0e", "description": "This workflow performs the first step of RNA-seq analysis - alignment to a reference genome and transcriptome. \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is used in this workflow. **STAR** is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main idea behind this workflow is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nThe important notes about the workflow are: \n\n- The workflow consists of two steps: **STAR Genome Generate**, which generates the necessary index files for the alignment procedure, and **STAR** itself, which does the alignment.\n- The main input to the workflow are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- A **Reference or index** file also needs to be provided. If the index archive is already generated for the desired reference, it can be provided instead of the reference in order to skip indexing and save computation time. Otherwise, a reference FASTA file is required here.\n- If a FASTA file is provided as an input, a **Splice junctions GTF file** (`--sjdbGTFfile`) should also be provided. It\u2019s generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this workflow is the **Aligned reads** output in coordinate-sorted BAM format. The **Transcriptome aligned reads** BAM file is also produced by default. \n- Gene counts are also outputted by default on the **Reads per gene** output. \n- STAR can detect chimeric transcripts, but the parameter **Min segment length** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki, which is set by default). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis (using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads are outputted by default, both in FASTQ format on the **Unmapped reads** output, and within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs. \n- A basic **Two-pass mode** is also turned on during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass. \n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2 or more then one single-end FASTQ file), the **Paired-end** metadata field must be set. Read Groups will be formed using metadata fields following this hierarchy: **Sample ID/Library ID/Platform Unit ID/File Segment Number**. If **No read groups** parameter is set to **True**, read groups won\u2019t be formed.   \n\nAdditionally, if you have long reads available and wish to map them with STAR, setting the **STARlong** option will run the **STARlong** algorithm instead, which uses a more efficient seed stitching algorithm for long reads (>200b), and also uses different array allocations. Selecting this boolean option will also automatically change the following parameters of STAR to comply with long read alignment best practices:\n`--outFilterMultimapScoreRange 20`\n`--outFilterScoreMinOverLread 0`\n`--outFilterMismatchNmax 1000`\n`--winAnchorMultimapNmax 200`\n`--seedSearchLmax 30`\n`--seedSearchStartLmax 12`\n`--seedPerReadNmax 100000`\n`--seedPerWindowNmax 100`\n`--alignTranscriptsPerReadNmax 100000`\n`--alignTranscriptsPerWindowNmax 10000`\n\nThe **STARsolo** algorithm is run with the **STARsolo** option. Selecting this option will turn the `--soloType Droplet` parameter in the command line, which will essentially run the **STARsolo** algorithm, this time assuming that the input reads are provided in the following fashion: the first file has to be the cDNA read, and the 2nd file has to be the barcode (cell+UMI) read, i.e. `--readFilesIn cDNAfragmentSequence.fastq.gz CellBarcodeUMIsequence.fastq.gz`. To invoke this behavior, please set the **Paired End** metadata on these files to 1 and 2, respectively (1 for the cDNA file and 2 for the barcode file). \n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- The GTF and FASTA files need to have compatible transcript IDs and chromosome names.\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in a subsequent analysis (i.e. downstream alignment). This is changed in this workflow - unmapped reads are by default sorted by read ID. This option can be turned off to decrease workflow runtime by a small margin, if unmapped reads are not of further interest. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The workflow can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will be inferred automatically using a built-in **Samtools** script. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the following workflow options in mind - indexing is not performed, unmapped reads are sorted by read id, output BAM is sorted by coordinate and basic two pass mode is turned on:\n\n\n\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 230 MB |     Yes    |     1M     |     101     |   18min   | $0.40 | c4.8xlarge |\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   30min   | $0.60 | c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M    |     101     |   64min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'sjdbGTFfile': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'reference_or_index': list(api.files.query(project=project_id, names=['reference_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='RNA-seq alignment - STAR 2.5.4b - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)", "input": [{"name": "Splice junctions GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Reference or index", "encodingFormat": "application/x-tar"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "\"Overhang\" length"}, {"name": "Exon parent name"}, {"name": "Gene name"}, {"name": "Limit Genome Generate RAM"}, {"name": "Bins size"}, {"name": "Output unmapped reads"}, {"name": "Output format"}, {"name": "Output sorting type"}, {"name": "Strand field flag"}, {"name": "Write unmapped in SAM"}, {"name": "Sorting in SAM"}, {"name": "Min chimeric segment"}, {"name": "Quantification mode"}, {"name": "Reads to process in 1st step"}, {"name": "Two-pass mode"}, {"name": "\"Overhang\" length"}, {"name": "Extra alignment score"}, {"name": "Chimeric output type"}, {"name": "Prohibit alignment type"}, {"name": "Limit BAM sorting memory"}, {"name": "Chimeric filter"}, {"name": "Unmapped output file names"}, {"name": "No read groups"}, {"name": "Number of threads"}, {"name": "Output file name prefix"}, {"name": "BAM remove duplicates type"}, {"name": "BAM remove duplicates mate2 bases"}, {"name": "Output BAM sorting threads"}, {"name": "STARlong"}, {"name": "STARsolo"}], "output": [{"name": "Unmapped reads", "encodingFormat": "text/fastq"}, {"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Transcriptome aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Chimeric alignments", "encodingFormat": "application/x-sam"}, {"name": "Chimeric junctions"}, {"name": "Splice junctions"}, {"name": "Reads per gene"}, {"name": "Log files"}, {"name": "STARsolo outputs"}], "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/archive/2.7.0e.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Alignment"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1617276567, "dateCreated": 1552473823, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-alignment-star-2-7-3a-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-alignment-star-2-7-3a-cwl1-0/7", "applicationCategory": "Workflow", "name": "RNA-seq alignment - STAR 2.7.3a", "description": "This workflow performs the first step of RNA-seq analysis - alignment of the reads to a reference genome. \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is used in this workflow. **STAR** is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main idea behind this workflow is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nThe important notes about the workflow are: \n\n- The workflow consists of two steps: **STAR Genome Generate**, which generates the necessary index files for the alignment procedure, and **STAR** itself, which does the alignment.\n- The main input to the workflow are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- A **Reference or index** file also needs to be provided. If the index archive is already generated for the desired reference, it can be provided instead of the reference in order to skip indexing and save computation time. Otherwise, a reference FASTA file is required here.\n- If a FASTA file is provided as an input, a **Splice junctions GTF file** (`--sjdbGTFfile`) should also be provided. It\u2019s generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this workflow is the **Aligned reads** output in coordinate-sorted BAM format. The **Transcriptome aligned reads** BAM file is also produced by default. \n- Gene counts are also outputted by default on the **Reads per gene** output. \n- STAR can detect chimeric transcripts, but the parameter **Min segment length** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki, which is set by default). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis (using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads are outputted by default, both in FASTQ format on the **Unmapped reads** output, and within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs. \n- A basic **Two-pass mode** is also turned on during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass. \n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2 or more then one single-end FASTQ file), the **Paired-end** metadata field must be set. Read Groups will be formed using metadata fields following this hierarchy: **Sample ID/Library ID/Platform Unit ID/File Segment Number**. If **No read groups** parameter is set to **True**, read groups won\u2019t be formed.     \n\nAdditionally, if you have long reads available and wish to map them with STAR, setting the **STARlong** option will run the **STARlong** algorithm instead, which uses a more efficient seed stitching algorithm for long reads (>200b), and also uses different array allocations. Selecting this boolean option will also automatically change the following parameters of STAR to comply with long read alignment best practices:\n`--outFilterMultimapScoreRange 20`\n`--outFilterScoreMinOverLread 0`\n`--outFilterMismatchNmax 1000`\n`--winAnchorMultimapNmax 200`\n`--seedSearchLmax 30`\n`--seedSearchStartLmax 12`\n`--seedPerReadNmax 100000`\n`--seedPerWindowNmax 100`\n`--alignTranscriptsPerReadNmax 100000`\n`--alignTranscriptsPerWindowNmax 10000`\n\n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- The GTF and FASTA files need to have compatible transcript IDs and chromosome names.\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in a subsequent analysis (i.e. downstream alignment). This is changed in this workflow - unmapped reads are by default sorted by read ID. This option can be turned off to decrease workflow runtime by a small margin, if unmapped reads are not of further interest. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The workflow can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will be inferred automatically using a built-in **Samtools** script. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes with the default workflow options (indexing is not performed, size of genome generate archive - 27.8 GB):\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:----------------:|:-------------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 232 MB |     Yes    |     1M     |     101     |   15min   | $0.44 | c5.9xlarge |\n|     RNA-Seq     |  2 x 2.2 GB |     Yes    |     9.5M     |     101     |   20min   | $0.56 | c5.9xlarge |\n|     RNA-Seq     |  2 x 10.8GB  |     Yes   |     47.5M   |     101     |   44min  | $1.23 | c5.9xlarge |\n|     RNA-Seq     |  2 x 21.5GB  |     Yes   |     95M    |     101     |   1h 4min  | $1.79 | c5.9xlarge |\n\n\nRuntime and task cost for one of previous samples with the default workflow options when indexing is performed (size of reference file - 2.9 GB, size of GTF file - 1.1 GB):\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:----------------:|:-------------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 2.2 GB |     Yes    |     9.5M     |     101     |   46min   | $1.29 | c5.9xlarge |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_gene_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_reference_or_sgg_archive': list(api.files.query(project=project_id, names=['reference_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='RNA-seq alignment - STAR 2.7.3a - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)\n[2] [Manual](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)", "input": [{"name": "Read sequence", "encodingFormat": "application/x-bam"}, {"name": "Reference/Index files", "encodingFormat": "application/x-tar"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Bins size"}, {"name": "Exon parent name"}, {"name": "Gene name"}, {"name": "\"Overhang\" length"}, {"name": "Limit Genome Generate RAM"}, {"name": "Output unmapped reads"}, {"name": "Output format"}, {"name": "Output sorting type"}, {"name": "Strand field flag"}, {"name": "Write unmapped in SAM"}, {"name": "Sorting in SAM"}, {"name": "Library ID"}, {"name": "Platform"}, {"name": "Min chimeric segment"}, {"name": "Quantification mode"}, {"name": "Reads to process in 1st step"}, {"name": "Two-pass mode"}, {"name": "\"Overhang\" length"}, {"name": "Chimeric output type"}, {"name": "Prohibit alignment type"}, {"name": "Limit BAM sorting memory"}, {"name": "Chimeric filter"}, {"name": "Unmapped output file names"}, {"name": "No read groups"}, {"name": "Number of threads"}, {"name": "Output file name prefix"}, {"name": "BAM remove duplicates type"}, {"name": "BAM remove duplicates mate2 bases"}, {"name": "Output BAM sorting bins"}, {"name": "Output BAM sorting threads"}, {"name": "STARlong"}, {"name": "Memory per job"}], "output": [{"name": "Unmapped reads", "encodingFormat": "text/fastq"}, {"name": "Transcriptome alignments", "encodingFormat": "application/x-bam"}, {"name": "Splice junctions"}, {"name": "Reads per gene"}, {"name": "Log files"}, {"name": "Chimeric junctions"}, {"name": "Chimeric alignments", "encodingFormat": "application/x-sam"}, {"name": "Aligned SAM/BAM", "encodingFormat": "application/x-bam"}, {"name": "Summary statistics log"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/rna-seq-alignment-star-2-7-3a-cwl1-0/7.png", "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/tree/master/source", "https://github.com/alexdobin/STAR/archive/2.7.3a.tar.gz", "https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf"], "applicationSubCategory": ["Alignment", "Transcriptomics", "CWL1.0"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["v1.0"], "dateModified": 1617276502, "dateCreated": 1581091555, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-alignment-tophat/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-alignment-tophat/15", "applicationCategory": "Workflow", "name": "RNA-seq Alignment - TopHat", "description": "RNA-Seq technology represents a powerful method to interrogate gene expression. In addition to determining total gene expression levels, RNA-Seq allows quantitation of isoforms, identification of novel transcripts, and interrogation of RNA editing events. The first step in profiling the transcriptome is the alignment of RNA-Seq reads against the reference genome. This step reveals the location in the genome from which the reads originated.\n\nThis pipeline uses the popular split-read aligner, TopHat, to map reads to a reference genome, and it is set up to accommodate the most common experimental conditions (e.g. RNA-Seq experiments of samples from well annotated transcriptomes such as Human and Mouse). It utilizes a transcript annotation file (GTF) to speed read mapping across known splice junctions. This pipeline will generate alignment files that can then be compared for differential expression, analyzed to discover novel transcripts, or viewed directly in a genome browser. TopHat is highly versatile and by building pipelines, you are able to exploit its many functions including the use of experimentally identified junctions, insertions and deletions.\n\nThis pipeline can be used in combination with the \"RNA-Seq Differential Expression\" (available in Public Apps) to take you all the way from raw sequencing reads to a list of differentially expressed genes.\n\nAlignment of RNA-Seq reads to a reference genome is performed using the split read aligner TopHat. TopHat incorporates the ultrafast short read aligner **Bowtie 2**. While Bowtie 2 is able to align tens of millions of reads per CPU hour, it does not allow alignments between the read and genome to contain large gaps. This limitation precludes the use of Bowtie 2 to align reads that span introns. TopHat was built to overcome this restriction - any reads that cannot be initially aligned to the genome are broken up by TopHat into smaller pieces which, when processed independently can be aligned by Bowtie 2. When read segments are found to align to the genome far apart from each other, TopHat infers that the read spans a splice junction and estimates the location of the splice sites. While TopHat can build up an index of splice sites in the transcriptome without a priori gene or splice site annotations, alignment speed and accuracy is increased by providing this information during the mapping process. \n\n###Inputs###\n\n**Reads**: This pipeline accepts both single stranded or paired-end RNA-Seq data in FASTQ format. If paired-end reads are used, the read pair metadata fields must be set as 1 and 2. The metadata field Sample ID should be unique for each biological sample.\n\n**Reference or index files**: For proper TopHat performance (which relies on Bowtie 2 for alignment) Bowtie 2 requires that the reference genome is indexed before read alignment can be performed. We have added **Bowtie 2 Indexer** to this pipeline for reference file indexing. This indexing can be time intensive, and in order to optimize for execution time of this pipeline, you can index reference file separately using Bowtie 2 Indexer. You may create a short pipeline with this tool and reuse index archive if you intend to perform several alignments with the same reference. By default, you will be provided with the tar bundle containing index files (`human_g1k_v37_decoy.phiX174_bowtie2-2.2.6.tar` obtained from `human_g1k_v37_decoy.phiX174.fasta`) as a suggested file.\n\n**FASTA reference**: FASTA file containing reference genome. Pipeline is provided with `human_g1k_v37_decoy.phiX174.fasta` as a suggested reference file.\n\n**GTF annotations**: Gene Transfer Format file containing known gene annotations. Using a GTF file will increase mapping speed and accuracy but it is not required. It is critical that the chromosome numbering schema used in the GTF file matches that used in the Reference file (UCSC convention is to number chromosomes as Chr _number_, whereas ensembl simply numbers chromosomes just with _number_). Suggested file for this input is `Homo_sapiens.GRCh37.75.gtf` annotation file.\n\n###Q&A###\n\n***Q: What should I do if I already have Bowtie2 index files, not archived as tar bundle?***\n\n***A***: You can provide your *.bt2 files to **SBG Compressor** app from our public apps and set \"TAR\" as your output format. After the task is finished, **you should assign common prefix of the index files to the `Reference genome` metadata field** and your TAR is ready for use.\n\n***Example:***\nIndexed files: chr20.1.bt2, chr20.2.bt2, chr20.3.bt2, chr20.4.bt2, chr20.rev.1.bt2, chr20.rev.2.bt2\n\nMetadata - `Reference genome`: **chr20**\n\n###Common issues###\nOne of the most common issues when running TopHat is incompatibility between reference genome and annotations. Please, make sure that you are using compatible FASTA (from which you have created tar bundle with index files) and GTF files in order to run tasks successfully. If you suspect your task has failed due to this incompatibility, you can check the last line in `job.err.log` file which would look as following if your assumptions are correct: `Error: Couldn't build bowtie index with err = 1`.\n\n**Important note: In case of paired-end alignment it is crucial to set metadata `Paired-end` field to \"1\" or \"2\". Sequences specified as \"1\" must correspond file-for-file and read-for-read with those specified as \"2\". Reads may be a mix of different lengths. In case of unpaired reads, the same metadata field should be set to '-'. Only one type of alignment can be performed at once, so all specified reads should be either paired or unpaired.**", "input": [{"name": "FASTA reference", "encodingFormat": "application/x-fasta"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "GTF annotations"}, {"name": "Reference or index files", "encodingFormat": "application/x-tar"}, {"name": "Transcriptome only"}, {"name": "Transcriptome max hits"}, {"name": "Splice mismatches"}, {"name": "Segment mismatches"}, {"name": "Segment length"}, {"name": "Report secondary alignments"}, {"name": "Read realign edit distance"}, {"name": "Read mismatches"}, {"name": "Read gap length"}, {"name": "Read edit distance"}, {"name": "Prefilter multihits"}, {"name": "Disable BAM sorting"}, {"name": "No novel juncs"}, {"name": "No novel indels"}, {"name": "Disable mixed alignments"}, {"name": "Disable discordant alignments"}, {"name": "Minimum segment intron"}, {"name": "Minimum intron length"}, {"name": "Minimum coverage intron"}, {"name": "Minimum anchor length"}, {"name": "Microexon search"}, {"name": "Maximum segment intron"}, {"name": "Maximum multihits"}, {"name": "Maximum intron length"}, {"name": "Maximum insertion length"}, {"name": "Maximum deletion length"}, {"name": "Maximum coverage intron"}, {"name": "Mate standard deviation"}, {"name": "Mate inner distance"}, {"name": "Library type"}, {"name": "Keep FASTA order"}, {"name": "Fusion search"}, {"name": "Fusion read mismatches"}, {"name": "Fusion multireads"}, {"name": "Fusion multipairs"}, {"name": "Fusion minimum distance"}, {"name": "Fusion ignore chromosomes"}, {"name": "Fusion anchor length"}, {"name": "Coverage search"}, {"name": "Bowtie -n"}, {"name": "Function type"}, {"name": "Coefficient B"}, {"name": "Constant A"}, {"name": "Reference gap penalties"}, {"name": "Read gap penalties"}, {"name": "Bowtie2 preset"}, {"name": "Ambiguous character penalty"}, {"name": "Function type"}, {"name": "Coefficient B"}, {"name": "Constant A"}, {"name": "Mismatch penalty"}, {"name": "Function type"}, {"name": "Coefficient B"}, {"name": "Constant A"}, {"name": "Disallow gaps"}, {"name": "Max number of re-seed"}, {"name": "Allowed mismatch number"}, {"name": "Seed substring length"}, {"name": "Seed extension attempts"}], "output": [{"name": "Output BAM file", "encodingFormat": "application/x-bam"}, {"name": "FastQC archived report", "encodingFormat": "application/zip"}, {"name": "Unmapped reads"}, {"name": "TopHat junctions"}, {"name": "TopHat insertions"}, {"name": "Tophat deletions"}, {"name": "TopHat summary metrics"}, {"name": "Picard summary metrics", "encodingFormat": "text/plain"}], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033583, "dateCreated": 1453800054, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-de-novo-assembly-and-analysis-trinity-2-4-0/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-de-novo-assembly-and-analysis-trinity-2-4-0/12", "applicationCategory": "Workflow", "name": "RNA-Seq De Novo Assembly and Analysis - Trinity 2.4.0", "description": "**RNA-Seq De Novo Assembly and Analysis - Trinity 2.4.0** is a workflow for the de novo reconstruction of a transcriptome and basic downstream analysis using **Trinity** software package.\n\nThe workflow takes RNA-Seq data in the FASTQ format. Input files can also be gzip-compressed, in which case they should have a .gz extension. The outputs are assembled transcripts, basic assembly statistics, samples aligned to transcriptome (BAMs), transcript abundance estimation, differentially expressed transcripts, and features and clusters of features with common expression profiles.\n\n**Trinity**, developed at the [Broad Institute](https://www.broadinstitute.org/) and the [Hebrew University of Jerusalem](http://www.cs.huji.ac.il/), represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-seq data. **Trinity Assembler** combines three independent software modules: _Inchworm_, _Chrysalis_, and _Butterfly_, applied sequentially to process large volumes of RNA-seq reads. Briefly, the process works as follows:\n\n+ _Inchworm_ assembles the RNA-seq data into the unique sequences of transcripts, often generating full-length transcripts for a dominant isoform, but then reports just the unique portions of alternatively spliced transcripts.\n+ _Chrysalis_ clusters the Inchworm contigs into clusters and constructs complete de Bruijn graphs for each cluster. Each cluster represents the full transcriptional complexity for a given gene (or sets of genes that share sequences). Chrysalis then partitions the full read set among these disjoint graphs.\n+ _Butterfly_ processes those individual graphs in parallel, tracing the paths that reads and pairs of reads take within the graph, ultimately reporting full-length transcripts for alternatively spliced isoforms, and teasing apart transcripts that correspond to paralogous genes. [1]\n\nAfter the assembly portion, this workflow uses Trinity software package tools which perform downstream analysis such as:\n\n+ **Trinity Stats** - Obtaining the basic statistics for FASTA file assembled by Trinity Assembler.\n+ **Trinity Component Distribution** - Providing the distribution of number of transcripts per gene.\n+ **Trinity Align and Estimate Abundance** - Aligning the raw RNA-Seq reads to assembled transcripts and determining the transcript abundance estimation based on resulting alignments\n+ **Trinity Count Features** - Counting the number of genes (components) and isoforms (transcripts) that are expressed above a certain FPKM expression threshold.\n+ **Trinity Run DE Analysis** & **Trinity analyse DE** -\nIdentifying and analysing differentially expressed transcripts\n+ **Trinity Define Clusters** - Creating clusters of features with common expression profiles.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThis workflow is designed for the de novo reconstruction of a transcriptome and required inputs are RNA-Seq data in FASTQ (input files can be gzip-compressed, in which case they should have a .gz extension) and parameter for either k-means (**Number Of Clusters To Define Via K-means** (`-K`)) or hierarchical tree clustering (**Number Of Clusters To Cut Tree** (`--Ktree`) or **Cut Percentage For Tree** (`--Ptree`)) for **Trinity Define Clusters** tool in order to work properly in the workflow.  \n\nThe workflow outputs are:   \n  \n+ Assembled **Transcripts** - full-length and nearly full-length transcripts across a broad range of expression levels and sequencing depths.\n+ Statistical values (**Stats**, **Trancscript Lengths**, **Component Sizes**...) - number of genes (components) and isoforms (transcripts), and contiguity of the assembly, etc.\n+ Alignments (**BAMs**) - RNA-Seq reads mapped to the assembled transcriptome, which can be visualized on the Platform using the genome browser.\n+ Transcript abundance estimation (**Gene Abundances** and **Isoform Abundances**) - isoform and gene expression value estimation based on the resulting alignments.\n+ Differentially expressed transcripts and plots (**DE Analysis**, **DE Analysis Plots**, **FPKM Reduced**, **numDE Features**) - identified and analysed with Trinity software wrapper around Bioconductor edgeR.\n+ Clusters of features with common expression profiles (**All FPKMs**, **Cluster Plots**, **Clustered Heatmap**) - plots with the mean-centered expression patterns for each cluster, and a heatmap plot.\n\nAt the beginning of the workflow, FASTQ validation is performed with **FastqValidator** tool which runs in parallel with **Trinity Assembly**. This tool will check if input FASTQ files are formatted properly and it will **fail the task** if they are not, causing failure at the very beginning of the workflow and preventing the wastage of time and recourses on an execution will fail on the **Trinity** tools.  \n\nThe workflow can handle both pair end and single end reads. Large RNA-Seq data sets, such as those exceeding 300M pairs, are best suited for in silico normalization prior to running **Trinity** [2]. This version of **Trinity** will perform in silico normalization by default. You can turn it off with **Do *not* run in silico normalization of reads** (`--no_normalize_reads`) **Trinity Assembler** parameter.      \n\nTrinity workflow works better with strand-specific data, but it can also accommodate non-strand-specific data. **Strand Specific RNA-Seq Read Orientation** (`--ss_lib_type`) parameter can be set in **Trinity Assembler** and **Trinity Align and Estimate Abundance** tools. If you have strand-specific data, specify the library type. There are four library types. For paired reads **RF** - first read (/1) of fragment pair is sequenced as antisense (reverse(R)), and second read (/2) is in the sense strand (forward(F)) and **FR** - first read (/1) of fragment pair is sequenced as sense (forward), and second read (/2) is in the antisense strand (reverse). For unpaired (single) reads **F** - the single read is in the sense (forward) orientation and **R** - the single read is in the antisense (reverse) orientation. By setting this parameter to one values from above, you are indicating that the reads are strand-specific. By default, reads are treated as not strand-specific. [3]   \n\nThe raw input reads needs to be of high quality: free from adapters, bar-codes, and other contaminating sub-sequences. To perform quality trimming of inputted FASTQ files directly in this workflow, use **Trimmomatic** (`--trimmomatic`) **Trinity Assembler** parameter. This will generate quality-trimmed reads that will be used for assembly.    \n\n**Samples File**, optional input for the workflow, represents tab-delimited text file indicating biological replicate relationships in downstream differential expression analysis. \n\n### Changes Introduced by Seven Bridges\n\n+ Long Reads (`--long_reads`) parameter of **Trinity Assembler** tool is not added since it is still under development and thus marked as experimental.\n+ Full Cleanup (`--full_cleanup`) parameter of **Trinity Assembler** tool is not added because it could not find it's usability on SBG Platform.\n+ At the end of the command line of **Trinity Assembler** a command was added (`rm -R -f -- */`) for removing  unnecessary temporary files.\n\n### Common Issues and Important Notes\n\n+ All input FASTQ/FASTA files on the **Reads** input node must have **Sample ID** metadata field set and it should be unique for each pair of pair-end reads, and for each single-end read file. If **Paired-end** metadata field is empty, input will be treated as single-end reads file in **Trinity Assembler**.\n+ This workflow does not work with single sample input file(s). For de novo reconstruction of a transcriptome for single sample input, use **RNA-Seq De Novo Assembly - Trinity 2.4.0** workflow.\n+ Parameters for either k-means (**Number Of Clusters To Define Via K-means** (`-K`)) or hierarchical tree clustering (**Number Of Clusters To Cut Tree** (`--Ktree`) or **Cut Percentage For Tree** (`--Ptree`)) are required for **Trinity Define Clusters** tool in order to work properly in the workflow.\n+ The raw **input reads** need to be of high quality: free from adapters, bar-codes, and other contaminating sub-sequences. Otherwise, it is highly recommended to perform pre-processing steps (quality trimming and read filtering). To perform quality trimming of inputted FASTQ files directly in this workflow, use **Trimmomatic** (`--trimmomatic`) **Trinity Assembler** parameter. This will generate quality-trimmed reads that will be used for assembly.\n+  The workflow will **FAIL** if input FASTQ file(s) do not pass the **FastqValidator** tool. This is implemented to break the workflow at the very beginning and prevent the waste of time and recourses on an execution that will fail on the **Trinity** tools.\n\n### Performance Benchmarking\n\nMemory (RAM) for **Trinity Assembler** is locked to 200 GB in order to work properly for every input file size. The maximum number of parallel instances is set to 10 to speed up the **Align and Estimate Abundance** part of the workflow when multiple samples are aligned.\n\nIn the following table, you can find estimates of the **RNA-Seq De Novo Assembly and Analysis - Trinity** workflow running time and cost. All tasks in the table perform RNA-Seq De Novo Assembly on the paired-end reads.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*      \n\n| Experiment size | Input size | # of reads | Duration | Cost | Instances (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| Small     | 4 x 232 MB         | 4M            | 46min.   | $1.4          | r4.8xlarge, c4.8xlarge, c4.2xlarge     |\n| Medium     | 4 x 4.5 GB        | 40M             | 8h 45min.    | $16.5               | r4.8xlarge, c4.8xlarge, c4.2xlarge     |\n| Large     | 2x17.4, 2x19.1, 2x22.0 GB        | 76,84,97M x 2            | 1d 18h 44min.   |  $84.72                | r4.8xlarge, m2.4xlarge, c4.8xlarge, c4.2xlarge     |\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {'reads': list(api.files.query(project=project_id, names=['G20479.HCC1143.2.converted.pe_1_1Mreads.fastq', \n                                                                   'G20479.HCC1143.2.converted.pe_2_1Mreads.fastq'])),\n          'K_means': 2,\n          'K_means_1': 2}\ntask = api.tasks.create(name='WES Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Trinity Home Page](https://github.com/trinityrnaseq/trinityrnaseq/wiki)    \n[2] [Trinity Insilico Normalization](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Insilico-Normalization)    \n[3] [Running Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity)", "input": [{"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "Samples File"}, {"name": "Dispersion"}, {"name": "Dispersion"}, {"name": "Cut Percentage For Tree"}, {"name": "Number Of Clusters To Cut Tree"}, {"name": "Number Of Clusters To Define Via K-means"}, {"name": "Cut Percentage For Tree"}, {"name": "Number Of Clusters To Cut Tree"}, {"name": "Number Of Clusters To Define Via K-means"}, {"name": "Strand Specific RNA-Seq Read Orientation"}, {"name": "Strand Specific RNA-Seq Read Orientation"}, {"name": "Do *not* run in silico normalization of reads"}], "output": [{"name": "FastqValidator Out", "encodingFormat": "text/plain"}, {"name": "Assembly Stats", "encodingFormat": "text/plain"}, {"name": "Transcript Lengths", "encodingFormat": "text/plain"}, {"name": "Component Sizes", "encodingFormat": "text/plain"}, {"name": "Isoform Counts", "encodingFormat": "text/plain"}, {"name": "Isoform Abundances"}, {"name": "Gene Counts", "encodingFormat": "text/plain"}, {"name": "Gene Abundances"}, {"name": "BAM Files", "encodingFormat": "application/x-bam"}, {"name": "BAM Indexes"}, {"name": "DE Analysis Plots Isoforms"}, {"name": "DE Analysis Plots Genes"}, {"name": "numDE Isoform Features"}, {"name": "numDE Gene Features"}, {"name": "FPKM Reduced Isoforms"}, {"name": "FPKM Reduced Genes"}, {"name": "DE Analysis Isoforms"}, {"name": "DE Analysis Genes"}, {"name": "Clustered Heatmap Isoforms"}, {"name": "Clustered Heatmap Genes"}, {"name": "Cluster Plots Isoforms"}, {"name": "Cluster Plots Genes"}, {"name": "All FKPMs Isoform"}, {"name": "All FKPMs Gene"}, {"encodingFormat": "application/x-fasta"}], "codeRepository": ["https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running%20Trinity", "https://github.com/trinityrnaseq/trinityrnaseq/blob/master/LICENSE"], "applicationSubCategory": ["RNA-Seq", "Assembly"], "project": "SBG Public Data", "creator": "Broad Institute, Hebrew University of Jerusalem", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1515515997, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-de-novo-assembly-trinity-2-4-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-de-novo-assembly-trinity-2-4-0/11", "applicationCategory": "Workflow", "name": "RNA-Seq De Novo Assembly - Trinity 2.4.0", "description": "**RNA-Seq De Novo Assembly - Trinity 2.4.0** is a workflow for the de novo reconstruction of a transcriptome using the **Trinity** software package.\n\nThe workflow takes RNA-Seq data which is formatted as FASTQs or FASTAs. FASTQ files can also be gzip-compressed, in which case they should have a .gz extension. The outputs are assembled transcripts, basic assembly statistics, samples aligned to transcriptome (BAMs) and transcript abundance estimation.\n\n**Trinity**, developed at [Broad Institute](https://www.broadinstitute.org/) and [Hebrew University of Jerusalem](http://www.cs.huji.ac.il/), represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-seq data. **Trinity Assembler** combines three independent software modules: _Inchworm_, _Chrysalis_, and _Butterfly_, applied sequentially to process large volumes of RNA-seq reads. Briefly, the process works as follows:\n\n+ _Inchworm_ assembles the RNA-seq data into the unique sequences of transcripts, often generating full-length transcripts for a dominant isoform, but then reports just the unique portions of alternatively spliced transcripts.\n+ _Chrysalis_ clusters the Inchworm contigs into clusters and constructs complete de Bruijn graphs for each cluster. Each cluster represents the full transcriptional complexity for a given gene (or sets of genes that share sequences). Chrysalis then partitions the full read set among these disjoint graphs.\n+ _Butterfly_ processes those individual graphs in parallel, tracing the paths that reads and pairs of reads take within the graph, ultimately reporting full-length transcripts for alternatively spliced isoforms, and teasing apart transcripts that correspond to paralogous genes. [1]\n\nAfter the assembly portion, this workflow utilizes Trinity software package tools which performs basic downstream analysis such as:\n\n+ **Trinity Stats** - Obtaining the basic statistics for FASTA file assembled by Trinity Assembler.\n+ **Trinity Component Distribution** - Providing the distribution of the number of transcripts per gene.\n+ **Trinity Align and Estimate Abundance** - Aligning the raw RNA-Seq reads to assembled transcripts and determining the transcript abundance estimation based on resulting alignments.\n+ **Trinity Count Features** - Counting the number of genes (components) and isoforms (transcripts) that are expressed above a certain FPKM expression threshold.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThis workflow is designed for the de novo reconstruction of a transcriptome and required input is RNA-Seq data in FASTQ or FASTA format (FASTQ files can be gzip-compressed, in which case they should have a .gz extension).\nThe workflow outputs are:   \n\n+ Assembled **Transcripts** - full-length and nearly full-length transcripts across a broad range of expression levels and sequencing depths.\n+ Statistical values (**Stats**, **Trancscript Lengths**, **Component Sizes**...) - number of genes (components) and isoforms (transcripts), and the contiguity of the assembly, etc.\n+ Alignments (**BAMs**) - RNA-Seq reads mapped to the assembled transcriptome, which can be visualized on the Platform using the Genome Browser.\n+ Transcript abundance estimation (**Gene Abundances** and **Isoform Abundances**) - isoform and gene expression value estimation based on the resulting alignments.\n\nAt the beginning of the workflow, FASTQ validation is performed with **FastqValidator** tool, which runs in parallel with **Trinity Assembly**. This tool will check if input FASTQ files are formatted properly and it will **fail the task** if they are not, causing failure at the very beginning of the workflow and preventing the task from using time and recourses on an execution that will fail on the **Trinity** tools.    \nThe workflow can handle both pair end and single end reads. Large RNA-Seq data sets, such as those exceeding 300M pairs, are best suited for in silico normalization prior to running **Trinity** [2]. This version of **Trinity** will perform in silico normalization by default. You can turn it off with **Do *not* run in silico normalization of reads** (`--no_normalize_reads`) **Trinity Assembler** parameter.   \nTrinity workflow works better with strand-specific data. But, it can also accommodate non-strand-specific data. **Strand Specific RNA-Seq Read Orientation** (`--ss_lib_type`) parameter can be set in **Trinity Assembler** and **Trinity Align and Estimate Abundance** tools. If you have strand-specific data, specify the library type. There are four library types. For paired reads **RF** - first read (/1) of fragment pair is sequenced as antisense (reverse(R)), and second read (/2) is in the sense strand (forward(F)) and **FR** - first read (/1) of fragment pair is sequenced as sense (forward), and second read (/2) is in the antisense strand (reverse). For unpaired (single) reads **F** - the single read is in the sense (forward) orientation and **R** - the single read is in the antisense (reverse) orientation. By setting this parameter to one values from above, you are indicating that the reads are strand-specific. By default, reads are treated as not strand-specific. [3]    \n\nThe raw input reads needs to be of high quality: free from adapters, bar-codes, and other contaminating sub-sequences. To perform quality trimming of inputted FASTQ files directly in this workflow, use **Trimmomatic** (`--trimmomatic`) **Trinity Assembler** parameter. This will generate quality-trimmed reads that will be used for assembly.\n\n### Changes Introduced by Seven Bridges\n\n+ Long Reads (`--long_reads`) parameter of **Trinity Assembler** tool is not added since it is still under development and is marked as experimental.\n+ Full Cleanup (`--full_cleanup`) parameter of **Trinity Assembler** tool is not added because it could not find its usability on SBG Platform.\n+ At the end of the command line of **Trinity Assembler** a command was added (`rm -R -f -- */`) for removing unnecessary temporary files.\n\n### Common Issues and Important Notes\n\n+ All input FASTQ/FASTA files on the **Reads** input node must have **Sample ID** metadata field set and it should be unique for each pair of pair-end reads, and for each single-end read file. If **Paired-end** metadata field is empty, input will be treated as single-end reads file in **Trinity Assembler**.\n+ The raw **input reads** need to be of high quality: free from adapters, bar-codes, and other contaminating sub-sequences. Otherwise, it is highly recommended to perform pre-processing steps (quality trimming and read filtering). To perform quality trimming of inputted FASTQ files directly in this workflow, use the **Trimmomatic** (`--trimmomatic`) **Trinity Assembler** parameter. This will generate quality-trimmed reads that will be used for assembly.\n+ The workflow will **FAIL** if input FASTQ file(s) do not pass the **FastqValidator** tool. This is implemented to break the workflow at the very beginning and prevent the waste of time and recourses on an execution that will fail on the **Trinity** tools.\n\n### Performance Benchmarking\n\nMemory (RAM) for **Trinity Assembler** is locked to 200 GB in order to work properly for every input file size. The maximum number of parallel instances is 10 to speed up the **Align and Estimate Abundance** part of the pipeline when multiple samples are aligned.\n\nIn the following table you can find estimates of the **RNA-Seq De Novo Assembly - Trinity** workflow running time and cost. All tasks in the table perform RNA-Seq De Novo Assembly on the paired-end reads.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n| Experiment size | Input size |  # of reads | Duration | Cost | Instances (AWS) |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| Small     | 4 x 232 MB         | 2M            | 41min.   | $1          | r4.8xlarge, c4.8xlarge, c4.2xlarge     |\n| Medium     | 4 x 4.5 GB             | 40M             | 8h 43min.    | $16.5               | r4.8xlarge, c4.8xlarge, c4.2xlarge     |\n| Large     | 2x17.4, 2x19.1  2x22.0 GB         | 76,84,97M            | 1d 18h 39min.   |  $84.7                | r4.8xlarge, m2.4xlarge, c4.8xlarge, c4.2xlarge     |\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {'reads': list(api.files.query(project=project_id, names=['G20479.HCC1143.2.pe_1.fastq', \n                                                                   'G20479.HCC1143.2.pe_2.fastq']))}\ntask = api.tasks.create(name='WES Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Trinity Home Page](https://github.com/trinityrnaseq/trinityrnaseq/wiki)    \n[2] [Trinity Insilico Normalization](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Insilico-Normalization)    \n[3] [Running Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity)", "input": [{"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "Strand Specific RNA-Seq Read Orientation"}, {"name": "Strand Specific RNA-Seq Read Orientation"}, {"name": "Do *not* run in silico normalization of reads"}], "output": [{"name": "FastqValidator Out", "encodingFormat": "text/plain"}, {"name": "Assembly Stats", "encodingFormat": "text/plain"}, {"name": "Component Sizes", "encodingFormat": "text/plain"}, {"name": "Transcript Lengths", "encodingFormat": "text/plain"}, {"name": "Isoform Counts", "encodingFormat": "text/plain"}, {"name": "Genes Count", "encodingFormat": "text/plain"}, {"name": "Isoform Abundances"}, {"name": "Gene Abundances"}, {"name": "BAMs", "encodingFormat": "application/x-bam"}, {"name": "BAM Indexes"}, {"encodingFormat": "application/x-fasta"}], "codeRepository": ["https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running%20Trinity", "https://github.com/trinityrnaseq/trinityrnaseq/blob/master/LICENSE"], "applicationSubCategory": ["RNA-Seq", "Assembly"], "project": "SBG Public Data", "creator": "Broad Institute, Hebrew University of Jerusalem", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1515511865, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-differential-expression/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-differential-expression/14", "applicationCategory": "Workflow", "name": "RNA-seq Differential Expression", "description": "The pipeline described here can be used to perform differential gene expression analysis between two groups/conditions. The first step in the pipeline performs gene/transcript quantification using Cuffquant tool. The output of Cuffquant is then fed into Cuffdiff tool. Cuffdiff finds significant differences in gene/transcripts expression between groups of samples. The Cuffdiff algorithm incorporates both biological and technical variability in order to identify differentially expressed genes and transcripts. Thus, the ability to detect true significant changes (and limit false positive detections) is determined by the number of replicates included in an experiment and the inter-replicate variability.\n\nThis pipeline also performs basic quality control analysis of your differential expression experiment powered by CummeRbundQC.\n\nOn the Seven Bridges Platform, you can use this pipeline in combination with the RNA-Seq Alignment \u2013 TopHat Public Pipeline to take you all the way from raw sequencing reads to visualization and a list of differentially expressed genes and transcripts.\n\n###Required Inputs\n\nThe pipeline has **three** required inputs:\n\n1. BAM group \"A\" (ID: \"BAM_Group_A\") - One or more BAM/SAM files for condition-1. Refer to the **NOTE** section below for more information.\n\n2. BAM group \"B\" (ID: \"BAM_Group_B\") - One or more BAM/SAM files for condition-2. Refer to the **NOTE** section below for more information.\n\n3. Gene annotation file (ID: \"Annotations\") - Known transcript annotation in GTF format. GTF file needs to be augmented with tss_id and p_id attributes in order to look for changes in primary transcript expression, splicing, coding output, and promoter use. For Human samples we recommend Ensembl gene annotations \"Homo_sapiens.GRCh37.75.gtf\".\n\n###Optional Input\n\n1. Reference FASTA file (ID: \"Reference\") - FASTA file containing reference genome for sequence bias correction.\n\n###Optional Parameters\n\n1. Group names (IDs: \"group_name\" and \"group_name_1\") - The group names defined here will be used in the final report.\n\n###Outputs\n\n1. Cuffdiff output (ID: \"zipped\") - Zipped directory containing the output of Cuffdiff.\n\n2. HTML output (ID: \"html\") - The final report in the b64 html format that can be viewed on the platform.\n\n3. CummeRbund QC output (ID: \"archive\") - Zipped directory containing the html output.\n\n###NOTE:\n\n1. For example if you have three replicates (A, B, & C) of an experimental condition (X), set the \"Group name\" parameter in \"SBG Group Input\" app to X and add BAM files A, B, & C to the group. The number of replicates doesn\u2019t need to be the same in each experimental condition. If you use fewer than 3 replicates per condition, by default Cuffdiff will not test for significant alterations in splicing, tss, or cds usage.\n\n2. If there are more than two groups, modify the pipeline by adding more \"SBG Group Input\" apps and connecting these to the \"SBG Flatten\" input.\n\n###Common Issues\n\n1. Make sure that the BAM files used in the workflow, are mapped to the same reference as the one used here as an input.\n2. Compatibility with STAR aligner: for unstranded RNA-seq data, Cuffinks/Cuffdiff require spliced alignments with XS strand attribute, which STAR will generate with `--outSAMstrandField intronMotif` option. As required, the XS strand attribute will be generated for all alignments that contain splice junctions. The spliced alignments that have undefined strand (i.e. containing only non-canonical unannotated junctions) will be suppressed.", "input": [{"name": "#Reference", "encodingFormat": "application/x-fasta"}, {"name": "#Annotations", "encodingFormat": "application/x-gtf"}, {"name": "#BAM_Group_B", "encodingFormat": "application/x-bam"}, {"name": "#BAM_Group_A", "encodingFormat": "application/x-bam"}, {"name": "Group name"}, {"name": "Group name"}], "output": [{"name": "zipped", "encodingFormat": "application/zip"}, {"name": "html"}, {"name": "archive", "encodingFormat": "application/zip"}], "applicationSubCategory": ["RNA-Seq", "Differential Expression"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033583, "dateCreated": 1453799660, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rna-seq-quantification-hisat2-stringtie/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rna-seq-quantification-hisat2-stringtie/14", "applicationCategory": "Workflow", "name": "RNA-Seq Quantification (HISAT2, StringTie)", "description": "The **RNA-Seq Quantification (HISAT2, StringTie)** workflow described here can be used to perform a gene abundance estimation of RNA-Seq data (i.e. quantification) of a unified set of genes common for all samples in the analysis. The workflow is based on the\u00a0Nature protocol paper [1] with the absence of the last step which carries out testing for differential expression - __Ballgown__. The first part of the workflow indexes and aligns RNA-Seq reads to the reference using tools from the __HISAT2__ toolkit. If the indexed reference already exists, the TAR bundle can be provided to **HISAT2 Build** instead of a FASTA file, in which case the indexing component will be skipped and the provided TAR bundle will be forwarded to the __HISAT2__ aligner. Aligned reads are then fed to the first run of __StringTie__ which performs 'reference guided' transcriptome assembly for each sample. Assembled transcripts and reference annotation transcripts are then merged into a uniform set of transcripts. The second run of __StringTie__ performs a quantification of merged transcripts and genes in each sample. For each sample, the workflow outputs the abundance estimation in the __Ballgown__ input format (one TAR bundle containing 5 .ctab tables), two files containing transcript and gene expression values in the __DESeq2__ input format, and a TAB file containing FPKM, TPM, and Coverage values for each gene. In addition to files containing the abundance estimation, the workflow also outputs a GTF file with merged transcripts, __GffCompare__ output files with comparison results between reference annotation transcripts, and merged transcripts and alignments in the BAM format for each sample.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n\n### Common Use Cases\n\n- This workflow can be used to perform the quantification of RNA-Seq data of a unified set of genomic features across all samples in an analysis suitable for a downstream differential expression step. Once quantification values are obtained, downstream differential expression analysis can be performed using **DESeq2** or **Ballgown** tools on the Seven Bridges Platform.\n- Though this is not the primary purpose of this workflow, it can be also used as a scattered spliced aware aligner which is able to process multiple samples at once since it outputs aligned BAM files by default. Additional costs for performing the quantification step is negligible compared to the indexing and alignment steps.\n\n### Changes Introduced by Seven Bridges\n\n* In order to facilitate and accelerate further RNA-Seq analysis, an additional toolkit __Sambamba (0.6.6)__ is integrated into the same Seven Bridges tool with __HISAT2__. Besides the standard __HISAT2__ SAM output, __HISAT2__ has been extended to provide sorted BAM files as well. Thus, the sorting step as described in the protocol paper [1] is not visible in the workflow\u2019s main page.\n\n* An additional, Seven Bridges-designed tool, __SBG Pair FASTQs by Metadata__, is added at the beginning of the workflow in order to enable the processing of multiple samples in a single run.\n\n* The workflow\u2019s indexing step is optional. The __HISAT2 Build__ is implemented in a way that it can accept a TAR bundle containing an already indexed reference instead of reference FASTA file(s). If a TAR bundle is provided, the indexing step is skipped and the TAR bundle is forwarded to the output. If this is the case, please set the __Use small instance__ parameter to True in order to use a smaller instance for the indexing step, and thus reduce execution costs. You can find pre-built index files (grch38\\_tran.tar.gz, grch37\\_tran.tar.gz, hg38_tran.tar.gz) on the Seven Bridges Platform under the Public Reference Files section. Pre-built index files for organisms other than human can be found [here](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data).\n\n* In order to enable __StringTie__ to produce quantification tables tailored for __DESeq2__ or [edgeR](http://bioconductor.org/packages/release/bioc/html/edgeR.html), the Python script [`prepDE.py`](https://ccb.jhu.edu/software/stringtie/dl/prepDE.py) [2] that extracts raw counts from the **Ballgown** input tables is embedded within the __StringTie__ tool.\n\n### Common Issues and Important Notes\n\n* For paired-end reads, the __Paired-end__ metadata field should be set appropriately in all FASTQ (or FASTA) files that are found in the **List of FASTQ files** input node.\n\n* All input FASTQ files (or FASTA files depending on the format of input reads) must have the __Sample ID__ metadata field appropriately set.\n\n* If index files are already available, the TAR bundle containing the index files can be provided to __HISAT2 Build__ instead of reference FASTA files, and indexing step will be skipped. You can find pre-built index files (grch38\\_tran.tar.gz, grch37\\_tran.tar.gz, hg38_tran.tar.gz) on the Seven Bridges Platform under the Public Reference Files section. Pre-built index files for organisms other than human can be found [here](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data).\n\n* The total size of the input RNA-Seq reads should not exceed approximately 1.4TB (for reads in the FASTQ format) or 1TB (for reads in the FASTQ.GZ format). In the case that the total file size exceeds these values, you can (*recommended*) contact Seven Bridges support team or modify the workflow to attach more [instance storage](https://docs.sevenbridges.com/docs/set-computation-instances#section-set-attached-storage-size).\n\n### Performance Benchmarking\n\nThe workflow requires approximately 110GB of RAM to process 16 samples in parallel without indexing the reference (index files are provided at the input). With reference indexing, the workflow requires 170GB. The default instance for the workflow is set to __r4.8xlarge__ (AWS). In the following table you can find estimates of the workflow run time and its cost on the default instance. Total input size refers to the total size of the input FASTQ/FASTQ.GZ files.\n\n*The cost of running __RNA-Seq Quantification (HISAT2, StringTie)__ can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| # of samples | Input format | Input size | Paired-end | # of reads | Read length | Duration  | Cost   |\n|--------------|--------------|------------|------------|------------|-------------|-----------|--------|\n| 100          | FASTQ.GZ     | 30 GB      | No         | 4-30M      | 36          | 1h 41min  | $4.08  |\n| 400          | FASTQ.GZ     | 115 GB     | No         | 4-30M      | 36          | 6h 15min  | $15.25 |\n| 50           | FASTQ.GZ     | 198 GB     | Yes        | 20-45M     | 75          | 7h 35min  | $18.59 |\n| 200          | FASTQ.GZ     | 803 GB     | Yes        | 20-45M     | 75          | 26h 43min | $64.36 |\n| 10           | FASTQ        | 625 GB     | Yes        | 60-125M    | 101         | 7h 30min  | $18.66 |\n| 16           | FASTQ        | 575 GB     | Yes        | 74-83M     | 50          | 5h        | $12.15 |\n| 16           | FASTQ        | 338 GB     | No         | 78-83M     | 75          | 2h 51min  | $6.88  |\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"references_or_index\": \"file_object_list - list(api.files.query(project=project_id, names=['enter_filename', 'enter_filename']))\",\n    \"fastq_list\": \"file_object_list - list(api.files.query(project=project_id, names=['enter_filename', 'enter_filename']))\",\n    \"input_GTF\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n}\ntask = api.tasks.create(name='RNA-Seq Quantification (HISAT2, StringTie) - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [HISAT, StringTie, Ballgown protocol paper](https://www.nature.com/articles/nprot.2016.095)\n\n[2] [StringTie manual page - using StringTie with DESeq2 and edgeR](https://ccb.jhu.edu/software/stringtie/index.shtml?t=manual#deseq)", "input": [{"name": "References or Index files", "encodingFormat": "application/x-tar"}, {"name": "List of FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Reference annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Use small instance"}], "output": [{"name": "Merged transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Gene abundance estimation"}, {"name": "Archived Ballgown input tables", "encodingFormat": "application/x-tar"}, {"name": "DESeq2 transcript count matrix"}, {"name": "DESeq2 gene count matrix"}, {"name": "GffCompare stats files", "encodingFormat": "application/x-gtf"}, {"name": "Output alignment", "encodingFormat": "application/x-bam"}], "codeRepository": [], "applicationSubCategory": ["RNA", "Quantification", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1585150599, "dateCreated": 1513351732, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rsem-with-star-workflow-1-3-1/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rsem-with-star-workflow-1-3-1/17", "applicationCategory": "Workflow", "name": "RSEM with STAR Workflow 1.3.1", "description": "This **RSEM** workflow for quantifying gene expression is based on the **STAR** aligner. \n\nThe parameters have been set so that indices are built and alignment is done using **STAR**. Therefore, this workflow is optimized to work with FASTQ input files. \nAligned files in BAM/SAM/CRAM format can also be supplied, in which case alignment will not be performed and only the quantification process will be run (Expectation Maximization algorithm), but please note that in this case the input BAM/SAM/CRAM needs to be formatted properly for **RSEM** to work (**RSEM** does not accept gapped alignment currently - more info on this can be found in the tool description or in the RSEM documentation [1]). \n\nOther aligners that can be chosen instead of **STAR** are **Bowtie** and **Bowtie2**, though **STAR** is set as the default.\n\nThe versions of aligners that **RSEM** internally uses are **Bowtie 1.1.2**, **Bowtie2 2.3.4.3** and **STAR 2.6.1d**.\nFor sorting BAM files, **RSEM** uses **SAMtools 1.3**.  \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n1. A proper reference FASTA file and GTF annotation file need to be provided. If the **RSEM Index Archive** is already built with these two files, it can instead be provided to the **Reference FASTA or RSEM index archive** input, so the **RSEM Prepare Reference** tool will be skipped, and the whole execution will be sped up by a margin. \n\n2. The workflow can be run in two modes: from FASTQ files (in which case alignment is first performed with the **Aligner** of choice, the default being **Bowtie**, followed by the EM procedure), or from a BAM file (in which case alignment is skipped, and EM is directly performed). \n\n3. For single-end data, it is strongly recommended that the user provides the fragment length distribution parameters - **Mean fragment length** and **Fragment length standard deviation** (`--fragment-length-mean` and `--fragment-length-sd`, respectively).  For paired-end data, **RSEM** will automatically learn a fragment length distribution from the data.\n\n4. Some aligner parameters have default values different from their original settings.\n\n5. With the **Calculate posterior mean estimates** (`--calc-pme`) option, posterior mean estimates will be calculated in addition to maximum likelihood estimates.\n\n6. With the **Calculate credibility intervals**' (`--calc-ci`) option, 95% credibility intervals and posterior mean estimates will be calculated in addition to maximum likelihood estimates.\n\n7. The temporary directory and all intermediate files will be removed when **RSEM** finishes unless **Keep temporary files generated by RSEM** (`--keep-intermediate-files`) is specified (specify this if you want to keep **STAR** logs, for example). \n\n### Common issues and important notes\n\n1. If **STAR** parameter is chosen, a larger instance will be required by the tool. \n\n2. In case of paired-end FASTQ files, it is crucial to set the **Paired End** metadata field to 1/2.\n\n3. For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: sample_id/library_id/platform_unit_id/file_segment_number).\n4. If running this workflow from BAM files, the BAM needs to be in transcript coordinates and without gapped alignment. Typically, if **STAR** is first run, turning the option `--quantMode TranscriptomeSAM` will produce a BAM file in transcript coordinates, which is suited for **RSEM**. \n\n### Changes Introduced by Seven Bridges\n\n1. All output files will be prefixed by the input **Sample ID**. \n\n2. The options to choose aligners have been merged into a single enum (**Aligner** option), instead of being three separate boolean options. The default is left to be **Bowtie**. \n\n3. If running **RSEM Calculate Expression** from BAM files, **SAMtools** will be run first to determine whether the BAM file came from paired-end or single-end data, thus eliminating the need for the user for specify so, and allowing for batch running of samples. \n\n4. The options that are most often used in a standard **RSEM** pipeline run have been exposed, so please check through those first and see if they meet your **RSEM** needs before changing any of the other parameters.\n\n5. The workflow is built to use the **STAR** aligner by default, but other aligners (**Bowtie**, **Bowtie2**) can be chosen as well. If this is the case, make sure to specify the correct aligner under the **Aligner** option, and also to specify which index files are ought to be build in the **RSEM Prepare Reference** step (**Bowtie**, **Bowtie2** or **STAR** index files).\n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the indexing part skipped (a TAR index archive is provided instead of the FASTA/GTF combination):\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq - FASTQ     |  2 x 232 MB |     Yes    |     1M     |     101     |   16min   | $0.35| c4.8xlarge |\n|     RNA-Seq - FASTQ    |  2 x 4.5 GB |     Yes    |     20M     |     101     |   50min   | $1.20| c4.8xlarge |\n|     RNA-Seq - FASTQ    | 2 x 17.4 GB |     Yes    |     76M     |     101     |   2h33min  | $4 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [RSEM manual](http://deweylab.biostat.wisc.edu/rsem/README.html)", "input": [{"name": "Reference FASTA or RSEM index archive"}, {"name": "GTF or GFF3"}, {"name": "Input Read Files"}, {"name": "STAR splice junction database overhang"}, {"name": "STAR"}, {"name": "Bowtie 2"}, {"name": "Bowtie"}, {"name": "No BAM output"}, {"name": "Output genome BAM"}, {"name": "Calculate credibility intervals"}, {"name": "Minimum fragment length"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Estimate RSPD"}, {"name": "Keep temporary files generated by RSEM"}, {"name": "Sort BAM by coordinate"}, {"name": "Strandedness"}, {"name": "Number of threads"}, {"name": "Aligner"}], "output": [{"name": "Transcript BAM", "encodingFormat": "application/x-bam"}, {"name": "STAR Log Files"}, {"name": "Isoforms results"}, {"name": "Genes results"}, {"name": "Genome BAM", "encodingFormat": "application/x-bam"}, {"name": "STAR splice junctions"}, {"name": "RSEM Plot Model PDF File"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/rsem-with-star-workflow-1-3-1/17.png", "codeRepository": ["https://github.com/deweylab/RSEM", "https://github.com/deweylab/RSEM/archive/v1.3.1.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Quantification"], "project": "SBG Public Data", "creator": "Bo Li, Colin Dewey", "softwareVersion": ["sbg:draft-2"], "dateModified": 1569331289, "dateCreated": 1546032686, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rsem-workflow-1-3-3/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rsem-workflow-1-3-3/3", "applicationCategory": "Workflow", "name": "RSEM workflow 1.3.3", "description": "This **RSEM** workflow for quantifying gene expression is based on the four different aligners, where **STAR** is set as the default.\n\nThe parameters have been set so that indices are built and alignment is done using **STAR**. Therefore, this workflow is optimized to work with FASTQ input files. \nAligned files in BAM/SAM/CRAM format can also be supplied, in which case alignment will not be performed and only the quantification process will be run (Expectation Maximization algorithm), but please note that in this case the input BAM/SAM/CRAM needs to be formatted properly for **RSEM** to work (**RSEM** does not accept gapped alignment currently - more info on this can be found in the tool description or in the RSEM documentation [1]). \n\nOther aligners that can be chosen instead of **STAR** are **Bowtie**, **Bowtie2** and **HISAT2**.\n\nThe versions of aligners that **RSEM** internally uses are **Bowtie 1.2.3**, **Bowtie2 2.4.1**, **STAR 2.7.3a** and **HISAT2 2.2.0**\nFor sorting BAM files, **RSEM** uses **SAMtools 1.3**.  \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n1. A proper reference FASTA file and GTF annotation file need to be provided. If the **RSEM Index Archive** is already built with these two files, it can instead be provided to the **Reference FASTA or RSEM index archive** input, so the **RSEM Prepare Reference** tool will be skipped, and the whole execution will be sped up by a margin. \n\n2. The workflow can be run in two modes: from FASTQ files (in which case alignment is first performed with the **Aligner** of choice, followed by the EM procedure), or from a BAM file (in which case alignment is skipped, and EM is directly performed). \n\n3. For single-end data, it is strongly recommended that the user provides the fragment length distribution parameters - **Mean fragment length** and **Fragment length standard deviation** (`--fragment-length-mean` and `--fragment-length-sd`, respectively).  For paired-end data, **RSEM** will automatically learn a fragment length distribution from the data.\n\n4. Some aligner parameters have default values different from their original settings.\n\n5. With the **Calculate posterior mean estimates** (`--calc-pme`) option, posterior mean estimates will be calculated in addition to maximum likelihood estimates.\n\n6. With the **Calculate credibility intervals**' (`--calc-ci`) option, 95% credibility intervals and posterior mean estimates will be calculated in addition to maximum likelihood estimates.\n\n7. The temporary directory and all intermediate files will be removed when **RSEM** finishes unless **Keep temporary files generated by RSEM** (`--keep-intermediate-files`) is specified (specify this if you want to keep **STAR** logs, for example). \n\n### Common issues and important notes\n\n1. In case of paired-end FASTQ files, it is crucial to set the **Paired End** metadata field to 1/2.\n\n2. For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: sample_id/library_id/platform_unit_id/file_segment_number).\n\n3. If running this workflow from BAM files, the BAM needs to be in transcript coordinates and without gapped alignment. Typically, if **STAR** is first run, turning the option `--quantMode TranscriptomeSAM` will produce a BAM file in transcript coordinates, which is suited for **RSEM**. \n\n### Changes Introduced by Seven Bridges\n\n1. All output files will be prefixed by the input **Sample ID**. \n\n2. The options to choose aligners have been merged into a single enum (**Aligner** option), instead of being four separate boolean options. The default is set to **STAR**,  but If left empty, **Bowtie** will be used. \n\n3. If running **RSEM Calculate Expression** from BAM files, **SAMtools** will be run first to determine whether the BAM file came from paired-end or single-end data, thus eliminating the need for the user for specify so, and allowing for batch running of samples. \n\n4. The options that are most often used in a standard **RSEM** pipeline run have been exposed, so please check through those first and see if they meet your **RSEM** needs before changing any of the other parameters.\n\n5. The workflow is built to use the **STAR** aligner by default, but other aligners (**Bowtie**, **Bowtie2** or **HISAT2**) can be chosen as well. If this is the case, make sure to specify the correct aligner under the **Aligner** option, and also to specify which index files are ought to be build in the **RSEM Prepare Reference** step (**Bowtie**, **Bowtie2**, **STAR** or **HISAT2** index files).\n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the default workflow options (indexing is not performed, a TAR index archive (size 32.2 GB) is provided instead of the FASTA/GTF combination), executed on the on-demand AWS instance:\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq - FASTQ     |  2 x 590 MB |     Yes    |     5M     |     101     |   21min   | $0.56| c5.9xlarge |\n|     RNA-Seq - FASTQ    |  2 x1.9 GB |     Yes    |     16M     |     101     |   28min   | $0.75| c5.9xlarge |\n|     RNA-Seq - FASTQ    | 2 x 5.6 GB |     Yes    |     50M     |     101     |   46min  | $1.23 | c5.9xlarge |\n|     RNA-Seq - FASTQ    | 2 x 18.5 GB |     Yes    |     163M     |     101     |   1h41min  | $2.69 | c5.9xlarge |\n\n\n\nRuntime and task cost for one of previous samples with the default workflow options when indexing is performed (size of reference file - 2.9 GB, size of GTF file - 1.2 GB):\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:----------------:|:-------------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 590 MB |     Yes    |     5M     |     101     |   45min   | $1.20 | c5.9xlarge |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_gene_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_references_or_rsem_index_archive': list(api.files.query(project=project_id, names=['reference_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='RSEM workflow 1.3.3 - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [RSEM manual](http://deweylab.biostat.wisc.edu/rsem/README.html)", "input": [{"name": "Reference fasta file(s) or RSEM index archive", "encodingFormat": "application/x-fasta"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Read files", "encodingFormat": "application/x-sam"}, {"name": "Bowtie"}, {"name": "Bowtie 2"}, {"name": "STAR"}, {"name": "HISAT2"}, {"name": "STAR splice junction database overhang"}, {"name": "Memory per job"}, {"name": "Memory per job"}, {"name": "Aligner"}, {"name": "Number of threads"}, {"name": "Strandedness"}, {"name": "Sort BAM by coordinate"}, {"name": "Output genome BAM"}, {"name": "No BAM output"}, {"name": "Minimum fragment length"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Estimate RSPD"}, {"name": "Keep temporary files generated by RSEM"}, {"name": "Calculate credibility intervals"}], "output": [{"name": "BAM in transcript coordinates", "encodingFormat": "application/x-bam"}, {"name": "STAR splice junctions"}, {"name": "STAR log files"}, {"name": "Isoform level expression estimates"}, {"name": "BAM in genome coordinates", "encodingFormat": "application/x-bam"}, {"name": "Gene level expression estimates"}, {"name": "PDF plot model file"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/rsem-workflow-1-3-3/3.png", "codeRepository": ["https://github.com/deweylab/RSEM", "https://github.com/deweylab/RSEM/archive/v1.3.3.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Quantification", "CWL1.0"], "project": "SBG Public Data", "creator": "Bo Li, Colin Dewey", "softwareVersion": ["v1.0"], "dateModified": 1590513120, "dateCreated": 1590512891, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-workflow-0-9-1/19", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-workflow-0-9-1/19", "applicationCategory": "Workflow", "name": "Salmon Workflow 0.9.1", "description": "The **Salmon workflow** infers maximum likelihood estimates of transcript abundances from RNA-Seq data using a process called **Quasi-mapping**.\n\n**Quasi-mapping** is a process of assigning reads to transcripts without doing an exact base-to-base alignment. The **Salmon** tool implements a procedure geared towards knowing the transcript from which a read originates rather than the actual mapping coordinates, since the former is crucial to estimating transcript abundances [1, 2]. \n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilize the full likelihood model while obtaining near-optimal probabilistic RNA-seq quantification results [1, 2, 3]. \n\nThe latest version of Salmon (0.9.x) introduces some novel concepts, like **Rich Factorization Classes**, which further increases the precision of the results, at a very negligible increase in runtime. This version of Salmon also supports quantification from already aligned BAM files, utilizing the full likelihood model (the same one as in RSEM), whereby the results are the same as RSEM but the execution time is much shorter than in RSEM, this time due only to engineering [3].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Salmon Index**, **Salmon Quant**, and **Salmon Quantmerge**.\n- The main input to the workflow are **FASTQ read files** (single end or paired end). \n- A **Transcriptome FASTA file** (`--transcripts`) also needs to be provided in addition to an optional **Gene map** (`--geneMap`) file (which should be of the same annotations that were used in generating the **Transcriptome FASTA file** - usually a GTF file can be provided here) if gene-level abundance results are desired. \n- An already generated **Salmon index archive** can be provided to the **Salmon Index** tool (**Transcriptome FASTA or Salmon Index Archive** input) in order to skip indexing and save some time. \n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript Abundance Estimates**), and an optional file containing **Gene Abundance Estimates**, if the input **Gene map** (`--gene-map`) file is provided. \n- In addition to the default output (**Quantification file**), additional outputs can be produced if the proper options are turned on for them (e.g. **Equivalent class counts** by setting `--dumpEq`, **Unmapped reads** by setting `--writeUnmappedNames`, **Bootstrap data** by setting `--numBootstraps` or `--numGibbsSamples`, **Mapping info** by setting `--write-mappings`...).\n- A **Transcript Expression Matrix** and a **Gene Expression Matrix** will be generated if more than one sample is provided. \n- The **GC bias correction** option (`--gcBias`) will correct for GC bias and improve quantification accuracy but at the cost of increased runtime (a rough estimate would be a **double** increase in runtime per sample). \n- The workflow is optimized to run in scatter mode. To run it successfully, just supply it with multiple samples (paired end or single end, with properly filled out **Sample ID** and **Paired End** metadata). \n- The use of *data-driven likelihood factorization* is turned on with the **Range factorization bins** parameter (`--rangeFactorizationBins=4`) by default in this workflow, as it can bring an increase in accuracy at a very small increase in runtime [3]. \n- The **Salmon Quant archive** output can be used for downstream differential expression analysis tools, like Sleuth. \n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), instead of having identical names between runs. \n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to properly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated Salmon index archive) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs. \n- If the location of the project where the workflow is running is set to Google US West, to run the task successfully, please reduce the default number of CPUs by setting the **Number of CPUs** parameter of the **SBG Pair FASTQs** to 32.\n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge instance (AWS) is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   16min   | $0.40| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   45min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'gtf': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'transcriptome_fasta_or_salmon_index_archive': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Salmon 0.9.1 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Salmon paper](biorxiv.org/content/biorxiv/early/2016/08/30/021592.full.pdf)   \n[2] [Rapmap paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908361/)   \n[3] [Data-driven likelihood factorization](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977)", "input": [{"name": "Transcriptome FASTA or Salmon index archive", "encodingFormat": "application/x-fasta"}, {"name": "FASTQ Read Files", "encodingFormat": "text/fastq"}, {"name": "Gene map or GTF file", "encodingFormat": "application/x-gtf"}, {"name": "K-mer length"}, {"name": "Keep duplicates"}, {"name": "GENCODE FASTA"}, {"name": "Maximum number of parallel jobs"}, {"name": "Allow orphans in FMD mode"}, {"name": "Sequence-specific bias correction"}, {"name": "Incompatible prior probability"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Forgetting factor"}, {"name": "Maximum (S)MEM occurance"}, {"name": "Maximum read occurence"}, {"name": "No effective length correction"}, {"name": "No fragment length distribution"}, {"name": "Number of bias samples"}, {"name": "Number of auxiliary model samples"}, {"name": "Number of pre auxiliary model samples"}, {"name": "Use Variational Bayesian optimization"}, {"name": "Number of Gibbs samples"}, {"name": "Number of bootstraps"}, {"name": "GC bias correction"}, {"name": "Consistent hits"}, {"name": "Dump equivalence class counts"}, {"name": "GC size sample"}, {"name": "Bias speed sample"}, {"name": "Strict intersect"}, {"name": "Initialize uniform parameters"}, {"name": "No bias length threshold"}, {"name": "Per transcript prior"}, {"name": "VBEM prior"}, {"name": "Write unmapped names"}, {"name": "Write mappings"}, {"name": "Thinning factor"}, {"name": "Meta"}, {"name": "Dump equivalence class weights"}, {"name": "No length correction"}, {"name": "Quasi coverage"}, {"name": "Discard orphans in Quasi-mapping mode"}, {"name": "Alternative initialization mode"}, {"name": "Minimum assigned fragments"}, {"name": "Reduce GC memory"}, {"name": "Faster mapping"}, {"name": "Range factorization bins"}, {"name": "Position bias"}], "output": [{"name": "Unmapped reads", "encodingFormat": "text/plain"}, {"name": "Transcript abundance estimates"}, {"name": "Gene abundance estimates"}, {"name": "Mapping info", "encodingFormat": "application/x-sam"}, {"name": "Bootstrap data", "encodingFormat": "application/x-tar"}, {"name": "Equivalence classes", "encodingFormat": "text/plain"}, {"name": "Transcript Expression Matrix", "encodingFormat": "text/plain"}, {"name": "Gene Expression Matrix", "encodingFormat": "text/plain"}, {"name": "Salmon Quant archive", "encodingFormat": "application/x-tar"}, {"name": "Salmon Quant log"}], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v0.9.1"], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Rob Patro, Carl Kingsford, Steve Mount, Mohsen Zakeri", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468512, "dateCreated": 1514565762, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-workflow-0-9-1-cwl-1-0/33", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-workflow-0-9-1-cwl-1-0/33", "applicationCategory": "Workflow", "name": "Salmon Workflow 0.9.1 CWL1.0", "description": "The **Salmon workflow** infers maximum likelihood estimates of transcript abundances from RNA-Seq data using a process called **Quasi-mapping**.\n\n**Quasi-mapping** is a process of assigning reads to transcripts without doing an exact base-to-base alignment. The **Salmon** tool implements a procedure geared towards knowing the transcript from which a read originates rather than the actual mapping coordinates, since the former is crucial to estimating transcript abundances [1, 2]. \n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilize the full likelihood model while obtaining near-optimal probabilistic RNA-seq quantification results [1, 2, 3]. \n\nThe latest version of Salmon (0.9.x) introduces some novel concepts, like **Rich Factorization Classes**, which further increases the precision of the results, at a very negligible increase in runtime. This version of Salmon also supports quantification from already aligned BAM files, utilizing the full likelihood model (the same one as in RSEM), whereby the results are the same as RSEM but the execution time is much shorter than in RSEM, this time due only to engineering [3].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Salmon Index**, **Salmon Quant**, and **Salmon Quantmerge**.\n- The main input to the workflow are **FASTQ read files** (single end or paired end). \n- A **Transcriptome FASTA file** (`--transcripts`) also needs to be provided in addition to an optional **Gene map** (`--geneMap`) file (which should be of the same annotations that were used in generating the **Transcriptome FASTA file** - usually a GTF file can be provided here) if gene-level abundance results are desired. \n- An already generated **Salmon index archive** can be provided to the **Salmon Index** tool (**Transcriptome FASTA or Salmon Index Archive** input) in order to skip indexing and save some time. \n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript Abundance Estimates**), and an optional file containing **Gene Abundance Estimates**, if the input **Gene map** (`--gene-map`) file is provided. \n- In addition to the default output (**Quantification file**), additional outputs can be produced if the proper options are turned on for them (e.g. **Equivalent class counts** by setting `--dumpEq`, **Unmapped reads** by setting `--writeUnmappedNames`, **Bootstrap data** by setting `--numBootstraps` or `--numGibbsSamples`, **Mapping info** by setting `--write-mappings`...).\n- A **Transcript Expression Matrix** and a **Gene Expression Matrix** will be generated if more than one sample is provided. \n- The **GC bias correction** option (`--gcBias`) will correct for GC bias and improve quantification accuracy but at the cost of increased runtime (a rough estimate would be a **double** increase in runtime per sample). \n- The workflow is optimized to run in scatter mode. To run it successfully, just supply it with multiple samples (paired end or single end, with properly filled out **Sample ID** and **Paired End** metadata). \n- The use of *data-driven likelihood factorization* is turned on with the **Range factorization bins** parameter (`--rangeFactorizationBins=4`) by default in this workflow, as it can bring an increase in accuracy at a very small increase in runtime [3]. \n- The **Salmon Quant archive** output can be used for downstream differential expression analysis tools, like Sleuth. \n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), instead of having identical names between runs. \n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to properly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated Salmon index archive) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs. \n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge instance (AWS) is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   16min   | $0.40| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   45min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'gtf': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'transcriptome_fasta_or_salmon_index_archive': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Salmon 0.9.1 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Salmon paper](biorxiv.org/content/biorxiv/early/2016/08/30/021592.full.pdf)   \n[2] [Rapmap paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908361/)   \n[3] [Data-driven likelihood factorization](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977)", "input": [{"name": "Transcriptome FASTA or Salmon index archive", "encodingFormat": "application/x-fasta"}, {"name": "FASTQ Read Files", "encodingFormat": "text/fastq"}, {"name": "Gene map or GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Maximum number of parallel jobs"}, {"name": "K-mer length"}, {"name": "GENCODE FASTA"}, {"name": "Keep duplicates"}, {"name": "Write unmapped names"}, {"name": "Write mappings"}, {"name": "VBEM prior"}, {"name": "Use Variational Bayesian optimization"}, {"name": "Thinning factor"}, {"name": "Strict intersect"}, {"name": "Sequence-specific bias correction"}, {"name": "Reduce GC memory"}, {"name": "Range factorization bins"}, {"name": "Quasi coverage"}, {"name": "Position bias"}, {"name": "Per transcript prior"}, {"name": "Number of pre auxiliary model samples"}, {"name": "Number of Gibbs samples"}, {"name": "Number of bootstraps"}, {"name": "Number of bias samples"}, {"name": "Number of auxiliary model samples"}, {"name": "No length correction"}, {"name": "No fragment length distribution"}, {"name": "No effective length correction"}, {"name": "No bias length threshold"}, {"name": "Minimum assigned fragments"}, {"name": "Meta"}, {"name": "Maximum read occurence"}, {"name": "Maximum (S)MEM occurance"}, {"name": "Initialize uniform parameters"}, {"name": "Incompatible prior probability"}, {"name": "GC size sample"}, {"name": "GC bias correction"}, {"name": "Forgetting factor"}, {"name": "Fragment length standard deviation"}, {"name": "Mean fragment length"}, {"name": "Maximum fragment length"}, {"name": "Faster mapping"}, {"name": "Dump equivalence class weights"}, {"name": "Dump equivalence class counts"}, {"name": "Discard orphans in Quasi-mapping mode"}, {"name": "Consistent hits"}, {"name": "Bias speed sample"}, {"name": "Alternative initialization mode"}, {"name": "Allow orphans in FMD mode"}], "output": [{"name": "Unmapped reads", "encodingFormat": "text/plain"}, {"name": "Transcript abundance estimates"}, {"name": "Gene abundance estimates"}, {"name": "Mapping info", "encodingFormat": "application/x-sam"}, {"name": "Bootstrap data", "encodingFormat": "application/x-tar"}, {"name": "Equivalence classes", "encodingFormat": "text/plain"}, {"name": "Transcript Expression Matrix", "encodingFormat": "text/plain"}, {"name": "Gene Expression Matrix", "encodingFormat": "text/plain"}, {"name": "Salmon Quant archive", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v0.9.1"], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Rob Patro, Carl Kingsford, Steve Mount, Mohsen Zakeri", "softwareVersion": ["v1.0"], "dateModified": 1648468512, "dateCreated": 1523623643, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-workflow-1-0-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-workflow-1-0-0/11", "applicationCategory": "Workflow", "name": "Salmon workflow 1.0.0", "description": "The **Salmon Workflow** estimates transcript abundances from RNA-Seq data using an improved mapping algorithm named **Selective Alignment (SA)**. SA is designed to remain fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Salmon Index**, **Salmon Quant**, and **SBG Create Expression Matrix**.\n- **FASTQ read files** is the required input port that accepts raw sequencing reads. \n- **Transcript FASTA or Salmon Index** is also required and accepts a transcriptome reference file or a pre-built salmon index file.\n- **Genome FASTA** accepts the reference genome file used for extracting decoy sequences.\n- **Gene annotation** file to be used for creating transcripts to genes mapping file required for gene-level aggregation of quantification results.\n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript-level quantifications**), and an optional file containing **Gene-level quantifications** if the **Gene annotation** input is provided. \n- In addition to the quantification outputs, additional outputs can be produced if the proper options are set. These files will be accessible in the TAR archive on the **Salmon Quant archive** output port. \n- A **Transcript expression matrix** and a **Gene expression matrix** will be generated if more than one sample is provided.\n- The workflow is optimized to run in scatter mode. To run it successfully, supply multiple samples (paired-end or single-end, with adequately filled out **Sample ID** and **Paired End** metadata). \n- The **Salmon Quant archive** output can be used for downstream differential expression analysis tools, like DESeq2. \n\n### Changes Introduced by Seven Bridges\n\n- All output files produced with **Salmon Quant - Reads** will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, or from filename otherwise), instead of having identical names between runs.\n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to correctly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated Salmon index archive) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e., two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2), proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs.\n- If the location of the project where the workflow is running is set to Google US West, to run the task successfully, please reduce the default number of CPUs by setting the **Number of CPUs** parameter of **SBG Pair FASTQs** to 32.\n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge instance (AWS) is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   18min   | $0.44| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   46min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. The file names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_transcriptome_or_index': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Salmon 1.0.0 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)", "input": [{"name": "Transcript FASTA or Salmon Index", "encodingFormat": "application/x-tar"}, {"name": "FASTQ read files", "encodingFormat": "text/fastq"}, {"name": "Gene annotation", "encodingFormat": "application/x-gtf"}, {"name": "Genome FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Output file name"}, {"name": "Column name"}, {"name": "Column name"}, {"name": "Output file name"}, {"name": "Alternative initialization mode"}, {"name": "Bias speed sample"}, {"name": "Bootstrap reproject"}, {"name": "Consensus slack [quasi-mapping mode only]"}, {"name": "Discard orphans in Quasi-mapping mode [quasi-mapping mode only]"}, {"name": "Dump equivalence class counts"}, {"name": "Dump equivalence class weights"}, {"name": "Faster mapping [Developer]"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Forgetting factor"}, {"name": "The value given to a gap extension in an alignment [quasi-mapping mode only]"}, {"name": "The value given to a gap opening in an alignment [quasi-mapping mode only]"}, {"name": "GC bias correction"}, {"name": "GC size sample"}, {"name": "Incompatible prior probability"}, {"name": "Initialize uniform parameters"}, {"name": "Library type"}, {"name": "Maximum read occurence"}, {"name": "Meta"}, {"name": "Minimum assigned fragments"}, {"name": "Minimum score fraction [quasi-mapping mode only]"}, {"name": "No bias length threshold"}, {"name": "No effective length correction"}, {"name": "No fragment length distribution"}, {"name": "No gamma draw"}, {"name": "No length correction"}, {"name": "Number of auxiliary model samples"}, {"name": "Number of bias samples"}, {"name": "Number of bootstraps"}, {"name": "Number of Gibbs samples"}, {"name": "Number of pre auxiliary model samples"}, {"name": "Per transcript prior"}, {"name": "Position bias"}, {"name": "Range factorization bins"}, {"name": "The value given to a match between read and reference nucleotides in an alignment [quasi-mapping mode only]"}, {"name": "The value given to a mismatch between read and reference nucleotides in an alignment [quasi-mapping mode only]"}, {"name": "Reduce GC memory"}, {"name": "Sequence-specific bias correction"}, {"name": "Significant digits"}, {"name": "Use the EM algorithm"}, {"name": "Use Variational Bayesian optimization"}, {"name": "Validate mappings [quasi-mapping mode only]"}, {"name": "VBEM prior"}, {"name": "Write Mappings"}, {"name": "Maximum MMP Extension [quasi-mapping mode only]"}, {"name": "Bandwidth [quasi-mapping mode only]"}, {"name": "Allow Dovetail [quasi-mapping mode only]"}, {"name": "Recover Orphans [quasi-mapping mode only]"}, {"name": "Mimic BT2 [quasi-mapping mode only]"}, {"name": "Mimic Strict BT2 [quasi-mapping mode only]"}, {"name": "Hard Filter [quasi-mapping mode only]"}, {"name": "Skip Quant"}, {"name": "Write Orphan Links"}, {"name": "Write Unmapped Names"}, {"name": "Thinning Factor"}, {"name": "Softclip overhangs [selective-alignment mode only]"}, {"name": "Disable selective-alignment [quasi-mapping mode only]"}, {"name": "Full length alignment [selective-alignment mode only]"}, {"name": "Hit filter policy"}, {"name": "Maximum occurrences per hit"}, {"name": "Skip fragment length estimate for SE reads"}, {"name": "Nucleotide level prior"}, {"name": "K-mer length"}, {"name": "Gencode FASTA"}, {"name": "Sparse"}, {"name": "Keep duplicates"}, {"name": "Number of threads"}, {"name": "Filter size"}], "output": [{"name": "Transcript-level quantification"}, {"name": "Gene-level quantification"}, {"name": "Salmon Quant archive", "encodingFormat": "application/x-tar"}, {"name": "Salmon quant log"}, {"name": "Expression matrix transcripts", "encodingFormat": "text/plain"}, {"name": "Expression matrix genes", "encodingFormat": "text/plain"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v1.0.0"], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Rob Patro, Avi Srivastava", "softwareVersion": ["v1.0"], "dateModified": 1648468512, "dateCreated": 1575463486, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-workflow-1-2-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-workflow-1-2-0/4", "applicationCategory": "Workflow", "name": "Salmon workflow 1.2.0", "description": "The **Salmon Workflow** estimates transcript abundances from RNA-Seq data using an improved mapping algorithm named **Selective Alignment (SA)**. SA is designed to remain fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The workflow consists of three steps: **Salmon Index**, **Salmon Quant**, and **SBG Create Expression Matrix**.\n- **FASTQ read files** is the required input port that accepts raw sequencing reads. \n- **Transcript FASTA or Salmon Index** is also required and accepts a transcriptome reference file or a pre-built salmon index file.\n- **Genome FASTA** accepts the reference genome file used for extracting decoy sequences.\n- **Gene annotation** file to be used for creating transcripts to genes mapping file required for gene-level aggregation of quantification results.\n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript-level quantifications**), and an optional file containing **Gene-level quantifications** if the **Gene annotation** input is provided. \n- In addition to the quantification outputs, additional outputs can be produced if the proper options are set. These files will be accessible in the TAR archive on the **Salmon Quant archive** output port. \n- A **Transcript expression matrix** and a **Gene expression matrix** will be generated if more than one sample is provided.\n- The workflow is optimized to run in scatter mode. To run it successfully, supply multiple samples (paired-end or single-end, with adequately filled out **Sample ID** and **Paired End** metadata). \n- The **Salmon Quant archive** output can be used for downstream differential expression analysis tools, like DESeq2. \n\n### Changes Introduced by Seven Bridges\n\n- All output files produced with **Salmon Quant - Reads** will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, or from filename otherwise), instead of having identical names between runs.\n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to correctly set the **Paired End** metadata field on your read files.\n- The input FASTA file (if provided instead of the already generated Salmon index archive) should be a transcriptome FASTA, not a genomic FASTA.\n- For FASTQ reads in multi-file format (i.e., two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2), proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs.\n- If the location of the project where the workflow is running is set to Google US West, to run the task successfully, please reduce the default number of CPUs by setting the **Number of CPUs** parameter of **SBG Pair FASTQs** to 32.\n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge instance (AWS) is used by default. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  4 x 4.5 GB |     Yes    |     20M     |     101     |   18min   | $0.44| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB, 2 x 19 GB |     Yes    |     76M & 84M    |     101     |   46min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. The file names below are just as an example.\ninputs = {\n        'in_reads': list(api.files.query(project=project_id, names=['sample_pe1.fq', 'sample_pe2.fq'])),\n        'in_annotation': list(api.files.query(project=project_id, names=['gtf_file.gtf'])),\n        'in_transcriptome_or_index': list(api.files.query(project=project_id, names=['transcriptome_fasta_file.fa']))\n        }\n\n# Run the task\ntask = api.tasks.create(name='Salmon 1.0.0 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)", "input": [{"name": "FASTQ read files", "encodingFormat": "text/fastq"}, {"name": "Transcript FASTA or Salmon Index", "encodingFormat": "application/x-tar"}, {"name": "Genome FASTA", "encodingFormat": "application/x-fasta"}, {"name": "GTF annotation", "encodingFormat": "application/x-gtf"}, {"name": "Maximum number of parallel jobs"}, {"name": "Number of CPUs"}, {"name": "Alternative initialization mode"}, {"name": "Bias speed sample"}, {"name": "Bootstrap reproject"}, {"name": "Consensus slack"}, {"name": "Discard orphans in Quasi-mapping mode"}, {"name": "Dump equivalence class counts"}, {"name": "Dump equivalence class weights"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Forgetting factor"}, {"name": "The value given to a gap extension in an alignment"}, {"name": "The value given to a gap opening in an alignment"}, {"name": "GC bias correction"}, {"name": "GC size sample"}, {"name": "Incompatible prior probability"}, {"name": "Initialize uniform parameters"}, {"name": "Library type"}, {"name": "Maximum read occurence"}, {"name": "Meta"}, {"name": "Minimum assigned fragments"}, {"name": "Minimum score fraction"}, {"name": "No bias length threshold"}, {"name": "No effective length correction"}, {"name": "No fragment length distribution"}, {"name": "No gamma draw"}, {"name": "No length correction"}, {"name": "Number of auxiliary model samples"}, {"name": "Number of bias samples"}, {"name": "Number of bootstraps"}, {"name": "Number of Gibbs samples"}, {"name": "Number of pre auxiliary model samples"}, {"name": "Per transcript prior"}, {"name": "Position bias"}, {"name": "Range factorization bins"}, {"name": "The value given to a match between read and reference nucleotides in an alignment"}, {"name": "The value given to a mismatch between read and reference nucleotides in an alignment"}, {"name": "Reduce GC memory"}, {"name": "Sequence-specific bias correction"}, {"name": "Significant digits"}, {"name": "Use the EM algorithm"}, {"name": "Use Variational Bayesian optimization"}, {"name": "VBEM prior"}, {"name": "Write Mappings"}, {"name": "Bandwidth"}, {"name": "Allow Dovetail"}, {"name": "Recover Orphans"}, {"name": "Mimic BT2"}, {"name": "Mimic Strict BT2"}, {"name": "Hard Filter"}, {"name": "Skip Quant"}, {"name": "Write Orphan Links"}, {"name": "Write Unmapped Names"}, {"name": "Thinning Factor"}, {"name": "Softclip overhangs"}, {"name": "Full length alignment [selective-alignment mode only]"}, {"name": "Hit filter policy"}, {"name": "Maximum occurrences per hit"}, {"name": "Skip fragment length estimate for SE reads"}, {"name": "Nucleotide level prior"}, {"name": "Column name"}, {"name": "Output file name"}, {"name": "Column name"}, {"name": "Output file name"}, {"name": "K-mer length"}, {"name": "Gencode FASTA"}, {"name": "Keep duplicates"}], "output": [{"name": "Transcript-level quantification"}, {"name": "Gene-level quantification"}, {"name": "Salmon Quant archive", "encodingFormat": "application/x-tar"}, {"name": "Salmon quant log"}, {"name": "Expression matrix transcripts", "encodingFormat": "text/plain"}, {"name": "Expression matrix genes", "encodingFormat": "text/plain"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0"], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Rob Patro, Avi Srivastava", "softwareVersion": ["v1.0"], "dateModified": 1648468512, "dateCreated": 1586797701, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-mpileup-parallel/27", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-mpileup-parallel/27", "applicationCategory": "Workflow", "name": "SAMtools Mpileup parallel", "description": "The purpose of this workflow is to allow user to execute the SAMtools Mpileup tool in parallel using all of the instance resources. \n\nInstead of analyzing whole BAM at once, this workflow obtains contigs information from FASTA reference index (FAI). This file contains a list of contigs where pileup or BCF should be generated. This is done by using parameter (-I or --positions FILE) of the SAMtools Mpileup tool. Jobs will be then executed per chromosome on each of the available core of the instance, while all small contigs will be executed together in a single job on one core.\n\nAs a result, execution time will be order of magnitude faster compared to the execution of the whole BAM at once. Also, output of the workflow can be an integral file for the whole BAM, or list of files per chromosome which is defined by the parameter \"output_state\". Finally, user can choose format of the output file or list of files by the SAMtools mpileup parameter output_type. Available output formats are PILEUP, VCF and BCF.\n\n######Required inputs:\n\n- Reference file-The faidx-indexed reference file in the FASTA format.\n\n- BAM file-One or a list of input BAM files.\n\n- FAI file-One of these two is required input. FAI file is used to extract intervals that will be used to scatter the SAMtools mpileup job. FAI file can be generated using SAMtools faidx. \n\n- Merging mode-This defines the way this pipeline outputs its results. Options are:\n      - \"Merge\": to produce VCF, PILEUP, or BCF file acquired after merging results of the scattered pileup.\n      - \"Pass nonempty files\"  to pass through all nonempty PILEUP, VF, BCF files without merging.\n      - \"Pass all files\": to output all PILEUP, VF, BCF Separately.\n\n- Output file format -This parameter can be used to change the output format (available choices are VCF/BCF and pileup)\n\n###### Outputs:\nThis pipeline has one output port and the output depends on the selected input parameters (merge mode and output file format). Please refer to these parameters for more details.\n\n###### Common Issues\nBAM files should be sorted by coordinates before using SAMmpileup.", "input": [{"name": "bams", "encodingFormat": "application/x-bam"}, {"name": "reference_fasta", "encodingFormat": "application/x-fasta"}, {"name": "fai_file"}, {"name": "Skip bases with baseQ/BAQ smaller than"}, {"name": "Required flags"}, {"name": "Region in which pileup is generated"}, {"name": "Recalculate BAQ on the fly"}, {"name": "Apply -m and -F thresholds per sample"}, {"name": "Parameter for adjusting mapQ"}, {"name": "Generate uncompress BCF/VCF output"}, {"name": "Output mapping quality"}, {"name": "Output file format"}, {"name": "Output base positions on reads"}, {"name": "Do not perform indel calling"}, {"name": "Comma-separated list of FORMAT and INFO tags to output"}, {"name": "Minimum fraction of gapped reads for candidates"}, {"name": "Minimum gapped reads for indel candidates"}, {"name": "Max per-BAM depth"}, {"name": "Skip INDEL calling if the average per-sample depth is above"}, {"name": "Skip alignments with mapQ  smaller than"}, {"name": "Ignore RG tags"}, {"name": "Disable read-pair overlap detection"}, {"name": "Coefficient for homopolymer errors"}, {"name": "Phred-scaled gap open sequencing error probability"}, {"name": "Phred-scaled gap extension seq error probability"}, {"name": "Filter zero coverage lines."}, {"name": "Filter flags"}, {"name": "Disable BAQ computation"}, {"name": "Count anomalous read pairs"}, {"name": "Comma separated list of platforms for indels"}, {"name": "Assume the quality is in the Illumina-1.3+ encoding"}, {"name": "Output state"}], "output": [{"name": "output_pileup", "encodingFormat": "application/x-vcf"}], "codeRepository": ["https://github.com/samtools/samtools"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li, Sanger Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039727, "dateCreated": 1453799466, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-deep-learning-image-classification-exploratory-workflow/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-deep-learning-image-classification-exploratory-workflow/14", "applicationCategory": "Workflow", "name": "SBG Deep Learning Image Classification Exploratory Workflow", "description": "**SBG Deep Learning Image Classification Exploratory Workflow**  is an image classifier pipeline. \n\nIt relies on the transfer learning approach, which allows the use of pre-trained models as the starting point for building a model adjusted to given image datasets. Furthermore, the pipeline allows to train the model for a variety of hyperparameter combinations, while detailed metrics and visualizations help to determine the best configuration that can later be used to make predictions on new data instances.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\nMost common use case of **SBG Deep Learning Image Classification Exploratory Workflow**  is to find a combination of hyperparameter values that generates the best model for a given dataset and criteria. This best model configuration can then be used in the final classification of unlabeled images, which is done with the **SBG Deep Learning Prediction** tool.\n\n\nThe pipeline takes labeled images split into **Train directory**, **Test directory** and optionally - **Validation directory** as its input. Image labels can be defined either by a sub-directories structure or with an image class manifest (see **Common Issues and Important Notes**). \n\nThe pipeline consists of four steps:\n1. Defining sets of hyperparameters\n\n    A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data. When a machine learning algorithm is tuned for a specific problem, the hyperparameters of the model are tuned in order to discover the parameters of the model that result in the most skilful predictions. \n\n   **SBG Create Input JSON** tool takes one or more values for any available hyperparameter as its input, as well as the desired number of output configurations, and creates combinations of hyperparameters in the form of JSON files (see **Common Issues and Important Notes**). These files are then used by the **SBG Deep Learning Image Processing** tool as input configurations for the model training. \n2. Importing the datasets and conversion of medical images formats \n\n    The user should provide train, test and optionally, validation directories containing labeled images. If no validation directory is provided, a portion of the train directory, determined by the **Validation data split factor** parameter, will be used for validation. If the input data are medical images in a non standard format (SVS, TIFF or DICOM), they will be automatically converted to PNG format by using the **SBG Image Convert** tool. Otherwise, if the images are in PNG or JPEG format, no conversion is needed and the **SBG Image Convert** step is skipped. \n3. Loading the datasets and composing the model\n\n    The actual image processing part - building, training and evaluation of the model - is done by the  **SBG Deep Learning Image Processing** tool. Images are loaded from input directories using dataset preprocessing utilities from Keras[1], a deep learning API written in Python, running on top of the machine learning platform TensorFlow[2]. After the datasets are loaded, rescaling of the images and data augmentation is performed. Data augmentation is used when the input dataset doesn\u2019t contain a large number of images , so it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. \n\n    After the data is loaded, the model is composed by loading in the pre-trained base model (and pre-trained weights) and stacking the classification layers on top. A pre-trained model is a model that was trained on a large benchmark dataset to solve a problem similar to the one that we want to solve. Accordingly, due to the computational cost of training such models, it is common practice to import and use models from published literature (e.g. [VGG](https://keras.io/api/applications/vgg/), [Xception](https://keras.io/api/applications/xception/), [MobileNet](https://keras.io/api/applications/mobilenet/)). The pipeline supports eleven pre-trained models most commonly found in papers and challenges dealing with similar problems. The user can select the desired base model by setting  the **Pre-trained model** parameter. \n\n    The final step in creating a model is compilation. At this stage the loss function, the optimiser and the metrics are defined. Once the compilation is done, the model enters the training phase, and finally, metrics can be collected. \n\n4. Generating the outputs\n\n    For a single combination of hyperparameters, **SBG Deep Learning Image Processing** generates visualisations and CSV with quality metrics, such as confusion matrix, ROC, AUC, accuracy, F-score etc. When **SBG Deep Learning Image Classification Exploratory Workflow** is run for multiple configurations, all the CSV metric files are gathered and processed by the **SBG Merge Metrics** tool, which concatenates all tables and sorts them by the criteria selected via the **Sort by** parameter. The resulting merged CSV file is the first output of the pipeline, labeled as **Metrics from all configurations**.\n\n    **SBG Prepare Reports** tool takes both visualizations and merged CSV on input and outputs **Visualizations from all the configurations**  in ZIP format, as well as separate PNG files for each figure for the best configuration (**Visualizations from the best configuration**). Additionally, the **SBG Create HTML Report** tool generates an interactive **HTML report** which contains an interactive, sortable table containing metrics from the model training process for all generated models, as well as various plots for the best model.\n\n\n### Common Issues and Important Notes\n* **SBG Deep Learning Image Classification Exploratory Workflow** supports various classification types - binary, multi-class and multi-class multi-label classification. The classification type is determined automatically. \n* Supported image formats are PNG, JPG/JPEG, SVS, DCM, DICOM and TIFF.\n* Both **Train directory** and **Test directory** are required inputs, but if the user has all their images inside one folder, the images could be split into train and test by using the **SBG Split Folders** tool outside the workflow.\n* Directories containing images should be organized in one of two ways:\n    * Under each of the dataset directories, there can be subdirectories, one for each class where the actual images are placed \n    * All the images are under the main directory, together with a single CSV file that has only two columns, where the first column has all the image names, and the second column contains corresponding classes. \n* The user can select number of configurations that will be trained by setting the **Number of output configurations** parameter, maximum possible value being 512 configurations in a single run. Please note that if the selected number of configurations is higher than number of available hyperparameter combinations, maximum possible number of configurations will be trained.\n* The **Dense layers** parameter enables you to set the number of dense layers and the number of neurons in each of them. Layers should be comma separated, represented by the number of neurons inside (for example: 100,100,100 means three dense layers, each with a hundred neurons). Default value is two layers with a hundred neurons each (100,100).\n* Model configuration string which can be used in the final classification of unlabeled images with **SBG Deep Learning Prediction** is located in the first column of the metrics table. It is a combination of hyperparameter values used for building the model, separated by colons. It is present in both **Metrics from all configurations** and **HTML report** output files.\n* **SBG Deep Learning Image Classification Exploratory Workflow** relies on AWS GPU instances, available for projects located on AWS. To use this tool, please run it from an AWS project.\n\n### Performance Benchmarking\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size | Number of configurations| Number of parallel instances| Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|---|---|---|\n|940|250 KiB|10|1|3h 57m 22s|$3.39|p2.xlarge|\n|5856|200 KiB|1|1|42m 43s|$0.77|p2.xlarge|\n|5856|200 KiB|24|1|2d 5h 18m 33s|$55.59|p2.xlarge|\n|5856|200 KiB|24|3|8h 56m 45s|$42.48|p2.xlarge|\n|650| 20 MiB |1|1|54m 39s|$0.99|p2.xlarge|\n|650| 20 MiB |24|5|5h 57m 23s|$47.53|p2.xlarge|\n|650| 90 MiB |24|5|1d 4h 54m|$285.45|p2.xlarge|\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1]  Fran\u00e7ois Chollet and others. *Keras*, 2015. [https://keras.io](https://keras.io)\n\n[2] Mart\u00edn Abadi, Yuan Yu and others. *TensorFlow: Large-scale machine learning on heterogeneous systems*, 2015. Software available from [tensorflow.org](https://tensorflow.org).", "input": [{"name": "Test directory"}, {"name": "Train directory"}, {"name": "Validation directory"}, {"name": "Sort by"}, {"name": "Number of output configurations"}, {"name": "Batch size"}, {"name": "Image size"}, {"name": "Pre-trained model"}, {"name": "Dense layers"}, {"name": "Dropout layer"}, {"name": "Learning rate"}, {"name": "Epochs - initial"}, {"name": "Epochs - additional"}, {"name": "Size for conversion"}, {"name": "Validation data split factor"}, {"name": "Average type"}], "output": [{"name": "Best configuration visualizations"}, {"name": "Visualizations from all configurations", "encodingFormat": "application/zip"}, {"name": "Merged CSV"}, {"name": "HTML report", "encodingFormat": "text/html"}], "softwareRequirements": ["LoadListingRequirement", "ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/sbg-deep-learning-image-classification-exploratory-workflow/14.png", "applicationSubCategory": ["SBGTools", "Imaging", "Machine Learning"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648468626, "dateCreated": 1628084721, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-tcia-archive-viewer/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-tcia-archive-viewer/3", "applicationCategory": "Workflow", "name": "SBG TCIA Archive Viewer", "description": "The TCIA Archive Viewer consists of two steps:\n\n* The first step involves unpacking of the archive file using the SBGDecompressor.\n* The second step involves generation of (a) a Base64-encoded HTML file that displays a preview of all the images in the provided archive and (b) a new archive containing the HTML gallery and a JSON representation of the DICOM images.\n\n##Inputs and outputs\n\n###Inputs:\n* **TCIA archive file** - Archive containing the DICOM images\n\n###Outputs:\n* **Base64 DICOM gallery** - A Base64-encoded HTML that can be viewed on the platform\n* **Downloadable DICOM gallery** -  An archive containing an HTML file representing the gallery, an images folder containing the JSON representation of the DICOM images, and a dist folder containing the necessary library files.\n\n##Instructions for using the downloaded gallery\n\nAfter downloading and uncompressing the .tar.gz file, double click the HTML file to open it in your browser. Click on the Load images button and select the desired JSON files from the images folder. Files are named based on the slice number shown for each image in the Base64 output file from the task. After  you have selected images to load, click on any thumbnail to open a pop-up window with the full-size image. Click on the pop-up window to close it.\n\n##Common issues\n\n* Metadata in the downloaded HTML file is specific to the executed task", "input": [{"name": "Input TCIA archive file", "encodingFormat": "application/zip"}], "output": [{"name": "Base64 DICOM gallery"}, {"name": "Compressed DICOM gallery", "encodingFormat": "application/x-tar"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/sbg-tcia-archive-viewer/3.png", "applicationSubCategory": ["Imaging"], "project": "SBG Public Data", "creator": "SBG", "softwareVersion": ["sbg:draft-2"], "dateModified": 1511192354, "dateCreated": 1511192354, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-x-ray-image-preprocessing-workflow-1-0/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-x-ray-image-preprocessing-workflow-1-0/12", "applicationCategory": "Workflow", "name": "SBG X-Ray Image Preprocessing Workflow", "description": "**X-ray Images Preprocessing Workflow** performs the selected X-ray image enhancement algorithm: UM, HEF or CLAHE.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n### Common Use Cases\nBesides resizing and normalization (according to the deep neural network used), in many cases no additional preprocessing is performed on X-ray images. Few preprocessing methods can be found in literature [1] and community [2]. Three most common algorithms are implemented within the **SBG X-Ray Image Preprocessing Workflow** -  unsharp masking (UM), high-frequency emphasis filtering (HEF) or contrast limited adaptive histogram equalization (CLAHE).\n\n1 - Unsharp Masking (UM)\n\nUnsharp masking is a linear filter that is capable of amplifying high-frequencies of an image. \nThe first step of the algorithm is to copy the original image and apply whether a median or gaussian blur into it, which should be selected by setting the **Filter type** parameter. \n\nBlur intensity is controlled by the **Radius** parameter which defines the size of the edges. If we subtract the blurred image from the original image, we will obtain only the edges created by the blur. This is what we call unsharpened mask. Finally, the enhanced image is collected after applying the following formula:\n\n`sharpened image = original image + amount * (unsharpened mask)`\n\nThe **Amount** setting, on the other hand, controls the intensity of the edges (how dark or light it will be).\n\n2 - High-frequency Emphasis filtering (HEF)\n\nHigh-frequency Emphasis filtering is a technique that uses Gaussian High Pass Filter to emphasize and accentuate the edges. The edges tend to be expressed in the high-frequency spectrum since they have more drastic changes of intensity. This technique produces a low contrast image and the use of Histogram Equalization is required to increase both sharpness and contrast.\n\nThe first step of the algorithm is to apply a gaussian high pass filter into it. The image has to go through the Fourier transformation and the filter function is calculated onto it. After the inverse transformation we will have a filtered image. Secondly, the contrast of the image will be adjusted with simple Histogram Equalization:\n\n`sharpened image = (original image + (Gaussian Highpass Filter)) * (Histogram Equalization)`\n\n3 - Contrast Limited Adaptive Histogram Equalization (CLAHE) \n\nContrast Limited Adaptive Histogram Equalization method is a histogram-based method used to improve contrast in images. This technique computes the histogram for the region around each pixel in the image, improving the local contrast and enhancing the edges in each region. Since AHE can over-amplify noise in the image, CLAHE prevents this by limiting the amplification.\nTo apply CLAHE to the images, we first convert them to grayscale and then normalize. This approach is similar to N-CLAHE, but we do not used a log normalization. After this step, the image is padded by reflecting the pixels in the borders, so we can process it all.\n\nThen, for each pixel in the image, we calculate the clipped histogram for the region around it, i.e., we define the maximum number of occurrences a pixel should have. If the occurrence is greater than the clip limit, we cut the excess and redistribute to all pixels. To improve this technique, this process can be repeated a certain number of times.\nWith the clipped histogram, we calculate the probability of each pixel in it and compute the CDF (Cumulative Distribution Function), using the cumulative sum of the ordered pixels, and multiply each value of the function by 255, to limit the image's values to [0, 255]. Since these are float values, the floor operation is used.\n\nAfter calculating the CDF, all pixels will have a transformation value. We now apply this transformation to the pixel in the center of the region.\nThis CLAHE implementation expects 3 inputs: \n* **Window size** - which is the size of the rectangular region around the pixel to be processed. \n* **Clip limit** - which is the maximum number of occurrences of the pixel in the histogram. \n* **Number of iterations** - which is the number of clipping iterations. It can reduce noise in the image, but increases the needed computational power drastically and isn't useful in all cases.\n\n### Common Issues and Important Notes\n- The workflow consists of one tool only - **X-ray Images Preprocessing**. Implementation like this enables parallelisation and speed improvement over an otherwise slow process.\n- The **Image enhancement algorithm** parameter is required for running the workflow. The user should select one of the three available filter types. The rest of the parameters have default values that can be changed if desired. Please make sure to set only parameters relevant for the selected filter type.\n- Since CLAHE computes a histogram for each pixel, its complexity is very high and demands a lot of time and resources to finish its task. For example, running CLAHE algorithm with only one iteration and for 1341JPEG images of average size ~250 kB  on its input, took approximately 16 hours to complete on c5.12xlarge instance, while it failed on both the default one, c4.2xlarge, and the larger c5.9xlarge. Please make sure you set the **Memory per job** and **CPU per job** parameters in your tasks accordingly.\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size | Algorithm |Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|---|\n|390|200 KiB|UM (gaussian)|13m 36s|$0.17|c4.2xlarge|\n|390|200 KiB|UM (median)|19m 27s|$0.23|c4.2xlarge|\n|390|200 KiB|HEF|24m 14s|$0.28|c4.2xlarge|\n|234|250 KiB|CLAHE|17h 39m 11s|$15.5|c4.2xlarge|\n|1341|250 KiB|UM (gaussian)|45m 40s|$0.43|c4.2xlarge|\n|1341|250 KiB|UM (median)|1h 3m 22s|$0.59|c4.2xlarge|\n|1341|250 KiB|HEF|2h 42m 22s|$1.48|c4.2xlarge|\n|1341|250 KiB|CLAHE|16h 6m 41s|$35.34|c5.12xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1] Udeshani, K.A.G. & Meegama, Gayan & Fernando, T.G.I.. (2011). [Statistical Feature-based Neural Network Approach for the Detection of Lung Cancer in Chest X-Ray Image](https://www.researchgate.net/publication/275719469_Statistical_Feature-based_Neural_Network_Approach_for_the_Detection_of_Lung_Cancer_in_Chest_X-Ray_Image). International Journal of Image Processing (IJIP). 5. 425-434. \n\n[2]  Andr\u00e9 Scheibel de Almada, Cho Young Lim & Eduardo Garcia Misiuk. (2021). [X-Ray Images Enhancement](https://github.com/asalmada/x-ray-images-enhancement) [GitHub repository].", "input": [{"name": "Input images"}, {"name": "Filter type"}, {"name": "D0 value for High cut"}, {"name": "Image enhancement algorithm"}, {"name": "Radius"}, {"name": "Amount"}, {"name": "Window size"}, {"name": "Clip limit"}, {"name": "Number of iterations"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output images"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/sbg-x-ray-image-preprocessing-workflow-1-0/12.png", "applicationSubCategory": ["SBGTools", "Imaging", "Machine Learning"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648468627, "dateCreated": 1628085644, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sc-pseudobulk-de/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sc-pseudobulk-de/2", "applicationCategory": "Workflow", "name": "Single Cell Multi Sample Pairwise Differential Expression Workflow", "description": "This pipeline performs differential expression analysis on single cell data between pairs of user defined conditions.\n\nA powerful use of scRNA-seq technology lies in the design of replicated multi-condition experiments to detect changes in expression between conditions. This pipeline enables pairwise differential expression (DE) analyses between samples belonging to multiple conditions. DE analysis can be performed by using all cells that belong to given conditions or the user might choose the option to perform data integration, batch correction and clustering prior to DE analysis. This pipeline is based on SingleCellExperiment 1.12.0,  scater 1.18.3, scran 1.18.3, scanorama 1.7, bbknn 1.3.12, edgeR 3.32.1 & Seurat 3.2.3 packages.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n### Common Use Cases\n\nIn this section we briefly describe parameters that can be set at each step of the pipeline.\n\n##### Single cell data input\n\nSingle cell data can be provided in the form of gene-cell count matrices produced by a wide variety of methods for single-cell data processing. Count matrices can be created with the following tools available on the Seven Bridges Platform: \n\n1. *Cell Ranger Counts*\n2. *Salmon Alevin*\n3. *Kallisto BUStools Workflow*\n4. *zUMIs*\n5. *Single-Cell Smart-seq2 Workflow v3.0.0*\n6. *STAR (STARsolo option)*\n\nThe user needs to provide multiple gene-cell count matrix files to the **Gene-cell count matrices** input port and chose an adequate value for the **Input Type** parameter.\nAlong with the count matrices, the metadata file should be provided to the **Metadata table** input port. Here\u2019s an example of a metadata table, it is mandatory to keep the same column names as shown below:\n\n|input_name|sample_name|batch|condition|\n|--|--|--|--|--|\n|pbmc1k_v3_alevin_output.tar.gz|pbmc1k|1|control|\n|pbmc4k_v2_alevin_output.tar.gz|pbmc4k|2|treatment|\n|pbmc8k_v2_alevin_output.tar.gz|pbmc8k|3|treatment|\n\nInstead of providing a metadata table, one can set the metadata values using the [visual interface](https://docs.sevenbridges.com/docs/set-metadata-using-the-visual-interface). Use the **Sample ID** field to set the sample names, and **Batch number** field to set the batch label. To add a **condition** field, one should use a manifest file, explained in more detail [here](https://docs.sevenbridges.com/docs/format-of-a-manifest-file). When the metadata values are properly set, the metadata table will be automatically generated.\n\nAdditionally, one can provide multiple single-cell samples combined in a single *SingleCellExperiment* object and saved as an RDS file used for storing R objects. In this case, SingleCellExperiment object colData must contain \"sample\", \"condition\" and \"batch\" information.\n\n##### Set up QC parameters\n\nLow quality cells are detected and removed based on library size, spike-ins and mitochondrial genes calculation.\nAdditionally, all genes expressed in less cells than the **Minimum number of cells** parameter value are discarded. Also, all cells that have less genes than the **Minimum number of genes** parameter value are removed as well.\n\n\n##### Chose if clustering should be used for DE\n\nTo perform DE inside clusters, set **Use clustering for DE** to TRUE. ***If clusters are not used for DE then integration and clustering steps will be skipped.*** \n\n##### Detection of Highly Variable Genes (HVGs)\n\nFitted mean-variance trend is used to determine HVGs.\n\nIf ERCC spike-ins are available, it is recommended to set **Use ERCC for mean-variance modeling** to TRUE. If spike-ins are not available, but single cell data is produced by using UMI based protocol, then it is recommended to set **Poisson mean-variance modeling** to TRUE. \n\nIt is important to preview mean-variance graphs in the HTML report (produced as one of the outputs) to check the quality of fit. Poor quality fitting is an indication that analysis should be repeated with different options for HVGs detection.\n\n#####  Batch effect removal\n\nIn this step, if data is coming from multiple batches, **Scanorama** [2] or **BBKNN** [3] is used for batch effect removal. Scanorama provides better data integration but BBKNN is much faster and less computationally expensive. It is recommended to use Scanorama when the number of cells is less than 100K and to use BBKNN when the number of cells is larger.\n\n##### Clustering\n\nClustering is performed using the **Seurat** R package [1].Since clustering execution is fast, it is recommended to use multiple clustering resolutions. Multiple clustering resolutions will result in various cell groupings. Low clustering resolution (~ 0.1 results in a small number of clusters while higher values (~1) result in a higher number of clusters. Use the *Clustering resolutions* parameter to set clustering resolutions. If none of the resolutions is set and the *Use clustering for DE* parameter is set to TRUE, the following resolutions will be used by default: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 and 1.\n\nAdditionally, we use the **clustree** R package to generate a visualisation for interrogating clusterings for the following resolutions : 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 and 1. Clustree displays the relation of clusters that were obtained by using various resolutions. If **Use provided resolutions for clustree** is set to TRUE, then user-provided resolutions will be included in clustree alongside default resolutions. Clustree can be used to determine stability of clustering resolutions.\n\n##### Using interactive report\n\nInteractive report can be used to preview results when multiple clustering resolutions are used. Interactive report is an RMD file that is outputted by the analysis. This file can be executed in RStudio, using the [Interactive Analysis](https://docs.sevenbridges.com/docs/interactive-analysis-on-the-platform) feature of the Seven Bridges Platform following these steps:\n1. Start RStudio using Data Cruncher within Interactive Analysis feature\n2. Access Interactive Analysis within a project, select Data Cruncher and start an RStudio analysis.\n3. Copy the interactive report to the RStudio workspace\n4. When the file is copied, locate the *SingleCellExperiment output* of your task.\n5. Copy the name of the *SingleCellExperiment output* file and assign this name to the `rds_file_name` variable that is located at the bottom of the first cell of the .RMD file.\n6. Click the `Run document` button within RStudio.\n7. After installing the required dependencies, an interactive report will open in a new tab.\n\n### Changes Introduced by Seven Bridges\n\nWorkflow created by Seven Bridges Bioinformatics Team.\n\n### Common Issues and Important Notes\n\nNone\n\n### Performance Benchmarking\n\nWe allocated *r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)* as the default AWS instance type. When executing tasks, the selection of AWS instance type can be changed in **Execution Settings**. Here are the runtime and task costs for different experimental settings:\n\n| Experimental settings | # of cells | Integration method  | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|:------------:|\n|4 samples 2 groups|16895|BBKNN|13 min|$0.14|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|4 samples 2 groups|16895|Scanorama|13 min|$0.15|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|8 samples 2 groups|26068|BBKNN|20 min|$0.22|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|8 samples 2 groups|26068|Scanorama|23 min|$0.25|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|16 samples 2 groups|56667|BBKNN|47 min|$0.52|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|16 samples 2 groups|56667|Scanorama|1 h 2 min|$0.68|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|18 samples 3 groups|66538|BBKNN|1 h 2 min|$0.68|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|18 samples 3 groups|66538|Scanorama|1 h 42 min|$1.90|r5.4xlarge (1024GB EBS, 16vCPUs, 128GB RAM)|\n|20 samples 4 groups|72062|BBKNN|1 h 20 min|$0.88|r5.2xlarge (1024GB EBS, 8vCPUs, 64GB RAM)|\n|20 samples 4 groups|72062|Scanorama|2 h 23 min|$2.76|r5.4xlarge (1024GB EBS, 16vCPUs, 128GB RAM)|\n\n      \n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'in_counts': list(api.files.query(project=project_id, names=['sample_kallisto_gene_counts.Rdata'])), \n        'input_type': \"kallisto_bus\",\n\n        }\n\n# Run the task\ntask = api.tasks.create(name='Multi-Sample Pairwise Single-Cell Differential Expression Workflow- API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1]  [Seurat documentation](https://satijalab.org/seurat/)\n\n[2] [Scanorama](https://github.com/brianhie/Scanorama)\n\n[3] [BBKNN](https://github.com/Teichlab/bbknn)\n\n[4] [clustree](https://cran.r-project.org/web/packages/clustree/index.html)", "input": [{"name": "Gene-cell count matrices", "encodingFormat": "application/x-tar"}, {"name": "Metadata table"}, {"name": "Clustering resolutions"}, {"name": "Use clustering for DE"}, {"name": "Input Type"}, {"name": "Output name prefix"}, {"name": "Minimum number of cells"}, {"name": "Minimum number of genes"}, {"name": "Species"}, {"name": "Use provided resolutions for clustree"}, {"name": "Use ERCC for mean-variance modeling"}, {"name": "Poisson mean-variance modeling"}, {"name": "Integration method"}], "output": [{"name": "SingleCellExperiment output"}, {"name": "Interactive report"}, {"name": "Html report", "encodingFormat": "text/html"}, {"name": "Cluster DE results"}, {"name": "DE results"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/sc-pseudobulk-de/2.png", "applicationSubCategory": ["Single Cell", "RNA-Seq"], "project": "SBG Public Data", "creator": "dalibor_veljkovic", "softwareVersion": ["v1.1"], "dateModified": 1649687799, "dateCreated": 1618324101, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/velocity-analysis-with-scvelo-0-2-4/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/velocity-analysis-with-scvelo-0-2-4/2", "applicationCategory": "Workflow", "name": "Single cell RNA-seq velocity analysis with scVelo 0.2.4", "description": "This pipeline performs preprocessing, marker gene analysis and velocity analysis [1] of single cell expression data. It is based on SingleCellExperiment 1.14.1, Seurat 4.0.4, scran 1.20.1, scater 1.20.1, AnnotationHub 3.0.2, scuttle 1.2.1 and scVelo 0.2.4 \n\n**Warning** : This pipeline does not contain the batch effect correction step due to lack of batch effect correction solutions for velocity analysis.  Perform an analysis of multiple samples only if you are confident that batch effects are not present in the data.\n\nThe pipeline contains of the following steps:\n - Data loading - Spliced and unspliced counts are loaded from loom files. Data is stored in SingleCellExperiment format [2]. \n - Preprocessing - Quality control and filtering are performed in order to remove low quality cells [3]. Scran pooling normalization is performed in order to remove artifacts coming from cDNA transcription and PRC amplification [4]. Highly variable genes are detected.\n - Clustering and marker gene detection - PCA is performed on highly variable genes. UMAP and tSNE data embeddings are calculated based on top PCA components. Data is clustered to find cell communities. Marker gene analysis is performed for each cluster to assist interpretation of velocity results.\n - Velocity analysis - scVelo is used to perform velocity analysis [1]. Clusters and dimensionality reductions calculated in previous steps are used during this analysis.\n \n_A list of  **all inputs and parameters**  with corresponding descriptions can be found at the bottom of this page._\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\nIn this section we briefly describe parameters that can be set at each step of the pipeline.\n\n##### Single cell data input\nSingle cell data can be loaded in LOOM file format. One or multiple LOOM files can be provided to the **Loom files** input. The metadata file (CSV file) should be provided to the **Metadata table** input port. Information from the metadata table will be used during the automatic QC step and for sample-based cell coloring on UMAP plot (useful for batch effect detection via visual inspection). Example metadata table with mandatory column names is shown below:\n|input_name|sample_name\n|--|--|\n|file1.loom|sample_1_name|\n|file2.loom|sample_2_name|\n|file3.loom|sample_3_name|\n\nInstead of providing a metadata table, the user can set the sample names by using the **Sample ID** metadata field. If neither, the metadata table nor the file metadata are provided, file names will be used as sample ids. After data loading, Annotation Hub [5] will be used to find Ensembl gene names and contig for each gene.  In order to annotate gene names, the user must provide genome name by using the **Genome name**  input (example : \"GRCh38\" for version 38 of human genome)  and ensembl version (example: \"Ensembl 97\") based on the gtf file that was used during alignment and quantification by using the **Ensembl version** input.\n\n#### Single cell data preprocessing\nMultiple parameters are available for preprocessing customization.\n##### Quality control and filtering\nQuality control step is performed in order to remove low quality cells which are detected based on several criteria. \"Library size\" and \"Number of detected features\" criteria are always used while \"The proportion of reads mapped to spike-in transcripts\" and \"The proportion of reads mapped to mitochondrial transcripts\" are used if mitochondrial and spike-in genes are available in the dataset. Filtering thresholds can be inferred automatically or  set manually based on the **Quality control type** input. In automatic mode, thresholds are set by using the _quickPerCellQC_ function from the scuttle package. This method assumes that the majority of cells in the dataset are of good quality. In manual mode thresholds are user defined.\n\n##### Highly variable genes (HVGs) detection\nThe goal of HVGs detection is to select genes that contain useful information about the biology of the system while removing genes whose expression is only influenced by technical variation. HVGs detection is performed by using the scran library. Number of top variable genes that will be used for PCA dimensionality reduction is determined based on the **Number of highly variable genes** input.\n\n##### Clustering and marker gene detection\nCell clustering is performed using Seurat[6] in order to detect different cell populations. The number of PCA components that will be used during clustering can be set via the **Number of PCA components** input and clustering resolution can be chosen via the **Clustering resolution** input. Marker genes are calculated for each cluster in order to aid velocity result interpretation [6]. Additionally, it is possible to perform differential expression testing for each cluster pair by setting the **Perform pairwise differential expression** workflow input to TRUE.\n\n#### Dataset subset analysis and re-analysis\nDataset subset can be analyzed by providing \"subset metadata table\" to the **Cell subset metadata table** input. Names of all cells (as named in the loom file) to be analyzed must be specified in the **cell_name** column.  Additional columns of arbitrary names can be provided in this table. All additional columns will be added to the Seurat object as cell metadata based on the **cell_name** column.\n\nThe following table represents an example of a subset_metadata_table.\n\n|cell_name|cell_metadata_1|cell_metadata_2\n|--|--|--|--|\n|cell1_name|cell1_metadata_1|cell1_metadata_2|\n|cell2_name|cell2_metadata_1|cell2_metadata_2|\n|cell3_name|cell2_metadata_1|cell3_metadata_2|\n\n**Clustering name**  input defines which metadata field from the Seurat object will be used to denote cell clustering during velocity analysis and marker gene analysis. The default value is \"seurat_clusters\" which represents clusters obtained by running Seurat clustering. If additional metadata was added to the Seurat object via **Cell subset metadata table**, names of these metadata fields can be provided to **Clustering name** and used instead of Seurat clustering. In this case, Seurat clustering will not be performed. \n\nIf **Cell subset metadata table** is provided, QC and normalization will still be performed by using all cells. Dataset will be subsetted between normalization and highly variable detection steps. \n\n### Changes Introduced by Seven Bridges\n\nNo changes to the original libraries were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\nNone\n\n### Performance benchmarking\n| # of cells | Pairwise differential expression  |  Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:----------:|:------------:|\n|       1.2k     |     No   |      9 min   |   $0.06    |    r5.xlarge    |\n|       4.6k     |     No  |    9 min   |   $0.06    |    r5.xlarge    |\n|       8.7k     |     No    |   17 min   |   $0.11    |    r5.xlarge    |\n|       11.7k     |    No    |   45 min   |   $0.3    |    r5.xlarge    |\n|       1.2k     |     Yes   |      10 min   |   $0.07    |    r5.xlarge    |\n|       4.6k     |     Yes  |    14 min   |   $0.09    |    r5.xlarge    |\n|       8.7k     |     Yes    |   20 min   |   $0.13    |    r5.xlarge    |\n|       11.7k     |    Yes    |   1h 11 min   |   $0.49    |    r5.xlarge    |\n\n_Cost can be significantly reduced by using  **spot instances**. Visit the  [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances)  for more details._\n\n### Portability\n\n**Single cell RNA-seq velocity analysis with scVelo 0.2.4**  was tested with cwltool 3.1.20220124184855.\n\n### References\n\n[1] [scVelo documentation](https://scvelo.readthedocs.io/)\n\n[2] [SingleCellExperiment documetation](https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html)\n\n[3] [Orchestrating Single-Cell Analysis with Bioconductor - QC](http://bioconductor.org/books/3.14/OSCA.basic/quality-control.html)\n\n[4] [Scran documentation](http://bioconductor.org/packages/release/bioc/html/scran.html)\n\n[5] [Annotation Hub bioconductor page](https://bioconductor.org/packages/release/bioc/html/AnnotationHub.html)\n\n[6] [Seurat documentation](https://satijalab.org/seurat/)", "input": [{"name": "Output name prefix"}, {"name": "Genome name"}, {"name": "Ensembl version"}, {"name": "Metadata table"}, {"name": "Loom files"}, {"name": "Cell subset metadata table"}, {"name": "Clustering name"}, {"name": "Number of highly variable genes"}, {"name": "Minimal number of counts per cell"}, {"name": "Maximal number of counts per cell"}, {"name": "Minimal number of genes detected per cell"}, {"name": "Maximal number of genes detected per cell"}, {"name": "Maximum percentage of mitochondrial genes"}, {"name": "Maximum percentage of spike-in genes"}, {"name": "Quality control type"}, {"name": "Number of PCA components"}, {"name": "Perform pairwise differential expression"}, {"name": "Clustering resolution"}], "output": [{"name": "scVelo html report", "encodingFormat": "text/html"}, {"name": "Velocity overall top likelihood genes"}, {"name": "Velocity per cluster top likelihood genes"}, {"name": "Basic data processing report", "encodingFormat": "text/html"}, {"name": "Seurat object"}, {"name": "Cluster pairwise DE results"}, {"name": "Cluster marker genes"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/velocity-analysis-with-scvelo-0-2-4/2.png", "codeRepository": [], "applicationSubCategory": ["Single Cell"], "project": "SBG Public Data", "creator": "Dalibor Veljkovic", "softwareVersion": ["v1.2"], "dateModified": 1646834964, "dateCreated": 1646679783, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/single-cell-smart-seq2-workflow/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/single-cell-smart-seq2-workflow/8", "applicationCategory": "Workflow", "name": "Single-Cell Smart-seq2 Workflow v3.0.0", "description": "This workflow is designed by the [Data Coordination Platform](https://data.humancellatlas.org/about) of the [Human Cell Atlas](https://www.humancellatlas.org/) to process single-cell RNAseq (scRNA-seq) data generated by [Smart-seq2 assays](https://www.nature.com/articles/nmeth.2639) [1]. It enables processing of stranded or unstranded, paired-end or single-end, scRNA-seq data from individual cells.\nThe Smart-seq2 workflow is divided into two branches which run in parallel. The first branch aligns reads to the genome and performs quality control on genome-aligned BAMs. The second branch aligns reads to the transcriptome and generates gene expression estimates from transcriptome-aligned BAMs. The pipeline returns reads and expression estimates in BAM format, and read counts and QC metrics in table format. An optional Zarr format output is also available.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n###Common Use Cases\n - Process a single or multiple samples by providing single-end or paired-end sequencing reads in FASTQ format to the **Input reads** input port.\n - Provide HISAT2 transcriptome index file required to the **HISAT2 transcriptome index** input port.\n - Provide HISAT2 genome index file to the **HISAT2 genome index** input port.\n - Provide a genome reference file, required by the HISAT2 tool, to the **Genome reference** input port.\n - Provide the location of rRNA sequences in the genome, required by the Picard CollectRnaMetrics tool, to the **Picard rRNA intervals** input port.\n - Provide a file containing gene annotations in RefFlat format, required by Picard CollectMultipleMetrics tool, to the **Picard RefFlat** input port.\n - Provide an archive file containing indexed transcriptome reference required by RSEM Expression tool, to the **RSEM reference index** input port.\n\n###Changes Introduced by Seven Bridges\n - To support the processing of multiple samples, we introduced the **SBG Pair FASTQs by Metadata** tool that creates a [job](https://docs.sevenbridges.com/docs/review-task-page) for each sample. The scatter functionality of the Common Workflow Language allows multiple jobs to be processed in parallel on a single AWS instance.\n - We introduced the **SBG Create Basename** tool, which creates output and sample names based on the name of the FASTQ file(s).\n - We changed CPU and RAM requirements for HISAT2 and Picard tools to optimize task executions on the cloud infrastructure.\n - We disabled saving of the BAM files, metrics produced with Picard's tools and Zarr output files, by default. If required, these outputs can be saved by setting the appropriate options in the App Settings.\n\n###Common Issues and Important Notes\n - To build the reference index files required for this workflow please refer to the instructions given [here](https://github.com/HumanCellAtlas/skylab/blob/master/pipelines/smartseq2_single_sample/Creating_Smartseq2_References.md). Also, we have pre-generated index files for mouse and human genomes available on-demand. To be granted access to these files please write to our support team at [support@sevenbridges.com](support@sevenbridges.com).\n - Before processing files that contain paired-end sequencing reads, set the appropriate values in **Paired-end** and **Sample ID** metadata fields of the FASTQ files.\n - When processing a single sample you can set the **Output name** and **Sample name or Cell ID** options. If not set, these values will be created based on the **Sample ID** metadata field or the FASTQ filename. *Do not* set these values when processing multiple samples because all the output files of the same type will have the same name.\n\n###Performance Benchmarking\nThe entire workflow is running on a c5.9xlarge AWS instance having 72GB of RAM and 36 CPUs. The maximum number of parallel instances is set to 10 enabling the processing of more than 100 samples in parallel.\n\n| Experiment type |  Number of Samples | Paired-end |  Duration |  AWS Instance Cost (spot) | AWS Instance Cost (on-demand) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|\n|     Smart-Seq2     |  50 |     Yes    |   18min   | 1.67$ | 3.49$ | \n|     Smart-Seq2     |  100 |     No    |   23min   | 2.13$ | 4.52$ | \n|     Smart-Seq2     | 200 |     No    |   51min   | 4.27$ | 10.02$ | \n|     Smart-Seq2     | 500 |     No    |   1h42min   | 9.20$ | 22.01$ |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###API Python Implementation\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id = \"your_username/project\"\nworkflow_id = \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n    \"in_reads\": api.files.query(\n        project=project_id, \n        names=['SRR6259130.fastq', 'SRR6259129.fastq']),\n    \"in_transcriptome_index\": api.files.query(\n        project=project_id, \n        names=['GRCm38.primary_assembly.genome.transcripts.HISAT2-2.1.0.index_files.tar'])[0],\n    \"in_genome_index\": api.files.query(\n        project=project_id, \n        names=['GRCm38.primary_assembly.genome.HISAT2-2.1.0.index_files.tar'])[0],\n    \"in_rsem_index\": api.files.query(\n        project=project_id, \n        names=['GRCm38.primary_assembly.genome.gencode.vM21.primary_assembly.annotation.rsem-1.3.1.tar'])[0],\n    \"in_ref_flat\": api.files.query(\n        project=project_id, \n        names=['GRCm38.primary_assembly.refFlat.txt'])[0],\n    \"in_intervals\": api.files.query(\n        project=project_id, \n        names=['GRCm38.primary_assembly.rRNA.interval_list'])[0],\n    \"in_reference\": api.files.query(\n        project=project_id, \n        names=['GRCm38.primary_assembly.genome.fa'])[0],\n}\n\ntask = api.tasks.create(\n    name='Single-Cell Smart-Seq2 Workflow - API Example', \n    project=project_id, app=workflow_id, inputs=inputs, run=False)\n#For running a batch task\ntask = api.tasks.create(\n    name='Single-Cell Smart-Seq2 Workflow - API Batch Example', \n    project=project_id, app=workflow_id, inputs=inputs, run=False, \n    batch_input='in_reads', batch_by = { 'type': 'CRITERIA', 'criteria': [ 'metadata.sample_id'] })\n```\n\nInstructions for installing and configuring the API Python client, are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Smart-seq2 Single Sample Pipeline Overview](https://github.com/HumanCellAtlas/skylab/tree/master/pipelines/smartseq2_single_sample)", "input": [{"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "HISAT2 transcriptome index", "encodingFormat": "application/x-tar"}, {"name": "HISAT2 genome index", "encodingFormat": "application/x-tar"}, {"name": "RSEM reference index", "encodingFormat": "application/x-tar"}, {"name": "Picard RefFlat", "encodingFormat": "text/plain"}, {"name": "Picard rRNA intervals"}, {"name": "Genome reference", "encodingFormat": "application/x-fasta"}, {"name": "Strand specificity"}, {"name": "Save BAM/SAM files"}, {"name": "Save BAM/SAM files"}, {"name": "Output name prefix"}, {"name": "Output name prefix"}, {"name": "Create zarr output"}, {"name": "Sample name or Cell ID"}, {"name": "Output name"}], "output": [{"name": "HISAT2 RSEM output bam", "encodingFormat": "application/x-bam"}, {"name": "HISAT2 PairedEnd output bam", "encodingFormat": "application/x-bam"}, {"name": "Zarr outputs"}, {"name": "Group metrics"}, {"name": "RSEM isoform expression matrix"}, {"name": "RSEM gene expression matrix"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/single-cell-smart-seq2-workflow/8.png", "codeRepository": ["https://github.com/HumanCellAtlas/skylab/tree/master/pipelines/smartseq2_single_sample", "https://github.com/HumanCellAtlas/skylab/blob/master/pipelines/smartseq2_single_sample/SmartSeq2SingleSample.wdl", "https://github.com/HumanCellAtlas/skylab/releases"], "applicationSubCategory": ["Transcriptomics", "Single Cell", "CWL1.0"], "project": "SBG Public Data", "creator": "Data Coordination Platform of the Human Cell Atlas", "softwareVersion": ["v1.0"], "dateModified": 1591199143, "dateCreated": 1591198972, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/solvebio-integration/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/solvebio-integration/15", "applicationCategory": "Workflow", "name": "SolveBio Integration", "description": "The purpose of this workflow is to integrate our Seven Bridges Data Analysis Platform with SolveBio, via each system's API.\n\nThe workflow takes a variant file (VCF), which is annotated using SnpEff, the output of which is then converted into JSON format and compressed (JSON.GZ) before being pushed to SolveBio, which allows for functional annotations and clinical impact to be explored.\n\n***Required Inputs:***\n\n1. VCF file - Input variant file(s), must be in VCF format. Set to batch, user can run multiple files at once and create a task per file. [VCF].\n2. SnpEff Database - A zip archive that can be downloaded from the SnpEff official site, or using the SnpEff download app [ZIP].\n\n***Required Parameters:***\n\n1. Genome reference used for variant calling. It must also match the name of the SnpEff database file.\n2. Dataset - Name of the SolveBio dataset to append to or create. Should follow the formatting {Depository}/{Version}/{Dataset}. Can be existing or new.\n3. API Token - SolveBio API token (must create account at SolveBio to obtain).\n\n***Outputs:***\n\n1. SnpEff Summary Text - SnpEff summary file in text format [TXT].\n3. Indexed VCF - Tabix indexed archive [VCF.GZ].\n4. Push2SolveBio Summary HTML - Output file which links to the \"Import Activity\" and \"Data Vault\" URLs at SolveBio [B64HTML].", "input": [{"name": "SnpEff Database", "encodingFormat": "application/zip"}, {"name": "VCF File", "encodingFormat": "text/x-bed"}, {"name": "SolveBio API token"}, {"name": "Genome reference"}, {"name": "SolveBio dataset"}], "output": [{"name": "SnpEff Summary Text", "encodingFormat": "text/plain"}, {"name": "Push2SolveBio Summary HTML"}, {"name": "Indexed VCF", "encodingFormat": "application/x-vcf"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/solvebio-integration/15.png", "codeRepository": [], "applicationSubCategory": ["integration"], "project": "SBG Public Data", "creator": "Lucia Alvarado / Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1502804269, "dateCreated": 1479143675, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-filtering-v1-0-5-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-filtering-v1-0-5-0/6", "applicationCategory": "Workflow", "name": "SomaticSniper + Filtering", "description": "This workflow is implementation of the best practice filtering procedure suggested by the authors. More detail about the filtering procedure can be found on the Documentation link under the section \"Basic filtering with provided Perl scripts\". Provided scripts are ported to CGC and incorporated into this workflow. In order to run this workflow successfully it is necessary to provide indexed tumor BAM on the input.", "input": [{"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Normal BAM"}, {"name": "Tumor BAM index"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}], "output": [{"name": "hc_vcf", "encodingFormat": "application/x-vcf"}], "codeRepository": ["https://github.com/genome/somatic-sniper", "https://github.com/genome/somatic-sniper/archive/v1.0.5.0.tar.gz"], "applicationSubCategory": ["Variant Calling", "VCF Processing"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649345777, "dateCreated": 1453799683, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/st-pipeline-with-spotty/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/st-pipeline-with-spotty/8", "applicationCategory": "Workflow", "name": "Spatial Transcriptomics Pipeline with Spotty", "description": "The Spatial Transcriptomics Pipeline with Spotty is a workflow for processing raw \nsequence data (paired FASTQ-files) generated using the Spatial Transcriptomics \ntechnology. The pipeline produces a table of spatially distributed gene counts \nfor downstream analysis.\n\nThis workflow consists of two nested workflows:\n\n1. ST Pipeline which performs the demultiplexing and decoding of the RNA-Seq reads\n2. The Spotty workflow with post processing, perform automatic spots identification \n   and pairing between the fluorescent (Cy3) and tissue (HE) image. Spotty is \n   proprietary and available only on the Seven Bridges Platforms.\n\n### Common Use Cases\n\nSpatial Transcriptomics is a method that allows visualisation and quantitative \nanalysis of the transcriptome in individual tissue sections. By placing histological \nsections on glass slides with arrayed oligonucleotides containing positional \nbarcodes, it is possible to generate high quality cDNA libraries with precise \npositional information for RNA-Seq. This provides transcriptome data in a \nversatile format for bioinformatics analyses of gene expression within the tissue \ncontext, which will be valuable in both research and diagnostics [1].\n\nThrough integration of the Spatial Transcriptomics workflow on the Seven Bridges \nPlatform, together with our own proprietary Spotty, we provide an end-to-end \nsolution for the generation, processing, and interpretation of this novel and \npowerful data type.\n\nThe following steps are performed by our pipeline:\n\n- Quality trimming (read 1 and read 2) :\n    - Removal of low quality bases\n    - Check UMI quality\n    - Remove artifacts (PolyT, PolyA, PolyG, PolyN and PolyC) of user defined length\n    - Check for AT and GC content\n    - Discard reads with a minimum number of bases that failed any of the checks above\n- Contaminant filter e.x. rRNA genome (Optional)\n- Mapping with STAR (only read 2)\n- Demultiplexing with Taggd (only read 1)\n- Keep reads (read 2) that contain a valid barcode and are correctly mapped\n- Annotate the reads with htseq-count (slightly modified version)\n- Group annotated reads by barcode(spot position), gene and genomic location \n  (with an offset) to get a read count\n- In the grouping/counting only unique molecules (UMIs) are kept\n- The histological images and fluorescent images are processed by Spotty, an \n  automated image recognition algorithm that registers both images together and \n  allows association of the RNA-Seq results with each spot.\n\nThe main output of the pipeline is an ST data file (in tsv format) with the number \nof unique molecules detected for each gene at each spot coordinate on the array \nand an alignment matrix file. These two files, together with the images form a \ncomplete dataset that can be loaded into ST Viewer [2] for subsequent interactive \nanalysis.\n\nRequired input files/parameters:\n\n- FASTQ files (Read 1 containing the spatial information and the UMI and read 2 \n  containing the genomic sequence)\n- A genome index generated with STAR\n- An annotation file in GTF or GFF3 format\n- The slide batch number (a string referencing a file with the array coordinates \n  for the barcode sequences)\n- Fluorescent (Cy) and tissue (HE) images.\n\nOptional input parameters:\n\nThe ST Pipeline has been optimized for speed, robustness. The pipeline is easy \nto use with many optional parameters making it possible to adjust most the \nsettings if desired. Most of the optional parameters are related to trimming, \nmapping and annotation. Generally the default values are good enough.\n\n\n### Common Issues and Important Notes\n\n1) There are pre-made GRCh37 and GRCh38 STAR index files in our Public Reference Files or you can build your own using STAR Genome Generate from the Public Apps repository.\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n    \"fastq_fw\": api.files.query(project=project_id, names=['SRR3382373_1.fastq.gz'])[0],\n    \"fastq_rv\": api.files.query(project=project_id, names=['SRR3382373_2.fastq.gz'])[0],\n    \"ref_map\": api.files.query(project=project_id, names=['GRCm38.primary_assembly.genome_star-2.5.1b-index-archive.tar'])[0],\n    \"genes_annotation_gtf\": api.files.query(project=project_id, names=['gencode.vM11.annotation.gtf'])[0],\n    \"cy\": api.files.query(project=project_id, names=['MOB11_Cy.jpg'])[0],\n    \"he\": api.files.query(project=project_id, names=['MOB11_HE.jpg'])[0],\n    \"idFile\" : '1000L5'\n}\ntask = api.tasks.create(name='ST workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n1. [St\u00e5hl, P. L., Salm\u00e9n, F., Vickovic, S., Lundmark, A., Navarro, J. \nF., Magnusson, J., Fris\u00e9n, J. (2016). Visualization and analysis of gene \nexpression in tissue sections by spatial transcriptomics. Science, 353(6294), \n78\u201382.](https://doi.org/10.1126/science.aaf2403)\n2. [ST Viewer](https://github.com/SpatialTranscriptomicsResearch/st_viewer)", "input": [{"name": "He image"}, {"name": "Cy image"}, {"name": "FASTQ Reverse reads", "encodingFormat": "text/fastq"}, {"name": "FASTQ Forward reads", "encodingFormat": "text/fastq"}, {"name": "Reference genome fasta and STAR index", "encodingFormat": "application/x-tar"}, {"name": "LP Slide Batch Number"}, {"name": "Reference annotation", "encodingFormat": "application/x-gtf"}], "output": [{"name": "Spotty report"}, {"name": "Adjusted counts matrix"}, {"name": "Mapped filtered reads", "encodingFormat": "application/x-bam"}, {"name": "Raw counts"}, {"name": "Mapped reads", "encodingFormat": "application/x-bam"}, {"name": "Alignment matrix"}], "applicationSubCategory": ["RNA-Seq"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033582, "dateCreated": 1525105363, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sra-download-and-set-metadata-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sra-download-and-set-metadata-0/10", "applicationCategory": "Workflow", "name": "SRA Download and Set Metadata", "description": "### Description\n\n**SRA Download and Set Metadata** downloads metadata associated with SRA accession via SRA Run Info CGI, (on-demand instance) FASTQ files and sets corresponding metadata.\n\nSRA Run files do not contain any information about the metadata (sample information, etc.) linked to the data themselves. In order to access related metadata, either [Run Selector](https://www.ncbi.nlm.nih.gov/Traces/study/?go=home) or [Run Browser](https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=run_browser) are normally used. This workflow incorporates a tool that allows command line access to SRA metadata with the SRA Run Info CGI [1]. This tool downloads a file containing SRA accessions and corresponding metadata, downloads FASTQ files and sets the metadata. **SRA fasterq-dump** is used to download the data, and is highly optimised for in-cloud in parallel running. **Sample ID** and **Paired end** metadata fields are set automatically, while custom metadata fields are made based on the information in the metadata file.\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nThe **SBG Prepare metadata for SRA Download** tool is responsible for downloading the metadata file. The tool runs a simple wget command listed below, where *<query>* is replaced with the string listed in the **Accession (SRA Accession)** input. The input string can contain anything ranging from SRR, SRP, SRX accession, but can also contain a pre-built Entrez query containing any set of Entrez parameters. Users may refine a search using Entrez and then copy the query string to the input. Note that Entrez groups by Experiment accession, but the CGI does not. It is, therefore, to be expected that Run Info CGI will return a longer list of results than Entrez, but will still contain the same datasets [2]. \n\n`wget -O <file_name.csv> 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?\nsave=efetch&db=sra&rettype=runinfo&term=<query>'`\n\nAn output filename is generated from the input string if it contains no spaces and is shorter than 15 characters, otherwise a default name *metadata_info.csv* is used. By default the output file format is CSV.\n\nInput **Metadata file** specifies already downloaded metadata file from Run Selector or Run Browser, and if provided with such file the tool returns a list of accessions which are further processed by downstream tools in the workflow. Note that a metadata file needs to have a *Run* column which has the associated SRR accession.\n\nThe **SRA fasterq-dump** tool converts SRA data into FASTQ format while using temporary files and multi-threading to speed up the extraction, and is used in this workflow to download the FASTQ files listed in the metadata file.\n\n**NOTE: SRA toolkit requires an *interactive configuration* since version 2.10.1 [3]. Running this tool on the platform triggers the configuration *automatically*!  Please find more information in the *'Common Issues and Important Notes'* section.**\n\n###Changes introduced by Seven Bridges\n\n- In order to access even the public data, the tool needs to be configured [3].  The authors have provided the interactive solution to this problem, but since this solution is not perfect for environments such as Seven Bridges Platform and many other, further solutions are being discussed [4][5][6]. The current solution on the platform entails that the blank configuration file containing just the UUID (universally unique identifier) is created inside a specific folder in the root directory. This approach is taken from the authors' [Dockerfile](https://github.com/ncbi/sra-tools/blob/master/build/docker/Dockerfile).\n- **Due to security issues, *downloading controlled data is not recommended*. The following input ports have been omitted from the tool wrapper: NGC file (--ngc), Permission file (--perm) and Cart file (--cart_file).**\n\n\n### Common Issues and Important Notes\n\n- **SRA fasterq-dump v2.10.8** used in this workflow enables multithreaded download of FASTQ files which is modulated by the **Number of threads** (`--threads`) parameter. The default value for this is 4, as recommended in the manual, but can be modified if needed. Default instance for this workflow is c4.8xlarge which has 36 CPUs and allows 9 parallel SRA fastq-dump jobs. Increasing the parallel instance limit allows running more jobs in parallel, and is recommended when the metadata file contains a lot of SRR accessions.\n- Note that the **Split files** (`--split-files`) parameter is set to *True* by default.\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the number of bases in the SRA file. The following table shows pricing and duration for several use-cases. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n| BioProject Accession | Bases    | Size (SRA files) | Size (Total outputs) | Runs | Assay Type        | Library Layout | Used Instance(s) | Max. Parallel Instance Limit | Task duration | Price |\n|----------------------|----------|------------------|----------------------|------|-------------------|----------------|------------------|------------------------------|---------------|-------|\n| PRJNA19655           | 130.73 M | 308.28 Mb        | 300 Mb               | 2    | WGS               | SINGLE         | c4.8xlarge       | 1                            | 2 minutes     | $0.05 |\n| PRJNA646083          | 9.28 G   | 4.88 Gb          | 35.3 Gb              | 8    | RNA-Seq           | SINGLE         | c4.8xlarge       | 1                            | 13 minutes    | $0.34 |\n| PRJNA629289          | 17.87 G  | 15.14 Gb         | 83.6 Gb              | 42   | RNA-Seq           | SINGLE         | c4.8xlarge       | 1                            | 34 minutes    | $0.93 |\n| PRJDB8039            | 259.61 G | 101.38 Gb        | 796 Gb               | 29   | RNA-Seq, ChIP-Seq | PAIRED         | c4.8xlarge       | 1                            | 3h 35 minutes | $5.70 |\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\ninputs = {\n    \"sra_accession_1\": \"SRP259119\"\n}\ntask = api.tasks.create(name='SRA Download and Set Metadata - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\n### References\n\n[1] [SRA Handbook - Download Guide](https://www.ncbi.nlm.nih.gov/books/NBK242621/pdf/Bookshelf_NBK242621.pdf)\n\n[2] [NCBI help manual - Entrez help](https://www.ncbi.nlm.nih.gov/books/NBK3837/pdf/Bookshelf_NBK3837.pdf)\n\n[3] [Instructions for SRA toolkit installation and configuration](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration)\n\n[4] [SRA Github issue #282](https://github.com/ncbi/sra-tools/issues/282)\n\n[5] [SRA Github issue #291](https://github.com/ncbi/sra-tools/issues/291)\n\n[6] [SRA Github issue #310](https://github.com/ncbi/sra-tools/issues/310)", "input": [{"name": "SRA metadata file", "encodingFormat": "text/plain"}, {"name": "Accession (SRA accession)"}, {"name": "Split files"}, {"name": "Split spot"}, {"name": "Minimum read leangth"}, {"name": "Skip technical"}, {"name": "Inculde technical"}, {"name": "Split 3"}, {"name": "Number of threads"}, {"name": "Concatenate reads"}], "output": [{"name": "Metadata file"}, {"name": "Out reads", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/sra-download-and-set-metadata-0/10.png", "codeRepository": ["https://github.com/ncbi/ncbi-vdb"], "applicationSubCategory": ["Utilities", "FASTQ Processing"], "project": "SBG Public Data", "softwareVersion": ["v1.1", "v1.0"], "dateModified": 1648036775, "dateCreated": 1619084490, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/strelka2-somatic-workflow/19", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/strelka2-somatic-workflow/19", "applicationCategory": "Workflow", "name": "Strelka2 Somatic Workflow", "description": "**Strelka2 Somatic workflow** is a workflow made for analysis of somatic variation in tumor/normal sample pairs.\n\nThis workflow uses additional tools so that **Strelka2** variants are made more precise and informative. To achieve better variant calling **Manta** structural variant caller is added and its \"*Candidate small INDELs*\" file is provided to **Strelka2**. For ease of use, the workflow combines the SNVs and INDELs into a single VCF, which is then genotyped and indexed. Genotyping is performed by a custom tool, **SBG Genotyping Strelka**, which records genotyping information into the output VCF, based on the information in the INFO field of each variant.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n* Strelka2 accepts input read mappings from BAM or CRAM files, and optionally candidate and/or forced-call alleles from VCF. It works only with matched **Tumor-Normal** BAM (CRAM) files. Tumor-only mode isn't supported. [1]\n* For best somatic INDEL performance, Strelka2 Somatic is designed to be run with the Manta structural variant and INDEL caller, which provides additional INDEL candidates up to a given maximum INDEL size (by default this is 50). By design, Manta and Strelka2 Somatic run together with default settings to provide complete coverage over all INDEL sizes (in addition to all SVs and SNVs) for clinical somatic and germline analysis scenarios. [2]\n\n### Changes Introduced by Seven Bridges\n \n * SBG Strelka Genotyping tool is added for genotyping Strelka2 variants. Genotype will be added both for tumor and normal samples. \n * Strelka2 variant caller outputs compressed VCF files in VCF.GZ format. We added **SBG Decompress** tools for decompressing these outputs. \n * Since SNVs and INDELs are separated in two separate VCF files, we added GATK Merge VCFs for creating one unique VCF which will contain both Strelka2 SNVs and INDELs.\n * Output is renamed using the **SampleID** metadata field from input Tumor BAM file. Output with SNVs and INDELs will be named: *Sample_id.Strelka2.vcf*\n \n### Common Issues and Important Notes\n\n* **Strelka** requires a matched normal sample to make somatic calls. The matched normal is used to distinguish both germline variation and sequencing artifact from somatic variation. The general depth guideline for the normal sample is either one half the tumor depth or ~30x, whichever is higher.\n* BAM/CRAM files used as inputs should be sorted and indexed. This can be done by using **Samtools Sort** and **Samtools Index** apps available in the public apps gallery.\n* Alignments cannot contain the \"=\" character in the SEQ field.\n* RG (read group) tags are ignored - each alignment file must represent one sample.\n* Alignments with basecall quality values greater than 70 will trigger a runtime error (these are not supported on the assumption that the high basecall quality indicates an offset error)\n* Setting the **Type of Analysis** to \"WES\" turns off depth filters.\n\n### Performance Benchmarking\n\nDefault instance for this workflow has been set to AWS **c5.2xlarge**. For faster execution, but increased cost, consider running the workflow on a larger instance.\n\nIn the following table you can find estimates of **Strelka2 Somatic workflow** run times and costs. All samples are aligned against **human_g1k_v37_decoy reference** using an appropriate regions file (**exome_targets.b37.bed** for WES, and **human_g1k_v37_decoy.breakpoints.bed** for WGS samples)\n\n|Tumor File Size|Normal File Size|Strategy|Time|Cost|Instance|\n|--:|--:|:--:|--:|--:|:--|\n|129.5 MiB|218.5 MiB|WES|10 min|$0.06|c5.2xlarge|\n|4.5 GiB|5.3 GiB|WES|14 min|$0.08|c5.2xlarge|\n|159.1 GiB|143.6 GiB|WGS|4 h, 49 min|$1.64|c5.2xlarge|\n|129.5 MiB|218.5 MiB|WES|9 min|$0.10|c5.4xlarge|\n|4.5 GiB|5.3 GiB|WES|13 min|$0.15|c5.4xlarge|\n|159.1 GiB|143.6 GiB|WGS|2 h, 54 min|$1.97|c5.4xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\t \n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"type_of_analysis\": \"WGS\", \n\t\"in_normal_alignments\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_tumorl_alignments\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_reference_and_index\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"Strelka2 Somatic workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/).\n\n### References\n\n[1] [Strelka2 and Tumor-only Calling](https://github.com/Illumina/strelka/issues/98)\n\n[2] [Strelka2 GitHub](https://github.com/Illumina/strelka)", "input": [{"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Type of Analysis"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Normal BAM/CRAM file", "encodingFormat": "application/x-bam"}, {"name": "Tumor BAM/CRAM file", "encodingFormat": "application/x-bam"}], "output": [{"name": "Output variants", "encodingFormat": "application/x-vcf"}, {"name": "Internal VC statistics in XML"}, {"name": "Internal VC statistics"}], "softwareRequirements": ["MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/strelka2-somatic-workflow/19.png", "codeRepository": ["https://github.com/Illumina/strelka", "https://github.com/Illumina/strelka/tree/master/docs/userGuide", "https://github.com/Illumina/strelka/releases/tag/v2.9.10", "https://github.com/Illumina/strelka/tree/v2.9.x/docs/userGuide#somatic-configuration-example"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "pavle.marinkovic", "softwareVersion": ["v1.0"], "dateModified": 1606234337, "dateCreated": 1606234336, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/targeted-capture-analysis-bwa-gatk-2-3-9-lite/27", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/targeted-capture-analysis-bwa-gatk-2-3-9-lite/27", "applicationCategory": "Workflow", "name": "Targeted Capture Analysis - BWA + GATK 2.3.9-Lite", "description": "Targeted Capture Analyses detects variants in selected regions of a genome. The advantages of this method are lower cost and shorter turnover time than Whole Exome Sequencing (WES) and Whole Genome Sequencing (WGS). In addition, this method provides greater coverage warranting increased sensitivity and specificity, increased confidence in heterozygous calls, increased likelihood of detecting mosaicism or low-level heterogeneity in mitochondrial or oncological applications, and greater ability to interpret the findings in a clinical context since sequenced regions have an established role in the disease. (H L Rehm, S J Bale et al. ACMG clinical laboratory standards for next-generation sequencing, Genet Med. 2013 September ; 15(9): 733\u2013747. doi:10.1038/gim.2013.92.) \nThe pipeline is constructed following the Broad Institute best practice and utilizing Broad Institute's GATK tools. A separate step is undertaken to assess the quality of sequenced reads using Babraham Institute's tool FastQC. \nSequenced reads are aligned with the BWA tool after which duplicates are removed. The next step uses algorithms developed by the Broad Institute to improve alignment around indels followed by the re-evaluation of the qualities of sequenced bases. Generated SAM files are pooled together and joint variant calling is performed. The key difference between this pipeline and WES pipeline is the restriction to predetermined regions of genome (genes or parts of genes), as oppose to WES where intervals of interest include all genes. \nDetected variants are subjected to additional analysis resulting in refined, high quality set of identified variants (for more information on how variant calling is performed, please refer to the [Broad Institute's web site](https://www.broadinstitute.org/gatk/guide/topic?name=methods/)). \nThe pipeline utilizes human reference genome hg19 as well as several public databases (dbSNP v142 and database of known indels generated by Seven Bridges).", "input": [{"name": "#FASTQ"}, {"name": "Reference or TAR with BWA reference indices"}, {"name": "Target BED", "encodingFormat": "application/x-vcf"}, {"name": "Known SNPs", "encodingFormat": "application/x-vcf"}, {"name": "Known Indels", "encodingFormat": "application/x-vcf"}], "output": [{"name": "FastQC report", "encodingFormat": "text/html"}, {"name": "BaseRecalibrator plot PDF"}, {"name": "Alignment summary metrics", "encodingFormat": "text/plain"}, {"name": "Filtered VCF", "encodingFormat": "application/x-vcf"}, {"name": "Coverage BEDs", "encodingFormat": "text/x-bed"}, {"name": "Raw VCF", "encodingFormat": "application/x-vcf"}], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/targeted-capture-analysis-bwa-gatk-2-3-9-lite/27.png", "applicationSubCategory": ["Targeted-sequencing"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1597676808, "dateCreated": 1453798839, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sciclone-workflow/61", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sciclone-workflow/61", "applicationCategory": "Workflow", "name": "Tumor Heterogeneity SciClone-based workflow", "description": "The **Tumor Heterogeneity SciClone-based workflow** infers the heterogeneity of a tumor sample for WES or WGS data. \n\nA tumor usually consists of a founding clone and a number of subclones that are each characterized by different mutations that give rise to the heterogeneity of the tumor sample. Having proper insight into which mutations are specific to which subclone is important in designing effective treatment strategies.\n\nThe main approach with this workflow is based on clustering patterns of allele frequencies at somatic point mutations. The idea is that variant allele frequencies (VAFs) will group around 50% for any heterozygous somatic mutations  in copy-number neutral regions. Any other points clustering at regions with VAFs less than 50% would essentially represent subclones that arose later in the tumors evolution (because less reads aligning to these mutations indicate different cells sequenced in the bulk-sequencing experiment).\n\nThe approach with the variant allele frequencies is described in detail in the __sciClone__ paper, [1] and as a solution for this problem, __sciClone__ is used as the main tool in this Tumor heterogeneity workflow. __sciClone__ results are then further processed by tools such as __CloneEvol__ and __Fishplot__ to produce additional plots that describe the structure of the tumor. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n- The Tumor Heterogeneity workflow takes a VCF file obtained from any of the somatic callers as the main input (on the **VCF files** input). In order to increase the precision of the results, clustering should only be performed in copy-number neutral regions [1]. As such, copy-number calling results obtained from a copy number calling tool like **CNVkit** or **Control-FREEC**, can be provided to the workflow as an optional (but highly-recommended) input (on the **Copy number calls** input). The pairing of VCF samples and copy-number data is done by properly filling in the **Sample ID** metadata values.\n- The workflow will produce a set of results describing the heterogeneity of the tumor samples, including the information about the number of clusters and which cluster each mutation belongs to (**SciClone clusters**), different plots showing the clustering of mutations (**SciClone plots**) and tracking the the tumor's evolution (**ClonEvol plots** && **Fishplot plots**), as well as a report with more detailed information about the clustering process, like cluster mean information, the read depth threshold used, as well as the estimated tumor purity (**Tumor heterogeneity report**). \n- The workflow can work with multiple VCF files (multiple samples per one case, if tracking the evolution of a tumor, for example), but in that case accompanying BAM files for each sample should also be provided. In the case of only one sample, a BAM file is not required. The pairing of VCF samples and BAM files is done by properly filling in the **Sample ID** metadata values.\n- When working with multiple samples, it is usually of interest to preserve the chronological order of sequenced samples, so that the tumor's evolution can properly be tracked by downstream tools such as **ClonEvol** and **Fishplot**. To achieve this, the parameter **Sample Order** can be used by adding string values of **Sample IDs** (or VCF filenames) in their proper chronological order (if adding filenames, the sample IDs must be contained in their respective filenames). There should be as much as rows as there are samples/VCFs used in the **sciClone** analysis. If only one sample is being processed with **sciClone**, this input is not needed. If multiple sample are being processed, but this input is not provided (or the number of samples specified is not the as the number of input samples), the samples will be sorted alphabetically.\n\n### Changes Introduced by Seven Bridges\n\n- The **sciClone** tool will fail if not enough mutations are present for the clustering procedure. There is a parameter inside the tool which can control minimum read depth of the mutations used for clustering (**Minimum read depth** -\n default value is 80 in the workflow). To avoid failure the value for this parameter is determined in the workflow before **sciClone** itself by checking if enough mutations are present for clustering, and if not - the tool will iteratively lower down the minimum read depth by a fixed resolution (**Read depth resolution** parameter) until enough mutations are present. The determined value for the minimum read depth is then written to the output **Tumor heterogeneity report**. This value is calculated by the **SBG SciClone Parameters** tool, and is then passed into the **sciClone** tool. \n\n### Common Issues and Important Notes\n\n- Under the **sciClone** parameters, please specify the correct copy-number caller used to infer the input copy number data, if provided (use the **CNV caller** input for this).\n- **Fishplot** and **Clonevol** currently in rare occasions might not output any plots, if very few mutations are available during the **sciClone** clustering process.\n- A GTF file and a known cancer database (like COSMIC) are optional inputs, as they are used for generating additional plots by the **ClonEvol tool**.\n- If providing multiple file types to the workflow (VCF, BAM, CNV files), make sure that the naming of chromosomes is compatibles across the files (i.e. >1, >2, ... or >chr1, >chr2, ...). \n- VCF files usually have different formats inside of them. Therefore, instead of the user specifying the type of VCF file they use, the workflow will try and recognize the variant caller from which the VCF came. If the caller is not recognized, and unknown caller error will be returned. The current list of supported callers includes: Mutect1, Mutect2, Varscan, Vardict, Sentieon TNsnv, Muse, Strelka1, Strelka2, GATK Haplotype Caller, Sentien Haplotyper, Sentieon TNscope, EBcall.\n- Similar goes for CNV files - the workflow will automatically try to infer from which CNV caller your CNV results came, and properly parse so that sciClone has no issues in successfully using the file. The currently supported list of CNV callers includes: CNVkit, ControlFREEC, PureCN, Facets, Sequenza, ICR96, GATK, SBG-Conseca.  \n\n### Performance Benchmarking\n\nThe workflow is not computationally challenging, being that it starts from VCF files, so all tasks finish successfully on the default instance, which is c4.2xlarge (AWS), with no problems, and pretty much in the same time range (6-7 minutes), and costing around $0.10 using on-demand instances. \n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'copyNumberCalls': list(api.files.query(project=project_id, names=['copy_number_results.txt'])),\n        'input_files': list(api.files.query(project=project_id, names=['somatic_mutations.vcf'])),\n        'cnv_caller': 'CNVkit'\n        }\n\n# Run the task\ntask = api.tasks.create(name='TH workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [sciClone paper](http://journals.plos.org/ploscompbiol/article/asset?id=10.1371%2Fjournal.pcbi.1003665.PDF)", "input": [{"name": "VCF files", "encodingFormat": "application/x-vcf"}, {"name": "Tumor BAM files", "encodingFormat": "application/x-bam"}, {"name": "Copy number calls", "encodingFormat": "text/plain"}, {"name": "GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Known cancer genes", "encodingFormat": "text/plain"}, {"name": "Project ID"}, {"name": "copyNumberMargins"}, {"name": "useSexChrs"}, {"name": "Output naming"}, {"name": "VAF threshold"}, {"name": "VCF tumor column name"}, {"name": "Sample order"}, {"name": "Copy numbers in log2 format"}, {"name": "Cluster method"}, {"name": "Report name"}, {"name": "Report format"}, {"name": "Add or remove chromosome tag"}], "output": [{"name": "SciClone plots"}, {"name": "Cluster summary", "encodingFormat": "text/plain"}, {"name": "Tumor heterogeneity report", "encodingFormat": "text/plain"}, {"name": "SciClone clusters", "encodingFormat": "text/plain"}, {"name": "ClonEvol plots"}, {"name": "Fishplot plots"}, {"name": "Tumor purity", "encodingFormat": "text/plain"}, {"name": "Clonal Evolution Models", "encodingFormat": "text/plain"}, {"name": "Tumor Heterogeneity Results", "encodingFormat": "application/zip"}], "codeRepository": ["https://github.com/genome/sciclone", "https://github.com/hdng/clonevol", "https://github.com/chrisamiller/fishplot"], "applicationSubCategory": ["Tumor-Heterogeneity"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1622023956, "dateCreated": 1512152756, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vardict-single-sample-calling/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vardict-single-sample-calling/9", "applicationCategory": "Workflow", "name": "Vardict Single Sample Calling", "description": "**Vardict 1.5.1 Single Sample Calling** workflow is based on the **VarDict 1.5.1 Java** tool and the utility scripts used for paired (tumor normal) or single-sample variant calling. This workflow is used or germline analysis or for somatic mutation calling when the normal sample is not available.\n\nThe analysis consists of 3 steps:\n\n#### 1. Create Tabular output\n* **VarDict 1.5.1 Java** - Paired Sample Calling\n    * VarDict is a sensitive variant caller from BAM files. It implements several novel features such as amplicon bias aware variant calling from targeted sequencing experiments, the rescue of long indels by realigning BWA soft clipped reads, and better scalability than many other Java-based variant callers. The Java port is around ten times faster than the original Perl implementation. [1]\n#### 2. Test each candidate\n* **VarDict 1.5.1 TestSomatic**\n    * In this step, a Fisher Exact Test is performed in order to determine strand bias and to check if a variant is truly a somatic variant.\n#### 3. Cast to VCF\n* **VarDict 1.5.1 Var2VCF Somatic**\n    * The program converts the variant output from Test Somatic tool into a validated VCF file. In addition, it has a number of filtering options.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThis workflow is used for somatic mutation calling when the normal sample is **not** available. This workflow can also be used for analyzing germline samples if the use case requires it.\n\n### Changes Introduced by Seven Bridges\n\n* Input **BED file** is split into manageable intervals using the SBG Tool BED Splitter\n* Output VCF is naturally reordered by using the SBG Tool VCF Reorder.\n\n### Common Issues and Important Notes\n\n* Input BAM file on the **Normal BAM** input should be indexed (e.g. with the Sambamba Index tool)\n* **Reference FASTA** file should have a .fai index (created with e.g. SBG FASTA Indices tool)\n* It is required to either set a **BED file** as an input **or** to set a **Region of interest** parameter\n* The main tool of the workflow defaults to 8 cores and 15 GB of RAM to utilize all the resources of the default instance, c4.2xlarge. In the case that this workflow is used on a larger instance, please adjust the number of cores and the amount of RAM.\n* For **Whole Genome** data please use a BED file with regions accessible for sequencing (e.g. human_*_breakpoints.bed from Public Reference Files section). The intervals will be divided into smaller intervals in order to increase the computational performance.\n* **VarDict Java** tool silently freezes and causes indefinite computation. Common reasons include malformed or unindexed files. If the computation lasts unusually long, consider checking the inputs.\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Performance Benchmarking\n\nWorkflow requires 1.5-2 GB of RAM per CPU in order to work properly. The bottleneck tool is VarDict 1.5.1 Java.\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference index** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n| Input Size (GB) | Duration (min) | Cores | Cost ($) | Instance (AWS)  |\n|-----------------|----------------|-------|----------|------------|\n| 0.2             | 8              | 8     | 0.05     | c4.2xlarge |\n| 6.0             | 14             | 8     | 0.09     | c4.2xlarge |\n| 10.9            | 21             | 8     | 0.14     | c4.2xlarge |\n| 6.0             | 16             | 32    | 0.42     | c4.8xlarge |\n| 10.9            | 24             | 32    | 0.63     | c4.8xlarge |\n| 143.6           | 55             | 8     | 0.36     | c4.2xlarge |\n| 143.6           | 57             | 32    | 1.51     | c4.8xlarge |\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"tumor_bam\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"reference_fasta\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"bed_file\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n}\ntask = api.tasks.create(name='Vardict 1.5.1 Single Sample Calling - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n[1] [VarDict Github](https://github.com/AstraZeneca-NGS/VarDict)", "input": [{"name": "Sample Name"}, {"name": "BED File", "encodingFormat": "text/x-bed"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Input BAM", "encodingFormat": "application/x-bam"}, {"name": "CPU per job"}, {"name": "Size of one interval"}, {"name": "Pass-through"}, {"name": "Overlap between sequences in splited region"}, {"name": "Remove duplicate reads"}, {"name": "Phred score for a base to be considered good"}, {"name": "Indicates whether to perform local realignment"}, {"name": "Number of mismatches allowed"}, {"name": "Minimum variant reads"}, {"name": "Minimum Mean MapQ"}, {"name": "Minimum individual MapQ"}, {"name": "Minimum allele frequency"}, {"name": "Memory per job in megabytes"}, {"name": "Memory overhead per job"}, {"name": "Hexical annotation (0x...) to filter reads using samtools"}, {"name": "Amplicon based calling"}, {"name": "Memory for job in megabytes"}, {"name": "Number of instance CPUs to use"}, {"name": "Minimum signal to noise ratio"}, {"name": "Output only variants that passed the filters"}, {"name": "Minimum total depth"}, {"name": "The minimum mean position of variants in the read"}, {"name": "The minimum mean base quality"}, {"name": "Minimum mapping quality"}, {"name": "Minimum high quality variant depth"}, {"name": "Minimum allele frequency"}, {"name": "Minimum allele frequency for homozygous"}, {"name": "Memory for job in megabytes"}, {"name": "The maximum non-monomer MSI allowed for a HT variant with AF < 0.5"}, {"name": "Maximum mismatches allowed"}, {"name": "Filter PSTD"}, {"name": "Filter close SNVs"}, {"name": "Do not print END tag"}, {"name": "Number of instance CPUs to use per job"}, {"name": "Amplicon filtering"}, {"name": "Split mode"}], "output": [{"name": "Reordered VCF", "encodingFormat": "application/x-vcf"}], "codeRepository": ["https://github.com/AstraZeneca-NGS/VarDictJava"], "applicationSubCategory": ["Variant-Calling"], "project": "SBG Public Data", "creator": "AstraZeneca", "softwareVersion": ["sbg:draft-2"], "dateModified": 1584652202, "dateCreated": 1519853380, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vardict-somatic-calling-wf/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vardict-somatic-calling-wf/8", "applicationCategory": "Workflow", "name": "Vardict Somatic Calling", "description": "**Vardict 1.5.1 Somatic Calling** workflow is based on **VarDict 1.5.1 Java** tool and the utility scripts used for paired (tumor normal) or single-sample variant calling. This workflow is used for somatic mutation calling when the normal sample is available.\n\nThe analysis consists of 3 steps:\n\n#### 1. Create Tabular output\n* **VarDict 1.5.1 Java** - Paired Sample Calling\n    * VarDict is a sensitive variant caller from BAM files. It implements several novel features such as amplicon bias aware variant calling from targeted sequencing experiments, the rescue of long indels by realigning BWA soft clipped reads, and better scalability than many other Java-based variant callers. The Java port is around ten times faster than the original Perl implementation. [1]\n#### 2. Test each candidate\n* **VarDict 1.5.1 TestSomatic**\n    * In this step a Fisher Exact Test is performed in order to determine strand bias and check if a variant is truly a somatic variant.\n#### 3. Cast to VCF\n* **VarDict 1.5.1 Var2VCF Somatic**\n    * The program will convert the variant output from Test Somatic tool into a validated VCF file. In addition, it has a number of filtering options.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThis workflow is used for somatic mutation calling when the normal sample is available.\n\n### Changes Introduced by Seven Bridges\n\n* Input **BED file** is split into manageable intervals using the SBG Tool BED Splitter\n* Output VCF is naturally reordered by using the SBG Tool VCF Reorder\n* VCF subsets according to the STATUS tag are obtained with SBG Tool VarDict Split Variants\n\n### Common Issues and Important Notes\n\n* **When analyzing BAM files aligned to the HG38 assembly with no intention of doing analysis on the alternative contigs a user should use a BED file with primary assembly only. If the full BED file is used VarDict will run very slow on contigs which are not part of the primary assembly.**\n* Input BAM files on **Normal BAM** and **Tumor BAM** inputs should be indexed (e.g. with Sambamba Index tool)\n* **Reference FASTA** file should have a .fai index (created, for example, with the SBG FASTA Indices tool)\n* It is required to either set a **BED file** as an input **or** to set a **Region of interest** parameter\n* The main tool of the workflow defaults to 8 cores and 15 GB of RAM to utilize all the resources of the default instance c4.2xlarge. In the case that this workflow is used on a larger instance, please adjust the number of cores and the amount of RAM.\n* For **Whole Genome** data please use a BED file with regions accessible for sequencing (e.g. human_*_breakpoints.bed from Public Reference Files section). The intervals will be divided to smaller intervals in order to increase the computational performance.\n* **VarDict Java** tool silently freezes and causes indefinite computation. Common reasons include malformed or unindexed files. If the computation lasts unusually long, consider checking the inputs.\n\n### Performance Benchmarking\n\nWorkflow requires 1.5-2 GB of RAM per CPU in order to work properly. The bottleneck tool is VarDict 1.5.1 Java.\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference index** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n| Tumor + Normal Size (GB) | Duration (min) | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 12.6                     | 26             | 8     | 0.17     | c4.2xlarge |\n| 17.5                     | 34             | 8     | 0.23     | c4.2xlarge |\n| 12.6                     | 22             | 32    | 0.58     | c4.8xlarge |\n| 17.5                     | 24             | 32    | 0.72     | c4.8xlarge |\n| 302.7                    | 705            | 8     | 5.74     | c4.2xlarge |\n| 302.7                    | 705            | 32    | 18.22    | c4.8xlarge |\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"tumor_bam\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"reference_fasta\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"normal_bam\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n}\ntask = api.tasks.create(name='Vardict 1.5.1 Somatic Calling - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n[1] [VarDict Github](https://github.com/AstraZeneca-NGS/VarDict)", "input": [{"name": "Minimum Allele Frequency"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Minimum Mean Mapq"}, {"name": "Normal BAM", "encodingFormat": "application/x-bam"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Minimum Variant Reads"}, {"name": "Tumor Sample Name"}, {"name": "Remove filtered variants"}, {"name": "Output only somatic"}, {"name": "Normal sample name shown in VCF"}, {"name": "Minimum total depth"}, {"name": "Minimum signal to noise ratio"}, {"name": "Minimum mean position of variants in the read"}, {"name": "Minimum mean base quality"}, {"name": "Minimum allele frequency to consider to be homozygous"}, {"name": "Minimum allele frequency difference between two samples"}, {"name": "Maximum P-value"}, {"name": "Maximum non-monomer MSI allowed for a HT variant with AF < 0.6"}, {"name": "Maximum mean mismatches allowed"}, {"name": "Filter two somatic candidates withing this number of BP"}, {"name": "Chromosome names 1, 2, 3, ..."}, {"name": "Size of one interval"}, {"name": "Pass-through"}, {"name": "Overlap between sequences in splited region"}, {"name": "BED zero based or not"}, {"name": "Trim bases after this number"}, {"name": "Minimum number of reads for strand bias"}, {"name": "Remove duplicate reads"}, {"name": "Regular expression for acquiring samplet"}, {"name": "Region of interest"}, {"name": "BAM/SAM reading strictness"}, {"name": "Read position filter"}, {"name": "Qratio"}, {"name": "Phred score for a base to be considered good"}, {"name": "Indicates whether to perform local realignment"}, {"name": "Output splicing read counts"}, {"name": "Number of nucleotide to extend for each segment"}, {"name": "Number of CPU threads"}, {"name": "Move indels to 3-prime"}, {"name": "Number of mismatches allowed"}, {"name": "Minimum matches for a read to be considered"}, {"name": "Minimum individual MapQ"}, {"name": "Lowest frequency in normal sample"}, {"name": "Indel size"}, {"name": "Hexical annotation (0x...) to filter reads using samtools"}, {"name": "Extension of BP to look for mismatches"}, {"name": "Downsampling fraction"}, {"name": "Do pileup regardless of the frequency"}, {"name": "Delimeter used in BED file"}, {"name": "Debug mode"}, {"name": "Column number for segment starts in output"}, {"name": "Column number for segment ends in output"}, {"name": "Column number for region start in output"}, {"name": "Column number for region end in output"}, {"name": "Column number for gene name in output"}, {"name": "Column number for chromosome in output"}, {"name": "Amplicon based calling"}], "output": [{"name": "Reordered Full VCF", "encodingFormat": "application/x-vcf"}, {"name": "unclassified", "encodingFormat": "application/x-vcf"}, {"name": "loh", "encodingFormat": "application/x-vcf"}, {"name": "somatic", "encodingFormat": "application/x-vcf"}, {"name": "germline", "encodingFormat": "application/x-vcf"}], "codeRepository": ["https://github.com/AstraZeneca-NGS/VarDictJava"], "applicationSubCategory": ["Variant-Calling"], "project": "SBG Public Data", "creator": "AstraZeneca", "softwareVersion": ["sbg:draft-2"], "dateModified": 1562860376, "dateCreated": 1519853110, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-workflow-from-bam-v2-3-9/36", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-workflow-from-bam-v2-3-9/36", "applicationCategory": "Workflow", "name": "VarScan2 Workflow from BAM", "description": "VarScan workflow is extended methodology suggested by the authors, where user can start analysis from BAM files for tumor and normal samples. \nFirst step in the workflow is to prepare pileups for the both samples. For this purpose, parallel realization of the SAMtools mpileup tool is used, where BAM files are splitted into smaller files using the intervals provided in the FAI file. It then runs the mpileup on each file in parallel. \n\nAfterwards, the VarScan Somatic is used. This tool accepts pileups for tumor and normal samples and calls for indels and SNPs in these files. Scatter option is enabled in this tool, so each tumor pileup file with its interval corresponding normal pileup file resulted from the previous step will be considered as single job.\n\nNext downstream tool is VarScan SomaticFilter which is also designed to perform analysis per chromosome. So, indels and SNP's files from the previous step are paired according to the chromosome they are located in. Simple filtering is then done to remove SNPs around INDELs (refer to publication for more details). \n\nClassification of mutations using VarsScan2 ProcessSomatic tool is done for all mutations (SNPs and INDELs). Mutations are segregated into three groups: germline, LOH (loss of heterozygosity), and somatic. Each of these groups will have two files on the tool output: one contains all variants from this group; the second is the subset of these variants that is considered high confidence. The output format of the tool is VCF by default, where only high confidence (HC) mutations are taken into consideration for the further downstream analysis.  \n\nSince scattering of the pipeline is done per chromosome, it is necessary to concat resulting VCF files, sort and reorder them. This is done using VCFtool Concat and Sort, while the reordering is performed by a custom Seven Bridges tool SBG Reorder VCF.\n\n##### Required inputs:\n1. A Reference FASTA file (ID: *input_fasta_file*) - The reference file in the FASTA format.\n2. Input tumor BAM file (ID: *Tumor_bam_file*) \n2. Input normal BAM file (ID: *Normal_bam_file*) \n\n\n##### Outputs:\nTwo VCF files are outputted from this pipeline.\n1.  Somatic INDELs (ID: *High_Confidence_INDELs*)\n2. Somatic SNPs (ID: *High_Confidence_SNPs*)\n\n##### Common issues:\n1. BAM files should be coordinates sorted.\n2. The input FASTA file should be the same as the one used for mapping the input tumor/normal BAM files otherwise the pipeline return an error like \"Inputs are null for parallelization\".\n3. BAQ calculations were disable as the authors of Varscan proposed.\n4.Please notice that you should set the instance according to the input BAM files. Instance should have enough memory to store the intermediate pileup files for the input BAMs.\n5. zero-coverage lines of pileup files are filtered, this is important to avoid any varscan failures.", "input": [{"name": "Input_FASTA_file", "encodingFormat": "application/x-fasta"}, {"name": "#Tumor_BAM", "encodingFormat": "application/x-bam"}, {"name": "#Normal_BAM", "encodingFormat": "application/x-bam"}], "output": [{"name": "#High_Confidence_SNPs", "encodingFormat": "application/x-vcf"}, {"name": "#High_Confidence_INDELs", "encodingFormat": "application/x-vcf"}], "codeRepository": ["https://github.com/dkoboldt/varscan"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799076, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcf-to-gds/21", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcf-to-gds/21", "applicationCategory": "Workflow", "name": "VCF to GDS converter", "description": "**VCF to GDS** workflow converts VCF or BCF files into Genomic Data Structure (GDS) format. GDS files are required by all workflows utilizing the GENESIS or SNPRelate R packages.\n\nStep 1 (*VCF to GDS*) converts  VCF or BCF files (one per chromosome) into GDS files, with option to keep a subset of **Format** fields (by default only *GT* field). (BCF files may be used instead of  VCF.) \n\nStep 2 (Unique Variant IDs) ensures that each variant has a unique integer ID across the genome. \n\nStep 3 (Check GDS) ensures that no important information is lost during conversion. If Check GDS fails, it is likely that there was an issue during the conversion.\n**Important note:** This step can be time consuming and therefore expensive. Also, failure of this tool is rare. For that reason we allows this step to be optional and it's turned off by default. To turn it on check yes at *check GDS* port. For information on differences in execution time and cost of the same task with and without check GDS  please refer to Benchmarking section.  \n\n### Common use cases\nThis workflow is used for converting VCF files to GDS files.\n\n### Common issues and important notes\n* This pipeline expects that input **Variant files** are separated to be per chromosome and that files are properly named.  It is expected that chromosome is included in the file name in following format:  chr## , where ## is the name of the chromosome (1-24 or X, Y). Chromosome can be included at any part of the filename. Inputs can be in vcf, vcf.gz and bcf format.  Examples: Data_subset_chr1.vcf, Data_subset_chr1.vcf.gz, Data_chr1_subset.vcf, Data_subset_chr1.bcf. \n\n\n\n* **Number of CPUs** parameter should only be used when working with VCF files. The workflow is unable to utilize more than one thread when working with BCF files, and will fail if number of threads is set for BCF conversion.\n\n* **Note:** Variant IDs in output workflow might be different than expected. Unique variants are assigned for one chromosome at a time, in ascending, natural order (1,2,..,24 or X,Y). Variant IDs are integer IDs unique to your data and do not map to *rsIDs* or any other standard identifier. Be sure to use *variant_id* file for down the line workflows generated based on GDS files created by this workflow.\n\n* **Note** This workflow has not been tested on datasets containing more than 62k samples. Since *check_gds* step is very both ram and storage memory demanding, increasing sample count might lead to task failure. In case of any task failure, feel free to contact our support.\n\n* **Note** **Memory GB** should be set when working with larger number of samples (more than 10k). During benchmarking, 4GB of memory were enough when working with 50k samples. This parameter is related to *VCF to GDS* step, different amount of memory is used in other steps.\n\n\n### Changes introduced by Seven Bridges\nFinal step of the workflow is writing checking status to stderr, and therefore it is stored in *job_err.log*, available in the platform *task stats and logs page*. If workflow completes successfully, it means that validation has passed, if workflow fails on *check_gds* step, it means that validation failed.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. \n      \n\n| Sample Count | Total sample size (GB) | Duration  | Cost - spot ($) |  Instance (AWS)  |\n|-------------------|-------------------|------------------|----------|-------------|------------|------------|\n| 1k samples | 0.2                    | 6m                 | 0.34 |  1x c5.18xlarge |\n| 50k simulated samples (VCF.GZ) | 200        | 1d4h                 | 134     |   4x c5.18xlarge |\n| 62k real samples (BCF) | 400                    | 2d11h                 | 139     | 1x c5.18xlarge |\n\n\n\n*Cost shown here were obtained with **spot instances** enabled. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*      \n\n**Note** Both at 50k samples, and 62k samples, termination of spot instance occurred, leading to higher duration and final cost. These results are not removed from benchmark as this behavior is usual and expected, and should be taken into account when using spot instances.\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://f4c.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"input_gds_files\": api.files.query(project=project_id, names=[\"basename_chr1.gds\", \"basename_chr2.gds\", ..]),\n    \"phenotype_file\": api.files.query(project=project_id, names=[\"name_of_phenotype_file\"])[0],\n    \"null_model_file\": api.files.query(project=project_id, names=[\"name_of_null_model_file\"])[0]\n}\ntask = api.tasks.create(name='Single Variant Association Testing - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).", "input": [{"name": "Variants Files", "encodingFormat": "application/x-vcf"}, {"name": "memory GB"}, {"name": "Format"}, {"name": "Number of CPUs"}, {"name": "check GDS"}], "output": [{"name": "Unique variant ID corrected GDS files per chromosome"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/UW-GAC/analysis_pipeline"], "applicationSubCategory": ["GWAS", "File Format Conversion"], "project": "SBG Public Data", "creator": "TOPMed DCC", "softwareVersion": ["v1.2"], "dateModified": 1649763750, "dateCreated": 1602755593, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/annotation-vep-workflow-1/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/annotation-vep-workflow-1/15", "applicationCategory": "Workflow", "name": "VEP annotation workflow", "description": "The **VEP annotation** workflow is used for preprocessing, annotating, and filtering VCF files. \n\nVCF or VCF.GZ files can be processed using this workflow. The first step in this pipeline is an optional VCF normalization and decomposition performed using the vt toolkit. To minimize processing time, the input file is streamed to vt normalize and vt decompose, which are invoked as piped processes. Preprocessing with vt requires you to supply a FASTA file with the input VCF. The final step of preprocessing is bgzip compression and tabix indexing, which is set to occur even if the vt preprocessing of the input file is skipped. Input files are sorted (vt sort, sorting mode local or full, depending on the absence/presence of contigs in the input VCF header) before preprocessing and vt uniq is used to drop any potentially duplicated variant entries after decomposition. \n\nThe input file is annotated using **Variant Effect Predictor**. Annotations are added from VEP cache, plugins, or custom annotation sources (VCF, BED, GFF, GTF, BW, etc.). If you are using custom annotation sources, all associated parameters should be provided separately in the order of the provided input files.\n\nThe third step of the pipeline is the optional filtering of the annotated VCF file. This step uses the filter_vep script from the ensembl-vep toolkit (**Filter VEP**), which has been wrapped to allow pass-through (if no filters are selected) and with a number of convenient filters alongside the standard filter_vep syntax. All filtering nodes use GNU parallel and splitting/joining VCFs on chromosomes to speed up filtering.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThe **VEP annotation** workflow can be used to preprocess (vt normalize, vt decompose, tabix), annotate (**Variant Effect Predictor**), and filter (**Filter VEP**) VCF files.\n\nRunning this workflow on the Seven Bridges Platform requires using a VEP cache file. VEP cache files can be obtained from our Public Reference Files section (homo_sapiens_vep_90_GRCh37.tar.gz and homo_sapiens_vep_90_GRCh38.tar.gz) or imported as files to your project from the [Ensembl ftp site](ftp://ftp.ensembl.org/pub/current_variation/VEP/) using the [FTP/HTTP import](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server) feature.\n\n### Changes Introduced by Seven Bridges\n\n- Input **VCF preprocessing** is a convenient master toggle flag, controlling VCF preprocessing. If **VCF preprocessing** is set to True, inputs **Decompose multiallelic variants** and **VCF normalization** can be used to specify preprocessing steps. Otherwise, these inputs are ignored.\n- Input **Reference FASTA file** is not required for running the workflow but is **highly recommended** as it is required for optional VCF normalization during preprocessing and enables additional annotations. Please note that **Reference FASTA file** is required for obtaining some annotations, including HGVS identifiers (`--hgvs`).\n- When using custom annotation sources (`--custom` flag) input files and parameters are specified separately and both must be provided to run the tool (inputs **Custom annotation sources** and **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**). Additionally, separate inputs have been provided for BigWig custom annotation sources and parameters, as these files do not require indexing before use (inputs **Custom annotation - BigWig sources only** and **Annotation parameters for custom BigWig annotation sources only**). Tabix TBI indices are required for other custom annotation sources.\n- Additional boolean flags are introduced to activate the use of **Variant Effect Predictor** plugins (CSN, MaxEntScan, and LoFtool plugins can be accessed with parameters **Use CSN plugin**, **Use MaxEntScan plugin**, and **Use LoFtool plugin**, respectively).\n- Filtering node performance was improved by splitting the input VCF file on chromosomes and running concurrent GNU parallel-managed **Filter VEP** processes. All filters are optional - inputs **Run filtering** and **Custom filtering sets** are convenience toggle flags controlling filtering of input VCF file and allowing pass-through mode of filtering tools, if set to False.\n- For details about changes made to individual tools in the workflow, please see tool descriptions.\n\n### Common Issues and Important Notes\n\n* The workflow is intended for processing VCF files. Even though **Variant Effect Predictor** can output TSV and JSON files, please note that only VCF output is supported in the current workflow setup.\n* Inputs **Input VCF** (`--input_file`) and **Species cache** files are required. They represent a variant file containing variants to be annotated and a database cache file used for annotating the most common variants found in the particular species, respectively.    \n* Input **Reference FASTA file** is not required for running the workflow but is **highly recommended** as it is required for optional VCF normalization during preprocessing and enables additional annotations. Please note that **Reference FASTA file** is required for obtaining some annotations, including HGVS identifiers (`--hgvs`).\n* Please see flag descriptions or official documentation [1-3] for detailed descriptions of tool options and limitations.\n* The **Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)** (`--af_exac` or `--af_gnomAD`) VEP parameter: Please note that ExAC data has been superseded by gnomAD data and is only accessible with older (<90) cache versions. The Seven Bridges version of the tool will automatically swap flags according to the cache version reported.\n* The **Include Ensembl identifiers when using RefSeq and merged caches** (`--all_refseq`) and **Exclude predicted transcripts when using RefSeq or merged cache** (`--exclude_predicted`) parameters should only be used with RefSeq or merged caches\n* The **Add APPRIS identifiers** (`--appris`) parameter - APPRIS is only available for GRCh38.\n* The **Samples to annotate** (`--individual`) parameter requires that all samples of interest have proper genotype entries for all variant rows in the file. **Variant Effect Predictor** will not output multiple variant rows per sample if genotypes are missing in those rows.\n* A preprocessed dbNSFP file (input **dbNSFP database file**) and dbNSFP column names (parameter **Columns of dbNSFP to report**) should be provided to access dbNSFP annotations. dbNSFP column names should match the release of dbNSFP provided for annotation (for detailed list of column names, please consult the [readme files accompanying the dbNSFP release](https://sites.google.com/site/jpopgen/dbNSFP) used for annotation). If no dbNSFP column names are provided alongside a dbNSFP annotation file, the following example subset of columns applicable to dbNSFP versions 2.9.3 and 3.Xc will be used for annotation: `FATHMM_pred,MetaSVM_pred,GERP++_RS`. Likewise, if using the dbscSNV plugin, a dbscSNV file (input **dbscSNV database file**) should be provided.\n* The **Version of VEP cache if not default** parameter (`--cache_version`) must be supplied if not using a VEP 90 cache.\n* Forking and increasing the number of CPUs available will improve annotation performance, but users interested in only a few annotations and concerned about speed may wish to consider directly using a tool like **SnpEff**, **vcfanno** or **Variant Effect Predictor**.\n* **Filter VEP** used in filtering nodes implies that all separate filters (`--filter` flag) used are chained with a logical AND operator, which should be kept in mind when designing filters to use.\n* If **Run filtering** parameter is set to True on the optional filtering tools, at least one filter *must* be defined. Running the workflow with no filters and **Run filtering** enabled will cause the tasks to fail.\n* As convenient filters, five gene lists are provided: 59 ACMG-recommended secondary findings (v2.0) genes [5], 114 cancer predisposition genes collated by Rahman [6], 190 actionable cancer genes reported in 2015 for JAX-CPT [7], 1158 human MitoCarta 2.0 genes [8] (accessed February 2018), and 1751 Development Disorder Genotype - Phenotype Database (DDG2P) genes [9] (accessed February 2018). Users can supply their own gene lists for filtering by setting **Filter set to use** input to \"Custom gene list as file\" and providing a file with gene symbols to filter on (one gene symbol per line) as **User-defined gene list file** input.\n* Please note that using **Variant Effect Predictor** `--individual` flag requires that genotypes are present for all samples of interest, and recorded in a standard format (for example, Strelka-produced genotype fields are incompatible with this flag).\n\n\n### Performance Benchmarking\n\nVCF preprocessing with **VT normalize-decompose** is usually fast (~3 mins for a WGS VCF file), with neglible cost ($0.02 on c4.2xlarge).\n\nThe performance of the annotation step of the workfow using **Variant Effect Predictor** will vary greatly depending on the annotation options selected and input file size. Increasing the number of forks used with the parameter **Fork number** (`--fork`) and the number of processors will help. Additionally, tabix-indexing your supplied FASTA file, or setting the **Do not generate a stats file** (`--no_stats`) flag will speed up annotation. Preprocessing the VEP cache using the **convert_cache.pl** script included in the **ensembl-vep distribution** will also help if using **Check for co-located known variants** (`--check_existing`) flag or any of the allele frequency associated flags. VEP caches available on the Seven Bridges platform have been preprocessed in this way. Using **Add HGVS identifiers** (`--hgvs`) parameter will slow down the annotation process.\n\nFiltering steps are optional. If used, the increase in task duration depends on the filter sets used (5-10 mins for a 5 GB VEP-annotated WGS VCF file on c4.2xlarge instance, cost ~$0.05).\n\nIn the following table you can find estimates of **VEP annotation** running time and cost. Sample that was annotated was NA12878 genome (~100 Mb, as VCF.GZ).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n** ; Homo_sapiens_assembly38.fasta.gz; dbSNP v150; dbNSFP 3.5; dbscSNV 1.1 - parallelized filtering\n                   \n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n|  Annotation of NA12878 genome VCF (GRCh38)** | 85 min   | $0.56            |c4.2xlarge      |\n|  Annotation of NA12878 genome VCF (GRCh38)** | 49 min    | $0.97                | c4.8xlarge (VEP) c4.2xlarge (other nodes) |\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"input_vcf\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"reffasta\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"cache_file\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n}\ntask = api.tasks.create(name='VEP annotation workflow - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [VT toolkit documentation](https://genome.sph.umich.edu/wiki/Vt)\n\n[2] [Running VEP - Documentation page](https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html)\n\n[3] [Filter VEP - Documentation page](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html)\n\n[4] [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP)\n\n[5] [ACMG-recommended secondary findings (v2.0) genes](https://www.nature.com/articles/gim2016190)\n\n[6] [Cancer predisposition genes; Rahman 2014](https://www.ncbi.nlm.nih.gov/pubmed/24429628)\n\n[7] [JAX-CPT-reported actionable genes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481190/)\n\n[8] [Human MitoCarta 2.0](https://www.broadinstitute.org/files/shared/metabolism/mitocarta/human.mitocarta2.0.html)\n\n[9] [DDG2P database genes](https://decipher.sanger.ac.uk/ddd#ddgenes)", "input": [{"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "VEP cache", "encodingFormat": "application/x-tar"}, {"name": "dbscSNV file"}, {"name": "dbNSFP file"}, {"name": "Custom annotation sources"}, {"name": "Custom annotation BigWig sources"}, {"name": "User defined gene list file", "encodingFormat": "text/plain"}, {"name": "input_vcf", "encodingFormat": "application/x-vcf"}, {"name": "Filters to apply"}, {"name": "Filter set to use"}, {"name": "Run filtering (gene list + custom filters)"}, {"name": "VEP --individual parameter used"}, {"name": "Split input VCF by sample"}, {"name": "Samples used for VEP --individual flag"}, {"name": "Run filtering"}, {"name": "Include variants with variant quality greater than this cutoff"}, {"name": "Populations available for allele frequency filtering"}, {"name": "Include only variants with allele frequency lower than this cutoff in specified populations"}, {"name": "Filters to apply"}, {"name": "Filter out known variants"}, {"name": "Include only variants with HGVSp"}, {"name": "Include only variants with HGVSc"}, {"name": "Include only coding variants (variant effects subset)"}, {"name": "Include coding and splicing variants (variant effects subset)"}, {"name": "Include variants with the following VEP-assigned impact"}, {"name": "Include only these variant types (VARIANT_CLASS)"}, {"name": "Include variants with one of the following variant effects (Consequence field)"}, {"name": "Include variants associated with genes from the list"}, {"name": "Include variants from chromosome(s)"}, {"name": "Use MaxEntScan plugin"}, {"name": "Use LoFtool plugin"}, {"name": "Use CSN plugin"}, {"name": "Number of CPUs"}, {"name": "Do not generate a stats file [--no_stats]"}, {"name": "Exclude intergenic consequences from the output"}, {"name": "Output only the most severe consequence per variant"}, {"name": "Memory to use for the task"}, {"name": "Add HGVS identifiers"}, {"name": "Limit analysis to GENCODE basic transcript set"}, {"name": "Fields to configure the output format (VCF or tab only) with"}, {"name": "Shortcut flag to turn on most commonly used annotations [--everything]"}, {"name": "Columns of dbNSFP to report"}, {"name": "Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)"}, {"name": "Annotation parameters for custom BigWig annotation sources only"}, {"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Check for co-located known variants"}, {"name": "Version of VEP cache if not default"}, {"name": "VCF preprocessing"}], "output": [{"name": "VEP annotation summary file", "encodingFormat": "text/plain"}, {"name": "Annotated VCF files", "encodingFormat": "application/x-vcf"}, {"name": "Custom filtered VCF file"}, {"name": "Filtered VCF on gene list"}, {"name": "Custom gene list filtering summary", "encodingFormat": "text/plain"}, {"name": "VEP warnings file", "encodingFormat": "text/plain"}], "applicationSubCategory": ["Annotation", "Variant Filtration"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648047364, "dateCreated": 1520856583, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vep-annotation-workflow-101-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vep-annotation-workflow-101-0/8", "applicationCategory": "Workflow", "name": "VEP annotation workflow 101.0 CWL1.0", "description": "The **VEP annotation** workflow is used for preprocessing, annotating, and filtering VCF files. \n\nVCF or VCF.GZ files can be processed using this workflow. The first step in this pipeline is an optional VCF normalization and decomposition performed using the **vt toolkit**. To minimize processing time, the input file is streamed to **vt normalize** and **vt decompose**, which are invoked as piped processes. Preprocessing with **vt** requires you to supply a FASTA file with the input VCF. The final step of preprocessing is bgzip compression and tabix indexing, which is set to occur even if the **vt preprocessing** of the input file is skipped. Input files are sorted (**vt sort**, sorting mode local or full, depending on the absence/presence of contigs in the input VCF header) before preprocessing and **vt uniq** is used to drop any potentially duplicated variant entries after decomposition. \n\nThe input file is annotated using **Variant Effect Predictor**. Annotations are added from VEP cache, plugins, or custom annotation sources (VCF, BED, GFF, GTF, BW, etc.). If you are using custom annotation sources, all associated parameters should be provided separately in the order of the provided input files.\n\nThe third step of the pipeline is the optional filtering of the annotated VCF file. This step uses the filter_vep script from the ensembl-vep toolkit (**Filter VEP**), which has been wrapped to allow pass-through (if no filters are selected) and with a number of convenient filters alongside the standard filter_vep syntax.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nThe **VEP annotation** workflow can be used to preprocess (vt normalize, vt decompose, tabix), annotate (**Variant Effect Predictor**), and filter (**Filter VEP**) VCF files.\n\nRunning this workflow on the Seven Bridges Platform requires using a VEP cache file. VEP cache files can be obtained from our Public Reference Files section (`homo_sapiens_vep_101_GRCh37.indexed.tar.gz` and `homo_sapiens_vep_101_GRCh38.indexed.tar.gz`) or imported as files to your project from the [Ensembl ftp site](ftp://ftp.ensembl.org/pub/release-101/variation/indexed_vep_cache/) using the [FTP/HTTP import](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server) feature.\n\n### Changes Introduced by Seven Bridges\n\n- Input **VCF preprocessing** is a convenient master toggle flag, controlling VCF preprocessing. If **VCF preprocessing** is set to True, inputs **Decompose multiallelic variants** and **VCF normalization** can be used to specify preprocessing steps. Otherwise, these inputs are ignored.\n- Input **Reference FASTA file** is not required for running the workflow but is **highly recommended** as it is required for optional VCF normalization during preprocessing and enables additional annotations. Please note that **Reference FASTA file** is required for obtaining some annotations, including HGVS identifiers (`--hgvs`).\n- When using custom annotation sources (`--custom` flag), input files and parameters are specified separately and both must be provided to run the tool (inputs **Custom annotation sources** and **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**). Additionally, separate inputs have been provided for BigWig custom annotation sources and parameters, as these files do not require indexing before use (inputs **Custom annotation - BigWig sources only** and **Annotation parameters for custom BigWig annotation sources only**). Tabix TBI indices are required for other custom annotation sources.\n- Additional boolean flags are introduced to activate the use of **Variant Effect Predictor** plugins (CSN, MaxEntScan, and LoFtool plugins can be accessed with parameters **Use CSN plugin**, **Use MaxEntScan plugin**, and **Use LoFtool plugin**, respectively).\n- Filtering node performance was improved by splitting the input VCF file on chromosomes and running concurrent GNU parallel-managed **Filter VEP** processes. All filters are optional - inputs **Run filtering** and **Custom filtering sets** are convenience toggle flags controlling filtering of input VCF file and allowing pass-through mode of filtering tools, if set to False.\n- For details about changes made to individual tools in the workflow, please see tool descriptions.\n\n### Common Issues and Important Notes\n\n* The workflow is intended for processing VCF files. Even though **Variant Effect Predictor** can output TSV and JSON files, please note that only VCF output is supported in the current workflow setup.\n* Inputs **Input VCF** (`--input_file`) and **Species cache** files are required. They represent a variant file containing variants to be annotated and a database cache file used for annotating the most common variants found in the particular species, respectively.    \n* The **Reference FASTA file** input is not required for running the workflow but is **highly recommended** as it is required for optional VCF normalization during preprocessing and enables additional annotations. Please note that **Reference FASTA file** is required for obtaining some annotations, including HGVS identifiers (`--hgvs`).\n*  If **VCF preprocessing** is set to False and **Input VCF** is provided in compressed VCF.GZ format, then the **Skip bgzip/tabix processing of the output file** input should be set to True, otherwise the preprocessing will fail.\n* Please see flag descriptions or official documentation [1-3] for detailed descriptions of tool options and limitations.\n* The **Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)** (`--af_exac` or `--af_gnomAD`) VEP parameter: Please note that ExAC data has been superseded by gnomAD data and is only accessible with older (<90) cache versions. The Seven Bridges version of the tool will automatically swap flags according to the cache version reported.\n* The **Include Ensembl identifiers when using RefSeq and merged caches** (`--all_refseq`) and **Exclude predicted transcripts when using RefSeq or merged cache** (`--exclude_predicted`) parameters should only be used with RefSeq or merged caches\n* The **Add APPRIS identifiers** (`--appris`) parameter - APPRIS is only available for GRCh38.\n* The **Samples to annotate** (`--individual`) parameter requires that all samples of interest have proper genotype entries for all variant rows in the file. **Variant Effect Predictor** will not output multiple variant rows per sample if genotypes are missing in those rows.\n* A preprocessed dbNSFP file (input **dbNSFP database file**) and dbNSFP column names (parameter **Columns of dbNSFP to report**) should be provided to access dbNSFP annotations. dbNSFP column names should match the release of dbNSFP provided for annotation (for a detailed list of column names, please consult the [readme files accompanying the dbNSFP release](https://sites.google.com/site/jpopgen/dbNSFP) used for annotation). If no dbNSFP column names are provided alongside a dbNSFP annotation file, the following example subset of columns applicable to dbNSFP versions 2.9.3, 3.Xc and 4.0c will be used for annotation: `FATHMM_pred,MetaSVM_pred,GERP++_RS`. Likewise, if using the dbscSNV plugin, a dbscSNV file (input **dbscSNV database file**) should be provided.\n* The **Version of VEP cache if not default** parameter (`--cache_version`) must be supplied if not using a VEP 100 cache.\n* Forking and increasing the number of CPUs available will improve annotation performance, but users interested in only a few annotations and concerned about speed may wish to consider directly using a tool like **SnpEff**, **vcfanno** or **Variant Effect Predictor**.\n* **Filter VEP** used in filtering nodes implies that all separate filters (`--filter` flag) used are chained with a logical AND operator, which should be kept in mind when designing filters to use.\n* If **Run filtering** parameter is set to True on the optional filtering tools, at least one filter *must* be defined. Running the workflow with no filters and **Run filtering** enabled will cause the tasks to fail.\n* The following five gene lists are provided as convenient filters: 59 ACMG-recommended secondary findings (v2.0) genes [5], 114 cancer predisposition genes collated by Rahman [6], 190 actionable cancer genes reported in 2015 for JAX-CPT [7], 1158 human MitoCarta 2.0 genes [8] (accessed February 2018), and 1751 Development Disorder Genotype - Phenotype Database (DDG2P) genes [9] (accessed February 2018). Users can supply their own gene lists for filtering by setting the **Filter set to use** input to \"Custom gene list as file\" and providing a file with gene symbols to filter on (one gene symbol per line) as the **User-defined gene list file** input.\n* Please note that using the **Variant Effect Predictor** `--individual` flag requires that genotypes are present for all samples of interest, and recorded in a standard format (for example, Strelka-produced genotype fields are incompatible with this flag).\n\n\n### Performance Benchmarking\n\nVCF preprocessing with **VT normalize-decompose** is usually fast (~3 mins for a WGS VCF file), with negligible cost ($0.02 on c4.2xlarge).\n\nThe performance of the annotation step of the workflow using **Variant Effect Predictor** will vary greatly depending on the annotation options selected and input file size. Increasing the number of forks used with the **Fork number** (`--fork`) and the number of processors will help. Additionally, tabix-indexing your supplied FASTA file, or setting the **Do not generate a stats file** (`--no_stats`) flag will speed up annotation. Preprocessing the VEP cache using the **convert_cache.pl** script included in the **ensembl-vep distribution** will also help if using the **Check for co-located known variants** (`--check_existing`) flag or any of the allele frequency associated flags. VEP caches available on the Seven Bridges Platform have been preprocessed in this way. Using the **Add HGVS identifiers** (`--hgvs`) parameter will slow down the annotation process.\n\nFiltering steps are optional. If used, the increase in task duration depends on the filter sets used (5-10 mins for a 5 GB VEP-annotated WGS VCF file on a c4.2xlarge instance, cost ~$0.05).\n\nIn the following table you can find estimates of **VEP annotation** running time and cost. The sample that was annotated was NA12878 genome (~100 Mb, as VCF.GZ).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n** ; Homo_sapiens_assembly38.fasta.gz; dbNSFP 4.0; dbscSNV 1.1 - parallelized filtering\n                   \n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n|  Annotation of NA12878 genome VCF (GRCh38)** | 1 h 55 min  | $1.03 (Instances: $0.76, Attached disks: $0.27)             |c4.2xlarge      |\n|  Annotation of NA12878 genome VCF (GRCh38)** | 1h    | $1.17 (Instances: $1.03, Attached disks: $0.14)                | c5.9xlarge (VEP) c4.2xlarge (other nodes) |\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"in_variants\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"in_references\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"cache_file\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n}\ntask = api.tasks.create(name='VEP annotation workflow - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [VT toolkit documentation](https://genome.sph.umich.edu/wiki/Vt)\n\n[2] [Running VEP - Documentation page](https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html)\n\n[3] [Filter VEP - Documentation page](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html)\n\n[4] [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP)\n\n[5] [ACMG-recommended secondary findings (v2.0) genes](https://www.nature.com/articles/gim2016190)\n\n[6] [Cancer predisposition genes; Rahman 2014](https://www.ncbi.nlm.nih.gov/pubmed/24429628)\n\n[7] [JAX-CPT-reported actionable genes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481190/)\n\n[8] [Human MitoCarta 2.0](https://www.broadinstitute.org/files/shared/metabolism/mitocarta/human.mitocarta2.0.html)\n\n[9] [DDG2P database genes](https://decipher.sanger.ac.uk/ddd#ddgenes)", "input": [{"name": "Species cache file", "encodingFormat": "application/x-tar"}, {"name": "Custom annotation - BigWig sources only"}, {"name": "Custom annotation sources"}, {"name": "dbNSFP database file"}, {"name": "dbscSNV database file"}, {"name": "Reference Fasta file", "encodingFormat": "application/x-fasta"}, {"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "VEP --individual parameter used"}, {"name": "Samples used for VEP --individual flag"}, {"name": "User-defined gene list file", "encodingFormat": "text/plain"}, {"name": "Filters to apply"}, {"name": "Run filtering (gene list + custom filters)"}, {"name": "Filter set to use"}, {"name": "Filters to apply"}, {"name": "Run filtering"}, {"name": "Include variants with variant quality greater than this cutoff"}, {"name": "Include only variants with allele frequency lower than this cutoff in specified populations"}, {"name": "Include variants from chromosome(s)"}, {"name": "Include variants associated with genes from the list"}, {"name": "Include only these variant types (VARIANT_CLASS)"}, {"name": "Include variants with one of the following variant effects (Consequence field)"}, {"name": "Filter out known variants"}, {"name": "Populations available for allele frequency filtering"}, {"name": "Include only coding variants (variant effects subset)"}, {"name": "Include only variants with HGVSp"}, {"name": "Include only variants with HGVSc"}, {"name": "Include coding and splicing variants (variant effects subset)"}, {"name": "Include variants with the following VEP-assigned impact"}, {"name": "VCF preprocessing"}, {"name": "Skip bgzip/tabix processing of the output file"}, {"name": "Number of CPUs"}, {"name": "Memory to use for the task"}, {"name": "Assembly version"}, {"name": "Columns of dbNSFP to report"}, {"name": "Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)"}, {"name": "Annotation parameters for custom BigWig annotation sources only"}, {"name": "Optional file name for warnings file output"}, {"name": "Shortcut flag to turn on most commonly used annotations [--everything]"}, {"name": "Version of VEP cache if not default"}, {"name": "Do not generate a stats file [--no_stats]"}, {"name": "Report overlaps with regulatory regions [--regulatory]"}, {"name": "Add APPRIS identifiers"}, {"name": "Add HGVS identifiers"}, {"name": "Check for co-located known variants"}, {"name": "Add 1000 genomes phase 3 global allele frequency"}, {"name": "Add allele frequency from continental 1000 genomes populations"}, {"name": "Add allele frequency from NHLBI-ESP populations"}, {"name": "Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)"}, {"name": "Fields to configure the output format (VCF or tab only) with"}, {"name": "Limit analysis to GENCODE basic transcript set"}, {"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Exclude intergenic consequences from the output"}, {"name": "Output only the most severe consequence per variant"}, {"name": "Summary stats file name"}, {"name": "Use LoFtool plugin"}, {"name": "Use MaxEntScan plugin"}, {"name": "Use CSN plugin"}, {"name": "Species"}], "output": [{"name": "Optional file with VEP warnings and errors", "encodingFormat": "text/plain"}, {"name": "Annotation summary stats file", "encodingFormat": "text/plain"}, {"name": "Annotated VCF files", "encodingFormat": "application/x-vcf"}, {"name": "Filtered VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Custom filtering summary", "encodingFormat": "text/plain"}, {"name": "Custom filtered VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-vep"], "applicationSubCategory": ["VCF Processing", "Annotation"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648039726, "dateCreated": 1617277912, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vep-annotation-workflow-90-5-cwl1-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vep-annotation-workflow-90-5-cwl1-0/8", "applicationCategory": "Workflow", "name": "VEP annotation workflow 90.5 CWL1.0", "description": "The **VEP annotation** workflow is used for preprocessing, annotating, and filtering VCF files. \n\nVCF or VCF.GZ files can be processed using this workflow. The first step in this pipeline is an optional VCF normalization and decomposition performed using the **vt toolkit**. To minimize processing time, the input file is streamed to **vt normalize** and **vt decompose**, which are invoked as piped processes. Preprocessing with **vt** requires you to supply a FASTA file with the input VCF. The final step of preprocessing is bgzip compression and tabix indexing, which is set to occur even if the **vt preprocessing** of the input file is skipped. Input files are sorted (**vt sort**, sorting mode local or full, depending on the absence/presence of contigs in the input VCF header) before preprocessing and **vt uniq** is used to drop any potentially duplicated variant entries after decomposition. \n\nThe input file is annotated using **Variant Effect Predictor**. Annotations are added from VEP cache, plugins, or custom annotation sources (VCF, BED, GFF, GTF, BW, etc.). If you are using custom annotation sources, all associated parameters should be provided separately in the order of the provided input files.\n\nThe third step of the pipeline is the optional filtering of the annotated VCF file. This step uses the filter_vep script from the ensembl-vep toolkit (**Filter VEP**), which has been wrapped to allow pass-through (if no filters are selected) and with a number of convenient filters alongside the standard filter_vep syntax. All filtering nodes use GNU parallel and splitting/joining VCFs on chromosomes to speed up filtering.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThe **VEP annotation** workflow can be used to preprocess (vt normalize, vt decompose, tabix), annotate (**Variant Effect Predictor**), and filter (**Filter VEP**) VCF files.\n\nRunning this workflow on the Seven Bridges Platform requires using a VEP cache file. VEP cache files can be obtained from our Public Reference Files section (`homo_sapiens_vep_90_GRCh37.tar.gz` and `homo_sapiens_vep_90_GRCh38.tar.gz`) or imported as files to your project from the [Ensembl ftp site](ftp://ftp.ensembl.org/pub/current_variation/VEP/) using the [FTP/HTTP import](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server) feature.\n\n### Changes Introduced by Seven Bridges\n\n- Input **VCF preprocessing** is a convenient master toggle flag, controlling VCF preprocessing. If **VCF preprocessing** is set to True, inputs **Decompose multiallelic variants** and **VCF normalization** can be used to specify preprocessing steps. Otherwise, these inputs are ignored.\n- Input **Reference FASTA file** is not required for running the workflow but is **highly recommended** as it is required for optional VCF normalization during preprocessing and enables additional annotations. Please note that **Reference FASTA file** is required for obtaining some annotations, including HGVS identifiers (`--hgvs`).\n- When using custom annotation sources (`--custom` flag), input files and parameters are specified separately and both must be provided to run the tool (inputs **Custom annotation sources** and **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**). Additionally, separate inputs have been provided for BigWig custom annotation sources and parameters, as these files do not require indexing before use (inputs **Custom annotation - BigWig sources only** and **Annotation parameters for custom BigWig annotation sources only**). Tabix TBI indices are required for other custom annotation sources.\n- Additional boolean flags are introduced to activate the use of **Variant Effect Predictor** plugins (CSN, MaxEntScan, and LoFtool plugins can be accessed with parameters **Use CSN plugin**, **Use MaxEntScan plugin**, and **Use LoFtool plugin**, respectively).\n- Filtering node performance was improved by splitting the input VCF file on chromosomes and running concurrent GNU parallel-managed **Filter VEP** processes. All filters are optional - inputs **Run filtering** and **Custom filtering sets** are convenience toggle flags controlling filtering of input VCF file and allowing pass-through mode of filtering tools, if set to False.\n- For details about changes made to individual tools in the workflow, please see tool descriptions.\n\n### Common Issues and Important Notes\n\n* The workflow is intended for processing VCF files. Even though **Variant Effect Predictor** can output TSV and JSON files, please note that only VCF output is supported in the current workflow setup.\n* Inputs **Input VCF** (`--input_file`) and **Species cache** files are required. They represent a variant file containing variants to be annotated and a database cache file used for annotating the most common variants found in the particular species, respectively.    \n* The **Reference FASTA file** input is not required for running the workflow but is **highly recommended** as it is required for optional VCF normalization during preprocessing and enables additional annotations. Please note that **Reference FASTA file** is required for obtaining some annotations, including HGVS identifiers (`--hgvs`).\n*  If **VCF preprocessing** is set to False and **Input VCF** is provided in compressed VCF.GZ format, then the **Skip bgzip/tabix processing of the output file** input should be set to True, otherwise the preprocessing will fail.\n* Please see flag descriptions or official documentation [1-3] for detailed descriptions of tool options and limitations.\n* The **Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)** (`--af_exac` or `--af_gnomAD`) VEP parameter: Please note that ExAC data has been superseded by gnomAD data and is only accessible with older (<90) cache versions. The Seven Bridges version of the tool will automatically swap flags according to the cache version reported.\n* The **Include Ensembl identifiers when using RefSeq and merged caches** (`--all_refseq`) and **Exclude predicted transcripts when using RefSeq or merged cache** (`--exclude_predicted`) parameters should only be used with RefSeq or merged caches\n* The **Add APPRIS identifiers** (`--appris`) parameter - APPRIS is only available for GRCh38.\n* The **Samples to annotate** (`--individual`) parameter requires that all samples of interest have proper genotype entries for all variant rows in the file. **Variant Effect Predictor** will not output multiple variant rows per sample if genotypes are missing in those rows.\n* A preprocessed dbNSFP file (input **dbNSFP database file**) and dbNSFP column names (parameter **Columns of dbNSFP to report**) should be provided to access dbNSFP annotations. dbNSFP column names should match the release of dbNSFP provided for annotation (for a detailed list of column names, please consult the [readme files accompanying the dbNSFP release](https://sites.google.com/site/jpopgen/dbNSFP) used for annotation). If no dbNSFP column names are provided alongside a dbNSFP annotation file, the following example subset of columns applicable to dbNSFP versions 2.9.3 and 3.Xc will be used for annotation: `FATHMM_pred,MetaSVM_pred,GERP++_RS`. Likewise, if using the dbscSNV plugin, a dbscSNV file (input **dbscSNV database file**) should be provided.\n* The **Version of VEP cache if not default** parameter (`--cache_version`) must be supplied if not using a VEP 90 cache.\n* Forking and increasing the number of CPUs available will improve annotation performance, but users interested in only a few annotations and concerned about speed may wish to consider directly using a tool like **SnpEff**, **vcfanno** or **Variant Effect Predictor**.\n* **Filter VEP** used in filtering nodes implies that all separate filters (`--filter` flag) used are chained with a logical AND operator, which should be kept in mind when designing filters to use.\n* If **Run filtering** parameter is set to True on the optional filtering tools, at least one filter *must* be defined. Running the workflow with no filters and **Run filtering** enabled will cause the tasks to fail.\n* The following five gene lists are provided as convenient filters: 59 ACMG-recommended secondary findings (v2.0) genes [5], 114 cancer predisposition genes collated by Rahman [6], 190 actionable cancer genes reported in 2015 for JAX-CPT [7], 1158 human MitoCarta 2.0 genes [8] (accessed February 2018), and 1751 Development Disorder Genotype - Phenotype Database (DDG2P) genes [9] (accessed February 2018). Users can supply their own gene lists for filtering by setting the **Filter set to use** input to \"Custom gene list as file\" and providing a file with gene symbols to filter on (one gene symbol per line) as the **User-defined gene list file** input.\n* Please note that using the **Variant Effect Predictor** `--individual` flag requires that genotypes are present for all samples of interest, and recorded in a standard format (for example, Strelka-produced genotype fields are incompatible with this flag).\n\n\n### Performance Benchmarking\n\nVCF preprocessing with **VT normalize-decompose** is usually fast (~3 mins for a WGS VCF file), with negligible cost ($0.02 on c4.2xlarge).\n\nThe performance of the annotation step of the workflow using **Variant Effect Predictor** will vary greatly depending on the annotation options selected and input file size. Increasing the number of forks used with the **Fork number** (`--fork`) and the number of processors will help. Additionally, tabix-indexing your supplied FASTA file, or setting the **Do not generate a stats file** (`--no_stats`) flag will speed up annotation. Preprocessing the VEP cache using the **convert_cache.pl** script included in the **ensembl-vep distribution** will also help if using the **Check for co-located known variants** (`--check_existing`) flag or any of the allele frequency associated flags. VEP caches available on the Seven Bridges Platform have been preprocessed in this way. Using the **Add HGVS identifiers** (`--hgvs`) parameter will slow down the annotation process.\n\nFiltering steps are optional. If used, the increase in task duration depends on the filter sets used (5-10 mins for a 5 GB VEP-annotated WGS VCF file on a c4.2xlarge instance, cost ~$0.05).\n\nIn the following table you can find estimates of **VEP annotation** running time and cost. The sample that was annotated was NA12878 genome (~100 Mb, as VCF.GZ).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n** ; Homo_sapiens_assembly38.fasta.gz; dbSNP v150; dbNSFP 3.3; dbscSNV 1.1 - parallelized filtering\n                   \n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n|  Annotation of NA12878 genome VCF (GRCh38)** | 52 min   | $0.47 (Instances: $0.35, Attached disks: $0.12)             |c4.2xlarge      |\n|  Annotation of NA12878 genome VCF (GRCh38)** | 29 min    | $0.69 (Instances: $0.62, Attached disks: $0.07)                | c4.8xlarge (VEP) c4.2xlarge (other nodes) |\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"input_vcf\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"reffasta\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"cache_file\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\"\n}\ntask = api.tasks.create(name='VEP annotation workflow - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [VT toolkit documentation](https://genome.sph.umich.edu/wiki/Vt)\n\n[2] [Running VEP - Documentation page](https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html)\n\n[3] [Filter VEP - Documentation page](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html)\n\n[4] [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP)\n\n[5] [ACMG-recommended secondary findings (v2.0) genes](https://www.nature.com/articles/gim2016190)\n\n[6] [Cancer predisposition genes; Rahman 2014](https://www.ncbi.nlm.nih.gov/pubmed/24429628)\n\n[7] [JAX-CPT-reported actionable genes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481190/)\n\n[8] [Human MitoCarta 2.0](https://www.broadinstitute.org/files/shared/metabolism/mitocarta/human.mitocarta2.0.html)\n\n[9] [DDG2P database genes](https://decipher.sanger.ac.uk/ddd#ddgenes)", "input": [{"name": "Input VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "dbscSNV database file"}, {"name": "dbNSFP database file"}, {"name": "Custom annotation sources"}, {"name": "Custom annotation BigWig sources"}, {"name": "VEP cache", "encodingFormat": "application/x-tar"}, {"name": "User defined gene list file", "encodingFormat": "text/plain"}, {"name": "VEP --individual parameter used"}, {"name": "Samples used for VEP --individual flag"}, {"name": "Run filtering"}, {"name": "Include variants with variant quality greater than this cutoff"}, {"name": "Filters to apply"}, {"name": "Include only variants with allele frequency lower than this cutoff in specified populations"}, {"name": "Include variants from chromosome(s)"}, {"name": "Include variants associated with genes from the list"}, {"name": "Include only these variant types (VARIANT_CLASS)"}, {"name": "Include variants with one of the following variant effects (Consequence field)"}, {"name": "Filter out known variants"}, {"name": "Populations available for allele frequency filtering"}, {"name": "Include only coding variants (variant effects subset)"}, {"name": "Include only variants with HGVSp"}, {"name": "Include only variants with HGVSc"}, {"name": "Include coding and splicing variants (variant effects subset)"}, {"name": "Include variants with the following VEP-assigned impact"}, {"name": "Filter set to use"}, {"name": "Run filtering (gene list + custom filters)"}, {"name": "VCF preprocessing"}, {"name": "Number of CPUs"}, {"name": "Memory to use for the task"}, {"name": "Assembly version"}, {"name": "Columns of dbNSFP to report"}, {"name": "Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)"}, {"name": "Annotation parameters for custom BigWig annotation sources only"}, {"name": "Optional file name for warnings file output"}, {"name": "Shortcut flag to turn on most commonly used annotations [--everything]"}, {"name": "Version of VEP cache if not default"}, {"name": "Do not generate a stats file [--no_stats]"}, {"name": "Report overlaps with regulatory regions [--regulatory]"}, {"name": "Add APPRIS identifiers"}, {"name": "Add HGVS identifiers"}, {"name": "Check for co-located known variants"}, {"name": "Add 1000 genomes phase 3 global allele frequency"}, {"name": "Add allele frequency from continental 1000 genomes populations"}, {"name": "Add allele frequency from NHLBI-ESP populations"}, {"name": "Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)"}, {"name": "Fields to configure the output format (VCF or tab only) with"}, {"name": "Limit analysis to GENCODE basic transcript set"}, {"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Exclude intergenic consequences from the output"}, {"name": "Output only the most severe consequence per variant"}, {"name": "Summary stats file name"}, {"name": "Use LoFtool plugin"}, {"name": "Use MaxEntScan plugin"}, {"name": "Use CSN plugin"}, {"name": "Skip bgzip/tabix processing of the output file"}], "output": [{"name": "Annotated VCF files", "encodingFormat": "application/x-vcf"}, {"name": "Custom filtered VCF file"}, {"name": "Filtered VCF file"}, {"name": "Custom gene list filtering summary", "encodingFormat": "text/plain"}, {"encodingFormat": "text/plain"}, {"encodingFormat": "text/plain"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "applicationSubCategory": ["VCF Processing", "Annotation"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648039727, "dateCreated": 1560936368, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vep-slivar-trios-rare-diseases-analysis-0-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vep-slivar-trios-rare-diseases-analysis-0-1/6", "applicationCategory": "Workflow", "name": "VEP Slivar Trios Rare Diseases Analysis", "description": "**VEP Slivar Trios Rare Diseases Analysis** workflow analyzes WES and WGS family variants [1-3].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nThe **VEP Slivar Trios Rare Diseases Analysis** workflow can be used to preprocess (normalize and decompose with **VT**) [4,5], annotate (**Variant Effect Predictor**) [6], and analyze (**Slivar**) VCF files containing WES or WGS variants from related individuals (trios or families).\n\nThe workflow implements the Slivar Rare Disease analysis flow [2], with the added VEP annotation and conversion of output VCFs into TSV files with **Slivar Tsv**.\n\nAfter optional preprocessing (**VCF preprocessing** input) the **Input variants** file is annotated using **Variant Effect Predictor**. Annotations are added from VEP cache, plugins, or custom annotation sources. All associated parameters for custom annotation sources should be provided separately in the order of the provided input files.\n\nRunning this workflow on the Seven Bridges Platform requires using a VEP cache file. VEP cache files can be obtained from our Public Reference Files section (`homo_sapiens_vep_101_GRCh37.indexed.tar.gz` and `homo_sapiens_vep_101_GRCh38.indexed.tar.gz`) or imported as files to your project from the [Ensembl ftp site](ftp://ftp.ensembl.org/pub/release-101/variation/indexed_vep_cache/) using the [FTP/HTTP import](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server) feature.\n\nVEP-annotated variants are indexed with **Tabix Index** and sent to **Slivar Expr** for additional annotation (**Gnotation files** input) and filtering. The default filters match [2] and can be adjusted at task runtime. After filtering, the variants can optionally be annotated with **BCFtools CSQ** and are sent for compound heterozygosity analysis with **Slivar Compound-het** [2].\n\n### Changes Introduced by Seven Bridges\n\nThis workflow differs from the reference implementation [2] in the following:\n* Data preprocessing and VEP annotation steps have been added to the beginning of the flow. Preprocessing is added as **Slivar** tools rely on the AD field and expect decomposed inputs [4]. VEP annotation was added to enable the use of the `impactful` **Slivar** filters by default.\n* **BCFtools CSQ** annotation step was made optional and will only be executed if an appropriate GFF3 file is provided on input (**GFF annotation file**).\n* **Slivar Tsv** nodes were added to the end of the workflow to convert the VCF outputs into more readable TSV files.\n\n### Common Issues and Important Notes\n\n * Required inputs:\n    - **Input variants** - A VCF file with data for one or more WGS or WES trios/families. Please ensure that the variants contain AD information, to ensure proper data filtering.\n    - **Pedigree file**  - A pedigree file matching the samples provided in the input VCF file (**Input variants**). The format matches **Plink** FAM format.\n    - **Reference FASTA file** - Reference sequence matching the **Input variants** data.\n    - **Species cache file** - The VEP annotation cache. This file should match the assembly version used. If the version of the cache is not 101, **Version of VEP cache if not default** input parameter should be set to the corresponding cache version.\n    - **Gnotation files**  - One or more gnotation files to use. Please note that the default setup of the workflow expects GRCh38 data and the gnomAD and TopMED gnotation files, distributed by **Slivar** authors. If using different inputs or gnotation files, please check all exposed input fields of the **Slivar** tools to ensure that your data is processed correctly. Custom gnotation files can be prepared with the **Slivar Make-gnotate** tool.\n    - **VCF preprocessing** - This input indicates whether variants should be normalized and decomposed with **VT** tools.\n\n * **GFF annotation file** is a recommended input. If provided, this file will be used by **BCFtools CSQ** to add annotations to the data. Only GFF3 inputs following the Ensembl format are supported. The GFF must match the provided **Reference FASTA file** and **Input variants** data. \n\n * The default values of workflow filters have been exposed at task runtime and are set to match [2]. When customizing the workflow, please check the values of all input parameters carefully, especially when changing input data sources. For example, if TopMED frequencies are not used as one of the **Gnotation files** inputs, please omit the `INFO.topmed_af < 0.05` filter from **Expression using only INFO fields**.\n\n * The analysis flow uses allelic balance (AB) as a filtering criterion. The tools derive this value from the AD field in the FORMAT column of input VCFs, necessitating the presence of this field in input data. If input VCFs do not contain AD information, the workflow will fail.\n\n* Please note that the default Slivar JS file used by the workflow does not impose a DP filter (`var config = {min_GQ: 20, min_AB: 0.20, min_DP: 0}`). If a DP filter is of interest (for example, for processing WGS data, it may be desirable to set min_DP to 10), a custom JS functions file can be provided (**Javascript functions** input). Changing the first line of the [default JS file](https://github.com/brentp/slivar/blob/master/js/slivar-functions.js) to read `var config = {min_GQ: 20, min_AB: 0.20, min_DP: 10}` and using the altered file to run the workflow will introduce a DP filter in the analysis.\n\n\n### Performance Benchmarking\n\nThe most time consuming step of the workflow is annotation with **Variant Effect Predictor**. The performance of the annotation step of the workflow depends on the annotation options selected and input file size. Increasing the number of processors will speed up the execution. \n\n                   \n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WES trio  | 16 min | $0.11 |  c4.2xlarge 100 GB EBS |\n| WGS trio | 68 min | $0.52 + $0.02  |  c4.2xlarge 100 GB EBS |\n| WGS trio | 41 min | $0.53 + $0.01  |  c5.4xlarge 100 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**VEP Slivar Trios Rare Diseases Analysis** was tested with cwltool 3.1.20211107152837. The `cache_file`, `in_reference`, `in_ped`, `in_variants`, `in_gnotate_files`, `preprocessing`, `info_field`, `info_field_1`, `sample_field`, `info`, `family_expr` and `trio` inputs were provided in the job.yaml/job.json file and used for testing. Cwltool command line parameters `--no-read-only` and `--no-match-user` were required for successful task execution.\n\n### References\n\n[1] [Slivar publication](https://www.biorxiv.org/content/10.1101/2020.08.13.249532v3)\n\n[2] [Slivar rare diseases analysis](https://github.com/brentp/slivar/wiki/rare-disease)\n\n[3] [Slivar toolkit documentation](https://github.com/brentp/slivar)\n\n[4] [Slivar inputs - decomposing](https://github.com/brentp/slivar/wiki/decomposing-and-subsetting-vcfs)\n\n[5] [VT toolkit documentation](https://genome.sph.umich.edu/wiki/Vt)\n\n[6] [Running VEP - Documentation page](https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html)", "input": [{"name": "Species cache file", "encodingFormat": "application/x-tar"}, {"name": "Custom annotation sources"}, {"name": "dbNSFP database file"}, {"name": "dbscSNV database file"}, {"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "GFF annotation file"}, {"name": "Pedigree file", "encodingFormat": "text/plain"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Javascript functions"}, {"name": "Number of CPUs"}, {"name": "Memory to use for the task"}, {"name": "Assembly version"}, {"name": "Columns of dbNSFP to report"}, {"name": "Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)"}, {"name": "Optional file name for warnings file output"}, {"name": "Shortcut flag to turn on most commonly used annotations [--everything]"}, {"name": "Version of VEP cache if not default"}, {"name": "Do not generate a stats file [--no_stats]"}, {"name": "Report overlaps with regulatory regions [--regulatory]"}, {"name": "Add APPRIS identifiers"}, {"name": "Add HGVS identifiers"}, {"name": "Check for co-located known variants"}, {"name": "Add 1000 genomes phase 3 global allele frequency"}, {"name": "Add allele frequency from continental 1000 genomes populations"}, {"name": "Add allele frequency from NHLBI-ESP populations"}, {"name": "Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)"}, {"name": "Fields to configure the output format (VCF or tab only) with"}, {"name": "Limit analysis to GENCODE basic transcript set"}, {"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Exclude intergenic consequences from the output"}, {"name": "Output only the most severe consequence per variant"}, {"name": "Summary stats file name"}, {"name": "Use LoFtool plugin"}, {"name": "Use MaxEntScan plugin"}, {"name": "Use CSN plugin"}, {"name": "Gene description files", "encodingFormat": "text/plain"}, {"name": "Gnotation files", "encodingFormat": "application/zip"}, {"name": "VCF preprocessing"}, {"name": "Skip bgzip/tabix processing of the output file"}, {"name": "Change ID=AD header entry (GATK VCFs)"}, {"name": "Expressions to apply to trios"}, {"name": "Expressions to apply to families"}, {"name": "Expressions to apply to groups"}, {"name": "Expressions to apply to each sample"}, {"name": "Expression using only INFO fields"}, {"name": "CSQ subfields to extract"}, {"name": "Info fields with passing samples"}, {"name": "INFO fields that should be added"}, {"name": "CSQ subfields to extract"}, {"name": "INFO fields that should be added"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "Optional file with VEP warnings and errors", "encodingFormat": "text/plain"}, {"name": "Annotation summary stats file", "encodingFormat": "text/plain"}, {"name": "Compound heterozygous variants", "encodingFormat": "application/x-vcf"}, {"name": "Output VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Annotated variants", "encodingFormat": "application/x-vcf"}, {"name": "CompHet TSV file"}, {"name": "Slivar TSV file"}], "softwareRequirements": ["InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/brentp/slivar/wiki/rare-disease", "https://github.com/brentp/slivar"], "applicationSubCategory": ["Annotation", "Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.1", "v1.0"], "dateModified": 1648047366, "dateCreated": 1612278642, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/warp-targetedsomaticsinglesample-pipeline/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/warp-targetedsomaticsinglesample-pipeline/8", "applicationCategory": "Workflow", "name": "WARP TargetedSomaticSingleSample Pipeline", "description": "**WARP TargetedSomaticSingleSample Pipeline** is designed for somatic human targeted sequencing data analysis. \n\n\nThe pipeline covers three main steps:    \n1) Data alignment and preprocessing by the **UnmappedBamToAlignedBam** inner workflow,     \n2) Converting BAM file to CRAM format file and the produced file validation by the **BamToCram** inner workflow,      \n3) Calculating different quality metrics by the **AggregatedBamQC** inner workflow, and **CollectHsMetrics** and **ConvertSequencingArtifactToOxoG** tools.     \n\nAdditionally, **GenerateSubsettedContaminationResources** tool is added at the very beginning of the pipeline. It uses **BEDtools Intersect** tool and a shell script in order to generate subsetted contamination files for **CheckContamination** tool within the **UnmappedBamToAlignedBam** inner workflow.\n\nRequired input files are:\n\n* **Unmapped BAM** - an array of unmapped SAM or BAM format files to extract reads from.     \n* **Reference** and **Reference dictionary** - human reference FASTA, FA, FA.GZ or FASTA.GZ format file, and a reference human DICT format file.           \n* **Reference Index TAR** - a reference file with its **BWA** index files packed in a TAR archive.    \n* **Target interval list** - an INTERVAL_LIST format file for selecting targeted regions.      \n* **Interval list of baits** - an INTERVAL_LIST format file that contains the locations of the baits used. It is required for the **CollectHsMetrics** step.         \n* **Contamination sites UD**, **Contamination sites MU** and **Contamination sites BED** - UD, MU and BED format files with contaminations for subsetting by the **GenerateSubsettedContaminationResources** tool.        \n* **Known sites** - one or more databases of known polymorphic sites in VCF, VCF.GZ, or BED format, used to exclude regions around known polymorphisms from analysis. Both SNPs and INDELs databases should be added to this input.     \n       \n\nStandard outputs of the workflow grouped by the inner workflows/tools are:     \n\n* Outputs produced by the\u00a0**UnmappedBamToAlignedBam**\u00a0inner workflow:       \n    * **Quality yield metrics** - a set of metrics used to describe the general quality of a BAM file.    \n    * **Unsorted read group quality distribution PDF**, **Unsorted read group quality distribution metrics**, **Unsorted read group quality by cycle PDF**, **Unsorted read group quality by cycle metrics**, **Unsorted read group insert size metrics**, **Unsorted read group insert size histogram PDF**, **Unsorted read group base distribution by cycle PDF**, **Unsorted read group base distribution by cycle metrics** - unsorted read group quality metrics.     \n    * **SelfSM** - a SELFSM format file with 2 rows and 19 columns. The first row contains the keys (e.g., SEQ_SM, RG, FREEMIX), while the second row contains the associated values.     \n    * **Contamination** - a float-adjusted contamination estimate for use in variant calling.           \n    * **Duplicate metrics** - a METRICS format file to which the duplication metrics will be written.       \n    * **BQSR report** - a CSV format file with gathered BQSR reports.      \n* Outputs produced by the\u00a0**AggregatedBamQC**\u00a0inner workflow:     \n    * **Aggregated alignment summary metrics** - aggregated alignment summary metrics in a text format file.    \n    * **Aggregated GC bias summary metrics**, **Aggregated GC bias PDF** and **Aggregated GC bias detail metrics** - aggregated GC bias metrics.    \n    * **Read group alignment summary metrics** - read group alignment summary metrics in a text format file.    \n    * **Read group GC bias detail metrics**, **Read group GC bias summary metrics** and **Read group GC bias PDF** - read group GC bias metrics.      \n    * **Calculate read group checksum MD5** - MD5 checksum of input BAM or SAM file.         \n    * **Aggregated bait bias detail metrics** and **Aggregated bait bias summary metrics** - aggregated bait bias metrics.      \n    * **Aggregated error summary metrics** - aggregated error summary metrics in a text format file.      \n    * **Aggregated insert size metrics** and **Aggregated insert size histogram PDF** - aggregated insert size metrics.          \n    * **Aggregated pre adapter summary metrics** and **Aggregated pre adapter detail metrics** - aggregated pre adapter metrics.      \n    * **Aggregated quality distribution metrics** and **Aggregated quality distribution PDF** - aggregated quality distribution metrics.            \n* Outputs produced by the\u00a0**BamToCram**\u00a0inner workflow:     \n    * **CRAM MD5 file** - a CRAM MD5 file.     \n    * **Validation report** - a validation report file.     \n    * **Indexed CRAM file** - an output CRAM, along with its index as a secondary file.      \n* Outputs produced by the\u00a0**CollectHsMetrics** tool:     \n    * **Hybrid selection metrics** - a TXT format file with HS metrics produced by the **CollectHsMetrics** tool.     \n* Outputs produced by the\u00a0**ConvertSequencingArtifactToOxoG** tool:     \n    * **OxoG metrics** - a TXT format file with OxoG metrics produced by the **ConvertSequencingArtifactToOxoG** tool.    \n\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**WARP TargetedSomaticSingleSample Pipeline** takes human single sample uBAM input files which will be converted into FASTQ reads and mapped to a reference file. The obtained alignment files will be passed to the quality control tools, calculating and producing different quality metrics. Output targeted somatic alignment files can be further used for variant calling or other analyses by different tools/pipelines, while metrics outputs can give various quality and statistical calculations about input data and the produced alignment file. \n\nTaking human single sample uBAM input files, together with reference and its indices, interval files, known and contaminations sites files, **WARP TargetedSomaticSingleSample Pipeline** produces an alignment CRAM file, and various metric files for unmapped inputs and aligned outputs.        \n\n\n**WARP TargetedSomaticSingleSample Pipeline** integrates several inner workflows and tools.     \n\nThe pipeline steps are briefly described below:    \n\n* First step is covered by the **GenerateSubsettedContaminationResources** tool. It is designed for processing contamination reference files, producing contamination output files covering only the targeted regions.    \n* Next steps are included in the complex **UnmappedBamToAlignedBam** inner workflow.      \n    * **UnmappedBamToAlignedBam** starts with **CollectQualityYieldMetrics** tool. It calculates quality metrics for uBAM inputs.     \n    * If uBAM input is larger than 20Gb, it will be passed to the **SplitLargeReadGroup** inner workflow. **SplitLargeReadGroup** covers the following steps:    \n        * **SamSplitter** tool splits the input alignment file so as to include 48000000 reads per file.     \n        * **SamToFastqAndBwaMemAndMba** is used for alignment.      \n        * **GatherUnsortedBamFiles** receives **SamToFastqAndBwaMemAndMba** outputs and create the final alignment output file.     \n    * If uBAM input is smaller than/equal to 20Gb, it will be passed to the following step:    \n        * **SamToFastqAndBwaMemAndMba** for alignment.    \n    * Scattered alignment files from the **SplitLargeReadGroup** or **SamToFastqAndBwaMemAndMba** will be passed to the **CollectUnsortedReadgroupBamQualityMetrics** tool. In this step unsorted alignment files' quality metrics will be calculated.     \n    * **MarkDuplicates** is able to take multiple BAM inputs and write out a single marked duplicates output.     \n    * Sorting an aggregated+deduplicated BAM file and fixing tags is done by the **SortSam** tool.      \n    * Next step depends on the optional haplotype database file. If added, **CrossCheckFingerprints** quality control step will be done.     \n    * Creating a list of sequences for scatter-gather parallelization is done by the **CreateSequenceGroupingTSV** tool. For this step, only the reference dict file is the required input.     \n    * Estimation level of cross-sample contamination is done by the **CheckContamination** tool.     \n    * **BaseRecalibrator** step is scattered by the **CreateSequenceGroupingTSV** output files. It generates the recalibration model by interval and passes it to the **GatherBqsrReports** tool.     \n    * **GatherBqsrReports** merges the recalibration reports resulting from by-interval recalibration.     \n    * **ApplyBQSR** step is scattered by the **CreateSequenceGroupingTSV** output files, as well. It applies the recalibration model by interval.     \n    * **GatherSortedBamFiles** is used to merge the recalibrated BAM files resulting from by-interval recalibration.    \n* After **UnmappedBamToAlignedBam** inner workflow, data goes to the **AggregatedBamQC** inner workflow. It calculates quality control metrics for the aggregated BAM files. **AggregatedBamQC** covers the following steps:    \n    * **CollectReadgroupBamQualityMetrics** is the first step of the **AggregatedBamQC** inner workflow. It calculates quality metrics of the final BAM file (consolidated after scattered BQSR).    \n    * **CollectAggregationMetrics** calculates additional quality metrics.     \n    * Next step depends on the optional haplotype database and fingerprint genotypes files. If added, **CheckFingerprints** quality control step will be done against the sample array.     \n    * **CalculateReadGroupChecksum** generates a checksum per read group in the final BAM file.    \n* **ConvertSequencingArtifactToOxoG** will calculate oxog metrics.     \n* **BamToCram** is another inner workflow. It covers the following steps:    \n    * **ConvertToCram** which converts the final merged recalibrated BAM file to CRAM format.    \n    * **CheckPreValidation** step checks whether the data has massively high duplication or chimerism rates.    \n    * Validation of the CRAM file is done by **ValidateSamFile**.    \n* **CollectHsMetrics** is the last step of the pipeline. It calculates Hs metrics for the aligned BAM file.       \n\n\nToolkit versions used in the pipeline are:     \n\n* genomes-in-the-cloud:2.4.7    \n* picard-cloud:2.23.8    \n* gatk:4.1.8.0    \n* python:2.7    \n* bedtools:2.27.1    \n\n\n\n### Changes Introduced by Seven Bridges\n\n**WARP TargetedSomaticSingleSample Pipeline** is designed in a manner to implement [WARP WDL #190](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/beta-pipelines/broad/somatic/single_sample/targeted/TargetedSomaticSingleSample.wdl) instructions available on [GitHub](https://github.com/broadinstitute/warp/tree/cec97750e3819fd88ba382534aaede8e05ec52df).     \n\nMinor differences are introduced in order to successfully adapt the pipeline to the **Seven Bridges** platform:    \n\n* All the scripts called by the pipeline tools are copied to **File Requirements** and their code will not be visible in the command lines.     \n* **SamToFastqAndBwaMemAndMba** step from WDL has been broken down to its components:     \n    * **Picard SamToFastq**,     \n    * **BWA MEM** and     \n    * **Picard MergeBamAlignment**.    \n* **SumFloats** tool is crucial for calculating *preemptible_tries* and needed memory. As these values are set within the main WDL *(PapiSettings papi_settings = {\"preemptible_tries\": 3, \"agg_preemptible_tries\": 3})*, it has been skipped.    \n* **Disable sanity check** parameter of the **UnmappedBamToAlignedBam** inner workflow has been exposed, so it could be switched on when needed.      \n* **CramIndex** tool has been added to the **BamToCram** inner workflow for indexing converted CRAM files.      \n* **SBG Pickvalue StrArray** tool is included in the **UnmappedBamToAlignedBam** inner workflow as a temporary fix for a bug that occurs when running code with cwltool. Running *cwltool --validate* throws a validation error which is incorrect. It seems like the pickValue field is ignored in validation.    \n* **CreateSequenceGroupingTSV** tool is altered to produce *output.json* file in order to bypass the limit of 64KB for load content and prevent any issues with cwltool run.    \n* All the output files will be named using the **Output file name prefix** (user defined) and file specific suffix. In case **Output file name prefix** is not provided, output prefix will be the same as the **Sample ID** metadata field from the input unmapped BAM file, if the **Sample ID** metadata field exists. Otherwise, output prefix will be inferred from the **Unmapped BAM** basename. If multiple flowcell-level unmapped BAMs are provided on input, **Quality yield metrics** and **Unsorted read group metrics** will be ported for each of the input BAMs. In order to avoid naming conflict if the output files have the same prefix, nameroot of the corresponding input BAM will be appended to the name prefix of the output metrics name.    \n\n\n### Common Issues and Important Notes    \n\n* **WARP TargetedSomaticSingleSample Pipeline** expects unmapped BAM (uBAM) file format as the main input. One or more read groups, one per uBAM file, all belonging to a single sample (SM).    \n* **Unmapped BAM** (`--in_reads`) - provided uBAM file should be in query-sorted order and all reads must have RG tags. Also, input uBAM files must pass validation by **ValidateSamFile**.    \n* For each tool in the workflow, equivalent parameter settings to the one listed in the corresponding WDL file are set as defaults.     \n* Setting **Disable sanity check** parameter to False might cause pipeline run to fail if the number of available markers in the input file is less than 10% of the number of markers in the reference matrix.    \n* Reference genome inputs and its indices must correspond to **Hg38** with ALT contigs.    \n\n### Limitations\n\nBoolean *provide_bam_output*,  *hard_clip_reads* and *bin_base_qualities* parameters' default values are False and they mean the following:      \n* If set to True, *provide_bam_output* parameter would cause **UnmappedBamToAlignedBam** inner workflow to port the final gathered BAM file. If this BAM file is needed, **Gathered BAM** output can be ported out in the workflow's editor and this file will be provided when running a task.    \n* *hard_clip_reads* parameter corresponds to the **Clip overlapping reads** parameter. It is set to False by default and can be changed only within the editor.           \n* *bin_base_qualities* value together with *somatic* value affects the **Static quantized quals** parameter values. The value of *somatic* is always True. The default value of *bin_base_qualities* is False, and causes **Static quantized quals** default value to be null. However, if *bin_base_qualities* set to True, **Static quantized quals** would be 10, 20, 30, 40, 50, and these values can be changed only within the editor.           \n\n### Performance Benchmarking\n\nAnalyses with the human genome **Reference** and **Reference Index TAR** files show that running time and cost mostly depend on the size of the input **Unmapped BAM**.    \nFor **WARP TargetedSomaticSingleSample Pipeline**, instance hint is set to **c5.9xlarge\u00a0AWS Instance Type**\u00a0by default.     \n\nIn the following table you can find estimates of **WARP TargetedSomaticSingleSample Pipeline** running time and cost when using different sizes of **Unmapped BAM**. \n\n                   \n|Unmapped BAM size| Running time| AWS instance | Parallel instances | Cost |\n|--------------------------------|--------------------------|--------------|---------------------|--------------------------|--------------|-----------------|------------|\n| 36GB + 37.5GB | 7h 19 min | c5.24xlarge (on-demand)  | 4 | $44.72 | \n| 36GB + 37.5GB | 10h 47min  | c5.18xlarge (on-demand) | 1 |$34.56 |\n| 36GB + 37.5GB | 7h 59min | c5.18xlarge (on-demand) | 4 | $38.09 |\n| 38.4MB + 38.5MB | 15min | c5.4xlarge (on-demand) | 1 | $0.21 | \n| 38.4MB + 38.5MB | 13min  | c5.9xlarge (on-demand) | 1 | $0.36 |\n| 3.8MB + 3.8MB  | 12min | c5.4xlarge (on-demand) | 1 | $0.17 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**WARP TargetedSomaticSingleSample Pipeline** was tested with cwltool version 3.1.20211020155521. The in_reads, in_reference, in_index_tar, in_databases, in_intervals_target, in_contamination_sites_ud, in_contamination_sites_mu, in_contamination_sites_bed, in_intervals_bait, in_reference_dict, in_fingerprint_genotypes, in_haplotype_map and disable_sanity_check inputs were provided in the job.yaml/job.json file for testing. \n\n\n### References\n\n[1] [Github page](https://github.com/broadinstitute/warp/tree/cec97750e3819fd88ba382534aaede8e05ec52df)     \n[2] [WARP WDL #190](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/beta-pipelines/broad/somatic/single_sample/targeted/TargetedSomaticSingleSample.wdl)", "input": [{"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Haplotype map", "encodingFormat": "text/plain"}, {"name": "Target interval list"}, {"name": "Contamination sites UD"}, {"name": "Contamination sites MU"}, {"name": "Contamination sites BED", "encodingFormat": "text/x-bed"}, {"name": "Reference Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Known sites", "encodingFormat": "text/x-bed"}, {"name": "Unmapped BAM", "encodingFormat": "application/x-bam"}, {"name": "Output file name prefix"}, {"name": "Fingerprint genotypes file", "encodingFormat": "application/x-vcf"}, {"name": "Interval list of baits"}, {"name": "Reference dictionary"}, {"name": "Disable sanity check"}], "output": [{"name": "Fingerprint detailed metrics"}, {"name": "Quality yield metrics"}, {"name": "Unsorted read group quality distribution PDF"}, {"name": "Unsorted read group quality distribution metrics"}, {"name": "Unsorted read group quality by cycle PDF"}, {"name": "Unsorted read group quality by cycle metrics"}, {"name": "Unsorted read group insert size metrics"}, {"name": "Unsorted read group insert size histogram PDF"}, {"name": "Unsorted read group base distribution by cycle PDF"}, {"name": "Unsorted read group base distribution by cycle metrics"}, {"name": "Cross check fingerprints metrics"}, {"name": "SelfSM"}, {"name": "Contamination"}, {"name": "Duplicate metrics"}, {"name": "BQSR report"}, {"name": "Aggregated alignment summary metrics"}, {"name": "Aggregated GC bias summary metrics"}, {"name": "Aggregated GC bias PDF"}, {"name": "Aggregated GC bias detail metrics", "encodingFormat": "text/plain"}, {"name": "Read group alignment summary metrics"}, {"name": "Read group GC bias detail metrics", "encodingFormat": "text/plain"}, {"name": "Read group GC bias summary metrics"}, {"name": "Read group GC bias PDF"}, {"name": "Calculate read group checksum MD5"}, {"name": "Aggregated bait bias detail metrics"}, {"name": "Aggregated bait bias summary metrics"}, {"name": "Aggregated error summary metrics"}, {"name": "Aggregated insert size histogram PDF"}, {"name": "Aggregated insert size metrics"}, {"name": "Aggregated pre adapter detail metrics"}, {"name": "Aggregated pre adapter summary metrics"}, {"name": "Aggregated quality distribution metrics"}, {"name": "Aggregated quality distribution PDF"}, {"name": "Fingerprint summary metrics"}, {"name": "CRAM MD5 file"}, {"name": "Validation report"}, {"name": "Indexed CRAM file"}, {"name": "OxoG metrics"}, {"name": "Hybrid selection metrics"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "codeRepository": ["https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/beta-pipelines/broad/somatic/single_sample/targeted/TargetedSomaticSingleSample.wdl"], "applicationSubCategory": ["Alignment", "Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1649411819, "dateCreated": 1649411816, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/warp-wgs-dragen-gatk-single-sample-1-0-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/warp-wgs-dragen-gatk-single-sample-1-0-cwl1-2/4", "applicationCategory": "Workflow", "name": "WARP WGS DRAGEN-GATK Single Sample", "description": "**WARP WGS DRAGEN-GATK Single Sample** is a WGS single sample processing workflow with DRAGMAP and GATK [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**WARP WGS DRAGEN-GATK Single Sample** is a CWL implementation of the Broad Institute's WARP Whole Genome Germline Single Sample workflow DRAGMAP analysis modes [1]. The workflow supports `dragen_functional_equivalence` and `dragen_maximum_quality` analysis modes from the original WDL implementation.  To process data in `dragen_functional_equivalence` mode, **Disable spanning event genotyping** and **Unmap contaminant reads** input parameters should be set to `true` and `false`, respectively. Conversely, processing data in `dragen_maximum_quality` analysis mode requires that **Disable spanning event genotyping** be set to `false` and **Unmap contaminant reads** to `true`.\n\n**WARP WGS DRAGEN-GATK Single Sample** workflow requires a number of reference inputs for a successful execution, as well as unmapped read group-level BAM files  (**Input BAM files**) with sample data. For successful task executions, the **Input BAM files** should have the **Sample ID**, **Library ID** and **Platform unit ID** metadata fields set.\n\n### Changes Introduced by Seven Bridges\n\n* This CWL implementation matches WDL default parameters and corresponds to a specific GitHub commit (78b96b0628f71862518bcdf3f280a6a376839308).\n* Toolkit versions were matched between workflows: **Picard** (2.23.8), **GATK** (4.2.2.0), **Samtools** (1.11) and **DRAGMAP** (1.2.1).\n* Large read groups (RG) are split in the WDL implementation to improve performance. This step of the original WDL workflow (SplitRG.SplitLargeReadGroup) is skipped \n* Summing up of read-group level BAM sizes to approximate the size of the aggregated BAM (Utils.SumFloats step) is also skipped as storage is not dynamically allocated.\n* READ_NAME_REGEX and SORTING_COLLECTION_SIZE_RATIO parameters of Picard MarkDuplicates are always omitted (default value).\n* **VerifyBamID2** parameters `--UDPath`, `--MeanPath` and `--BedPath` are replaced with **SVD prefix** and **SVD files** input parameters in the wrapper as these three parameters are deprecated in the current version of **VerifyBamID2**. **VerifyBamID2** tool version in the CWL workflow (2.0.1) is newer than the commit used in the WDL implementation.\n* As BQSR is not used when data is processed with DRAGMAP, WDL BQSR processing steps have been skipped: Utils.CreateSequenceGroupingTSV, BaseRecalibrator, GatherBQSRReports, ApplyBQSR and GatherBamFiles. Note: As the input to GatherBamFiles step in this implementation would be just one input file, the step has been omitted.\n* QC.CheckFingerprint step (Picard CheckFingerprint) is optional in the WDL workflow and was omitted from the CWL workflow as it requires a fingerprints input file.\n* Utils.ScatterIntervalList WDL step uses a Python script to rename the output. This script is not needed on Seven Bridges platforms and has been excluded from the workflow.\n* A utility tool **rename-and-pack-rg-qc** was added to the workflow. This tool creates a TAR.GZ archive containing read-group level outputs of **Picard CollectMultipleMetrics** and **Picard CollectQualityYieldMetrics** tools. The files in the archive are named using **Library ID** and **Platform Unit ID** metadata fields from **Input BAM files**.\n\n### Common Issues and Important Notes\n\n*  **Input BAM files** input is required and the following metadata fields should be set for all files supplied for it: **Sample ID**, **LIbrary ID** and **Platform unit ID**. These metadata fields are required for successful processing of data with multiple read groups.\n* The following reference input files are required :\n    * **Reference and hash table TAR.GZ archive**\n        Description: A TAR.GZ archive containing the reference files and hash tables for DRAGMAP.\n        Preprocessing: This reference archive can be generated by **DRAGMAP**, or by packing reference files with any tool using tar.\n    * **Reference sequence**\n        * Description: Reference FASTA file.\n        * Secondary files: FAI, DICT\n    * **Haplotype map** \n        * Description: Haplotype map for the reference genome used.\n    * **SVD files** \n        * Description: A set of reference files (UD, MU, BED, V) used by **VerifyBamID2** to estimate contamination.\n        * Preprocessing: If necessary, these files can be prepared with **VerifyBamID2**.\n    * **WGS coverage intervals** \n        * Description: Interval list used by **Picard CollectRawWgsMetrics** and **Picard CollectWgsMetrics** tools to evaluate coverage QC metrics.\n    * **Reference STR file** \n        * Description: Reference STR file for **GATK CalibrateDragstrModel**. This file can be generated with **GATK ComposeSTRTableFile**.\n    * **Calling intervals**\n        * Description: Interval list with calling intervals, used in variant calling.\n    * **dbSNP** \n        * Description: dbSNP file.\n        * Secondary files: IDX or TBI\n    * **Sequence dictionary file**\n        * Description: Sequence dictionary file used by **Picard CollectVariantCallingMetrics**.\n    * **Evaluation intervals file**\n        * Description: Interval list with evaluation intervals used by **Picard CollectVariantCallingMetrics**.\n\n* A suitable set of reference files for the GRCh38 genome assembly can be obtained from Broad Institute-hosted Google Cloud buckets. Please consult the official workflow documentation and GitHub repo [1,2] for further details.\n\n* The following input parameters should always be set before running a task:\n    * **SVD prefix** is required and should match the shared file name of the **SVD files**. This parameter is used by **VerifyBamID2**.\n    * **Unmap contaminant reads** and **Disable spanning event genotyping** input parameters are required and determine which WDL processing mode will be used (`dragen_functional_equivalence` or `dragen_maximum_quality`). To use `dragen_functional_equivalence` analysis mode, please set the **Disable spanning event genotyping** to `true` and **Unmap contaminant reads** to `false`. The opposite parameter configuration should be set to use the `dragen_maximum_quality` processing mode.\n\n* **Disable sanity check** input parameter can be ignored, as it is exposed only for testing/debugging purposes. This parameter is used by **VerifyBamID2** when sanity checking the provided sample data.\n\n### Limitations\n\n* This workflow was tested only on human data, using the GRCh38 reference genome assembly.\n\n### Performance Benchmarking\n\n**WARP WGS DRAGEN-GATK Single Sample** workflow performance depends on the size of the input data and the allocated resources. For processing human data (GRCh38), the workflow has been set up to use c5.9xlarge AWS instances for the alignment and variant calling steps and to run the duplicate marking and aligned BAM sorting on a c4.2xlarge instance. The RAM requirements for individual tools were matched to those used in the WDL, for the majority of steps.\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS 24x 2 GB BAMs | 23 h 28 mins |$26.65 + $2.95| c5.9xlarge, c4.2xlarge  -  1024 GB EBS | \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**WARP WGS DRAGEN-GATK Single Sample** was tested with cwltool 3.1.20211107152837. Running the WF locally with the full human reference was not tested due to limited hardware resources. The workflow was tested with a mocked set of input files and without the **Picard CollectWGSMetrics** and **Picard CollectRawWGSMetrics** tool nodes. Only required input parameters were included in the tests.\n\n### References\n\n[1] [WARP Whole Genome Germline Single Sample Pipeline documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Whole_Genome_Germline_Single_Sample_Pipeline/README#running-the-dragen-gatk-implementation-of-the-wgs-pipeline)\n\n[2] [WARP Whole Genome Germline Single Sample Pipeline Github Repository](https://github.com/broadinstitute/warp/tree/develop/pipelines/broad/dna_seq/germline/single_sample/wgs); commit used: 78b96b0628f71862518bcdf3f280a6a376839308", "input": [{"name": "Reference  and hash table TAR.GZ archive", "encodingFormat": "application/x-tar"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Input BAM files", "encodingFormat": "application/x-bam"}, {"name": "Haplotype map", "encodingFormat": "application/x-vcf"}, {"name": "SVD files", "encodingFormat": "text/x-bed"}, {"name": "SVD prefix"}, {"name": "WGS coverage intervals"}, {"name": "Reference STR file"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Sequence dictionary file"}, {"name": "Evaluation intervals file"}, {"name": "Disable sanity check"}, {"name": "Calling intervals"}, {"name": "Unmap contaminant reads"}, {"name": "Disable spanning event genotyping"}], "output": [{"name": "MarkDuplicates metrics file"}, {"name": "CrosscheckFingerprints metrics"}, {"name": "VerifyBamID selfSM file"}, {"name": "Read group-level hash code", "encodingFormat": "text/plain"}, {"name": "Validation report", "encodingFormat": "text/plain"}, {"name": "WGS metrics", "encodingFormat": "text/plain"}, {"name": "Raw WGS metrics", "encodingFormat": "text/plain"}, {"name": "Variant calling summary metrics", "encodingFormat": "text/plain"}, {"name": "Variant calling detailed metrics", "encodingFormat": "text/plain"}, {"name": "GC bias summary metrics - RG"}, {"name": "GC bias detailed metrics - RG"}, {"name": "GC bias - RG"}, {"name": "Alignment summary metrics - RG"}, {"name": "MD5sum for alignments"}, {"name": "Alignment summary metrics file"}, {"name": "Bait bias detailed metrics"}, {"name": "Bait bias summary metrics"}, {"name": "Error summary metrics"}, {"name": "GC bias"}, {"name": "GC bias detailed metrics"}, {"name": "GC bias summary metrics"}, {"name": "Insert size histogram"}, {"name": "Insert size metrics file"}, {"name": "Pre-adapter detailed metrics"}, {"name": "Pre-adapter summary metrics"}, {"name": "Quality distribution metrics"}, {"name": "Quality distribution"}, {"name": "Merged variants", "encodingFormat": "application/x-vcf"}, {"name": "RG-level QC files", "encodingFormat": "application/x-tar"}, {"name": "CRAM alignments"}], "softwareRequirements": ["SubworkflowFeatureRequirement", "ScatterFeatureRequirement", "MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/warp-wgs-dragen-gatk-single-sample-1-0-cwl1-2/4.png", "codeRepository": ["https://github.com/broadinstitute/warp/commit/78b96b0628f71862518bcdf3f280a6a376839308", "https://github.com/broadinstitute/warp/releases/tag/WholeGenomeReprocessing_develop"], "applicationSubCategory": ["WGS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1649412248, "dateCreated": 1649412247, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/wgbs-analysis-bitmapperbs-with-methyldackel/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/wgbs-analysis-bitmapperbs-with-methyldackel/6", "applicationCategory": "Workflow", "name": "WGBS Analysis - BitMapperBS with MethylDackel", "description": "**WGBS Analysis - BitMapperBS with MethylDackel** is developed for BS-seq reads processing and DNA methylation analyses. It is designed for WGBS (whole-genome bisufite) samples from directional protocol.\n\nThe pipeline covers four main steps: 1) quality control and adapter trimming performed by **Trim Galore!**; 2) BS-seq reads alignment by **BitmapperBS Align**;  3) alignment files processing by the tools from the **Samtools** toolkit; 4) extraction of the methylated cytosines performed by the tools from the **MethylDackel** toolkit.\n\nThe workflow has three required input files:\n\n* **FASTQ files** - input files with single-end or paired-end reads; the files can be in FASTQ, FASTQ.GZ, FQ or FQ.GZ formats;\n* **Reference file** - a reference genome, that can be in FASTA or FA format;\n* **Index folder** - a directory containing FM-indexes of the reference file, generated by the **BitMapperBS Index** tool.\n\n\nStandard outputs of the workflow are:\n\n* **Pre-trimming FastQC HTML report** and **Post-trimming FastQC HTML report** - HTML reports generated before and after trimming by the **Trim Galore!** tool;\n* **Pre-processed alignment file** - a BAM file generated by the **BitMapperBS Align** tool, sorted by name;\n* **Alignment report** - a TXT format file with alignment metrics generated by the **BitMapperBS Align** tool;\n* **Processed alignment file** - a fixed, sorted by coordinate and deduplicated BAM file acceptable for the further analysis by the **MethylDackel** tools; it has its own index BAI file;\n* **CpG BedGraph file** - a file in BEDGRAPH format, produced by **MethylDackel Extract**, calculating metrics for Cytosines in a CpG context; it is applicable for the further analysis (differential methylation);\n* **CHG BedGraph file** - a file in BEDGRAPH format, produced by **MethylDackel Extract**, calculating metrics for Cytosines in a CHG context;\n* **CHH BedGraph file** - a file in BEDGRAPH format, produced by **MethylDackel Extract**, calculating metrics for Cytosines in a CHH context; if a Cytosine is close enough to the end of a chromosome/contig such that its context can not be inferred, it is categorised as CHH [1].\n* **Merged CpG bedGraph file** and **Merged CHG bedGraph file** - files in BEDGRAPH format, produced by **MethylDackel MergeContext**, containing per-CpG/CHG metrics rather than per-Cytosine metrics;\n* **Mbias plot** - mbias graphs in SVG format that can be viewed in most modern web browsers; \n* **Per-read report** - a TXT file, produced by **MethylDackel perRead**, containing per-read CpG methylation reports.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**WGBS Analysis - BitMapperBS with MethylDackel** is designed for analyses of WGBS samples. It takes **FASTQ files**, a **Reference file** and an **Index folder** as the required inputs. Optionally, users can provide a **BED file**, a **BigWig mappability data file** and a **BBM mappability data file** to filter reads and regions for inclusion. \n\n**WGBS Analysis - BitMapperBS with MethylDackel** integrates established algorithms for alignment, quality control, data\u00a0preprocessing\u00a0and methylation calling.\u00a0It proposes **BitMapperBS Align** as an ultrafast and memory-efficient tool for the alignment step [2], and the **MethylDackel** toolkit for methylations calling. **MethylDackel Extract** produces\u00a0files with extracted methylated regions that could be used for differential methylation analysis. It groups all Cytosines into one of the three sequence contexts: CpG, CHG, and CHH (H is the IUPAC ambiguity code for any nucleotide other than G) [1]. Besides the standard output of the **MethylDackel Extract** tool, **WGBS Analysis - BitMapperBS with MethylDackel** produces per-CpG/CHG metrics by **MethylDackel MergeContext**, per-read CpG methylation reports by **MethylDackel perRead**, and methylation bias (mbias) plots by **MethylDackel Mbias**. SVG mbias plots show suggested regions for inclusion during methylation extraction. \n\n### Changes Introduced by Seven Bridges\n\n**WGBS Analysis - BitMapperBS with MethylDackel** is designed by Seven Bridges Genomics, combining fast and memory efficient tools for the WGBS analysis. The main changes are introduced on the level of tools, for the purpose of pipeline optimisation. \n\n* **Split by** parameter in the **SBG FASTQ Split CWL1.0** tool is set to split **FASTQ files** into **36** files by default;\n\n* **Link Merge Method** of **FASTQ reads** in the **SBG FASTQ MERGE CWL1.0** tool is set to **Flattened** by default;\n\n* **Pre-processed alignment file** generated by **BitMapperBS Align** is in BAM format by default, as it is more convenient for further analysis;\n\n* **Samtools Fixmate CWL1.0** is set to add mate score tag by default, required for **Samtools Markdup CWL1.0** processing;\n\n* As a part of the pipeline, the **Samtools Index CWL1.0** parameter **Output indexed data file** is set to **True**;\n\n* **MethylDackel Extract** will calculate metrics for Cytosines in CpG, CHG and CHH context by default; \n\n* Merged bedGraph files generated by **MethylDackel MergeContext** have the BEDGRAPH extension;\n\n* All tools within the workflow are set to use the maximum number of threads for the **c4.2xlarge** instance by default.\n\n\n### Common Issues and Important Notes\n\n* **FASTQ files** must have metadata fields properly set: **Sample ID** for single-end, and **Sample ID** and **Paired-end** fields for paired-end samples; **Paired-end** field should be 1 or 2;\n\n* If paired-end reads are provided, the **Paired** option in the **Trim Galore!** parameters should be set;\n\n* **Index folder** input must be generated by the **BitMapperBS Index** tool;\n\n* A secondary FAI file of the **Reference file** must be provided; **Samtools Faidx CWL1.0** is recommended for the **Reference file** indexing;\n\n* Fragments longer than 10kb are currently not handled correctly by **Methyl Dackel perRead** [1];\n\n* Although it is remarked that **MethylDackel** may be quite slow when setting a large number of threads (recommended to use **2**, **3**, **4** or **8** threads) [2], **WGBS Analysis - BitMapperBS with MethylDackel** did not show this problem during development and testing;\n\n* **WGBS Analysis - BitMapperBS with MethylDackel**\u00a0pipeline will run on **c4.2xlarge** on-demand AWS instance by default, using\u00a0**8CPUs** and **15GB RAM**; instance type can be changed before task execution in the\u00a0**Execution Settings** field; \n\n* All the tools within the workflow are set to use maximum number of threads for the **c4.2xlarge** instance by default;\n\n* The **Number of threads** parameter is linked with the following tools: **SBG FASTQ Split CWL1.0**, **BitMapperBS Align**, **Samtools Fixmate CWL1.0**, **Samtools Sort CWL1.0**, **Samtools Markdup CWL1.0**, **Samtools Index CWL1.0**, **MethylDackel Extract**, **MethylDackel perRead** and **MethylDackel Mbias**;\n\n* The **Kmers** and **Min length** parameters are linked with both **FastQC CWL 1.0** tools within the pipeline; \n\n* The **Keep discordant** parameter is linked with the **MethylDackel Extract** and **MethylDackel Mbias** tools. \n\n### Performance Benchmarking\n\nAnalyses with the human genome **Reference file** and **Index folder** show that running time and cost mostly depend on the size of the input **FASTQ files**. \n\nIn the following table you can find estimates of **WGBS Analysis - BitMapperBS with MethylDackel** running time and cost when using different sizes of **FASTQ files** and the maximum number of threads for the chosen instance. For large **FASTQ files** (> 2x 10GB), it is recommended to set up an instance type with more CPUs and RAM Memory to gain better running time and cost. **FASTQ files** ~200GB and larger require providing additional attached storage.\n\n                   \n|FASTQ files size| Running time| AWS instance | Cost |Attached storage (TB) |\n|--------------------------------|--------------------------|---------------------|--------------------------|--------------|\n| 470 MB (GZ)|  15min | c4.2xlarge (on-demand)  |  $0.14 | - |\n| 470 MB (GZ)|  7min | c5.9xlarge (on-demand)  |  $0.20 | - |\n| 2x 700 MB (GZ)|  29min | c4.2xlarge (on-demand)  |  $0.26 | - |\n| 2x 700 MB (GZ)|  12min | c5.9xlarge (on-demand)  |  $0.34 | - |\n| 2x 12 GB (GZ)|  5h 32min | c4.2xlarge (on-demand)  |  $2.96 | - |\n| 2x 12 GB (GZ)|  2h 4min | c5.9xlarge (on-demand)  |  $3.44 | - |\n| 2x 12 GB (GZ)|  1h 43min | c5.18xlarge (on-demand)  |  $5.49 | - |\n| 2x 110 GB |  11h | c4.2xlarge (on-demand)  |  $5.88 | - |\n| 2x 110 GB |  4h | c5.9xlarge (on-demand)  |  $6.67 | - |\n| 2x 220 GB |  21h 48min | c4.2xlarge (on-demand)  |  $14.67 | 2 |\n| 2x 220 GB |  8h 6min | c5.9xlarge (on-demand)  |  $14.62 | 2 |\n| 2x 220 GB |  7h 39min | c5.18xlarge (on-demand)  |  $25.52 | 2 |\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \u00a0 \n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project.\ninputs = {\n    \"in_reads\": [\"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\", ... ],\n    \"in_reference\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"in_index\": \"file_object - api.files.query(project=project_id, names=['enter_filename'])[0]\",\n    \"paired\": True\n}\ntask = api.tasks.create(name='WGBS Analysis - BitMapperBS with MethylDackel - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n\n[1] [MethylDackel GitHub](https://github.com/dpryan79/MethylDackel)\n\n[2] [BitMapperBS paper](https://www.biorxiv.org/content/10.1101/442798v1.full.pdf+html)", "input": [{"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "FASTQ files", "encodingFormat": "text/fastq"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "BigWig mappability data file"}, {"name": "BBM mappability data file"}, {"name": "Index folder"}, {"name": "Number of threads"}, {"name": "Kmers"}, {"name": "Keep discordant"}, {"name": "5' clip read 1"}, {"name": "3' clip read 2"}, {"name": "3' clip read 1"}, {"name": "Paired"}, {"name": "5' clip read 2"}, {"name": "Edit distance rate"}, {"name": "Minimun observed template length"}, {"name": "Maximum observed template length"}, {"name": "Pbat protocol"}, {"name": "Phred value"}, {"name": "Maximum mismatch penalty"}, {"name": "Minimum mismatch penalty"}, {"name": "Ambiguous character penalty"}, {"name": "Gap open penalty"}, {"name": "Gap extension penalty"}, {"name": "Sensitive mode"}, {"name": "Keep duplicates"}, {"name": "Output in the format required by methylKit"}, {"name": "Output cytosine report"}, {"name": "Output binary bismap file"}, {"name": "Output tab separated metrics"}, {"name": "Min length"}], "output": [{"name": "Pre-trimming FastQC HTML report", "encodingFormat": "text/html"}, {"name": "Cytosine report", "encodingFormat": "text/plain"}, {"name": "Output binary bismap file"}, {"name": "Pre-processed alignment file", "encodingFormat": "application/x-sam"}, {"name": "Processed alignment file", "encodingFormat": "application/x-bam"}, {"name": "Post-trimming FastQC HTML report", "encodingFormat": "text/html"}, {"name": "MethylKit"}, {"name": "CpG BedGraph file"}, {"name": "CHH BedGraph file"}, {"name": "CHG BedGraph file"}, {"name": "Per-read report", "encodingFormat": "text/plain"}, {"name": "Tab separated metrics", "encodingFormat": "text/plain"}, {"name": "Mbias plot"}, {"name": "Merged CHG bedGraph file"}, {"name": "Merged CpG bedGraph file"}, {"name": "Alignment report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ScatterFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/wgbs-analysis-bitmapperbs-with-methyldackel/6.png", "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1612362750, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/whole-exome-sequencing-gatk-2-3-9-lite/79", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/whole-exome-sequencing-gatk-2-3-9-lite/79", "applicationCategory": "Workflow", "name": "Whole Exome Analysis - BWA + GATK 2.3.9-Lite (obsolete)", "description": "This version of GATK is obsolete. Use Whole Exome Sequencing GATK4 from App gallery.\n\nWES pipeline analyzes all protein-coding genes in a genome (known as Exome). The exome is estimated to comprise ~1\u20132% of the genome, yet contains ~85% of recognized disease-causing mutations. Exome sequencing is limited to variations affecting coding regions of the genes, thus leaving potential effects on regulatory regions and various control mechanisms undetected. This characteristic determines the applicability of the pipeline to areas where changes in proteins are expected with greater probability or are of higher significance. For example, WES can be used for detecting variants (i.e. mutations) in known disease-causing genes as well as for detection of novel gene-disease associations (H L Rehm, S J Bale et al. ACMG clinical laboratory standards for next-generation sequencing, Genet Med. 2013 September ; 15(9): 733\u2013747. doi:10.1038/gim.2013.92.). \nThe pipeline is constructed following the Broad Institute best practice and utilizing Broad Institute's GATK tools. A separate step is undertaken to assess the quality of sequenced reads using Babraham Institute's tool FastQC. \nThe sequenced reads are aligned with the BWA tool after which duplicates are removed and output BAM file is sorted. The next step uses algorithms developed by the Broad Institute to improve alignment around indels followed by the re-evaluation of the qualities of sequenced bases. Generated BAM files are passed to variant calling (GATK Unified Genotyper). The detected variants are subjected to additional analysis resulting in refined, high-quality set of identified variants (for more information on how variant calling is performed, please refer to the [Broad Institute's web site](https://www.broadinstitute.org/gatk/guide/topic?name=methods)). \nIn order to run successfully this pipeline it is necessary to set the following metadata fields of the input FASTQ files: \nSample ID, Platform and Paired-end.\nBED file used in this workflow should correspond to FASTQ files being processed (capture kit BED) in order to obtain all coding variants targeted during the sequencing process. The BED file suggested for this workflow will not 100% correspond to all FASTQ files.", "input": [{"name": "Reference or TAR with BWA reference indices", "encodingFormat": "application/x-fasta"}, {"name": "SnpEff Database", "encodingFormat": "application/zip"}, {"name": "#FASTQ", "encodingFormat": "text/fastq"}, {"name": "Target BED", "encodingFormat": "application/x-vcf"}, {"name": "Known Indels", "encodingFormat": "application/x-vcf"}, {"name": "Known SNPs", "encodingFormat": "text/plain"}], "output": [{"name": "FastQC report", "encodingFormat": "text/html"}, {"name": "BaseRecalibrator plot PDF"}, {"name": "Alignment summary metrics", "encodingFormat": "text/plain"}, {"name": "Raw VCF", "encodingFormat": "application/x-vcf"}, {"name": "SnpEff summary text", "encodingFormat": "text/plain"}, {"name": "SnpEff Annotated VCF", "encodingFormat": "text/x-bed"}, {"name": "Processed BAM", "encodingFormat": "application/x-sam"}, {"name": "SnpEff summary HTML", "encodingFormat": "text/html"}], "applicationSubCategory": ["WES(WXS)"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649152605, "dateCreated": 1453799556, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/whole-exome-sequencing-bwa-gatk-4-0/43", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/whole-exome-sequencing-bwa-gatk-4-0/43", "applicationCategory": "Workflow", "name": "Whole Exome Sequencing - BWA + GATK 4.0 (with Metrics)", "description": "This Whole Exome Sequencing (WES) workflow identifies variants from a human exome experiment by using the [Broad Institute's](https://software.broadinstitute.org/gatk/best-practices/) best-practices workflow for alignment and variant calling.\n\nThe WES workflos analyzes all protein-coding genes in a genome, known as the exome. The exome is estimated to comprise ~1-2% of the genome yet contains ~85% of recognized disease-causing mutations.\n\nExome sequencing achieves better coverage compared to whole genome sequencing. However, variations affecting a gene\u2019s coding regions can leave potential effects on regulatory regions and various control mechanisms undetected. As such, the WES workflow is optimally applied to areas where changes in proteins are expected with a greater probability or are of a higher significance. For example, WES can be used for detecting variants (i.e. mutations) in known disease-causing genes as well as for the detection of novel gene-disease associations.\n\n##Workflow structure\n\nThe workflow follows the Broad Institute\u2019s best practices and utilizes the Broad Institute's GATK tools with additional quality metrics reporting tools. Sequenced reads are first aligned with the BWA-MEM Bundle tool, which optionally removes duplicates and sorts BAM files. In parallel with the alignment the assessment of the quality of sequenced reads is performed using Babraham Institute's tool, FastQC. The next step uses algorithms developed by the Broad Institute to improve the qualities of sequenced bases (Base Quality Score Recalibration - BQSR). Generated BAM files are passed to GATK HaplotypeCaller for variant calling.  Detected variants are subjected to filtering with predefined hard thresholds and annotation (SnpEff).  For more information on how variant calling is performed, please refer to the [Broad Institute's web site](https://software.broadinstitute.org/gatk/best-practices/).\n\nThe WES workflow utilizes human reference genome hg19, hg37, and hg38, as well as several public databases. All reference files must correspond to the same reference genome (HG19, GRCh 37, HG38, etc.). If some of the reference files have contigs not listed in the reference genome, the workflow cannot be executed.\n\n## Required inputs\n\n- Reference genome (uncompressed) or TAR with BWA reference indices created by BWA-INDEX tool\n- FASTQ reads Illumina paired-end reads from the sequencer\n- Target exome BED file\n- DBSNP database - Database with known variants from the population used with BQSR and for annotation in variant calling (HaplotypeCaller)\n- Mills indel database used for BQSR (VCF)\n- Known indels 1000g for BQSR\n- SnpEff database archive used for variant annotation (ZIP)\n\n\n## Outputs\n- Coverage metrics showing the distribution of the number of reads covering regions of the exome\n- Alignment metrics with statistics about the quality of the alignment\n- Aligned Reads from BWA-MEM (BAM)\n- Annotated and filtered variant calling format file (VCF)\n- Raw variant calling format file (VCF)\n- Genomic variant calling format file (gVCF)\n- FastQC Report containing reports about quality of the sequenced data (adapter contamination, quality distribution, GC content,...)\n\n## Expected workflow performance\n\nBelow, we present three benchmarks showcasing the expected variant detection and run-time performance of the WES workflow.\n \nWe use the [Genome in a bottle](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release) truth dataset and the [hap.py](https://github.com/Illumina/hap.py) variant evaluation tool to determine the sensitivity and precision of the workflow for both SNPs and Indels following the GA4GH recommended practices. We note that the observed precision and sensitivity measures are in line with the expected performance for this sample. Further, we observe that both the transition to transversion ratio for SNPs and the ratio of heterozygous to homozygous indels calls are in the expected ranges for a whole-genome context. Only the calls that have the PASS flag in the VCF filter column were used for calculating the reported performance metrics, reflecting the performance that can be expected when the workflow is used in a production scenario.\n\n| Sample            | SNP Precision | SNP Recall | SNP F-measure | INDEL Precision | INDEL Recall | INDEL F-measure |\n|-------------------|---------------|------------|---------------|-----------------|--------------|-----------------|\n| HG001 TruSeq 135x | 99.3156       | 96.4127    | 97.8426       | 77.8433         | 79.5941      | 78.709          |\n| HG002 Oslo 190x   | 99.828        | 99.4878    | 99.6576       | 92.88           | 90.3208      | 91.5825         |\n\n\nFinally, we measured the end-to-end run-time of the workflow. The total execution time for 135x sample is 2 hours and 53 minutes on the C4.2xlarge AWS instance.\n\n| Sample-coverage | Size of Gzipped FASTQs [Gb] | Alignment and BAM preparation | Variant calling | Variant filtering and annotation | TotalCost (AWS spot instance) |\n|-----------------|-----------------------------|-------------------------------|-----------------|-------------------|-------------------------------|\n| NA12878-135x    | 7.6                         | 2h 14m                        | 30m             | 6m                | 0.40                          |\n| HG002 Oslo 190x | 7.2                          | 2h 21m                         | 26m             | 6m                | 0.47                          |\n\n## Important issues\n\n- In order to complete the execution of the workflow, the following fields in the metadata of FASTQ files must be set: **Paired-end, Sample ID, Platform and Library**.\n- BWA-MEM index files are packed together with the reference genome in the TAR files which are available on SBG Public files. With that indexing step in the pipeline can be skipped and its total execution will be faster.\n- If HG38 reference and high-coverage FASTQ files (e.g. larger than 150x) are used the GATK HaplotypeCaller might fail due to lack of memory. Providing more memory to its Java virtual machine and larger instance for the workflow should be done in that case.\n- If HG38 is used it will automatically perform alt contig processing from [Broad](https://gatkforums.broadinstitute.org/gatk/discussion/8017/how-to-map-reads-to-a-reference-with-alternate-contigs-like-grch38) by including additional alt index from bwa.kit. The alignments on primary assembly reference will be done correctly and for proper alignment on alt contigs please run additional post processing.\n- This workflow might not work with filenames containing some special characters e.g. **#**, **~**.", "input": [{"name": "FASTQ", "encodingFormat": "application/x-sam"}, {"name": "1000g phase1 indels"}, {"name": "SnpEff Database", "encodingFormat": "application/zip"}, {"name": "Reference or TAR with BWA reference indices", "encodingFormat": "application/x-fasta"}, {"name": "Mills"}, {"name": "dbsnp", "encodingFormat": "text/x-bed"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "Assembly (genome version)"}], "output": [{"name": "SnpEff Annotated VCF", "encodingFormat": "text/x-bed"}, {"name": "Raw VCF", "encodingFormat": "application/x-vcf"}, {"name": "FastQC report", "encodingFormat": "text/html"}, {"name": "SnpEff summary", "encodingFormat": "text/html"}, {"name": "gVCF", "encodingFormat": "application/x-vcf"}, {"name": "coverage"}, {"name": "percentage_coverage_larger_than_20"}, {"name": "HS Metrics", "encodingFormat": "text/plain"}, {"encodingFormat": "application/x-bam"}, {"encodingFormat": "text/plain"}], "applicationSubCategory": ["WES(WXS)"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035757, "dateCreated": 1509554705, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/whole-genome-analysis-bwa-gatk-2-3-9-lite/65", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/whole-genome-analysis-bwa-gatk-2-3-9-lite/65", "applicationCategory": "Workflow", "name": "Whole Genome Analysis - BWA + GATK 2.3.9-Lite (obsolete)", "description": "This version of GATK is obsolete. Use Whole Genome Sequencing GATK4 from App gallery.\n\nWGS pipeline is used to study the complete DNA sequence of an organism (known as Genome). Although WGS generally has lower coverage than WES, this method can detect variants outside of protein-coding areas and can detect changes affecting regulatory regions and various controlling mechanisms. This characteristic allows for wider application of the pipeline, especially in cases when novel variants are expected. For example, WGS can be used when the phenotype or family history strongly implicates genetic etiology but the phenotype does not correspondent to any specific disorder for which a testing targeting a specific gene is clinically available, or in case when a genetic disorder demonstrates high degree of genetic heterogeneity (H L Rehm, S J Bale et al. ACMG clinical laboratory standards for next-generation sequencing, Genet Med. 2013 September ; 15(9): 733\u2013747. doi:10.1038/gim.2013.92.). \nThe pipeline is constructed following the Broad Institute best practice and utilizing Broad Institute's GATK tools. A separate step is undertaken to assess the quality of sequenced reads using Babraham Institute's tool FastQC. \nSequenced reads are aligned with the BWA tool after which duplicates are removed. The next step uses algorithms developed by the Broad Institute to improve alignment around indels followed by the re-evaluation of the qualities of sequenced bases. Generated SAM files are pooled together and variant calling is performed. Detected variants are subjected to additional analysis resulting in refined, high-quality set of identified variants (for more information on how variant calling is performed, please refer to the [Broad Institute's web site](https://www.broadinstitute.org/gatk/guide/topic?name=methods)).\n\nIn order to obtain optimal usage of the computational instance\u2019s resources analysis is divided into the number of jobs that corresponds to the number of \u201cchromosomal\u201d regions in the input BED file plus one job for much smaller, mitochondrial and global contigs. Splitting of BED file (Target BED) into several smaller BED files is performed by SBG Pass Intervals tool. GATK RealignerTargetCreator uses these BED files to perform scatter (parallelization of execution) on its input intervals file and outputs for each execution intervals file used by GATK Indel Realigner, which performs scatter on this input and outputs BAM file for every interval. GATK BaseRecalibrator collects all the BAM files and use only those covered with BQSR intervals string input for creating the model for base quality score recalibration (BQSR). If BQSR intervals string is not set GATK BaseRecalibrator would work for more than 20 hours on Whole genome sample. For that reason this input is set to \"required\" with the **default value of 20** meaning only chromosome number 20 will be used for creating the model for BQSR. GATK PrintReads applies quality mapping table received from GATK BaseRecalibrator to the BAMs received from GATK IndelRealigner. It also works in scatter mode set on \u201creads\u201d input (one job per BAM file). GATK UnifiedGenotyper caller scatters by BAM file received from GATK PrintReads. It performs variant calling on each of the BAMs and outputs raw variant calling file (VCF). Final steps of the workflow are re-calibrating and annotating of variants.\nWhole Genome Sequencing workflow can be used for processing several pairs of FASTQ files but all coming from the same sample, but different lanes. It is not created for processing FASTQ files coming from different samples together, but rather by processing each of these samples individually using \"batch by the sample\" on FASTQ files input and setting correctly metadata of FASTQ files. The tools SBG Pair Fastqs by metadata will split into the groups FASTQ files came from the different lanes and pass them through different jobs to BWA-MEM. Later, they will be merged in GATK IndelRealigner.\n\nIn order to complete the execution of the workflow the following fields in the metadata of FASTQ files must be set: **Paired-end, Sample ID, Platform and Library**.", "input": [{"name": "SnpEff database", "encodingFormat": "application/zip"}, {"name": "FASTQ", "encodingFormat": "text/fastq"}, {"name": "Reference or TAR with BWA reference indices", "encodingFormat": "application/x-tar"}, {"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "dbSNP", "encodingFormat": "text/plain"}, {"name": "1000g phase1 snps", "encodingFormat": "application/x-vcf"}, {"name": "1000g Omni", "encodingFormat": "application/x-vcf"}, {"name": "HapMap", "encodingFormat": "application/x-vcf"}, {"name": "BQSR intervals optimal value is 20 or chr20"}, {"name": "Mills", "encodingFormat": "text/plain"}, {"name": "1000g p1 indels", "encodingFormat": "application/x-vcf"}], "output": [{"name": "FastQC report", "encodingFormat": "text/html"}, {"name": "Picard Alignment Metrics", "encodingFormat": "text/plain"}, {"name": "BaseRecalibrator Plot"}, {"name": "SnpEff Summary text", "encodingFormat": "text/plain"}, {"name": "Annotated VCF", "encodingFormat": "text/x-bed"}, {"name": "Raw VCF", "encodingFormat": "application/x-vcf"}, {"name": "SnpEff summary HTML", "encodingFormat": "text/html"}, {"name": "Genome Coverage"}, {"name": "Coverage Per Interval"}], "applicationSubCategory": ["WGS"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1597676806, "dateCreated": 1459852872, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/whole-genome-sequencing-bwa-gatk-4-0/75", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/whole-genome-sequencing-bwa-gatk-4-0/75", "applicationCategory": "Workflow", "name": "Whole Genome Sequencing - BWA + GATK 4.0 (with Metrics)", "description": "This Whole Genome Sequencing (WGS) workflow identifies variants from a human whole-genome resequencing experiment by using the [Broad Institute's](https://software.broadinstitute.org/gatk/best-practices/) best-practices workflow for alignment and variant calling.\n\nThis workflow performs optimally on data from experiments which utilize a PCR-free library preparation protocol and targets 30x mean coverage across the genome. However, it is also suitable for a range of coverages (verified up to 150x). Although WGS generally has lower coverage than Whole Exome Sequencing (WES), this method can detect variants outside of protein-coding areas and can detect changes affecting regulatory regions and various controlling mechanisms. These characteristics allow for a wider application of the workflow, especially in cases when novel variants are expected. For example, WGS can be used if the phenotype or family history strongly implicates genetic etiology but the phenotype does not correspond to a specific disorder for which a test targeting a particular gene is clinically available.\n\n## Common Use Cases\n\nThe workflow follows the Broad Institute\u2019s best practices and utilizes the Broad Institute's GATK 4.1.0.0 tools. A separate step is undertaken to assess the quality of sequenced reads using Babraham Institute's tool, FastQC. \n\nSequenced reads are first aligned with the BWA-MEM 0.7.17 tool. Then, duplicates are removed within the same tool using biobambam2 sortmadup. The next step uses algorithms developed by the Broad Institute to do a re-evaluation of the qualities of sequenced bases. Generated BAM files are pooled together, and variant calling is performed. For more information on how variant calling is performed, please refer to the [Broad Institute's web site](https://software.broadinstitute.org/gatk/best-practices/).\n\nTo obtain the optimal usage of a computational instance\u2019s resources, analysis is divided into a number of jobs which correspond to the number of *chromosomal* regions in the input BED file plus one additional job for much smaller, mitochondrial, and global contigs. The SBG PrepareIntervals tool splits the input BED file (Target BED) into several smaller BED files. GATK BaseRecalibrator, set to use only reads from chromosome 20, collects all the BAM files and uses only those covered with the BQSR intervals string input to create the model for base quality score recalibration (BQSR). This lowers execution time while preserving a similar quality of recalibration. GATK ApplyBQSR applies quality mapping table received from GATK BaseRecalibrator to the BAMs received from BWA-MEM. GATK ApplyBQSR also works in scatter mode set on intervals input (one job per BED file) received from the SBG PrepareIntervals tool. GATK HaplotypeCaller is scattered by BAM file received from GATK ApplyBQSR and performs variant calling on each of the BAMs, outputting raw genotype variant calling files (GVCF). Each of the GVCFs is passed to GATK GenotypeGVCFs which converts it to a VCF. All GVCFs and VCFs from the intervals are merged.\n\n## Workflow performance\n\n| Sample             | Instance type (c5 instance) | Execution time | Cost (Spot) | Cost (On demand) | Instance type (c4 instance) | Execution time | Cost (Spot) | Cost (On demand) |\n|--------------------|-----------------------------|----------------|-------------|------------------|-----------------------------|----------------|-------------|------------------|\n| HG001 30x PCR-free | c5.9xlarge (HDD 700GB)      | 7:24:04        | 5.51$       | 11.49$           | c4.8xlarge (HDD 700GB)      | 8:32:28        | 4.68$       | 13.78$           |\n| HG001 30x PCR-prep | c5.9xlarge (HDD 700GB)      | 10:09:22       | 7.56$       | 15.77$           | c4.8xlarge (HDD 700GB)      | 12:59:43       | 7.11$       | 20.97$           |\n| HG001 50x PCR-free | c5.9xlarge (HDD 700GB)      | 10:35:15       | 7.88$       | 16.44$           | c4.8xlarge (HDD 700GB)      | 12:44:03       | 7.56$       | 20.54$           |\n\n## Required inputs\n\n- Reference or TAR with BWA reference indices\n- Target BED file - Chromosomal intervals of this BED are used for parallelization (scattering)\n- DBSNP database - Database with known variants from the population used with base quality score recalibration, variant calling and variant quality score recalibration\n- Mills INDEL database - Database of known indels in the population used with variant recalibration (VCF)\n- Known indels 1000g fused with BQSR\n- FASTQ reads Illumina paired-end reads from the sequencer.\n- Threads for BWA MEM (default value is 36, which is equal to the number of CPUs on the c4.8xlarge instance)\n- Threads for Sambamba sort (default value is 36, which is equal to the number of CPUs on the c4.8xlarge instance)\n- Memory per job for HaplotypeCaller (default value is 2200 MB)\n\n## Outputs\n\n- Genome coverage metrics\n- Alignment metrics\n- Genotype variant calling format file (GVCF)\n- Aligned Reads from BWA-MEM ([BAM](https://samtools.github.io/hts-specs/SAMv1.pdf))\n- Raw variant calling format file (VCF)\n- FastQC Report\n\n## Common Issues and Important Notes\n\n- In order to complete the execution of the workflow, the following fields in the metadata of FASTQ files must be set: **Paired-end, Sample ID and Platform**.\n- All reference files must correspond to the same reference genome (HG19, GRCh 37, HG38,...). If some of the reference files has contigs not listed in reference genome the pipeline cannot be executed.\n- BWA-MEM index files are packed together with the reference genome in the TAR files which are available on SBG Public files. With that indexing step in the pipeline can be skipped and its total execution will be faster.\n- If HG38 is used, it is recommended to lower the number of BWA MEM and Sambamba sort threads. For input files with ~30x coverage the recommended value for both parameters is 15, while for input files with ~50x coverage the recommended value for both parameters is 10. If threads number is not set and HG38 is used the threads for BWA-MEM will be set to 10.\n- If HG38 is used, it is recommended to increase the memory per job for HaplotypeCaller from 2048 MB to at least 4096 MB depending on the type of sample and coverage. If memory_per_job for HaplotypeCaller is not set and HG38 is used the memory for HaplotypeCaller is set 4096 MB.\n-  If HG38 is used it will automatically perform alt contig processing from [Broad](https://gatkforums.broadinstitute.org/gatk/discussion/8017/how-to-map-reads-to-a-reference-with-alternate-contigs-like-grch38) by including additional alt index from bwa.kit. The alignments on primary assembly reference will be done correctly and for proper alignment on alt contigs please run additional post processing.\n- Variant Quality Score Recalibration (VQSR) has been left out of this workflow since it is not optimized for single sample filtering and is also going to be deprecated by Broad's upcoming [deep learning](https://software.broadinstitute.org/gatk/blog?id=10996) filtering. This workflow will feature the new deep learning tool once it becomes available.\n- This workflow might not work with filenames containing some special characters e.g. **#**, **~**.", "input": [{"name": "Target BED", "encodingFormat": "text/x-bed"}, {"name": "dbsnp", "encodingFormat": "text/x-bed"}, {"name": "Mills", "encodingFormat": "text/x-bed"}, {"name": "Fastq", "encodingFormat": "text/fastq"}, {"name": "Uncompressed FASTA or TAR with BWA reference indices", "encodingFormat": "application/x-fasta"}, {"name": "Known indels 1000g bqsr", "encodingFormat": "application/x-vcf"}, {"name": "Memory Per Job"}, {"name": "Threads"}], "output": [{"name": "Genome coverage"}, {"name": "Alignment Summary Metrics", "encodingFormat": "text/plain"}, {"name": "FastQC Report", "encodingFormat": "text/html"}, {"name": "gVCF", "encodingFormat": "application/x-vcf"}, {"name": "Raw VCF", "encodingFormat": "application/x-vcf"}, {"encodingFormat": "application/x-bam"}], "applicationSubCategory": ["WGS"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035173, "dateCreated": 1501857801, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/wgs-quality-control-v2-cwl1-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/wgs-quality-control-v2-cwl1-2/5", "applicationCategory": "Workflow", "name": "Whole Genome Sequencing - Quality Control", "description": "**Whole Genome Sequencing - Quality Control** is used for quality control of WGS data. The workflow can process raw WGS data (FASTQ files), alignments (BAM files) and variant calls (VCF.GZ files)\n\nIn addition to standard QC tools [1-9], the workflow includes a small script to estimate sample biological sex from coverage (by comparing the average coverage of the X chromosome to the minimum average coverage among autosomes (test1) and the average coverage of the Y chromosome to 1/10th of the minimum average coverage among autosomes (test2).\n\nAll outputs are collated using the **SBG Collate WGS QC** tool. This is a utility clean-up tool which renames all outputs for consistency (using metadata 'sample_id' value, followed by tool-specific suffixes), creates a summary JSON file with key metrics and packs all outputs (metrics files, plots and reports) into a TAR.GZ archive. Please note that not all outputs are parsed to create the JSON file and not all metrics are included. The core list of metrics can be easily extended on request. A custom metric ('fin_coverage') was added to Picard CollectWgsMetricsWithNonZeroOutputs, which corresponds to coverage calculated as per an Illumina technical note [10]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Changes Introduced by Seven Bridges\n\n* This workflow was assembled by Seven Bridges.\n\n\n### Common Use Cases\n\n**Whole Genome Sequencing - Quality Control** workflow is intended as a general-purpose QC workflow for users processing WGS data, offering plots which can be easily inspected, as well as structured data output suitable for aggregation and parsing in an automated setup (JSON and TAR.GZ archive with all QC files). As it may be of interest to keep the cost and duration of single-sample tasks to a minimum in large-scale sequencing projects, the workflow is designed to be modular, with tool nodes turned on/off on request, or segments completely skipped (based on input data availability and user preferences). Users are encouraged to modify the workflow to suit the intended QC scope of their analysis (by choosing which tool nodes to run and adjusting tool parameters to suit their needs).\n\nInputs for the workflow are alignments (and optionally raw reads and/or variant calls) corresponding to one WGS sample. The following toolkits (versions) are used in the workflow: **BCFtools** (1.14), **FastQC** (0.11.9), **FastQ Screen**  (0.14), **GATK** (4.2.0.0), **MultiQC** (1.9), **Picard** (2.25.7), **Samtools** (1.12), **SnpEff** (4.3t) and **VerifyBamID2** (2.0.1). Further documentation on component tools included in the workflow is listed in the References section [1-9].\n\n### Common Issues and Important Notes\n\n* An input BAM file (with .BAI index) is required and should be provided either via the **Input alignments** input. The input BAM file should have the **Sample ID** metadata field ('sample_id' for API users) set.\n* **Reference sequence** input is required (FASTA with .FAI and .DICT secondary files).\n* **Input reads** and **Input variants** inputs correspond to sample raw reads (FASTQ) and variants (VCF.GZ) and are optional. If these inputs are not provided, the corresponding tools will be skipped. If using FASTQ inputs, we highly recommend compressing the inputs to reduce task durations and reduce disk storage costs. \n* **SVD preset** input is required and determines which set of files will be used by **VerifyBamID2** [5].\n* **FastQ Screen** and **Picard CollectWgsMetricsWithNonZeroCoverage** nodes can be manually skipped when running a task by setting required **Run tool** input parameters for these two nodes to False.\n* In order to make the workflow modular, some of tool-required inputs are set to optional in the workflow, but should still be provided to ensure successful task execution. This is a complete list of such inputs, grouped by corresponding tool nodes:\n\t1. FastQ Screen:\n\t\t- **Genome reference archives for aligners**\n\t\t- **Basenames for genome reference files [in order]** (should match the files supplied as **Genome reference archives for aligners**)\n\t2. Picard CollectVariantCallingMetrics:\n\t\t- **dbSNP** (in VCF.GZ format with .TBI index)\n\t3. SnpEff\n\t\t- **SnpEff database**\n\t\t- **Genome assembly** (should match the SnpEff database assembly, for example GRCh38.86 or GRCh37.75)\n* If **Samtools Stats** input parameter **Coverage threshold** is set, **Target regions (Samtools)** file input must be provided, or the task will fail.\n*  **Interval list (Picard CollectVariantCallingMetrics)** and **Interval list (Picard)** inputs should be in Picard interval list format. BED files can be converted into this format using the **GATK BedToIntervalList** tool.\n* By default, the workflow will use 1 TB of disk storage. If using FASTQ inputs and **FastQ Screen**, please consider compressing the files to avoid storage issues or increase the available storage via [workflow execution hints](https://docs.sevenbridges.com/docs/set-execution-hints-at-workflow-level). Likewise, task costs can be significantly reduced by adjusting the requested storage to the expected data amounts.\n* Duration of the **Picard CollectMultipleMetrics** tool depends on the modules used (**Program** input parameter) and can be considerably reduced by omitting the CollectGcBiasMetrics module value from the list.\n* If **FastQ Screen** is included in the analysis and the tool-default read subsetting (100000 reads) is not used, please consider increasing the number of cores and RAM allocated to the tool (**Number of threads for aligners** and **Memory per job [MB]** input parameters, respectively).\n* Please note that not all outputs are parsed to create the final output JSON file and not all tool metrics are included in it. The core list of metrics can be easily extended on request.\n* SnpEff-annotated VCF file is not output by default. If required, this output can be obtained by adding an output node in the workflow editor.\n\n### Limitations\n\n* This workflow was tested only on human data, using the GRCh38 reference genome assembly.\n\n### Performance Benchmarking\n\n* Performance of the workflow greatly depends on input sizes/coverage and the selected set of QC tools to run.\n* Most of the tools in the workflow are single-core and require 1-2 GB of RAM. The notable exceptions are **FastQ Screen**, which internally uses aligners (BWA, Bowtie2) and therefore can theoretically require 30-60 CPUs with 30+ GB RAM and **SnpEff**, which requires 6-8 GB RAM. However, by default, **FastQ Screen** operates on a subset of data (100000 reads) and is set to consume 6 cores. If minimizing task duration is important and this tool has to be run, a reasonable default for it is 10 cores (for c5.4 and equivalent instances) or 15-20 cores (for c5.9 and equivalent instances), the rationale being that the remaining cores can be used by other tools in the workflow to minimize delays.\n* Storage becomes a consideration when supplying both BAM and FASTQ inputs and running **FastQ Screen**. Cost can be optimized by adjusting the requested storage to match the QC use case and expected data amounts.\n* Task durations can be further reduced by disabling the CollectGcBiasMetrics module of **Picard CollectMultipleMetrics** at task run time.\n\nThe cost and duration of a few typical usage scenarios are listed in the following table. 'Optional nodes off' indicates that **FastQ Screen** and **Picard CollectWgsMetricsWithNonZeroCoverage** were turned off, where applicable. All other tools were run with workflow defaults. The sample set used for testing included a Simons dataset HGDP01078 sample realigned to GRCh38 by Seven Bridges  (40 GB BAM, 30x), the corresponding FASTQ.GZ files obtained with biobambam2 bamtofastq (11.6 GB) and the VCF.GZ file converted from the corresponding gVCF file (240 MB).\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| 30x file set - all tool nodes | 223 min | $1.48 + $0.51 | c4.2xlarge - 1TB EBS | \n| 30x file set - optional nodes off | 162 min | $1.08 + $0.37 | c4.2xlarge - 1TB EBS | \n| 30x BAM file only - all tool nodes | 151 min | $1.00 + $0.35 | c4.2xlarge - 1TB EBS | \n| 30x BAM file only - optional nodes off | 138 min | $0.92 + $0.32 | c4.2xlarge - 1TB EBS | \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*      \n\n\n### Portability\n\n**Whole Genome Sequencing - Quality Control** workflow was tested with cwltool 3.1.20211107152837. The `in_alignments`, `in_reference`, `run_tool`, `run_tool_1`, `svd_preset` and `disable_sanity_check` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [FASTQC documentation](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n\n[2] [FastQ Screen documentation](http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/)\n\n[3] [Samtools documentation](http://www.htslib.org/doc/samtools.html)\n\n[4] [Picard documentation](https://broadinstitute.github.io/picard/)\n\n[5] [VerifyBamID2 documentation](https://github.com/Griffan/VerifyBamID)\n\n[6] [BCFtools documentation](http://www.htslib.org/doc/bcftools.html#stats)\n\n[7] [SnpEff documentation](http://snpeff.sourceforge.net/SnpEff_manual.html)\n\n[8] [GATK VariantEval documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.2.0/org_broadinstitute_hellbender_tools_walkers_varianteval_VariantEval.php)\n\n[9] [MultiQC documentation](https://multiqc.info/docs/)\n\n[10] [Illumina technical note on coverage](https://www.illumina.com/content/dam/illumina-marketing/documents/products/technotes/hiseq-x-30x-coverage-technical-note-770-2014-042.pdf)", "input": [{"name": "Input reads", "encodingFormat": "application/x-sam"}, {"name": "FastQC contaminants", "encodingFormat": "text/plain"}, {"name": "FastQC adapters", "encodingFormat": "text/plain"}, {"name": "Genome reference archives for aligners", "encodingFormat": "application/x-tar"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Interval list (Picard)"}, {"name": "dbSNP file", "encodingFormat": "application/x-vcf"}, {"name": "Target regions (Samtools)"}, {"name": "Coverage threshold"}, {"name": "Filtering flag [-F]"}, {"name": "Exclude reads marked as duplicates from statistics"}, {"name": "Remove overlaps"}, {"name": "Required flag"}, {"name": "Optional reference stats file with expected GC content"}, {"name": "Within ancestry"}, {"name": "Disable sanity check"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Target intervals (Picard CollectVariantCallingMetrics)"}, {"name": "Sequence dictionary file"}, {"name": "SnpEff database file", "encodingFormat": "application/zip"}, {"name": "Input comparison file(s)", "encodingFormat": "application/x-vcf"}, {"name": "Include unpaired"}, {"name": "Extra arguments for submodules"}, {"name": "Program"}, {"name": "Metric accumulation level"}, {"name": "Count unpaired"}, {"name": "Coverage cap"}, {"name": "Include base quality histogram"}, {"name": "Minimum base quality"}, {"name": "Minimum mapping quality"}, {"name": "Run tool"}, {"name": "Gvcf input"}, {"name": "Do not use standard modules"}, {"name": "Eval modules to apply"}, {"name": "Require strict allele match"}, {"name": "Basenames for genome reference files [in order]"}, {"name": "Aligner for mapping"}, {"name": "Number of reads to subset inputs to"}, {"name": "Number of threads for aligners"}, {"name": "Memory per job [MB]"}, {"name": "Run tool"}, {"name": "Assembly (genome version)"}, {"name": "Set insert size plot y-axis to log10"}, {"name": "SVD preset"}, {"name": "Apply filters"}, {"name": "Collapse"}, {"name": "Samples list"}], "output": [{"name": "MultiQC HTML report", "encodingFormat": "text/html"}, {"name": "Picard CollectWgsMetricsWithNonZeroCoverage chart"}, {"name": "Plot-bamstats HTML report", "encodingFormat": "text/html"}, {"name": "QC summary JSON"}, {"name": "FastQC HTML reports", "encodingFormat": "text/html"}, {"name": "FastQ Screen HTML report"}, {"name": "Estimated sex based on coverage", "encodingFormat": "text/plain"}, {"name": "BCFtools stats plots"}, {"name": "TGZ archive with QC files", "encodingFormat": "application/x-tar"}, {"name": "Simplified JSON with metrics"}], "softwareRequirements": ["MultipleInputFeatureRequirement", "InlineJavascriptRequirement", "StepInputExpressionRequirement"], "thumbnailUrl": "https://igor.sbgenomics.com/ns/brood/images/admin/sbg-public-data/wgs-quality-control-v2-cwl1-2/5.png", "applicationSubCategory": ["Quality Control", "WGS", "CWLtool Tested"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648209631, "dateCreated": 1648209630, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/abra2-12-germline/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/abra2-12-germline/6", "applicationCategory": "CommandLineTool", "name": "ABRA2 Germline", "description": "**ABRA2 Germline** is used for INDEL realignment on single WES/WGS sample.\n\nThe **ABRA2 Germline** tool is an assembly-based realigner uses an efficient and flexible localized de novo assembly followed by global realignment to more accurately remap reads. Existing NGS read mappers have difficulty accurately mapping short reads containing complex variation (i.e. more than a single base change), thus making identification of such variants difficult or impossible. This results in enhanced performance for INDELs detection as well as improved accuracy in variant allele frequency estimation, [1]. ABRA2 produces realigned, sorted BAM and optionally index BAI file.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**ABRA2 Germline** takes **Coordinate sorted and indexed BAM** (`--in`) file and realigns it on regions provided with **BED file** (`--targets`) input, if BED file is not provided ABRA2 Germline will try to realign whole genome. In addition VCF containing known (or suspected) variant sites can be provided with parameter **Input VCF** (`--in-vcf`).\n\nSince most tools in downstream analysis require sorted and indexed BAM file, **ABRA2 Germline** produces coordinate sorted and indexed BAM file.\n\n### Changes Introduced by Seven Bridges\n\n* **ABRA2 Germline** will create index file, unless **Do not sort output** (`--nosort`) parameter is used. \n* **Log file** (`--log`) doesn't work in current version, therefore parameter is not wrapped.\n* **Temporary directory** (`--tmpdir`) is by default set to temporary_directory because it doesn't contain any useful/readable output and it is used only for debugging purposes.\n* **Keep temporary directory** (`--keep-tmp`) is omitted because of the same reason. \n\n### Common Issues and Important Notes\n\n* For WGS files amount of needed resources and optimal instance type [2] can differ with sample size. The default instance (suitable for WES data) will probably not satisfy resource requirements.\n* Output file is sorted and indexed.\n* The tmpdir may grow large. Make sure you have disk space at least equal to the input file size.\n* **BED file** (`--targets`) argument is not required. When omitted, the entire sample will be eligible for realignment.\n\n### Performance Benchmarking\n\nThe tool requires at least 15 GB of RAM and 8 CPUs in order to work properly. It will use the default instance (suitable for WES data) unless specified otherwise. For WGS amount of needed resources and optimal instance type [2] can differ with sample size. In the following table you can find estimates of the **ABRA2 Germline** running time and costs on Amazon (AWS) instances.\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Experiment type | Input size | Paired-end  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|\n| WES         | 4.5 GB     | Yes        | 23 minutes   | $0.4            | c4.2xlarge      |\n| WES               | 13.1 GB       | Yes        | 1h 2min.   | $0.8                | c4.2xlarge      |\n\n### References\n\n[1] [ABRA ncbi paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4173014/)\n\n[2] [Seven Bridges documentation](https://docs.sevenbridges.com/docs/set-computation-instances)", "input": [{"name": "Coordinate sorted and indexed BAM", "encodingFormat": "application/x-bam"}, {"name": "FASTA tar with indexes", "encodingFormat": "application/x-tar"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Memory per job (MB)"}, {"name": "Memory overhead"}, {"name": "Number of threads"}, {"name": "Use positional consensus sequence"}, {"name": "Optional file for assembled contigs"}, {"name": "Optional assembly kmer size"}, {"name": "Log level"}, {"name": "Regions with average depth exceeding this value will be downsampled"}, {"name": "Minimum mapping quality"}, {"name": "Maximum pre-pruned nodes in regional assembly"}, {"name": "Minimum base quality for inclusion in assembly"}, {"name": "Assembly minimum contig length"}, {"name": "Max number of cached reads per sample per thread"}, {"name": "Min edge pruning ratio"}, {"name": "Maximum mismatch rate"}, {"name": "Assembly minimum node frequency"}, {"name": "Minimum read candidate fraction"}, {"name": "Skip assembly"}, {"name": "Target kmers", "encodingFormat": "text/x-bed"}, {"name": "Processing window size and overlap"}, {"name": "Single end input"}, {"name": "Equally mapped alignments"}, {"name": "Contig anchor"}, {"name": "BAM compression level"}, {"name": "Max read move distance"}, {"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Maximum assembled contigs"}, {"name": "Maximum read noise"}, {"name": "Max reads per region"}, {"name": "Max reads in memory while sorting"}, {"name": "Do not sort output"}, {"name": "Soft clip contig arguments"}, {"name": "Scoring alignment"}, {"name": "Skip realignment on chromosome"}, {"name": "Do not use original indels"}, {"name": "Skip soft clipped contigs"}, {"name": "Do not use unmapped reads"}, {"name": "Unset duplicate flag"}], "output": [{"name": "ABRA coordinate sorted BAM", "encodingFormat": "application/x-bam"}, {"name": "Contigs file"}, {"name": "BAM index file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/mozack/abra2"], "applicationSubCategory": ["Assembly", "Alignment", "WES(WXS)"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1512750093, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/abra2-12-somatic/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/abra2-12-somatic/3", "applicationCategory": "CommandLineTool", "name": "ABRA2 Somatic", "description": "**ABRA2 Somatic** is used for normal/tumor INDEL realignment on WES/WGS data.\n\nThe **ABRA2 Somatic** tool is an assembly-based realigner which performs efficient, flexible and localized de novo assembly followed by global realignment in order to more accurately remap reads. Existing NGS read mappers have difficulty accurately mapping short reads containing complex variations (i.e. more than a single base change), thus making identification of such variants difficult or impossible. This results in enhanced performance for INDEL detection as well as improved accuracy in variant allele frequency estimation [1]. **ABRA2** produces realigned, sorted BAM files and optionally index BAI files.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**ABRA2 Somatic** takes **Coordinate sorted and indexed normal BAM** (`--in`) and **Coordinate sorted and indexed tumor BAM** files (`--in`) and realigns them to regions provided with a **BED file** (`--targets`) input. If the BED file is not provided **ABRA2 Somatic** will try to realign both whole samples. In addition VCF files containing known (or suspected) variant sites can be provided with the parameter **Input VCF** (`--in-vcf`).\n\nSince most tools in the downstream analysis require sorted and indexed BAM files, **ABRA2 Somatic** produces  coordinate sorted and indexed BAM files for normal and tumor samples.\n\n\n### Changes Introduced by Seven Bridges\n\n* **ABRA2 Somatic** will create index files for both tumor and normal realigned BAM files, unless the **Do not sort output** (`--nosort`) parameter is used. \n* **Log file** (`--log`) doesn't work in the current version, therefore the parameter is not wrapped.\n* **Temporary directory** (`--tmpdir`) is by default set to temporary_directory because it doesn't contain any useful/readable output and it is used only for debugging purposes.\n* The **Keep temporary directory** (`--keep-tmp`) parameter is omitted because of the same reason. \n\n### Common Issues and Important Notes\n\n* For WGS files amount of needed resources and optimal instance type [2] can differ with sample size. The default instance will probably not satisfy resource requirements.\n* Output files are sorted and indexed.\n* The tmpdir may grow large. Be sure you have disk space at least equal to the input file size.\n* **BED file** (`--targets`) argument is not required. When omitted, the entire samples will be eligible for realignment.\n\n### Performance Benchmarking\n\nThe tool requires at least 15 GB of RAM and 8 CPUs in order to work properly. It will use the default instance (suitable for WES data) unless specified otherwise. For WGS amount of needed resources and optimal instance type [2] can differ with sample size. In the following table you can find estimates of the **ABRA2 Somatic** running time and costs on Amazon (AWS) instances.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Experiment type | Input size | Paired-end | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|\n| WES         | 4.5 GB and 5.3 GB     | Yes        | 35 minutes   | $0.4            | c4.2xlarge      |\n| WES               | 11.5 GB and 13.1 GB       | Yes        | 2h 10min.   | $1.2                | c4.2xlarge      |\n\n### References\n\n[1] [ABRA ncbi paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4173014/)\n\n[2] [Seven Bridges documentation](https://docs.sevenbridges.com/docs/set-computation-instances)", "input": [{"name": "Coordinate sorted and indexed normal BAM", "encodingFormat": "application/x-bam"}, {"name": "FASTA tar with indexes", "encodingFormat": "application/x-tar"}, {"name": "BED file input", "encodingFormat": "text/x-bed"}, {"name": "Memory per job (MB)"}, {"name": "Memory overhead"}, {"name": "Number of threads"}, {"name": "Use positional consensus sequence"}, {"name": "Optional file for assembled contigs"}, {"name": "Optional assembly kmer size"}, {"name": "Log level"}, {"name": "Regions with average depth exceeding this value will be downsampled"}, {"name": "Minimum mapping quality"}, {"name": "Maximum pre-pruned nodes in regional assembly"}, {"name": "Minimum base quality for inclusion in assembly"}, {"name": "Assembly minimum contig length"}, {"name": "Max number of cached reads per sample per thread"}, {"name": "Min edge pruning ratio"}, {"name": "Maximum mismatch rate"}, {"name": "Assembly minimum node frequency"}, {"name": "Minimum read candidate fraction"}, {"name": "Skip assembly"}, {"name": "Target kmers", "encodingFormat": "text/x-bed"}, {"name": "Processing window size and overlap"}, {"name": "Single end input"}, {"name": "Equally mapped alignments"}, {"name": "Contig anchor"}, {"name": "BAM compression level"}, {"name": "Max read move distance"}, {"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Maximum assembled contigs"}, {"name": "Maximum read noise"}, {"name": "Max reads per region"}, {"name": "Max reads in memory while sorting"}, {"name": "Do not sort output"}, {"name": "Soft clip contig arguments"}, {"name": "Scoring alignment"}, {"name": "Skip realignment on chromosome"}, {"name": "Do not use original indels"}, {"name": "Skip soft clipped contigs"}, {"name": "Do not use unmapped reads"}, {"name": "Unset duplicate flag"}, {"name": "Coordinate sorted and indexed tumor BAM", "encodingFormat": "application/x-bam"}], "output": [{"name": "Coordinate sorted realigned normal BAM", "encodingFormat": "application/x-bam"}, {"name": "Contigs file"}, {"name": "Normal index file"}, {"name": "Coordinate sorted realigned tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Tumor index file", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/mozack/abra2"], "applicationSubCategory": ["Assembly", "Alignment", "WES(WXS)"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1512750093, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/admixture-1-3-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/admixture-1-3-0/4", "applicationCategory": "CommandLineTool", "name": "Admixture", "description": "**Admixture** is a tool for estimating ancestry in a model-based manner from large autosomal SNP genotype datasets.\n\n**Admixture's** inputs are binary PLINK (BED), ordinary PLINK (PED), or EIGENSTRAT (GENO) formatted files and outputs are simple space-delimited files containing the parameter estimates.\nThe default optimization method used by **Admixture** is a block relaxation algorithm. An alternative method, an **EM algorithm** is also available. **Admixture**'s computational speed opens up the possibility of using a much larger set of markers in model-based ancestry estimation and that its estimates are suitable for use in correcting for population stratification in association studies.\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.\n\n**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly**.\n\n\n\n\n\n\n\n### Common Use Cases\n- **Admixture** can be used to estimate the number of underlying populations through cross-validation.\n- Individuals of known ancestry can be exploited in supervised learning mode to yield more precise ancestry estimates.\n- By penalizing small admixture coefficients for each individual, one can encourage model parsimony, often yielding more interpretable results for small datasets or datasets with large numbers of ancestral populations.\n\n\n\n\n\n\n\n\n\n### Common Issues and Important Notes\n- **Admixture** requires binary PLINK (**.bed**), ordinary PLINK (**.ped**), or EIGENSTRAT (**.geno**) formatted files and **K value** (number of ancestral populations).\n- The associated **BIM**, **FAM**, and **MAP** files should be provided together with main input files.\n-  In order to do the Bootstrapping, the **MAP** file is required.\n \n- **Cross-validation** (`--cv`) is enabled by simply choosing the number of folds in the app settings section. The cross-validation error is reported in the admixture.log that can be found in the View stats & logs task section among other log files.\n - **Supervised learning mode** (`--supervised`) when enabled it requires an additional file with a **.pop suffix**, specifying the ancestries of the reference individuals.\n-  **Haploid data** field requires a single string argument that describes the haploid sex and the haploid chromosomes, separated by a colon. For instance, for human data, sex-chromsomes can be supplied as an argument for **Admixture** as \u201cmale:23,24\u201d.  For details on analysing the haploid data, please see the tool Software Manual documentation [1].\n\n\n\n\n\n\n\n### Changes Introduced by Seven Bridges\n- To perform **Projection analysis**, a study input **BED** ,**PED**, or **GENO** files should be provided to the **Input PLINK study files** port  and the new command line will be generated to learn the population structure from the reference panel and project the study individuals on it. For more details please see the tool documentation [1].\n- Preview of the command line with **Projection analysis mode**:\n\n```\n/opt/dist/admixture_linux-1.3.0/admixture  reference.bed 3  && for i in *.P* ; do cp \"$i\" study.3.P.in; done && /opt/dist/admixture_linux-1.3.0/admixture -P study.bed 3 > admixture.log\n```\n\n\n\n\n\n\n\n\n## Performance Benchmarking \n\n**Admixture** is tested with the files for the HapMap3 dataset from the tool documentation [2]. \n\n| BED size \t| Duration  \t| Cost  \t| Instance(AWS) \t|\n|----------\t|-----------\t|-------\t|---------------\t|\n| 1.1 MB   \t| 2 minutes \t| $0.02 \t| c4.2xlarge    \t|\n\n\n\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Admixture 1.3 Software Manual](http://dalexander.github.io/admixture/admixture-manual.pdf)\n\n[2] [Admixture Download and documentation](http://dalexander.github.io/admixture/)", "input": [{"name": "Plink files", "encodingFormat": "text/x-bed"}, {"name": "K value"}, {"name": "Cross-validation"}, {"name": "Alternative algorithm"}, {"name": "Major termination criterion"}, {"name": "Minor termination criterion"}, {"name": "Acceleration method"}, {"name": "Random seed"}, {"name": "Bootstrapping"}, {"name": "Supervised learning mode"}, {"name": "Penalized estimation"}, {"name": "Penalized estimation"}, {"name": "Haploid data"}, {"name": "Input PLINK study files", "encodingFormat": "text/x-bed"}, {"name": "CPU per job"}, {"name": "Memory per job (in MB)"}], "output": [{"name": "Output Q value"}, {"name": "Output P value"}, {"name": "Bootstrapping standard errors"}, {"name": "Penalized estimation P"}, {"name": "Penalized estimation Q"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["CWL1.1", "GWAS", "Genomics"], "project": "SBG Public Data", "creator": "David Alexander", "softwareVersion": ["v1.1"], "dateModified": 1622113397, "dateCreated": 1622113397, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/annotationdbi-select-and-mapids-1-54-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/annotationdbi-select-and-mapids-1-54-1/4", "applicationCategory": "CommandLineTool", "name": "AnnotationDbi select and mapIds", "description": "**AnnotationDbi select and mapIds** is a tool that maps one type of IDs to another. It is based on *Bioconductor* annotation data packages [1].\n\nIt takes a tabular file with identifiers in the first column as input, and outputs two CSV files: \n\n1. A file that has the same structure as the input file, except that in the first column are now ID types that were chosen to be mapped to. This file is obtained using the *mapIds* function from the Bioconductor **AnnotationDbi** package.\n\n2. Symbols mapping file, obtained by the *select* function from the Bioconductor **AnnotationDbi** package, with two columns - first one with input identifiers and the others with ID types they have been mapped to.\n\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common use cases\n\nParameters required for this tool to work are:\n\n* **Input ID Type** - type of identifiers present in the input file \n\nID types supported for input are:\n\nENSEMBL: Ensembl gene IDs\n\nENSEMBLPROT: Ensembl protein IDs\n\nENSEMBLTRANS: Ensembl transcript IDs\n\nENTREZID: Entrez gene Identifiers\n\nSYMBOL: The official gene symbol\n\nGO: GO Identifiers\n\nPATH: KEGG Pathway Identifiers\n\nREFSEQ: Refseq Identifiers\n\n\n* **Organism** -  organism that the identifiers are from\n* **Output ID Type** - type of identifiers to be present in output files\n\nID types supported for mapping include all of the types described above for **Input ID Type**, as well as:\n\nALIAS: Commonly used gene symbols\n\nEVIDENCE: Evidence codes for GO associations with a gene of interest\n\nGENENAME: The full gene name\n\nONTOLOGY: For GO Identifiers, which Gene Ontology (BP, CC, or MF)\n\n* If the file contains a header row, the parameter **Header** should be set to **Yes**.\n\n###Changes Introduced by Seven Bridges\n\n* All the rows that were not mapped and thus contain NA values will be removed from the output file. Duplicate rows can also be removed, by setting the parameter **Remove duplicates** to **Yes**.\n\n* As both *select* and *mapIds* functions don't recognize Ensembl ID versions, the script will automatically delete the version suffix prior to mapping (e.g. \"ENSMUSG00000017167.6\" will be converted to \"ENSMUSG00000017167\"), when one of Ensembl ID types is selected as the **Input ID Type**.\n\n* If the selected output ID type has multiple matches for input IDs, *select* function will return one row in the output for each possible match, while *mapIds* returns only the first match. This results in two output files that will most often have different numbers of rows (identifiers). If the parameter **Match outputs** is set to **Yes**, features that are not present in both files will be omitted.\n\n### Common Issues and Important Notes\n\n* Supported organisms are **human, mouse, fruit fly and zebrafish**. The *org.db* packages that are used are primarily based on mapping using Entrez Gene identifiers [2-6]. \n\n### Limitations\n\n* This tool was tested only with human samples.\n\n### Performance benchmarking\n\nThe execution time takes several minutes on the default instance and the price is negligible (~ 0.01$). Unless specified otherwise, the default instance used to run the **AnnotationDbi select and mapIds** tool will be c4.2xlarge (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**AnnotationDbi select and mapIds** was tested with cwltool version 3.1.20211107152837. The '-in_tabular_file', 'in_id_type, 'out_id_type', 'match_outputs', 'remove_duplicates' inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References \n[1] [AnnotationDbi](https://www.bioconductor.org/packages/devel/bioc/vignettes/AnnotationDbi/inst/doc/IntroToAnnotationPackages.pdf)\n\n[2] [org.Hs.eg.db](https://bioconductor.riken.jp/packages/3.8/data/annotation/manuals/org.Hs.eg.db/man/org.Hs.eg.db.pdf)\n\n[3] [org.Mm.eg.db](https://bioconductor.org/packages/release/data/annotation/manuals/org.Mm.eg.db/man/org.Mm.eg.db.pdf)\n\n[4] [org.Dm.eg.db](https://bioconductor.org/packages/release/data/annotation/manuals/org.Dm.eg.db/man/org.Dm.eg.db.pdf)\n\n[5] [org.Dr.eg.db](http://bioconductor.org/packages/release/data/annotation/manuals/org.Dr.eg.db/man/org.Dr.eg.db.pdf)", "input": [{"name": "Input tabular file", "encodingFormat": "text/plain"}, {"name": "Input ID Type"}, {"name": "Organism"}, {"name": "Output ID Type"}, {"name": "Header"}, {"name": "Remove duplicates"}, {"name": "Match output files"}], "output": [{"name": "Output mapping files"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Bioconductor/AnnotationDbi/tree/RELEASE_3_14"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648649349, "dateCreated": 1648649349, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/annotsv-3-0-7-cwl1-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/annotsv-3-0-7-cwl1-1/5", "applicationCategory": "CommandLineTool", "name": "AnnotSV", "description": "**Annot SV** annotates and ranks structural variants  [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**AnnotSV** can be used to annotate and interpret structural variants [1,2] in BED or VCF/VCF.GZ format (**Input variants**) based on its annotation sources (**AnnotSV data sources**). If the variants are in BED format, the tool requires the **BED SV type column** input to be set to classify variants.\n\n**AnnotSV data sources** for supported organisms (human and mouse) are available for download from the [AnnotSV website](https://lbgi.fr/AnnotSV/annotations). For a phenotype-driven analysis (using **HPO terms**) **Exomiser** reference files should also be prepared as described in the **AnnotSV** documentation [2] and provided as **AnnotSV data sources**. Please check the terms of use and licensing of individual data sources before using the tool. \n\nThe tool outputs annotated and classified variants in TSV format (**AnnotSV results** output). If a variant record was not processed by AnnotSV, it will be included in the **Unprocessed SVs** with the reason it was skipped. This file is only created if there are unprocessed records. If a report on overlaps with regulatory elements is requested (**Create a report for regulatory elements**), an additional output will be created (**Optional regulatory elements overlap report**).\n\nThe columns of the **AnnotSV results** can be customized using the **Custom AnnotSV configuration file**. If provided, this file will override the default configuration file distributed with the tool. Please note that command line parameters take precedence over values given in the configuration file. For instructions on how to prepare the custom configuration file and a file example, please see the [default **AnnotSV** configuration file](https://github.com/lgmgeo/AnnotSV/blob/6f5794ea610dea22980512f021e4ee02ef9a1e91/etc/AnnotSV/configfile).\n\nThis wrapper also includes the **knotAnnotSV** utility tool (commit 89af07a3) [3] which can be used to create an HTML report from the AnnotSV output file (**Create knotAnnotSV HTML report** input). This output can be customized by providing the **knotAnnotSV YAML configuration file** input. For details, please consult the **knotAnnotSV** documentation [3].\n\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `-annotationsDir` and `-outputDir` have been hardcoded in the wrapper.\n* Parameters `-bcftools`, `-bedtools`, `-help` and `-overwrite` have been omitted from the wrapper.\n* If a custom configuration file is provided, it will replace the default **AnnotSV** configuration file in the tool installation directory.\n* To assist in task debugging, the tool stdout and stderr streams are captured in `AnnotSV.log` file, accessible with the other task execution logs. Please consult this file if you encounter any issues with task executions.\n* The wrapper includes the **knotAnnotSV** utility tool (commit 89af07a3). The following input parameters have been omitted from the wrapper: `--outDir`, `--outPrefix`, and `--datatableDir`. **Create knotAnnotSV HTML report** input should be used to obtain the corresponding output.\n* As the wrapper uses the current working directory of the task as the `-annotationsDir`, if using **Exomiser** data or a custom archive for the **AnnotSV data sources** input, please ensure that your data source archives reproduce the folder structure the tool expects once unpacked (e.g., `./Annotations_Exomiser/2007/2007_hg19/` and `./Annotations_Exomiser/2007/2007_phenotype/`).\n\n### Common Issues and Important Notes\n\n* **Input variants** and **AnnotSV data sources** inputs are required.\n* **AnnotSV** requires the SV type (e.g., DEL or DUP) to classify SVs. For BED inputs, the **BED SV type column** should be set to obtain classifications.\n* Unprocessed variant records are reported in the **Unprocessed SVs** output, with the accompanying reason for skipping.\n\n### Performance Benchmarking\n\nTypical **AnnotSV** task executions take 5-20 minutes ($0.05 - $0.15) on a c4.2xlarge AWS on-demand instance. The performance of the tool depends on the number of SV events being processed. Disk storage of <50 GB was sufficient for all testing tasks.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**AnnotSV** was tested with cwltool version 3.1.20211107152837. The `in_data_sources` and `in_variants` inputs were provided in the job.yaml/job.json file and used for testing. Cwltool command line parameters `--no-read-only` and `--no-match-user` were required for successful tool execution.\n\n### References\n\n[1] [AnnotSV publication](https://academic.oup.com/bioinformatics/article/34/20/3572/4970516)\n\n[2] [AnnotSV documentation](https://lbgi.fr/AnnotSV/Documentation/README.AnnotSV_latest.pdf)\n\n[3] [knotAnnotSV documentation - commit 89af07a3](https://github.com/mobidic/knotAnnotSV/tree/89af07a35d942cf4cf409ab7173ffa848e5c4af8)", "input": [{"name": "AnnotSV data sources", "encodingFormat": "application/x-tar"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Annotation mode"}, {"name": "Candidate genes", "encodingFormat": "text/plain"}, {"name": "Candidate genes filtering"}, {"name": "Filtered files for compound heterozygosity", "encodingFormat": "application/x-vcf"}, {"name": "Samples from filtered VCF inputs"}, {"name": "External gene annotations"}, {"name": "Genome build"}, {"name": "HPO terms"}, {"name": "Include CI"}, {"name": "Metrics"}, {"name": "Minimum number of individuals tested"}, {"name": "Output file name prefix"}, {"name": "Minimum overlap [%]"}, {"name": "Promoter size"}, {"name": "Rank filtering"}, {"name": "Reciprocal overlap"}, {"name": "Create a report for regulatory elements"}, {"name": "BED column reporting sample IDs"}, {"name": "False positive discovery files", "encodingFormat": "application/x-vcf"}, {"name": "Only use FILTER PASS variants"}, {"name": "Sample names from VCF files"}, {"name": "Extract SV INFO fields"}, {"name": "SV minimum size [bp]"}, {"name": "BED SV type column"}, {"name": "Origin of transcripts"}, {"name": "Preferred gene transcripts", "encodingFormat": "text/plain"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Custom AnnotSV configuration file"}, {"name": "Create knotAnnotSV HTML report"}, {"name": "knotAnnotSV YAML configuration file"}, {"name": "knotAnnotSV genome build"}, {"name": "knotAnnotSV LOEUF bin colour range"}], "output": [{"name": "AnnotSV results"}, {"name": "Unprocessed SVs"}, {"name": "Optional regulatory elements overlap report"}, {"name": "knotAnnotSV HTML report", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/lgmgeo/AnnotSV", "https://github.com/lgmgeo/AnnotSV/releases/tag/v3.0.7"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Veronique Geoffroy, Thomas Guignard", "softwareVersion": ["v1.1"], "dateModified": 1648045276, "dateCreated": 1619094420, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/augmented-filter-vep-101-0-cwl1-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/augmented-filter-vep-101-0-cwl1-0/2", "applicationCategory": "CommandLineTool", "name": "Augmented Filter VEP CWL1.0", "description": "**Augmented Filter VEP** tool is a customized wrapper of the `filter_vep` script from the **ensembl-vep** toolkit, modified to allow GNU parallel-scattered filtering of VEP-annotated VCFs split on chromosomes. \n\nThe only required input for this tool is **Input file** (`--input_file`), which should originate from a **Variant Effect Predictor** run (VCF file). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Augmented Filter VEP** tool is intended for filtering **Variant Effect Predictor** output (annotated VCF files) using customizable filters, as described in the [documentation](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html).\n\n### Changes Introduced by Seven Bridges\n\n- Performance was improved by splitting the input VCF file on chromosomes and running concurrent GNU parallel [2]-managed **Filter VEP** processes.\n- The **Run filtering (gene list + custom filters)** input is a convenience toggle flag indicating that the supplied input VCF file should be filtered, allowing pass-through mode for the tool, if set to False.\n- As convenience filters, five gene lists are provided: 59 ACMG-recommended secondary findings (v2.0) genes [3], 114 cancer predisposition genes collated by Rahman [4], 190 actionable cancer genes reported in 2015 for JAX-CPT [5], 1158 human MitoCarta 2.0 genes [6] (accessed February 2018), and 1751 Development Disorder Genotype - Phenotype Database (DDG2P) genes [7] (accessed February 2018). Users can supply their own gene lists for filtering by setting the **Filter set to use** input to \"Custom gene list as file\" and providing a file with gene symbols to filter on (one gene symbol per line) as the **User-defined gene list file** input.\n- The `--ontology` parameter was not included in the Seven Bridges version of the tool since it requires a database connection.\n- A simple summary file is provided, with counts of gene symbol occurrences in the filtered VCF (genes with count 0 are omitted). \n\n\n### Common Issues and Important Notes\n\n* **Input file** (`--input_file`) which should originate from a **Variant Effect Predictor** run.\n* Please note that the script expects VEP annotations to be added under CSQ entry in the INFO field. If a custom name for the VEP annotation field is used, annotation subfields will not be accessible for filtering unless the field name is replaced with CSQ or the file is re-annotated.\n* Multiple filters are treated as if joined with logical ANDs (all filters must pass for a line to be printed) [1]. Each filter should be given as an unquoted string.\n* The **User-defined gene list file** input should contain one gene symbol per line.\n\n### Performance Benchmarking\n\nFiltering VEP-annotated NA12878 genome (GRCh38, ~100 Mb as VCF.GZ file) took 6-9 minutes (depending on the filter set) using the c4.2xlarge instance.\n\n### References\n\n[1] [Documentation](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html)\n\n[2] [GNU parallel](https://www.gnu.org/software/parallel/)\n\n[3] [ACMG-recommended secondary findings (v2.0) genes](https://www.nature.com/articles/gim2016190)\n\n[4] [Cancer predisposition genes; Rahman 2014](https://www.ncbi.nlm.nih.gov/pubmed/24429628)\n\n[5] [JAX-CPT-reported actionable genes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481190/)\n\n[6] [Human MitoCarta 2.0](https://www.broadinstitute.org/files/shared/metabolism/mitocarta/human.mitocarta2.0.html)\n\n[7] [DDG2P database genes](https://decipher.sanger.ac.uk/ddd#ddgenes)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Filters to apply"}, {"name": "Run filtering (gene list + custom filters)"}, {"name": "Filter set to use"}, {"name": "Memory to use for the task [MB]"}, {"name": "Number of CPUs"}, {"name": "User-defined gene list file", "encodingFormat": "text/plain"}], "output": [{"name": "Output file"}, {"name": "Custom filtering summary", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-vep"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Ensembl", "softwareVersion": ["v1.0"], "dateModified": 1648039727, "dateCreated": 1617277730, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/ballgown-2-8-4/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/ballgown-2-8-4/11", "applicationCategory": "CommandLineTool", "name": "Ballgown", "description": "Ballgown is an R package designed to facilitate flexible differential expression analysis of RNA-Seq data. It also provides functions to organize, visualize, and analyze the expression measurements for your transcriptome assembly. It can be used for comparison between two or more groups as well as for timecourse experiments. [1]\n\n__Ballgown__ Seven Bridges app will perform testing for differential expression on gene and transcript level using FPKM expression measurement. It will output HTML summary report containing useful statistics and plots for both genomic features (genes and transcripts) as well as raw tables containing p-values and q-values for each genomic feature tested for differential expression.\n\nExpression measurements in __Ballgown__ compatible format can be obtained by Seven Bridges public app __StringTie__ or workflow __RNA-Seq Quantification (HISAT2, StringTie)__. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n* __Ballgown__ can be used to perform two or multi-group testing for differential gene and transcript expression.\n* __Ballgown__ can be used for time course experiments (testing if expression profiles of genomic features vary over time or over another continuous variable in the study). In this use case phenotype data table (check bellow for details) must contain a column with numerical values representing variable of interest which is time or some other continuous variable. In case of time course experiment parameter __Timecourse__ should be set to True.\n* To obtain expression data in __Ballgown__ input format  __StringTie__ app or __RNA-Seq Quantification (HISAT2, StringTie)__ workflow should be used.\n\n### Changes Introduces by Seven Bridges\n\n* Besides standard tables containing p and q-values for each genomic feature, __Ballgown__ produces a short HTML report containing visualized results of testing for differential expression. The report contains various statistics like box-plots of estimated expression measurement (FPKM) per sample, histograms of p-values and q-values, volcano plots and exon-intron structure of 5 most differentially expressed genes. All plots in the report are made using built-in functions in R packages 'ggplot2' and 'ballgown'.\n* As a preprocessing step, that comes just before testing for differential expression, variance filtration is being conducted. Purpose of this step is to remove zero-count or low-abundance genes which occur very often in RNA-Seq data sets. That said, transcripts with variance smaller than a given threshold (default is 1) among different samples will be excluded. To select threshold for variance filter use __Minimum abundance variance across samples__ parameter. For more details regarding implementation of variance filter refer to [2].\n\n### Common Issues and Important Notes\n\n* Phenotype information can be provided in one of the following two ways:\n\n     1. By including a CSV file (**Phenotype data** input) that contains a row for each sample, with Sample ID in the first column. These Sample IDs need to match those in input files metadata. Also, a single line header with variable names should be included. \n     2. By indicating API keys for metadata fields that need to be included in the design. Phenotype information will then consist of variables you listed as **Covariate of interest** and **Control variables**.\n\nAn example of CSV content can be found below:\n\n```\nsample_id,library,sex,condition\ntreated1,paired-end,male,treated\ntreated2,single-end,male,treated\ntreated3,paired-end,female,treated\nuntreated1,single-end,male,untreated\nuntreated2,paired-end,female,untreated\nuntreated3,paired-end,female,untreated\nuntreated4,paired-end,male,untreated\n```\n\nSupplying a CSV like this while entering \"condition\" for the value of the **Covariate of interest** parameter and \"library\" in **Control variables** will test for differential expression between treated and untreated samples, while controlling for effect of library preparation.\n\nThe information about sample belonging to the treated or the untreated group can also be kept in the metadata. To use a metadata field for splitting the samples into groups for testing, enter its metadata key for the **Covariate of interest** parameter. All the input files need to have this metadata field populated. To control for possible confounders, enter their API keys as **Control variables**.\n\n* Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and it's field be populated in all the samples (**Expression data**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. \n\n* Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end please refer to the table on [this link](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). To learn how to add custom metadata field to expression data files refer to the [following document](https://docs.sevenbridges.com/docs/format-of-a-manifest-file) (section: _Modifying metadata via the visual interface_).\n\n* Be careful when choosing covariates - generalized linear model fitting will fail if model matrix is not full rank!\n\n### Performance Benchmarking\n\nThe execution time for performing differential expression analysis on 12 samples (6 in each of two group) takes 3-4 minutes on the default instance; the price is negligible (~ 0.03$). Unless specified otherwise, the default instance used to run the __Ballgown__ tool will be c4.2xlarge (AWS).\n\n### References\n\n[1] [Ballgown GitHub page](https://github.com/alyssafrazee/ballgown)\n\n[2] [HISAT2, StringTie, Ballgown protocol paper](https://www.nature.com/articles/nprot.2016.095)", "input": [{"name": "Expression data", "encodingFormat": "application/x-tar"}, {"name": "Covariate of interest"}, {"name": "Timecourse"}, {"name": "Control variables"}, {"name": "Degrees of freedom"}, {"name": "Logarithmic transformation"}, {"name": "Minimum abundance variance across samples"}, {"name": "Analysis title"}, {"name": "Phenotype data"}], "output": [{"name": "Ballgown result table for transcript level", "encodingFormat": "text/plain"}, {"name": "Ballgown result table for gene level", "encodingFormat": "text/plain"}, {"name": "HTML report", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/alyssafrazee/ballgown/blob/master/README.md", "https://github.com/alyssafrazee/ballgown"], "applicationSubCategory": ["Differential Expression", "RNA-Seq"], "project": "SBG Public Data", "creator": "Johns Hopkins University,Center for Computational Biology", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033583, "dateCreated": 1523970158, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bam2nuc/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bam2nuc/1", "applicationCategory": "CommandLineTool", "name": "Bam2Nuc", "description": "**Bam2Nuc** is the tool which accepts BAM/CRAM files, calculates the mono- and di-nucleotide coverage of the reads and compares it to the average genomic sequence composition. Bam2Nuc handles both Bismark single-end and paired-end files (determined automatically). It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe Bam2Nuc has two required inputs:\n\n* **Bisulfite genome** is a compressed folder (in .TAR.GZ format), which contains reference genome. FASTA file and two subfolders with two conversions (C->T and G->A) of the original reference genome. \n\n* **Input file** is a file in BAM/CRAM format, which is generated by **Bismark**.\n\nThe tool Bam2Nuc can generate three outputs:\n\n* **Nucleotide coverage report**  is a file in TXT format which contains mono- and di-nucleotide coverage of the reads in comparison to the genomic sequence. It is generated if parameter **Genomic composition** (`--genomic_composition_only`) is not set to TRUE.\n\n* The output **Bisulfite genome with nucleotide freq file** is optional and it will be generated if file *genomic\\_nucleotide\\_frequencies.txt* doesn't exist in genome folder. This output represents compressed genome folder with prefix name  *with_nuc_freq_* with the mentioned file which will be written in it.\n\n* **Genomic Nucleotide Frequencies** is a file in TXT format. It is generated if parameter **Genomic composition** (`--genomic_composition_only`) is set to TRUE.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Bam2Nuc** is the tool which generates the file with the mono- and di-nucleotide coverage of the reads and the average genomic sequence composition (file genomic\\_nucleotide\\_frequencies.txt could be generated by the tool **Bismark Genome Preparation** and found in compressed genome folder (.TAR.GZ format) or by setting **Genomic composition** (`--genomic_composition_only`) to TRUE). Since the calculation of the average genomic (di-)nucleotide composition may take a while, Bam2Nuc attempts to write out a file called *genomic\\_nucleotide\\_frequencies.txt* to the genome folder if it wasn't there already and generates outputs **Nucleotide coverage report** (**Genomic Nucleotide Frequencies** (`--genomic_composition_only`) is not set to TRUE) and **Bisulfite genome with nucleotide freq file**. **Nucleotide coverage report** will not be created if  **Genomic Nucleotide Frequencies** is set to TRUE. Reads with InDels are not taken into consideration. Mono- or Dinucleotides which contains Ns are ignored as well.\n\n\n### Changes Introduced by Seven Bridges\n\n* The tool requires .TAR.GZ bisulfite genome folder generated by the tool **Bismark Genome Preparation**, instead of providing the name of genome folder (`--genome_folder` <path>). The name of the folder is taken from provided .TAR.GZ bisulfite genome folder.\n\n* If the output **Bisulfite genome with nucleotide freq file** is not generated it means that file *genomic\\_nucleotide\\_frequencies.txt* exists in provided input **Bisulfite genome**.\n\n### Common Issues and Important Notes\n\n* If **Genomic composition** (`--genomic_composition_only`) is set to TRUE, only  **Genomic Nucleotide Frequencies** will be created.\n\n* If the file genomic\\_nucleotide\\_frequencies.txt doesn't exist in the genome folder (it is not created by the tool **Bismark Genome Preparation**), the tool will create the output **Bisulfite genome with nucleotide freq file**.\n\n* The output **Nucleotide coverage report** can be used as input of the tool **Bismark2Report** for creating graphical HTML report page of whole Bismark analysis.\n\n### Performance Benchmarking\n\n| Genomic nuceotide frequence | Reference type (Size .TAR.GZ) | Total input size (BAM) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- | --- |\n| Exist | Human (8.4 GB) | 1.9 GB | c4.2xlarge | 19m | $0.17 |\n| Exist | Human (8.4 GB) | 33 GB | c4.2xlarge | 4h 14m | $2.27 |\n| Not exist | Human (8.4 GB) | 33 GB | c4.2xlarge | 4h 53m | $2.62 |\n| Exist | Human (12.2 GB) | 33 GB | c4.2xlarge | 5h 28m | $2.93 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Bisulfite genome", "encodingFormat": "application/x-tar"}, {"name": "Genomic composition"}, {"name": "Input file", "encodingFormat": "application/x-bam"}], "output": [{"name": "Nucleotide coverage report", "encodingFormat": "text/plain"}, {"name": "Bisulfite genome with nucleotide freq file", "encodingFormat": "application/x-tar"}, {"name": "Genomic Nucleotide Frequencies", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "File Format Conversion"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561065, "dateCreated": 1519389842, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bamtools-index-2-4-0/31", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bamtools-index-2-4-0/31", "applicationCategory": "CommandLineTool", "name": "BamTools Index", "description": "BamTools Index creates an index file (BAI or BTI) for a BAM file. If BAI file is present on the input the tool will skip indexing step and output BAM with provided BAI file.\n\n**Common issues:** Providing a BAI file on input will result in a pass-through without execution, even if a different index format is requested on the output (BTI instead of BAI).", "input": [{"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "BTI format"}, {"name": "Input BAI(BAM index) file"}, {"name": "Don't output indexed data file"}], "output": [{"name": "Output BAM file", "encodingFormat": "application/x-bam"}, {"name": "Generated index file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pezmaster31/bamtools", "https://github.com/pezmaster31/bamtools/wiki"], "applicationSubCategory": ["Indexing", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Derek Barnett, Erik Garrison, Gabor Marth, and Michael Stromberg", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799867, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bamtools-merge-2-4-0/21", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bamtools-merge-2-4-0/21", "applicationCategory": "CommandLineTool", "name": "BamTools Merge", "description": "BamTools Merge merges multiple BAM files into a single file.", "input": [{"name": "Input BAM files", "encodingFormat": "application/x-bam"}, {"name": "Region of interest"}], "output": [{"name": "Output BAM file", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pezmaster31/bamtools", "https://github.com/pezmaster31/bamtools/wiki"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Derek Barnett, Erik Garrison, Gabor Marth, and Michael Stromberg", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799132, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bamtools-split-2-4-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bamtools-split-2-4-0/11", "applicationCategory": "CommandLineTool", "name": "BamTools Split", "description": "BamTools Split splits a BAM file based on a user-specified property. It creates a new BAM output file for each value found.\n\n**Warning:** Splitting  by tags or reference can output a large number of files.\n\n**Common issues:** Splitting by tag can produce no output if the selected tag doesn't exist in the BAM file.", "input": [{"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "Reference prefix"}, {"name": "Tag prefix"}, {"name": "Tag split"}, {"name": "Split Options"}], "output": [{"name": "Output BAM files", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pezmaster31/bamtools", "https://github.com/pezmaster31/bamtools/wiki"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Derek Barnett, Erik Garrison, Gabor Marth, and Michael Stromberg", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799503, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-annotate/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-annotate/8", "applicationCategory": "CommandLineTool", "name": "Bcftools Annotate", "description": "**BCFtools Annotate**: Add or remove annotations. The columns ID, QUAL, FILTER, INFO and FORMAT can be edited, added or removed.\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\n* Remove fields: Provide comma separated strings you want to remove to the **Remove annotations** (`--remove`).\n```\n$bcftools annotate -x ID,INFO/DP,FORMAT/DP file.vcf.gz\n```\n\n* Add ID, QUAL and INFO/TAG, not replacing TAG if already present: Add ID,QUAL,+TAG in **Columns list** (`--columns`) input.\n```\n$bcftools annotate -a src.bcf -c ID,QUAL,+TAG dst.bcf\n```\n\n* Annotate from a BED file: Add a BED file on **Annotation file** (`--annotations`) input, add header file on the **Header lines** (`--header-lines`) input and list of columns to be annotated in **Columns list** (`--columns`) input.\n```\n$bcftools annotate -a annots.bed.gz -h annots.hdr -c CHROM,FROM,TO,TAG input.vcf\n```\n\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n* If BED file is given on input for annotation, we added subsequent `bgzip` and `tabix` commands if a BED file is provided on input. If VCF.GZ is given on input only indexing will be done.\n\n### Common Issues and Important Notes\n\n * It is expected for annotation file to be already gzipped and indexed. Otherwise, tool will fail. \n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "text/x-bed"}, {"name": "include expression"}, {"name": "Exclude expression"}, {"name": "Filter mode"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Samples list"}, {"name": "Samples file"}, {"name": "Threads"}, {"name": "Annotation file", "encodingFormat": "text/x-bed"}, {"name": "Columns list"}, {"name": "Header lines"}, {"name": "Assign set ID (format)"}, {"name": "Rename chromosomes", "encodingFormat": "text/plain"}, {"name": "Remove annotations"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Keep sites"}, {"name": "Collapse duplicate positions"}], "output": [{"name": "Annotated output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758802, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-call/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-call/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Call", "description": "**BCFtools Call**: replaces the former BCFtools View caller. \n\n\nSome of the original functionality has been temporarily lost in the process of transition under htslib, but will be added back due to popular demand. The original calling model can be invoked with the **Consensus caller** (`--consensus-caller`) input set to _Consensus_.\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nAfter creating an mpileup VCF with **BCFtools Mpileup**,  the user can call variants with **BCFtools Call** with the **Output variant sites only** (`--variants-only`) option set to True in order to output only variant sites, and with **Output type** (`--output-type`) to set the type of output.\n```\n$bcftools call --multiallelic-caller --variants-only --output-type b -output calls.bcf\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * Mpileup VCF has to be given as the input in order to successfully call variants, otherwise the tool will fail.\n\n### Performance Benchmarking\n\nIt took 4 minutes to execute this tool on AWS c4.2xlarge instance using an input of 332.3 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Threads"}, {"name": "Ploidy - Assembly list"}, {"name": "Ploidy file name"}, {"name": "Llist of samples to include"}, {"name": "Samples file"}, {"name": "Output all alternate"}, {"name": "List of format fields"}, {"name": "Output also gVCF blocks (parameter INT is the minimum per-sample depth)"}, {"name": "Insert missed sites"}, {"name": "Keep masked reference"}, {"name": "Skip variants (snps|indels)"}, {"name": "Output variant sites only"}, {"name": "Consensus caller"}, {"name": "Constrain alleles|trio"}, {"name": "Novel rate (list of coma-separated floats)"}, {"name": "Pval threshold (float)"}, {"name": "Expected substitution rate (float)"}, {"name": "Haploid output for male samples (Chromosome X)"}, {"name": "Haploid output for males and skips females (Chromosome Y)"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Prior allele frequency"}], "output": [{"name": "Output call file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-cnv/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-cnv/6", "applicationCategory": "CommandLineTool", "name": "Bcftools CNV", "description": "**BCFtools CNV**: Copy number variation caller, requires Illumina's B-allele frequency (BAF) and Log R Ratio intensity (LRR). \n\n\nThe tool uses Hidden Markov Model (HMM) to call copy number variants. The HMM considers the following copy number states: CN 2 (normal), 1 (single-copy loss), 0 (complete loss), 3 (single-copy gain).\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\n\nThe strength of this CNV caller is in the pairwise calling mode which was designed to detect differences between two samples. This greatly helps reduce the number of false calls and also allows the user to distinguish between normal and novel copy number variation.\n```\n$bcftools cnv --control-sample conrol_sample --query-sample query_sample --plot-threshold 0 file.vcf\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * If TAB files produced by BCFtools CNV are empty, no plots will be created.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 9.1 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Control sample"}, {"name": "Allele frequency"}, {"name": "Output directory"}, {"name": "Plot above quality"}, {"name": "Regions"}, {"name": "Regions File", "encodingFormat": "text/x-bed"}, {"name": "Query sample"}, {"name": "Targets"}, {"name": "Targets File"}, {"name": "Aberrant cells"}, {"name": "BAF weight"}, {"name": "BAF deviation"}, {"name": "Error probability"}, {"name": "LRR Weight"}, {"name": "LRR Smooth window"}, {"name": "Optimize"}, {"name": "Prior probablity"}, {"name": "XY probability"}], "output": [{"name": "Tab files"}, {"name": "Plot files"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-concat/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-concat/7", "applicationCategory": "CommandLineTool", "name": "Bcftools Concat", "description": "**BCFtools Concat**: Concatenate or combine VCF/BCF files. All source files must have the same sample columns appearing in the same order. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nCan be used, for example, to concatenate chromosome VCFs into one VCF, or combine a SNP VCF and an INDEL VCF into one. The input files must be sorted by chr and position. The files must be given in the correct order to produce sorted VCF on the output unless the **Allow overlaps** (`--allow-overlaps`) option is specified. \n\nWith the **Naive option** (`--naive`) set to True, the files are concatenated without being recompressed, which is very fast but dangerous if the BCF headers differ. \n```\n$bcftools concat --allow-overlaps --naive vcf_file1.vcf.gz vcf_file2.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added suvsequent `bgzip` and `index` commands afterwards if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n \n\n### Common Issues and Important Notes\n\n * All VCF files must have same sample names, otherwise tool will fail.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using two inputs sized 12.4 MB and 56 KB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Threads"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Allow overlaps"}, {"name": "Compact phase set"}, {"name": "Remove duplicates"}, {"name": "Remove all duplicates"}, {"name": "Ligate phased VCF"}, {"name": "No version"}, {"name": "Naive concat"}, {"name": "Min phase quality"}, {"name": "Regions"}, {"name": "Regions File", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Concatenated output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-consensus/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-consensus/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Consensus", "description": "**BCFtools Consensus**: Create a consensus sequence by applying VCF variants to a reference FASTA file. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nBy default, the program will apply all ALT variants to the reference FASTA to obtain the consensus sequence. Can be also used with the **SAMtools Faidx** tool where the desired part of reference can be extracted and then provided  to this tool.\n\n\nUsing the **Sample** (`--sample`) (and, optionally, **Haplotype** (`--haplotype`) option will apply genotype (haplotype) calls from FORMAT/GT. \n```\n$bcftools consensus -s NA001 -f in.fa in.vcf.gz > out.fa\n```\n\nApply variants present in sample \"NA001\", output IUPAC codes using **Output in IUPAC** (`--iupac-codes`) option\n```\nbcftools consensus --iupac-codes -s NA001 -f in.fa in.vcf.gz > out.fa\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequet bgzip and index commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n* By default, the program will apply all ALT variants to the reference FASTA to obtain the consensus sequence. \n\n * If the FASTA sequence does not match the REF allele at a given position, the tool will fail.\n\n### Performance Benchmarking\n\nIt took 5 minutes to execute this tool on AWS c4.2xlarge instance with a 56 KB VCF and a 3 GB reference FASTA file. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Include expression"}, {"name": "Exclude expression"}, {"name": "Output file name"}, {"name": "Sample"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Chain file"}, {"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Haplotype"}, {"name": "Output in IUPAC"}, {"name": "Mask file"}, {"name": "Missing genotypes"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-convert/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-convert/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Convert", "description": "**BCFtools Convert**: Apply conversions from different formats like gensample, hapsample, haplegendsample and TSV to VCF and vice versa.\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\n* Convert IMPUTE2 output to VCF with the **Convert GEN/SAMPLE to VCF (Output VCF file name)** (`--gensample2vcf`) option. The second column must be in the \"CHROM:POS_REF_ALT\" form to detect possible strand swaps; IMPUTE2 leaves the first one empty (\"--\") when sites from reference panel are filled in.\n\n* Convert from hap/sample format to VCF using the **Convert HAP/SAMPLE to VCF (Output VCF file name)** (`--hapsample2vcf`) option. The columns of the .hap file are similar to the .gen file above, but there are only two haplotype columns per sample. Note that the first column of the .hap file is expected to be in the following form: \"CHR:POS_REF_ALT(_END)?\"\n\n* Convert from hap/legend/sample format used by IMPUTE2 to VCF with the **Convert HAP/LEGEND/SAMPLE to VCF (Output VCF file name)** (`--haplegendsample2vcf`) option.\n\n* Convert from TSV (tab-separated values) format (such as the one generated by 23andMe) to VCF with **Convert TSV to VCF (Output VCF file name)** (`--tsv2vcf`). The input file fields can be tab- or space- delimited.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n * In order to run the tool properly for conversion to gensample, hapsample and haplegendsample from VCF, file name has to be given in **Convert VCF on the GEN/SAMPLE (Output GEN or SAMPLE file name)** (`--gensample`), **Convert VCF to HAP/SAMPLE (Output HAPS or SAMPLE file name)** (`--hapsample`), **Convert HAP/LEGEND/SAMPLE to VCF (Output VCF file name)** (`--haplegendsample`) inputs respectively.\n\n * In order to run the tool properly for conversion to VCF from gensample, hapsample and haplegendsample, file name has to be given on the **Convert GEN/SAMPLE to VCF (Output VCF file name)** (`--gensample2vcf`), **Convert HAP/SAMPLE to VCF (Output VCF file name)** (`--hapsample2vcf`), **Convert HAP/LEGEND/SAMPLE to VCF (Output VCF file name)** (`--haplegendsample2vcf`) inputs respectively.\n\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance with a 7 MB VCF. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Include expression"}, {"name": "Exclude expression"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file", "encodingFormat": "text/x-bed"}, {"name": "Samples"}, {"name": "Samples file"}, {"name": "Targets"}, {"name": "Targets file", "encodingFormat": "application/x-vcf"}, {"name": "Threads"}, {"name": "Convert GEN/SAMPLE to VCF (Output VCF file name)"}, {"name": "Convert VCF to GEN/SAMPLE (Output GEN or SAMPLE file name)"}, {"name": "Tagged GEN file"}, {"name": "Convert GVCF to VCF (Output VCF file name)"}, {"name": "Reference sequence in fasta format", "encodingFormat": "application/x-fasta"}, {"name": "Convert HAP/SAMPLE to VCF (Output VCF file name)"}, {"name": "Convert VCF to HAP/SAMPLE (Output HAPS or SAMPLE file name)"}, {"name": "Converts haploid genotypes to homozygous diploid"}, {"name": "Convert HAP/LEGEND/SAMPLE to VCF (Output VCF file name)"}, {"name": "Convert VCF to HAPS/LEGEND/SAMPLE (Output HAPS/LEGEND/SAMPLE file name)"}, {"name": "Convert TSV to VCF (Output VCF file name)"}, {"name": "Column list"}, {"name": "Memory in MB"}, {"name": "Number of CPUs"}, {"name": "Output chromosome in first column"}, {"name": "Output sex column in sample-file"}, {"name": "Output VCF IDs in second column"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155729, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-csq/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-csq/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Csq", "description": "**BCFtools CSQ**: Haplotype-aware consequence predictor. Correctly handles combined variants such as MNPs, SNPs separated by an intron or nearby frame-shifting INDELs.\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nThe output VCF is annotated with INFO/BCSQ and FORMAT/BCSQ tag (configurable with the **Custom tag** (`--custom-tag`) option). The latter is a bitmask of indexes to INFO/BCSQ, with interleaved haplotypes. The construction of the bitmask limits the number of consequences that can be referenced in the FORMAT/BCSQ tags. By default this is 16, but if more are required, see the **Number of consequences**  (`--ncsq`) option.\n\nThe program requires inputs in the form of a VCF/BCF file, the reference genome in FASTA format and genomic features in the GFF3 format downloadable from the Ensembl website, and outputs an annotated VCF/BCF file. Currently, only Ensembl GFF3 files are supported.\n\nBy default, the input VCF should be phased. If phase is unknown, or only partially known, the **Phase** (`--phase`) option can be used to indicate how to handle unphased data. Alternatively, haplotype aware calling can be turned off with the **Local CSQ** (`--local-csq`) option.\n\nIf conflicting (overlapping) variants within one haplotype are detected, a warning will be emitted and predictions will be based only on the first variant in the analysis.\n\nSymbolic alleles are not supported. They will remain unannotated in the output VCF and are ignored for the prediction analysis.\n\n```\n$bcftools csq -f hs37d5.fa -g Homo_sapiens.GRCh37.82.gff3.gz in.vcf -O b -o out.bcf\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent bgzip and index commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command. \n\n### Common Issues and Important Notes\n\n * No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\n\nIt took 5 minutes to execute this tool on AWS c4.2xlarge instance with a 56 KB VCF and a 3 GB reference FASTA file. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "include expression"}, {"name": "Exclude expression"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Samples list"}, {"name": "Samples file"}, {"name": "Threads"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Suppress warning messages"}, {"name": "Custom tag"}, {"name": "Local CSQ"}, {"name": "Number of consequences"}, {"name": "Phase"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "GFF file"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Run even if some sanity checks fail"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155729, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-filter/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-filter/7", "applicationCategory": "CommandLineTool", "name": "Bcftools Filter", "description": "**BCFtools Filter**: Apply fixed-threshold filters. This tool can be used to increase calling precision by filtering the VCF. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nTo exclude all variants with quality threshold below ten, give '%QUAL<10' on the **Exclude expression** (`--exclude`) input:\n```\n$bcftools filter --exclude '%QUAL<10' file.vcf.gz\n```\n\nTo include all variants with quality threshold above twenty and have allele count above four, give 'AC>4 && %QUAL>20' on the **Include expression** (`--include`) input:\n```\n$bcftools filter --include 'AC>4 && %QUAL>20' file.vcf.gz\n```\nTo give a concrete example, the following filter seemed to work quite well for one particular dataset (human data, exomes): '%QUAL<10 || (RPB<0.1 && %QUAL<15) || (AC<2 && %QUAL<15) || %MAX(DV)<=3 || %MAX(DV)/%MAX(DP)<=0.3'. When provided on the **Exclude expression** (`--exclude`) input, it will exclude all variants with quality below ten or whose quality is below fifteen and allele count below two\n\n```\n$bcftools filter --exclude '%QUAL<10 || (RPB<0.1 && %QUAL<15) || (AC<2 && %QUAL<15) || %MAX(DV)<=3 || %MAX(DV)/%MAX(DP)<=0.3' file.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * Expressions must be given within single quotations marks, but strings within need to have double quotes, for example: 'FILTER=\"PASS\"'. Otherwise, the tool will fail. \n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Include expression"}, {"name": "Exclude expression"}, {"name": "SNP gap"}, {"name": "Filter mode"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Soft filter"}, {"name": "Indel gap"}, {"name": "Set GTs"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Threads"}, {"name": "Don't append version to header"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}], "output": [{"name": "Filtered output VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758737, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-gtcheck/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-gtcheck/5", "applicationCategory": "CommandLineTool", "name": "Bcftools GTcheck", "description": "**BCFtools GTcheck**: Checks sample identity.  This tool can be used to find genotype discordance between samples in VCF. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nIf the file on the **Genotypes** (`--genotypes`) input is provided, the identity of the **Query samples** (`--query-sample`) from **query.vcf.gz** is checked against the samples in the given file. \n```\n$bcftools gtcheck --genotypes genotypes.vcf.gz --query_samples sample query.vcf.gz\n```\n\nWithout the **Genotypes** (`--genotypes`) option, multi-sample cross-check of samples in **query.vcf.gz** is performed.\n```\n$bcftools gtcheck --query_samples sample query.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Output file name"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Query samples"}, {"name": "Target sample"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Output all sites"}, {"name": "Cluster"}, {"name": "Genotypes", "encodingFormat": "application/x-vcf"}, {"name": "Use genotypes"}, {"name": "Homozygous only"}, {"name": "Plot"}], "output": [{"name": "Output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758864, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-index/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-index/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Index", "description": "**BCFtools Index**: Creates index for bgzip-compressed VCF/BCF files for random access. CSI (coordinate-sorted index) is created by default. \n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\n\nThe **CSI** format supports indexing of chromosomes up to 2^31 in length with **Generate CSI Index** (`--csi`). When loading an index file, BCFtools will try the **CSI** first and then the **TBI**.\n```\n$bcftools index --csi file.vcf.gz\n```\n**TBI** (tabix index) index files, which support chromosome lengths up to 2^29, can be created by using the **Generate TBI Index** (`--tbi`) option or using the tabix program packaged with **htslib**.\n```\n$bcftools index --tbi file.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* Added the **Output VCF file with index** option that allows you to get the index file along with the VCF on the output, to be used in tools that require VCFs with a corresponding secondary index file.\n\n### Common Issues and Important Notes\n\n * No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Output file name"}, {"name": "Threads"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Generate CSI Index"}, {"name": "Overwrite index"}, {"name": "Minimal interval"}, {"name": "Generate TBI Index"}, {"name": "Number of records"}, {"name": "Print stats"}, {"name": "Output VCF file with index"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}, {"name": "Index file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758864, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-isec/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-isec/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Isec", "description": "**BCFtools Isec**: Creates intersections, unions and complements of VCF files. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nDepending on the options, the program can output records from one (or more using the **Output positions present in that number of files** (`--nfiles`)) files which have (or do not have using the **Complement** (`--complement`)) corresponding records with the same position in the other files. \n```\n$bcftools isec --nfiles=3 A.vcf.gz B.vcf.gz C.vcf.gz\n```\n\nCreate intersection and complements of two sets.\n```\n$bcftools isec A.vcf.gz B.vcf.gz\n```\n\nFilter sites in A (require INFO/MAF>=0.01 with **Exclude expression** (`--exclude`)) and B (require INFO/dbSNP with **Include expression** (`--include`)) but not in C, and create an intersection, including only sites which appear in at least two of the files (using the **Output positions present in that number of files** (`--nfiles`) option) after filters have been applied.\n```\n$bcftools isec --exclude 'MAF<0.01' -include 'dbSNP=1' --nfiles +2 --exclude- A.vcf.gz B.vcf.gz C.vcf.gz \n```\n\nExtract and write records from A shared by both A and B using exact allele match with **Output positions present in that number of files** (`--nfiles`) and **List of files to write** (`--write`) options.\n```\n$bcftools isec --nfiles=2 --write 1 A.vcf.gz B.vcf.gz\n```\n\nPrint a list of records which are present in A and B but not in C and D adding \"~1100\" string to the **Output positions present in that number of files** (`--nfiles`) input and selecting all variants in the **Treat as identical** (`--collapse`) checkbox.\n```\n$bcftools isec --nfiles~1100 --collapse all A.vcf.gz B.vcf.gz C.vcf.gz D.vcf.gz\n```\n\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command. \n* If only file file is provided on input, since there is no intersection, that file will be output of tool.\n\n### Common Issues and Important Notes\n\n* For more than two files on input, **Output positions present in that number of files** (`--nfiles`) must be set. Otherwise, the tool will fail.\n\n* The **Readme.txt** output contains information about the records in output VCF files:\n           \n      * Which VCF contains records private to which VCF\n      * Which VCF contains records shared by both VCFs\n\n### Performance Benchmarking\n\nIt took 4 minutes to execute this tool on AWS c4.2xlarge instance using two inputs of 7 MB and 12.4 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "include expression"}, {"name": "Exclude expression"}, {"name": "Output directory name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Threads"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Treat as identical"}, {"name": "Complement"}, {"name": "Apply filters"}, {"name": "Do not append version to header"}, {"name": "Output positions present in that number of files"}, {"name": "Output to single file"}, {"name": "List of files to write"}], "output": [{"name": "Output VCF files", "encodingFormat": "application/x-vcf"}, {"name": "Readme TXT file", "encodingFormat": "text/plain"}, {"name": "Sites file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758864, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-merge/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-merge/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Merge", "description": "**BCFtools Merge**: Merge multiple VCF/BCF files from non-overlapping sample sets to create one multi-sample file. \n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\n\nFor example, when merging file A.vcf.gz containing samples S1, S2 and S3 and file B.vcf.gz containing samples S3 and S4, the output file will contain four samples named S1, S2, S3 and S4.\n```\n$bcftools merge A.vcf.gz B.vcf.gz\n```\n\nMerge two VCF files with different samples for SNPs only, using the **Merge** (`--merge`) option.\n```\n$bcftools merge --merge snps A.vcf.gz B.vcf.gz\n```\n\nMerge two VCF files (with one sample identical in both files) for all records with **Force samples** (`--force-samples`) option.\n```\n$bcftools merge --merge all --force_samples A.vcf.gz B.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n* If only file file is provided on input, since there is no merging, that file will be output of the tool.\n\n### Common Issues and Important Notes\n\n* Note that it is the responsibility of the user to ensure that the sample names are unique across all files. If they are not, the program will exit with an error unless the **force samples** (`--force-samples`) option is used. The sample names can also be given explicitly using the print header (`--print-header`) option.\n\n* Note that only records from different files can be merged, but not from the same file. For \"vertical\" merge take a look at BCFtools concat. At least two files must be given to merge.\n\n### Performance Benchmarking\n\nIt took 4 minutes to execute this tool on AWS c4.2xlarge instance using inputs of 12.4 MB and 56 KB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file", "encodingFormat": "text/x-bed"}, {"name": "Threads"}, {"name": "Tagged GEN file"}, {"name": "Memory in MB"}, {"name": "Number of CPUs"}, {"name": "Force samples"}, {"name": "Print header"}, {"name": "Use provided header", "encodingFormat": "application/x-vcf"}, {"name": "Assume missing genotypes"}, {"name": "Apply filters"}, {"name": "Filters logic"}, {"name": "Merge gVCF blocks", "encodingFormat": "application/x-fasta"}, {"name": "Info rules"}, {"name": "Merge option"}, {"name": "No version"}, {"name": "Output name"}], "output": [{"name": "Merged VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758864, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-mpileup/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-mpileup/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Mpileup", "description": "**BCFtools Mpileup**: Generate VCF or BCF containing genotype likelihoods for one or multiple alignment (BAM or CRAM) files. \n\n\nBCFtools Mpileup is based on the original SAMtools Mpileup command (with the -v or -g options) producing genotype likelihoods in VCF or BCF format, but not the textual pileup output. The `mpileup` command was transferred to BCFtools in order to avoid errors resulting from the use of incompatible versions of SAMtools and BCFtools when used in the mpileup+BCFtools Call pipeline. [1]\n\nIndividuals are identified from the SM tags in the @RG header lines. Multiple individuals can be pooled in one alignment file. Also one individual can be separated into multiple files. If sample identifiers are absent, each input file is regarded as one sample.\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed.\n\nMost commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed.\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nMake an uncompressed BCF with mpileup with SNPs and short INDELs from a BAM file using the **Output type** (`--output-type`) option.\n```\n$bcftools mpileup --output-type u --fasta-ref referece.fasta aln.bam\n```\n\nMake a compressed VCF with mpileup with SNPs only from a BAM file using the **Output type** (`--output-type`) option.\n```\n$bcftools mpileup --output-type z --skip-indels --fasta-ref referece.fasta aln.bam\n```\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n * Note that there are two orthogonal ways to specify locations in the input file; via the **regions** (`--regions`) and **targets** (`--targets`) positions. The former uses (and requires) an index to do random access while the latter streams through the file contents filtering out the specified regions, requiring no index. The two may be used in conjunction. For example, a BED file containing locations of genes in chromosome 20 could be specified by setting **regions** (`--regions`) to 20 and **targets** (`--targets`) to chr20.bed, meaning that the index is used to find chromosome 20 and then it is filtered for the regions listed in the BED file. Also note that the **regions** (`--regions`) option can be much slower than **targets** (`--targets`) with many regions and can require more memory when multiple regions and many alignment files are processed.\n\n### Performance Benchmarking\n\nIt took 5 minutes to execute this tool on AWS c4.2xlarge instance using a 220 MB BAM file as its input. The price is negligible ($0.03).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-bam"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Samples list"}, {"name": "Samples file"}, {"name": "Threads"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Quality is in the Illumina-1.3+ encoding"}, {"name": "Do not discard anomalous read pairs"}, {"name": "Disable BAQ"}, {"name": "Adjust mapping quality"}, {"name": "Max per-file depth"}, {"name": "Recalculate BAQ"}, {"name": "Fasta reference file", "encodingFormat": "application/x-fasta"}, {"name": "No reference"}, {"name": "Select read groups"}, {"name": "Minimum mapping quality"}, {"name": "Min base quality"}, {"name": "Ignore RG tags"}, {"name": "Include flags"}, {"name": "Exclude flags"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Disable read-pair overlap detection"}, {"name": "Annotate"}, {"name": "Output gVCF blocks"}, {"name": "No version"}, {"name": "Extension sequencing error probability"}, {"name": "Minimum fraction of gapped reads"}, {"name": "Coefficient for homopolymer errors"}, {"name": "Do not perform indel calling"}, {"name": "Maximum per-file depth for INDEL calling"}, {"name": "Minimum number gapped reads for indel candidates"}, {"name": "Open seq error probability"}, {"name": "Apply -m and -F per-sample for increased sensitivity"}, {"name": "List of platforms"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/develop.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758864, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-norm/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-norm/6", "applicationCategory": "CommandLineTool", "name": "Bcftools Norm", "description": "**BCFtools Norm** Left-align and normalize INDELs, check if REF alleles match the reference, split multiallelic sites into multiple rows; recover multiallelics from multiple rows. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nNormalize and left-align INDELs in given VCF.\n```\n$bcftools norm --fasta-ref reference.fasta input.vcf.gz\n```\n\nSplit multiallelic sites into biallelic records, or join biallelic sites into multiallelic records with **Split or join** option. An optional type string can follow which controls variant types which should be split or merged together with **Multiallelics type**. Normalize and left-align INDELs and break multiallelic SNPs in multiple rows in given VCF with **Multiallelics** option. \n```\n$bcftools norm --fasta-ref reference.fasta --multiallelics -snps input.vcf.gz\n```\n\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n* Left-alignment and normalization will only be applied if the FASTA reference is supplied.\n\n* Please note that you need to check one of the items from the **Check reference** (`--check-ref`) option when a missing REF allele is encountered: exit (e), warn (w), exclude (x), or set/fix (s) bad sites. Note that `s` can swap alleles and will update genotypes (GT) and AC counts, but will not attempt to fix PL or other fields. Moreover, and this cannot be stressed enough, `s` will NOT fix strand issues in your VCF, so please DO NOT use it for that purpose.\n\n* The **Remove duplicates** (`--remove-duplicates`) and **Multiallelics** options cannot be combined, otherwise the tool will fail.\n\n### Performance Benchmarking\n\nIt took 4 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Regions for processing"}, {"name": "Processing regions from file", "encodingFormat": "text/x-bed"}, {"name": "Number of CPUs"}, {"name": "Reference sequence in FASTA format", "encodingFormat": "application/x-fasta"}, {"name": "Memory in MB"}, {"name": "Output type"}, {"name": "Check reference"}, {"name": "Remove duplicate"}, {"name": "Multiallelics"}, {"name": "No version"}, {"name": "Do not normalize indels"}, {"name": "Merged site is PASS only if all sites being merged PASS."}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Buffer for sorting lines"}, {"name": "Threads"}], "output": [{"name": "Norm output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-query/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-query/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Query", "description": "**BCFtools Query**: Extracts fields from VCF or BCF files and outputs them in the user-defined format. \n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\n* Print chromosome, position, ref allele and the first alternate allele using the **Format** (`--format`) option.\n```\n$bcftools query --format '%CHROM  %POS  %REF  %ALT{0}\\n' file.vcf.gz\n```\n\n* Similar to above, but use tabs instead of spaces, add sample name and genotype.\n```\n$bcftools query --format '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' file.vcf.gz\n```\n\n* Print FORMAT/GT fields followed by FORMAT/GT fields.\n```\n$bcftools query --format 'GQ:[ %GQ] \\t GT:[ %GT]\\n' file.vcf\n```\n\n* Make a BED file: CHR, POS (0-based), END POS (1-based), ID.\n```\n$bcftools query --format '%CHROM\\t%POS0\\t%END\\t%ID\\n' file.bcf\n```\n\n* Print only samples with alternate (non-reference) genotypes using the **Include expression** (`--include`) option.\n```\n$bcftools query --format '[%CHROM:%POS %SAMPLE %GT\\n]' -i'GT=\"alt\"' file.bcf\n```\n\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done. Output type can for this tool is limited to VCF only.\n\n### Common Issues and Important Notes\n\n * No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "include expression"}, {"name": "Exclude expression"}, {"name": "Output file name"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Samples list"}, {"name": "Samples file"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Format"}, {"name": "Print header"}, {"name": "List samples"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Allow undefined tags"}], "output": [{"name": "Output VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758738, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-reheader/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-reheader/8", "applicationCategory": "CommandLineTool", "name": " Bcftools Reheader", "description": "**BCFtools Reheader**: Modify header of VCF/BCF files, and change sample names.\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\nChange the header of a VCF file using a header file on the **Header file** (`--header`) input\n```\n$bcftools reheader --header header_file.txt input.vcf.gz\n```\nChange sample names in a VCF file, one name per line, in the same order as they appear in the VCF file provided on the **Samples file** input or a list of strings on the **Sample strings** input.\n\n```\n$bcftools reheader --samples samples_file.txt input.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input only indexing will be done.\n\n### Common Issues and Important Notes\n\n * No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using a VCF input of 7 MB and header input of 8 KB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Output file name"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Header file", "encodingFormat": "text/plain"}, {"name": "Samples file", "encodingFormat": "text/plain"}, {"name": "Sample strings"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Threads"}, {"name": "Index output file"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/develop.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758737, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-roh/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-roh/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Roh", "description": "**BCFtools Roh**: A program for detecting runs of homo/autozygosity including exome data, using a hidden Markov model. Only bi-allelic sites are considered.\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\n\n* The `roh` command takes a VCF with FORMAT columns containing either genotype likelihoods (PL) or genotypes (GT) as its input. By default, genotype likelihoods are expected unless the **Genotype only** (`--GTs-only`) option is provided. The second required piece of information is the estimate of alternate allele frequencies in the population for each site with **Allele frequency default** (`--AF-dflt`).\n```\n$bcftools roh --GTs-only 30 --AF-dflt 0.4 file.vcf.gz\n```\n\n* A genetic map can be provided (**Genetic map** (`--genetic_map`)) so that the model can account for recombination hotspots. If not available, a constant recombination rate per-base can be given (**Recombination rate** (`--rec-rate`)). Note that both options can also be given.  In that case, the **Recombination rate** (`--rec-rate`) parameter will be interpreted differently by the program - as a fold increase of transition probabilities.\n```\n$bcftools roh --genetic_map genetic_map_combined_b37.txt --rec_rate 0.001 file.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input, only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * If the FORMAT/PL tag is not found in the header, the **Genotype only** (`--GTs-only`) option has to be checked to prevent the tool from failing.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "include expression"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Samples list"}, {"name": "Samples file", "encodingFormat": "text/plain"}, {"name": "Threads"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Allele frequency default"}, {"name": "Allele frequency tag"}, {"name": "Allele frequency file", "encodingFormat": "application/x-vcf"}, {"name": "Genotype only"}, {"name": "Ignore hom-ref genotypes"}, {"name": "Skip indels"}, {"name": "Genetic map"}, {"name": "Recombination rate"}, {"name": "Targets"}, {"name": "Targets file", "encodingFormat": "text/x-bed"}, {"name": "HW (Hardy-Weinberg) to AZ (autozygous) state"}, {"name": "AZ (autozygous) state to HW (Hardy-Weinberg)."}, {"name": "Estimate HMM parameters"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155731, "dateCreated": 1538758737, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-sort/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-sort/5", "applicationCategory": "CommandLineTool", "name": "Bcftools Sort", "description": "**BCFtools Sort**: Sort VCF/BCF file.\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\n* Sort a VCF file and choose output type with the **Output type** (`--output-type`) option.\n```\n$bcftools sort --output-type v file.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. \n\n### Common Issues and Important Notes\n\n * In order for tool to work, VCF file needs to have **contigs** in header. Otherwise tool will fail. \n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Maximum memory to use"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}], "output": [{"name": "Sorted output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155730, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-stats/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-stats/7", "applicationCategory": "CommandLineTool", "name": "Bcftools Stats", "description": "**BCFtools Stats**: Parses VCF or BCF and produces text file stats which are suitable for machine processing and can be plotted using the vcfstats plot. \n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n\n### Common Use Cases\n\n*  When one VCF file is specified on the command line, then stats by non-reference allele frequency, depth distribution, stats by quality and per-sample counts, singleton stats, etc. are printed.\n```\n$bcftools stats --fasta-ref reference.fasta file.vcf.gz > stats_file.stats ; plot-vcfstats stats_file.stats\n```\n\n* When two files are given, the program generates separate stats for intersection and the complements. By default only sites are compared, **Samples list**  (`--samples`) or **Samples file** (`--samples-file`) must be given to include also sample columns. When two VCF files are given, then stats such as concordance (Genotype concordance by non-reference allele frequency, Genotype concordance by sample, Non-Reference Discordance) and correlation are also printed. Per-site discordance (PSD) is also printed in verbose mode using **Verbose** (`--verbose`). Using the **Output stats** name option, the user can choose the name of the stats file, otherwise the file will be named after the first VCF file given on input.\n```\n$bcftools stats --fasta-ref reference.fasta A.vcf.gz B.vcf.gz > stats_file.stats ; plot-vcfstats stats_file.stats\n```\n\n\n### Changes Introduced by Seven Bridges\n\n* Plot-vcfstats script is added after BCFtools stats command. Tool outputs both stats file and plots.\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added `bgzip` and `index` commands afterwards if a VCF file is provided on input. If VCF.GZ is given on input, only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * Reference FASTA file is required for this tool.\n\n * Tool accepts no more than two VCF files. If more than two files are given on input, tool will fail.\n\n### Performance Benchmarking\n\n* It took 4 minutes to execute this tool on AWS c4.2xlarge instance using one input of 7 MB. The price is negligible ($0.02).\n\n* It took 4 minutes to execute this tool on AWS c4.2xlarge instance using two inputs sized 7 MB and 12.4 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Include expression"}, {"name": "Exclude expression"}, {"name": "Regions for processing"}, {"name": "Processing regions from file", "encodingFormat": "text/x-bed"}, {"name": "Samples list"}, {"name": "Samples file"}, {"name": "Targets"}, {"name": "Targets file", "encodingFormat": "application/x-vcf"}, {"name": "CPUs"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Memory in MB"}, {"name": "Plot statistics"}, {"name": "User Ts/Tv"}, {"name": "Number of threads"}, {"name": "Allele frequency bins"}, {"name": "Allele frequency tag"}, {"name": "Include only 1st allele"}, {"name": "Collapse"}, {"name": "Depth distribution"}, {"name": "Exons file"}, {"name": "Apply filters"}, {"name": "Split by ID"}, {"name": "Verbose"}, {"name": "Output stats name"}], "output": [{"name": "Output plots files"}, {"name": "Stats file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155729, "dateCreated": 1538758737, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bcftools-view/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bcftools-view/9", "applicationCategory": "CommandLineTool", "name": "Bcftools View", "description": "**BCFtools View**: View, subset and filter VCF or BCF files by position and filtering expression (Former BCFtools Subset).\n\n\n**BCFtools** is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. [1]\n\nA list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.\n\n### Common Use Cases\n\n* Convert between VCF and BCF using the **Output type** (`--output-type`) option\n```\n$bcftools view --output-type v input.bcf.gz\n```\n\n* Output only header of VCF using the **Header only** (`--header-only`) option\n```\n$bcftools view --header-only input.vcf.gz\n```\n\n* Subset VCF to output SNPs only using the **Variant types** (`--types`) option\n```\n$bcftools view --types snps input.vcf.gz\n```\n\n### Changes Introduced by Seven Bridges\n\n* BCFtools works in all cases with gzipped and indexed VCF/BCF files. To be sure BCFtools works in all cases, we added subsequent `bgzip` and `index` commands if a VCF file is provided on input. If VCF.GZ is given on input, only indexing will be done. Output type can still be chosen with the `output type` command.\n\n### Common Issues and Important Notes\n\n * No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\n\nIt took 3 minutes to execute this tool on AWS c4.2xlarge instance using an input of 7 MB. The price is negligible ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1 - BCFtools page](https://samtools.github.io/bcftools/bcftools.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Include expression"}, {"name": "Exclude expression"}, {"name": "Output file name"}, {"name": "Output type"}, {"name": "Regions for processing"}, {"name": "Processing regions from file"}, {"name": "Targets"}, {"name": "Targets file"}, {"name": "Threads"}, {"name": "Don't append version to header"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}, {"name": "Drop genotypes"}, {"name": "Header only"}, {"name": "Suppress header"}, {"name": "Compression level"}, {"name": "Trim alt alleles"}, {"name": "Do not update INFO fields"}, {"name": "Force samples"}, {"name": "Samples list"}, {"name": "Samples file", "encodingFormat": "text/plain"}, {"name": "Minimum allele count"}, {"name": "Maximum allele count"}, {"name": "Apply filters"}, {"name": "Genotype"}, {"name": "Known sites"}, {"name": "Novel sites"}, {"name": "Minimum alleles"}, {"name": "Maximum alleles"}, {"name": "Phased"}, {"name": "Exclude phased"}, {"name": "Minimum frequency"}, {"name": "Maximum frequency"}, {"name": "Uncalled genotype"}, {"name": "Exclude uncalled genotype"}, {"name": "Variant types"}, {"name": "Exclude variant types"}, {"name": "Private to subset samples"}, {"name": "Exclude private sites to subset samples"}], "output": [{"name": "Output VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/bcftools", "https://github.com/samtools/bcftools/wiki", "https://github.com/samtools/bcftools/archive/1.9.zip"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Petr Danecek, Shane McCarthy, John Marshall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155729, "dateCreated": 1538758803, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedgraphtobigwig-3-2-3/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedgraphtobigwig-3-2-3/4", "applicationCategory": "CommandLineTool", "name": "BedGraphToBigWig", "description": "BedGraphToBigWig converts a BEDGRAPH file to the BIGWIG file format.", "input": [{"name": "Input BEDGRAPH file"}, {"name": "Input chromosome sizes file", "encodingFormat": "text/plain"}, {"name": "Block size"}, {"name": "Items per slot"}, {"name": "Uncompressed"}], "output": [{"name": "Output BIGWIG file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["File Format Conversion"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799632, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtobigbed-3-2-3/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtobigbed-3-2-3/4", "applicationCategory": "CommandLineTool", "name": "BedToBigBed", "description": "BedToBigBed converts a BED file to the BIGBED file format.", "input": [{"name": "Input BED file", "encodingFormat": "text/x-bed"}, {"name": "Input chromosome sizes file", "encodingFormat": "text/plain"}, {"name": "Type"}, {"name": "Auto SQL file"}, {"name": "Block size"}, {"name": "Items per slot"}, {"name": "Uncompressed"}, {"name": "Tab separated"}, {"name": "Extra index"}], "output": [{"name": "Output BIGBED file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["File Format Conversion"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799048, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtools-coverage-2-25-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtools-coverage-2-25-0/11", "applicationCategory": "CommandLineTool", "name": "BEDTools Coverage", "description": "BEDTools coverage returns the depth and breadth of coverage of features from B on the intervals in A.\n\nThe BEDTools coverage computes both the depth and breadth of coverage of features in file B on the features in file A. For example, BEDTools coverage can compute the coverage of sequence alignments (file B) across 1 kilobase (arbitrary) windows (file A) tiling a genome of interest. One advantage that BEDTools coverage offers is that it not only counts the number of features that overlap an interval in file A, but it also computes the fraction of bases in the interval in A that were overlapped by one or more features. Thus, BEDTools coverage also computes the breadth of coverage observed for each interval in A.", "input": [{"name": "Input file A", "encodingFormat": "application/x-vcf"}, {"name": "Input file B", "encodingFormat": "application/x-vcf"}, {"name": "Fraction of A"}, {"name": "Fraction of B"}, {"name": "Require fraction overlap"}, {"name": "Require minimum fraction"}, {"name": "Require same strandedness"}, {"name": "Require different strandedness"}, {"name": "Split"}, {"name": "Genome file", "encodingFormat": "text/plain"}, {"name": "No name check"}, {"name": "Sorted"}, {"name": "Output as bed"}, {"name": "Header"}, {"name": "Disable buffered output"}, {"name": "Input buffer size"}, {"name": "Reported value"}, {"name": "Memory in MB"}], "output": [{"name": "Output result file", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/bedtools2", "https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz"], "applicationSubCategory": ["BED Processing", "Quality Control"], "project": "SBG Public Data", "creator": "Aaron R. Quinlan & Neil Kindlon", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453798828, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtools-genomecov-2-25-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtools-genomecov-2-25-0/6", "applicationCategory": "CommandLineTool", "name": "BEDTools Genomecov", "description": "BEDTools genomecov computes the coverage of a feature file in a genome.", "input": [{"name": "Input file", "encodingFormat": "application/x-bam"}, {"name": "Genome file", "encodingFormat": "text/plain"}, {"name": "Depth one-based coordinates"}, {"name": "Depht zero based coordinates"}, {"name": "Depth bedgraphformat"}, {"name": "Depth bedgraph with zeros"}, {"name": "Split"}, {"name": "Strand"}, {"name": "Coverage of 5prime positions"}, {"name": "Coverage of 3prime positions"}, {"name": "Max"}, {"name": "Scale"}, {"name": "Trackline"}, {"name": "Trackopts"}], "output": [{"name": "Output result file", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/bedtools2", "https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz"], "applicationSubCategory": ["BED Processing", "Quality Control"], "project": "SBG Public Data", "creator": "Aaron R. Quinlan & Neil Kindlon", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453799729, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtools-getfasta/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtools-getfasta/3", "applicationCategory": "CommandLineTool", "name": "BEDTools GetFasta", "description": "BEDTools GetFasta extracts sequences from a FASTA file for each of the intervals defined in a BED/GFF/VCF file.\n\n**Tips:**\n\n1. The headers in the input FASTA file must exactly match the chromosome column in the BED file.\n\n3. BED files containing a single region require a newline character at the end of the line, otherwise a blank output file is produced.", "input": [{"name": "Name"}, {"name": "Force strandedness"}, {"name": "Split"}, {"name": "Input fasta", "encodingFormat": "application/x-fasta"}, {"name": "Input BED/GTF/GFF", "encodingFormat": "application/x-gtf"}], "output": [{"name": "Output fasta"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/bedtools2", "https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz"], "applicationSubCategory": ["FASTA Processing", "BED Processing"], "project": "SBG Public Data", "creator": "Aaron R. Quinlan & Neil Kindlon", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1496069686, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtools-intersect-2-25-0/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtools-intersect-2-25-0/14", "applicationCategory": "CommandLineTool", "name": "BEDTools Intersect", "description": "BEDTools intersect reports the overlap between multiple feature files.\n\nThe most common question asked of two sets of genomic features is whether or not any of the features in the two sets \u201coverlap\u201d with one another. This is known as feature intersection. BEDTools intersect screens for overlaps between two sets of genomic features. Moreover, it allows the user to have fine control over how the intersections are reported. BEDTools intersect allows both BED/GFF/VCF and BAM files as inputs.\n\nWhen you are using Bedtools intersect, you can choose the compress level of bam, or output bed files. The time and storage would be quit different. All Input = 6GB bam file:\n\n* Output compressed bam file (-ubam=false, equal to bwa's compress level=1, most compressed): output bam=5.6Gb, Duration: 18 minutes\n* Output uncompressed bam file (-ubam=true, equal to bwa's compress level=9, least compressed): output bam=30.2Gb, Duration: 17 minutes\n* Output bed file (-bed=true): output bed=9.2Gb, Duration: 10 minutes", "input": [{"name": "Input file A", "encodingFormat": "application/x-vcf"}, {"name": "Input files B", "encodingFormat": "application/x-vcf"}, {"name": "Fraction of A"}, {"name": "Fraction of B"}, {"name": "Require fraction overlap"}, {"name": "Require minimum fraction"}, {"name": "Require same strandedness"}, {"name": "Require different strandedness"}, {"name": "Split"}, {"name": "Genome file", "encodingFormat": "text/plain"}, {"name": "No name check"}, {"name": "Sorted"}, {"name": "Output as bed"}, {"name": "Header"}, {"name": "Disable buffered output"}, {"name": "Input buffer size"}, {"name": "Write the original entry in A"}, {"name": "Write the original entry in B"}, {"name": "Perform left outer join"}, {"name": "Write A, B and overlap"}, {"name": "Write A, B, overlap and additional"}, {"name": "Write original A entry if found in B"}, {"name": "For entry in A write overlap if in B"}, {"name": "Write entry in A and no overlaps in B"}, {"name": "Uncompressed BAM output"}, {"name": "Names alias"}, {"name": "Show complete filename"}, {"name": "Sort output DB hits for each record"}], "output": [{"name": "Output result file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/bedtools2", "https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz"], "applicationSubCategory": ["BED Processing"], "project": "SBG Public Data", "creator": "Aaron R. Quinlan & Neil Kindlon", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453800015, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtools-merge-2-25-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtools-merge-2-25-0/10", "applicationCategory": "CommandLineTool", "name": "BEDTools Merge", "description": "BEDTools merge combines overlapping or \u201cbook-ended\u201d features in an interval file into a single feature which spans all of the combined features.\n\nBEDTools merge requires that you presort your data by chromosome and then by start position. For instance, sort -k1,1 -k2,2n in.bed > in.sorted.bed for BED files.", "input": [{"name": "Input file", "encodingFormat": "text/x-bed"}, {"name": "Force strandedness"}, {"name": "Force merge"}, {"name": "Maximum distance"}, {"name": "Specify columns"}, {"name": "Specify operations"}, {"name": "Print header"}, {"name": "Specify delimiter"}], "output": [{"name": "Output merged file", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/bedtools2", "https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz"], "applicationSubCategory": ["BED Processing"], "project": "SBG Public Data", "creator": "Aaron R. Quinlan & Neil Kindlon", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453799073, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bedtools-sort-2-25-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bedtools-sort-2-25-0/7", "applicationCategory": "CommandLineTool", "name": "BEDTools Sort", "description": "BEDTool sorts a feature file by chromosome and other criteria. By default, the program sorts a BED file by chromosome and then by start position in ascending order.", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Sort by feature size (asc order)"}, {"name": "Sort by feature size (desc order)"}, {"name": "Sort by chr, then by feature size (asc)"}, {"name": "Sort by chr, then feature size (desc)"}, {"name": "Sort by chr, then score (asc)"}, {"name": "Sort by chr, then score (desc)"}, {"name": "Memory in MB"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/bedtools2", "https://github.com/arq5x/bedtools2/releases/download/v2.25.0/bedtools-2.25.0.tar.gz"], "applicationSubCategory": ["BED Processing"], "project": "SBG Public Data", "creator": "Aaron R. Quinlan & Neil Kindlon", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453799286, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bigwigcorrelate-3-2-3/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bigwigcorrelate-3-2-3/2", "applicationCategory": "CommandLineTool", "name": "BigWigCorrelate", "description": "BigWigCorrelate correlates two or more BIGWIG files. The user can also specify target regions.", "input": [{"name": "Input BIGWIG files"}, {"name": "Restriction BIGBED file"}, {"name": "Threshold"}], "output": [{"name": "Output correlation file", "encodingFormat": "text/plain"}], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1471539440, "dateCreated": 1453799126, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bigwigtobedgraph-3-2-3/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bigwigtobedgraph-3-2-3/3", "applicationCategory": "CommandLineTool", "name": "BigWigToBedGraph", "description": "BigWigToBedGraph converts a BIGWIG file to the BEDGRAPH file format.", "input": [{"name": "Input BIGWIG file"}, {"name": "Chromosome"}, {"name": "Start"}, {"name": "End"}], "output": [{"name": "Output BEDGRAPH file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["File Format Conversion"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799181, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-0-21-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-0-21-0/5", "applicationCategory": "CommandLineTool", "name": "Bismark", "description": "**Bismark** is the tool which takes files with bisulfite-treated reads and aligns them to a specified bisulfite genome. \n\nAlong with aligning, **Bismark** writes information on methylation states for all cytosines in a read. It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe tool has two required inputs:\n\n* **Bisulfite converted genome folder** - compressed folder containing bisulfite converted FASTA reference files, generated by **Bismark Genome Preparation** tool;\n* **Bisulfite-treated reads** - input files with bisulfite-treated reads (single-end or paired-end); the files can be in FASTQ, FASTQ.GZ, FQ, FQ.GZ or FASTA formats and must have metadata fields properly set (**Sample ID** for single-end and **Sample ID** and **Paired-end** fields for paired-end samples; **Paired-end** field should be **1** or **2**).\n\nThe tool produces the following outputs:\n\n* **Alignment** file with aligned reads; it is in BAM format by default, but it can also be in SAM, CRAM, specified with **Output format** parameter;\n* a set of report files (**Report file** in TXT format, **Nucleotide stats** in TXT format, **Ambiguous BAM** in BAM format, **Unmapped reads** in FQ.GZ format and **Ambiguous reads**) with alignment and methylation call statistics.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nPrior to alignment, **Bismark** first transforms sequence reads into a bisulfite-converted forward strand version (C->T conversion) and into a bisulfite treated reverse strand (G->A conversion of the forward strand). Each of these reads is then aligned to bisulfite treated forward strand index of a reference genome (C->T converted) and a bisulfite treated reverse strand index of the genome (G->A conversion of the forward strand). Depending on the adapters used, BS-Seq libraries can be constructed in two different ways: directional or non-directional. If a library is directional, only reads which are (bisulfite converted) versions of the original top strand (OT) or the original bottom strand (OB) will be sequenced. If the library is non-directional, strands complementary to OT (CTOT) and OB (CTOB) which are generated in the BS-PCR step will be sequenced and in this case, all four strands (OT, CTOT, OB, CTOB) can produce valid alignments. \n\nBy default, Bismark assumes the library is directional and performs only 2 read alignments to the OT and OB strands. This behavior can be changed by setting **Sequencing library** to *non-directional* (`--non-directional`), in situations when input files in  **Bisulfite-treated reads** are produced with non-directional protocol.\n\nThe alignment itself is based on  [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml) or [HISAT2](https://ccb.jhu.edu/software/hisat2/index.shtml) aligners, with Bowtie2 being the default one. This could be changed by setting **Aligner** option to *HISAT 2* (`--hisat2`).\n\nThe output **Alignment file** will have the following line structure (the example is for paired-end samples, where one line corresponds to one sequence pair):\n\n1. seq-ID,\n2. alignment strand,\n3. chromosome,\n4. start,\n5. end,\n6. original bisulfite read sequence 1,\n7. equivalent genomic sequence 1 (+2 extra bp) (8) methylation call string 1,\n8. original bisulfite read sequence 2,\n9. equivalent genomic sequence 2 (+2 extra bp) (11) methylation call string 2,\n10. read 1 conversion,\n11. genome conversion,\n12. read 1 quality score (Phred33 scale),\n13. read 2 quality score (Phred33 scale).\n\nThe example line could look like this:\n\n    `HWUSI-EAS611_100205:2:1:13:1732#0\n    +\n    14\n    62880539\n    62880652\n    CGGGATTTCGCGGAGTACGGGTGATCGTGTGGAATATAGA\n    CGGGACTCCGCGGAGCACGGGTGACCGTGTGGAATACAGAGT\n    Z....h.xZ.Z....h.Z......xZ..........x...\n    CAACTATCTAAAACTAAAAATAACGCCGCCCAAAAACTCT\n    TCCGGCTGTCTGGAGCTGAAGATGGCGCCGCCCAGAAGCTCT\n    .zx..x...xh.h..x..h..hh.Z..Z....x..h....\n    CT\n    CT\n    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII`\n\nUpon completion, **Bismark** produces a run report containing information about the following: - Summary of alignment parameters used - Number of sequences analyzed - Number of sequences with a unique best alignment (mapping efficiency) - Statistics summarizing the bisulfite strand the unique best alignments came from - Number of cytosines analyzed - Number of methylated and unmethylated cytosines - Percentage methylation of cytosines in CpG, CHG or CHH context (where H can be either A, T or C).\n\nIt should be stressed that the percent methylation value (context) is just a very rough calculation performed directly at the mapping step. Actual methylation levels, after post-processing or filtering have been applied, may vary.\n\n### Changes Introduced by Seven Bridges\n\n* **Bismark** tool takes the compressed and archived folder with reference sequences (original and converted), as obtained from **Bismark Genome Preparation** tool, instead of just a path to genome folder (as required by the original Bismark script).\n\n\n### Common Issues and Important Notes\n\n* Files provided to the **Bisulfite-treated reads** input node must have metadata fields properly set. For single-end experiments, this refers only to **Sample ID** metadata field, while in paired-end case both **Sample ID** and **Paired-end** fields have to be filled. **Paired-end** field can contain values **1** or **2**.\n* Please note that **HISAT2** and **Bowtie 2** indexes are not compatible. The **Aligner** parameter setting and actual indexer used for indexing genome (in **Bismark Genome Preparation** tool) must match. The default value for both of them is *Bowtie 2*.\n* Memory and CPU requirements are set to 60GB and 32, which is optimal for directional libraries and human reference genome. In other situations, please set **Memory in MB per job** and **Number of CPUs per job** accordingly (you can use tables below as a hint).\n\n### Performance Benchmarking\nMemory and CPU requirements for **Bismark** strongly correlate with **Sequencing library** (directional or non-directional), a number of multicores (parameter **Multicore**) and the size of the reference file (**Bisulfite converted genome folder**). Running time depends on the size of input bsseq files (**Bisulfite-treated reads**) and the number of multicores. In such a situation, it is very hard to give a general advice on the parameter settings, especially because increasing one parameter, such as multicore number, speeds up the analysis, but also increases need for memory.\nIn order to somehow optimize the analysis, we focused on the most common case: directional libraries and human reference genome (8.4 GB bisulfate converted). Our testing on AWS instances showed that the optimal setting for the multicore parameter is 8 in this case. Higher values do not further speed up an analysis significantly, while lower values tend to be very slow and even more expensive (analysis of larger input files with 1 multicore did not finish after 24 hours). You can see some approximate running times and costs in the table below. \n\n| Bsseq file size | Multicore | Running time | Instance (AWS)| Running cost |\n| --- | --- | --- | --- | --- |\n| 2 x 700 MB | 1 | 2h 38m| c4.2xlarge| $1.15|\n| 2 x 700 MB | 8 | 39m | c4.8xlarge | $0.95|\n| 2 x 2 GB | 1 | 4h 49m | c4.4xlarge | $3.85 |\n| 2 x 2 GB | 8 | 1h 25m|\tr3.8xlarge | $3.77 |\n| 2 x 12 GB | 1\t| 24h + | - | - | - |\n| 2 x 12 GB | 8 | 8h 17m | r3.8xlarge | $22.03 |\n| 2 x 23 GB | 1 | 24h +\t| - | - |\n| 2 x 23 GB | 8\t| 19h 10m | c4.8xlarge | $30.48 |\n\nFor other cases, such as non-directional libraries or higher size genomes, we are giving here just a quick overview of some tests done at SBG, but we suggest that user specify memory and CPU demands explicitly.\n\n| Sequence library | Reference genome size | Bsseq file size | Multicore | RAM used | Running time | Instance (AWS) | Running cost |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Directional | 3 GB | 2 x 2 GB | 8 | 26 GB | 13m | r3.8xlarge | $0.58 |\n| Directional | 12.2 GB | 2 x 2 GB | 8 | 96GB | 1h 27m | r3.8xlarge | $3.86 |\n| Directional | 16.9 GB | 2 x 2 GB | 8 | 126GB | 1h 33m | r3.8xlarge | $4.12 |\n| Non-directional | 8.4 GB | 2 x 2 GB | 8 | 107.2 GB | 1h 50m | r3.8xlarge | $4.88 |\n\nBecause of high memory demands and relatively slow analysis for larger files, we suggest using our **Bismark Analysis 0.19.0** workflow, which uses scattering techniques to speed up the whole process.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pubmed/21493656). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Bisulfite converted genome folder", "encodingFormat": "application/x-tar"}, {"name": "Bisulfite-treated reads", "encodingFormat": "application/x-fasta"}, {"name": "Single-end"}, {"name": "Fasta reads"}, {"name": "Skip"}, {"name": "upto"}, {"name": "Fastq quality scale"}, {"name": "Aligner"}, {"name": "Minimum insert size"}, {"name": "Maximum insert size"}, {"name": "Multicore"}, {"name": "Sequencing library"}, {"name": "Sam no header"}, {"name": "RG tag"}, {"name": "RG id"}, {"name": "RG sample"}, {"name": "Unmapped"}, {"name": "Ambiguous reads"}, {"name": "Non bisulfite mismatch"}, {"name": "Cram reference", "encodingFormat": "application/x-fasta"}, {"name": "Prefix"}, {"name": "Basename"}, {"name": "Old flag"}, {"name": "Ambiguous BAM"}, {"name": "No mixed"}, {"name": "No discordant"}, {"name": "Dovetail"}, {"name": "Seed extension attempts"}, {"name": "Maximum reseed number"}, {"name": "Function type"}, {"name": "Constant A"}, {"name": "Coefficient B"}, {"name": "Read gap penalties"}, {"name": "Reference gap penalties"}, {"name": "Nucleotide coverage"}, {"name": "Output format"}, {"name": "Number of threads"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Seed mismatches"}, {"name": "Seed length"}, {"name": "No spliced alignment"}, {"name": "Truncate IDs"}], "output": [{"name": "Report file", "encodingFormat": "text/plain"}, {"name": "Alignment", "encodingFormat": "application/x-bam"}, {"name": "Nucleotide stats", "encodingFormat": "text/plain"}, {"name": "Ambiguous BAM", "encodingFormat": "application/x-bam"}, {"name": "Unmapped reads"}, {"name": "Ambiguous reads"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Alignment"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648560772, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-bismark2bedgraph-0-21-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-bismark2bedgraph-0-21-0/6", "applicationCategory": "CommandLineTool", "name": "Bismark2BedGraph", "description": "The **Bismark2BedGraph** is a tool used to generate bedGraph and coverage files sorted by chromosomal position. \n\n\nIt is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe tool has one required input:\n\n* **Context txt file(s)** which type is an array of files in TXT format and accept default output of the tool Bismark Methylation Extractor. These files represent the position of every single C depending on context (CpG, CHG or CHH) whereby methylated Cs will be labelled as forward reads (+), non-methylated Cs as reverse reads (-).\n\nThe Bismark2BedGraph produces two outputs:\n\n* **Coverage file** is compressed file (GZ) which reports the position of methylated or unmethylated cytosine, methylation percentage and count of methylated and unmethylated cytosine;\n\n* **BedGraph** is compressed file (GZ) with positions (start and end) of a given cytosine and its methylation state (in %);\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThe **Bismark2BedGraph** is useful when the input parameter **BedGraph** (`--bedGraph`) in the tool **Bismark Methylation Extractor** hasn't been selected, so it can create outputs **BedGraph** and **Coverage file**, by using default output files of the Bismark Methylation Extractor (CpG, CHH, CHG context). The **BedGraph** file uses 0-based genomic start and 1-based genomic end coordinates and should be UCSC compatible (if UCSC genomes were used for the alignment step). In addition, this tool will write out a **Coverage file** which is similar to the **BedGraph** file, but uses 1-based genomic coordinates and also reports the count of methylated and unmethylated cytosines for any covered position; this coverage file is required if you wish to generate a genome-wide cytosine report with the tool **Coverage2Cytosine**.\n\n### Changes Introduced by Seven Bridges\n\n* The basename of output files is determined automatically depending on input file name.\n\n### Common Issues and Important Notes\n\nFor larger size input files the tool may need more storage space than 1024GB which is set by default. The maximum size of the file was tested is 26GB and storage usage space was 70% of 1024GB.\n\n### Performance Benchmarking\n\n| Context | Total input size (Context CpG, CHH, CHG(TXT)) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- |\n| CpG | 626 MB | c4.2xlarge | 5m | $0.05 |\n| CX | 626 MB | c4.2xlarge | 27m | $0.24 |\n| CX | 26 GB | c4.2xlarge | 8h 17m | $4.44 |\n| CX | 26 GB | c4.8xlarge | 6h 55m | $11.96 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Cutoff value"}, {"name": "CX context"}, {"name": "Remove spaces"}, {"name": "Buffer Size"}, {"name": "Scaffolds"}, {"name": "Ample memory"}, {"name": "Zero based"}, {"name": "Context txt file(s)", "encodingFormat": "text/plain"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "BedGraph"}, {"name": "Coverage file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "File Format Conversion"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561064, "dateCreated": 1574861318, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark2report/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark2report/3", "applicationCategory": "CommandLineTool", "name": "Bismark2Report", "description": "**Bismark2Report** is a tool that generates a graphical HTML report page out of Bismark alignment, deduplication and methylation extraction (splitting) reports as well as M-bias files. It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe **Bismark2Report** has one required input:\n\n* **Alignment report** (`--alignment_report`) - the Bismark alignment report, generated by *Bismark* tool.\n\nOther reports can be included in the final HTML report with the following inputs:\n\n* **Deduplication report** (`--dedup_report`) - the deduplication report, generated with *Deduplicate Bismark* tool;\n* **Splitting report** (`--splitting_report`) - the splitting report, generated with *Bismark Methylation Extractor* tool;\n* **Mbias report** (`--mbias_report`) - the Mbias report, generated with *Bismark Methylation Extractor* tool;\n* **Nucleotide report** (`--nucleotide_report`) - the nucleotide report, generated with *Bismark* tool if its parameter **Nucleotide coverage** (`--nucleotide_coverage`) is set to TRUE.\n\nThe tool has one output, a **Html report** which is a graphical HTML report of alignment and methylation calling. This HTML report is produced in b64 converted format, so it could be easily integrated with SBG platform.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n**Bismark2Report** is commonly used after *Bismark* and *Bismark Methylation Extractor* tools, to produce the final HTML report which merges all reports generated by these two tools in the process of methylation calling. All the reports user wants to merge into the final one must be provided explicitly.\n\n### Changes Introduced by Seven Bridges\n\n* Although original bismark2report script searches for all the reports having the basename as the alignment report in the working directory and merges them into the final one, the tool **Bismark2Report** requires that all reports that need to be merged are given explicitly on the tool inputs. \n* The output of the tool is given in binary b64 HTML format, instead of textual HTML.\n\n### Common Issues and Important Notes\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\nThe execution time of **Bismark2Report** takes several minutes on the default instance. Unless specified otherwise, the default instance used to run it will be c4.2xlarge (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14", "input": [{"name": "Output filename"}, {"name": "Alignment report", "encodingFormat": "text/plain"}, {"name": "Deduplication report", "encodingFormat": "text/plain"}, {"name": "Splitting report", "encodingFormat": "text/plain"}, {"name": "Mbias report", "encodingFormat": "text/plain"}, {"name": "Nucleotide report", "encodingFormat": "text/plain"}], "output": [{"name": "Html report", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561065, "dateCreated": 1519389842, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark2summary/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark2summary/1", "applicationCategory": "CommandLineTool", "name": "Bismark2Summary", "description": "**Bismark2Summary** uses Bismark report files of several samples in a current running directory to generate a graphical summary HTML report as well as a big table (tab-delimited text) with all relevant alignment and methylation statistics which may be used in R, Excel etc. It is a part of the *Bismark* toolkit which represents a set of tools for the time-efficient analysis of Bisulfite-Seq (BS-Seq) data. Bismark performs alignments of bisulfite-treated reads to a reference genome and cytosine methylation calls at the same time.\n\nBismark2Summary has two inputs:\n\n* **Bam and reports** represents set of four input files: \n   * **Bam file** is generated by **Bismark** tool (mandatory file),\n   * **Alignment report** is generated by **Bismark** tool (mandatory file),\n   * **Deduplication report** is generated by **Bismark Deduplicate** tool,\n   * **Splitting report** is generated by **Bismark Methylation Extractor** tool,\n* **Output filename** - ability to enter output filename, otherwise it takes default name *bismark\\_summary\\_report*.\n\nTwo outputs are created by Bismark2Summary:\n\n* **Html summary report** (*ID:* html_report) - Graphical HTML report of alignment and methylation calling between samples.\n\n* **Txt summary report** (*ID:*  txt_report) -  Tab-delimited text report with of alignment and methylation calling results between samples.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Bismark2Summary** is intended to be used for comparison methylation analysis between samples. It uses Bismark BAM files and Bismark alignment reports as mandatory files and deduplication or methylation extractor (splitting) reports. If splitting reports are found they overwrite the methylation statistics of the initial alignment report.\n\n### Changes Introduced by Seven Bridges\n\nOriginal *bismark2summary* program assumes that input files all follow *Bismark naming conventions*. **Bismark2Summary** tool at SBG platform can take input files with names that slightly deviate from this convention. In this case, the script *prepareFiles.sh* will be automatically run first to make correction of the names of provided files. After name correction, all files of the same sample should have the same basename. \n\nProvided files may look like:\n\n   * **Bam file** - sample\\_bismark\\_bt2\\_pe.bam - bam file (basename will be *sample\\_bismark\\_bt2\\_pe*)\n   * **Alignment report** - \\_4\\_sample\\_bismark\\_bt2\\_PE\\_report.txt (prefix will be removed by the script prepareFiles.sh)\n   * **Deduplication report** - \\_4\\_sample\\_bismark\\_bt2_pe.sorted.deduplication\\_report.txt (prefix and \"sorted\" will be removed by the script prepareFiles.sh; Bismark name convention: basename.deduplication\\_report.txt) \n   * **Splitting report** - sample\\_bismark\\_bt2\\_pe.sorted.deduplicated\\_splitting\\_report.txt (\"sorted\" will be removed by the script prepareFiles.sh; Bismark name convention: basename.deduplicated\\_splitting\\_report.txt)\n\nThe *prepareFiles.sh* script will correct the file names as follows:\n\n   * **Bam file** - sample\\_bismark\\_bt2\\_pe.bam\n   * **Alignment report** - sample\\_bismark\\_bt2\\_PE_report.txt\n   * **Deduplication report** - sample\\_bismark\\_bt2\\_pe.deduplication\\_report.txt \n   * **Splitting report** - sample\\_bismark\\_bt2\\_pe.deduplicated\\_splitting\\_report.txt \n\n### Common Issues and Important Notes\n\n* If some files aren't recognized by the Bismark2Summary, probably their names don't follow [Bismark name convention](https://github.com/FelixKrueger/Bismark/tree/master/Docs#v-the-bismark-summary-report).\n\n### Performance Benchmarking\n\n| Number of samples | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- |\n| 2 | c4.2xlarge | 14m | $0.12 |\n| 4 | c4.2xlarge | 19m | $0.17 |\n| 8 | c4.2xlarge | 24m | $0.21 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Output filename"}, {"name": "Bam and reports"}], "output": [{"name": "Html summary report"}, {"name": "Txt summary report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561065, "dateCreated": 1519389842, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-bam2nuc-0-21-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-bam2nuc-0-21-0/5", "applicationCategory": "CommandLineTool", "name": "Bismark Bam2Nuc", "description": "**Bam2Nuc** calculates the mono- and di-nucleotide coverage and compares it to the average genomic sequence composition. \n\nBam2Nuc handles both Bismark single-end and paired-end files (determined automatically). It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe Bam2Nuc has two required inputs:\n\n* **Bisulfite genome** is a compressed folder (in .TAR.GZ format), which contains reference genome. FASTA file and two subfolders with two conversions (C->T and G->A) of the original reference genome. \n\n* **Input alignment file** is a file in BAM/CRAM format, which is generated by **Bismark**.\n\nThe tool Bam2Nuc can generate three outputs:\n\n* **Nucleotide coverage report**  is a file in TXT format which contains mono- and di-nucleotide coverage of the reads in comparison to the genomic sequence. It is generated if parameter **Genomic composition** (`--genomic_composition_only`) is not set to TRUE.\n\n* The output **Bisulfite genome with nucleotide freq file** is optional and it will be generated if file *genomic\\_nucleotide\\_frequencies.txt* doesn't exist in genome folder. This output represents compressed genome folder with prefix name  *with_nuc_freq_* with the mentioned file which will be written in it.\n\n* **Genomic Nucleotide Frequencies** is a file in TXT format. It is generated if parameter **Genomic composition** (`--genomic_composition_only`) is set to TRUE.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Bam2Nuc** is the tool which generates the file with the mono- and di-nucleotide coverage of the reads and the average genomic sequence composition (file genomic\\_nucleotide\\_frequencies.txt could be generated by the tool **Bismark Genome Preparation** and found in compressed genome folder (.TAR.GZ format) or by setting **Genomic composition** (`--genomic_composition_only`) to TRUE). Since the calculation of the average genomic (di-)nucleotide composition may take a while, Bam2Nuc attempts to write out a file called *genomic\\_nucleotide\\_frequencies.txt* to the genome folder if it wasn't there already and generates outputs **Nucleotide coverage report** (**Genomic Nucleotide Frequencies** (`--genomic_composition_only`) is not set to TRUE) and **Bisulfite genome with nucleotide freq file**. **Nucleotide coverage report** will not be created if  **Genomic Nucleotide Frequencies** is set to TRUE. Reads with InDels are not taken into consideration. Mono- or Di-nucleotides which contains Ns are ignored as well.\n\n\n### Changes Introduced by Seven Bridges\n\n* The tool requires .TAR.GZ bisulfite genome folder generated by the tool **Bismark Genome Preparation**, instead of providing the name of genome folder (`--genome_folder` <path>). The name of the folder is taken from provided .TAR.GZ bisulfite genome folder.\n\n* If the output **Bisulfite genome with nucleotide freq file** is not generated it means that file *genomic\\_nucleotide\\_frequencies.txt* exists in provided input **Bisulfite genome**.\n\n### Common Issues and Important Notes\n\n* If **Genomic composition** (`--genomic_composition_only`) is set to TRUE, only  **Genomic Nucleotide Frequencies** will be created.\n\n* If the file genomic\\_nucleotide\\_frequencies.txt doesn't exist in the genome folder (it is not created by the tool **Bismark Genome Preparation**), the tool will create the output **Bisulfite genome with nucleotide freq file**.\n\n* The output **Nucleotide coverage report** can be used as input of the tool **Bismark2Report** for creating graphical HTML report page of whole Bismark analysis.\n\n### Performance Benchmarking\n\n| Genomic nuceotide frequence | Reference type (Size .TAR.GZ) | Total input size (BAM) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- | --- |\n| Exist | Human (8.4 GB) | 1.9 GB | c4.2xlarge | 19m | $0.17 |\n| Exist | Human (8.4 GB) | 33 GB | c4.2xlarge | 4h 14m | $2.27 |\n| Not exist | Human (8.4 GB) | 33 GB | c4.2xlarge | 4h 53m | $2.62 |\n| Exist | Human (12.2 GB) | 33 GB | c4.2xlarge | 5h 28m | $2.93 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Bisulfite genome", "encodingFormat": "application/x-tar"}, {"name": "Genomic composition"}, {"name": "Input alignment file", "encodingFormat": "application/x-bam"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Nucleotide coverage report", "encodingFormat": "text/plain"}, {"name": "Bisulfite genome with nucleotide freq file", "encodingFormat": "application/x-tar"}, {"name": "Genomic Nucleotide Frequencies", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "File Format Conversion"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561065, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-bismark2report-0-21-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-bismark2report-0-21-0/6", "applicationCategory": "CommandLineTool", "name": "Bismark Bismark2Report", "description": "**Bismark2Report** uses Bismark alignment, deduplication and methylation reports to generate a graphical HTML report page.\n\nIt is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe **Bismark2Report** has one required input:\n\n* **Alignment report** (`--alignment_report`) - Bismark alignment report, generated by *Bismark* tool.\n\nOther reports can be included in the final HTML report with the following inputs:\n\n* **Deduplication report** (`--dedup_report`) file in TXT format, generated with *Deduplicate Bismark* tool;\n* **Splitting report** (`--splitting_report`) file in TXT format, generated with *Bismark Methylation Extractor* tool;\n* **Mbias report** (`--mbias_report`) file in TXT format, generated with *Bismark Methylation Extractor* tool;\n* **Nucleotide report** (`--nucleotide_report`) file in TXT format, generated with *Bismark* tool if its parameter **Nucleotide coverage** (`--nucleotide_coverage`) is set to TRUE.\n\nThe tool has one output, a **Html report** which is a graphical HTML report of alignment and methylation calling. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n**Bismark2Report** is commonly used after *Bismark* and *Bismark Methylation Extractor* tools, to produce the final HTML report which merges all reports generated by these two tools in the process of methylation calling. All the reports user wants to merge into the final one must be provided explicitly.\n\n### Changes Introduced by Seven Bridges\n\n* Although original bismark2report script searches for all the reports having the basename as the alignment report in the working directory and merges them into the final one, the tool **Bismark2Report** requires that all reports that need to be merged are given explicitly on the tool inputs. \n\n### Common Issues and Important Notes\n* **Alignment report** must have *Sample ID* metadata field set. \n\n### Performance Benchmarking\nThe execution time of **Bismark2Report** takes several minutes on the default instance. Unless specified otherwise, the default instance used to run it will be c4.2xlarge (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14", "input": [{"name": "Output filename"}, {"name": "Alignment report", "encodingFormat": "text/plain"}, {"name": "Deduplication report", "encodingFormat": "text/plain"}, {"name": "Splitting report", "encodingFormat": "text/plain"}, {"name": "Mbias report", "encodingFormat": "text/plain"}, {"name": "Nucleotide report", "encodingFormat": "text/plain"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Html report", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561065, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-bismark2summary-0-21-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-bismark2summary-0-21-0/6", "applicationCategory": "CommandLineTool", "name": "Bismark Bismark2Summary", "description": "**Bismark2Summary** uses Bismark report files of several samples to generate a graphical summary HTML report.\n\nIt is a part of the *Bismark* toolkit which represents a set of tools for the time-efficient analysis of Bisulfite-Seq (BS-Seq) data. Bismark performs alignments of bisulfite-treated reads to a reference genome and cytosine methylation calls at the same time.\n\nBismark2Summary has two inputs:\n\n* **Bam alignment files**, generated by **Bismark** tool (mandatory file), and\n* **Associated reports to Bismark alignment file**, which include **Alignment report** generated by **Bismark** tool (mandatory file), **Deduplication report** generated by **Bismark Deduplicate** tool and **Splitting report** generated by **Bismark Methylation Extractor** tool,\n\nTwo outputs are created by Bismark2Summary:\n\n* **Html summary report** (*ID:* html_report) - Graphical HTML report of alignment and methylation calling between samples.\n\n* **Txt summary report** (*ID:*  txt_report) -  Tab-delimited text report with of alignment and methylation calling results between samples.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Bismark2Summary** is intended to be used for comparison methylation analysis between samples. It uses Bismark BAM files and Bismark alignment reports as mandatory files and deduplication or methylation extractor (splitting) reports. If splitting reports are found they overwrite the methylation statistics of the initial alignment report.  In case **Output filename** is not defined, the tool takes the default name *bismark\\_summary\\_report*.\n\n\n### Changes Introduced by Seven Bridges\n\nInstead of providing the name of genome folder (`--genome_folder` <path>) at the input, the name of the folder, which is required in the command line, is taken from the current working directory. The tool requires all files, except for the BAM alignment files, that should initially be placed in a genome folder be provided at the **Associated reports to Bismark alignment file** input.\n\n### Common Issues and Important Notes\n\n* If some files aren't recognized by the Bismark2Summary, probably their names don't follow [Bismark name convention](https://github.com/FelixKrueger/Bismark/tree/master/Docs#v-the-bismark-summary-report). We strongly advise the user check the naming of the input files. The correct input files names should read as follows:\n\n   * **Bam file** - sample\\_bismark\\_bt2\\_pe.bam\n   * **Alignment report** - sample\\_bismark\\_bt2\\_PE_report.txt\n   * **Deduplication report** - sample\\_bismark\\_bt2\\_pe.deduplication\\_report.txt \n   * **Splitting report** - sample\\_bismark\\_bt2\\_pe.deduplicated\\_splitting\\_report.txt \n\n### Performance Benchmarking\n\n| Number of samples | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- |\n| 2 | c4.2xlarge | 14m | $0.12 |\n| 4 | c4.2xlarge | 19m | $0.17 |\n| 8 | c4.2xlarge | 24m | $0.21 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Output filename"}, {"name": "BAM alignment files", "encodingFormat": "application/x-bam"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Associated reports to Bismark alignment files", "encodingFormat": "text/plain"}], "output": [{"name": "Html summary report"}, {"name": "Txt summary report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561065, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-coverage2cytosine-0-21-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-coverage2cytosine-0-21-0/6", "applicationCategory": "CommandLineTool", "name": "Bismark Coverage2Cytosine", "description": "**Coverage2Cytosine** is a tool which generates a cytosine methylation report for a genome of interest. \n\nIt uses a sorted methylation input file produced by the tool **Bismark Methylation Extractor** when the **BedGraph** (`--bedGraph`) option was specified or by the tool **Bismark2BedGraph**. It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe Coverage2Cytosine has two mandatory inputs.\n\n* **Coverage file** is a file in COV or COV.GZ format. It reports the position of methylated or unmethylated cytosine, methylation percentage and count of methylated and unmethylated cytosine\n\n* **Reference genome** are files of genome sequences in FASTA or FA formats.\n\nThe tool has six outputs, where with default options it generates only output **CpG report**.\n\n* The outputs **CX report** and **CpG report** contain information on every single cytosine in the genome irrespective of its context.\n\n* **GpC report** is a file which contains methylation in GpC context. The format of this output is exactly the same as for the normal cytosine report. In addition, this will write out a **GpC coverage** file.\n\n* **Merge CpG** and **Discordant CpG** are files which are created when the option **Merge CpG** (`--merge_CpG`) is set to TRUE and the integer value is provided to option **Discordance** (`--discordance <int>`). This value represents maximum allowed discordance between the top and the bottom strand methylation values expressed as the absolute difference in percent methylation.\n\n* Outputs will be compressed if the option **Gzip** is set to TRUE.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Coverage2Cytosine** is a tool which generates a cytosine methylation report of specific context for a genome of interest. The input **Coverage file** is expected to use 1-based genomic coordinates. By default, the output uses 1-based chromosomal coordinates and reports CpG positions only (for both strands individually and not merged in any way). Output coordinates may be changed to 0-based coordinates using the option **Zero Based** (`--zero_based`).\n\n### Changes Introduced by Seven Bridges\n\n* The tool requires reference genome sequences in FA or FASTA format, instead of providing the name of genome folder (`--genome_folder` <path>). The name of the folder, which is required in the command line, is taken from the current working directory.\n\n### Common Issues and Important Notes\n\n* **Merge CpG** (`--merge_CpG`) doesn't work with the options **CX context** (`--CX_context`) or **Split By Chromosome** (`--split_by_chromosome`).\n\n* **NOMe-Seq** data requires a COV.GZ file as input which has been generated in non-CG mode **CX context** (`--CX_context`) in the tools Bismark Methylation Extractor or in Bismark2BedGraph, else the GpC output file will be empty.\n\n* Default value of the input parameter **Discordance** (`--discordance`) is 0 if no value is provided when the option **Merge CpG** is selected.\n\n### Performance Benchmarking\n\n| Context | Total input size (COV, COV.GZ) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- |\n| CpG | 333 MB | c4.2xlarge | 19m | $0.17 |\n| CX | 333 MB | c4.2xlarge | 56m | $0.5 |\n| CX | 730 MB(.GZ) | c4.2xlarge | 1h 17m | $0.69 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Reference genome file(s)", "encodingFormat": "application/x-fasta"}, {"name": "Coverage file"}, {"name": "CX context"}, {"name": "Merge CpG"}, {"name": "Split by chromosome"}, {"name": "Discordance"}, {"name": "GC context"}, {"name": "NOMe-Seq"}, {"name": "Four and five (tetra/penta) nucleotide context extraction."}, {"name": "Zero-based"}, {"name": "Gzip"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "CX report", "encodingFormat": "text/plain"}, {"name": "CpG report", "encodingFormat": "text/plain"}, {"name": "GpC report", "encodingFormat": "text/plain"}, {"name": "Discordant CpG"}, {"name": "Merged CpG"}, {"name": "GpC coverage"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "File Format Conversion"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561064, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-deduplicate-0-21-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-deduplicate-0-21-0/5", "applicationCategory": "CommandLineTool", "name": "Bismark Deduplicate", "description": "**Bismark Deduplicate** is a tool for removing duplicate Bisulfite-Sequencing (BS-Seq) reads from an alignment file. \n\nIt is a part of Bismark toolkit, a set of tools for time-efficient analysis of BS-Seq data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe tool requires the following input:\n\n* **Alignment file** - an alignment file(s) generated with **Bismark** tool, in SAM or BAM format.\n\nAfter the process of deduplication finishes, the **Bismark Deduplicate** will produce two output files:\n\n* **Deduplicated file(s)** - a deduplicated alignment file(s), which is by default in SAM format; the output format can be changed with parameter **Output format**;\n* **Report file** - a file(s) with metrics and statistics of deduplication process.\n\nThe **Output format** parameter can be set to *sam* (default), *bam* (`--bam`).\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Bismark Deduplicate** remove reads that are detected as duplicates from the input BAM or SAM file. For single-end alignments, only the start coordinate of a read will be used for deduplication. For paired-end alignments, the start coordinate of the first read and the end coordinate of the second read will be used for deduplication. If sequences align to the same genomic position but on different strands, they will be scored individually. The user can set whether the alignment file is single-end or paired-end with the parameter **Single-End**, which corresponds to using `--single` if it is set to TRUE, or to using `--paired` if it is set to FALSE.\n\nThe deduplication is not recommended for RRBS-type experiments, and in this case parameter **RRBS** should be set to TRUE. This will skip the whole deduplication process, which is important in situations when **Bismark Deduplicate** is the part of a larger workflow. \n\nHowever, if the **Barcode** parameter is set to TRUE (`--barcode`), the deduplication will be performed regardless of the **RRBS** parameter value. In addition to the chromosome, start position and orientation this will also take a potential barcode into consideration while deduplicating. The barcode needs to be the last element of the read ID and separated by a ':', e.g.: MISEQ:14:000000000-A55D0:1:1101:18024:2858_1:N:0:CTCC.\n\n### Changes Introduced by Seven Bridges\n\n* **RRBS** parameter is added, in order to skip deduplication processing the RRBS data.\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\nThe actual run time and cost of the tool depend on the input files size and available resources. Based on our testing, **Bismark Deduplicate** has increasing demand for RAM memory with larger input files, while the CPU number demand remains the same. That is why we suggest using some of the memory optimized AWS instances (i.e. m4.2xlarge or m4.4xlarge). Here are some examples of run times and costs:\n\n|Input BAM size|Instance (AWS)| Duration | Cost |\n| --- | --- | --- | --- |\n| 1.9 GB | c4.2xlarge | 10m | $0.07 |\n| 4.9 GB | c4.2xlarge | 18m 35s | $0.12 |\n| 27.8 GB | m4.2xlarge | 1h 39m | $0.66 |\n| 27.8 GB | m4.4xlarge | 1h 27m | $1.16 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pubmed/21493656). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Alignment file", "encodingFormat": "application/x-bam"}, {"name": "Output format"}, {"name": "Barcode"}, {"name": "Single-end/Paired-end"}, {"name": "RRBS"}, {"name": "Multiple"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Report file(s)", "encodingFormat": "text/plain"}, {"name": "Deduplicated file(s)", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648560772, "dateCreated": 1574861354, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-filter-non-conversion-0-21-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-filter-non-conversion-0-21-0/5", "applicationCategory": "CommandLineTool", "name": "Bismark Filter Non Conversion", "description": "**Filter Non Conversion** is used for filtering incomplete bisulfite conversion in non-CG context in Bismark BAM files. \n\nIt is a part of the *Bismark* toolkit which represents a set of tools for the time-efficient analysis of Bisulfite-Seq (BS-Seq) data. Bismark performs alignments of bisulfite-treated reads to a reference genome and cytosine methylation calls at the same time.\n\n**Filter Non Conversion** has one input:\n\n* **Input BAM file** is a file (BAM) produced by the tools **Bismark** or **Bismark Deduplicate**. Bam file should be sorted by name. This file is mandatory.\n\nThe tool creates three output files:\n\n* **Filtered BAM** is a file in BAM format which contains reads that passed filter.\n\n* **Removed sequences** is a file in BAM format, too. That represents sequences which are removed.\n\n* **Output report** is the text file of how many sequences have been analyzed and removed.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Filter Non Conversion** examines the methylation calls of reads, or read pairs for paired-end sequencing, and filters out reads that exceed a certain **Threshold** (`--threshold`) of methylated calls in non-CG context (the default is 3). This is extended to the possibility of setting a percentage **Percentage cutoff** (`--percentage_cutoff`) of methylation in non-CG context for any given read.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n* This kind of filtering is not advisable and will introduce biases if analysis considers the organisms with any appreciable levels of non-CG methylation (e.g. most plants [2]).\n\n* **Filter Non Conversion** in paired-end analysis expects Read 1 and Read 2 to follow each other in consecutive lines. The file should be sorted by read name.\n\n### Performance Benchmarking\n\n| Size of BAM file| Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- |\n| 1.9 GB | c4.2xlarge | 10m | $0.09 |\n| 27 GB | c4.2xlarge | 5h 14m | $2.8 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.\n\n[2] Niederhuth C. E., Schmitz R. J. , [Putting DNA methylation in context: from genomes to gene expression in plants](https://doi.org/10.1016/j.bbagrm.2016.08.009). Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms, Volume 1860, Issue 1, 2017, Pages 149-156, ISSN 1874-9399.", "input": [{"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "Single-end"}, {"name": "Threshold"}, {"name": "Consecutive"}, {"name": "Percentage cutoff"}, {"name": "Minimum count"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Filtered BAM", "encodingFormat": "application/x-bam"}, {"name": "Removed sequences", "encodingFormat": "application/x-bam"}, {"name": "Output report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Filtering"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561065, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-genome-preparation-0-21-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-genome-preparation-0-21-0/6", "applicationCategory": "CommandLineTool", "name": "Bismark Genome Preparation", "description": "**Bismark Genome Preparation** is a tool for converting a specified reference genome into bisulfite converted genome. \n\nIt is part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe **Bismark Genome Preparation** tool requires one input:\n- **FASTA reference genome** - a file with reference genome fasta sequences, in FASTA or FA format.\n\nThe output of the tool is a folder (in TAR.GZ format), which contains reference genome FASTA file and two subfolders with two conversions (C->T and G->A) of the original reference genome. Depending on the inputs and parameters settings, this output folder can contain some additional files, too. The name of compressed output folder contains the reference name (e.g. GRCh37) and the name of the indexer that was used (Bowtie 2 or HISAT 2).\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**The Bismark Genome Preparation** tool is used for converting a specified reference genome into two different bisulfite converted versions and index them for alignments with Bowtie 2 (default), or HISAT2 (default indexer can be changed to HISAT 2 with setting **Hisat2** parameter to TRUE). The first bisulfite genome will have all Cs converted to Ts (C->T), and the other one will have all Gs converted to As (G->A). Both bisulfite genomes will be stored in subfolders within the reference genome folder. Once the bisulfite conversion has been completed, the program will fork and launch\ntwo simultaneous instances of the Bowtie 2 or HISAT2 indexer (bowtie2-build or hisat2-build, respectively), depending on the settings of the **Hisat2** parameter. \n\n### Changes Introduced by Seven Bridges\n\n* The tool requires FASTA or FA file with reference genome sequences as an input, instead of the folder with the reference in it. The tool itself then creates the folder and uses it as an output, once the converted genomes are created in it.\n\n### Common Issues and Important Notes\n\n* Please note that HISAT2 and Bowtie 2 indexes are not compatible. To create a genome index for use with HISAT2 the parameter **HISAT2** must be set to TRUE (which corresponds to using `--hisat2` parameter in the command line).\n* Be aware that the indexing process can take up to several hours. This will mainly depend on genome size and system resources.\n\n### Performance Benchmarking\n\nBased on our experience, **Bismark Genome Preparation** needs more RAM memory with increasing the size of input FASTA files, while the consumption of CPUs remains stable and relatively low. That is why we chose r4.2xlarge AWS instance as a default instance to run this tool.The table below shows some approximate run times and costs. The true values will depend on genome files sizes and available resources.\n\n\n|Number of genomes|Total input size|Duration|Cost|\n| :---: | :---: | :---: | :---: |\n| 1 | 3 GB | 2h 15m | $1.19 |\n| 3 | 4.2 GB | 5h 28m | $3.74  |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "FASTA reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Genomic composition"}, {"name": "Single fasta"}, {"name": "Hisat2"}, {"name": "Slam"}, {"name": "Parallel threads"}, {"name": "Memory per job"}, {"name": "CPU per jpb"}], "output": [{"name": "Bisulfite converted genome folder", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Indexing"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561065, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-methylation-extractor-0-21-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-methylation-extractor-0-21-0/6", "applicationCategory": "CommandLineTool", "name": "Bismark Methylation Extractor", "description": "**Bismark Methylation Extractor** is a tool that extracts the methylation call for every Cytosine in Bismark result files. \n\n It is a part of Bismark toolkit, a set of tools for time-efficient analysis of BS-Seq data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1]. **Bismark Methylation Extractor** operates on Bismark alignment file and extracts the methylation call for every single C analyzed. The position of every single C will be written out to a new output file, depending on its context (CpG, CHG or CHH), whereby methylated Cs will be labelled as forward reads (+), non-methylated Cs as reverse reads (-). \n\nThe tool has two inputs (both required):\n\n* **Aligment file(s)** - one or more alignment files generated by **Bismark** or **Bismark Deduplicate**, in SAM, BAM or CRAM format;\n* **Reference genome file(s)** - a reference genome in FASTA or FA format; this input field corresponds to using the option `--genome-folder` when `--cytosine_report` is selected in original *bismark_methylation_extractor* script and it is required if parameter **Cytosine report** (`--cytosine_report`) is set to TRUE.\n\n**Bismark Methylation Extractor** produces several reports as outputs, depending on additional options. The possible outputs are:\n\n* **CHG context** - a report with methylation states of all cytosines in CHG context;\n* **CpG context** - a report with methylation states of all cytosines in CpG context;\n* **CHH context** - a report with methylation states of all cytosines in CHH context;\n* **Mbias** report- the methylation proportion across each possible position in the reads (position, count methylated, count unmethylated, % methylation, coverage);\n* **Splitting report** - summary methylation report;\n* **BedGraph** - if **BedGraph** input parameter is set to TRUE (`--bedGraph`), the sorted bedGraph file will be produced, with positions (start and end) of a given cytosine and its methylation state (in %);\n* **Coverage Output** - the file which reports the position of methylated or unmethylated cytosine, methylation percentage and count of methylated and unmethylated cytosine. **BedGraph** input parameter should be set to TRUE (`--bedGraph`);\n* **Cytosine Methylation Report** - genome-wide methylation report for all cytosines in the genome; this report is produced only if **Cytosine report** (`--cytosine_report`) is set to TRUE.\n* **Yacht file** is a file with additional information about the read a methylation call belongs to. NOMe\\_filtering tool in Bismark toolkit requires **Yacht** file as input. This file will be created when **Single-end** is set to TRUE, **CX\\_context** (`--CX`) is chosen and **mbias\\_only** (`--mbias_only`) in parameter **Mbias** is not selected.\n\n**CHG context**, **CpG context** and **CHH context** all have the same structure: one line corresponds to one cytosine found in a given context in a reference, and it consists of an ID of a sequence, its methylation state (Methylated cytosines receive a '+' orientation, unmethylated '-' orientation), chromosome name, position and methylation call.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Bismark Methylation Extractor** operates on **Bismark** result files and extracts the methylation call for every single C analysed. The position of every single C will be written out to a new output file, depending on its context (CpG, CHG or CHH), whereby methylated Cs will be labelled as forward reads (+), non-methylated Cs as reverse reads (-). The resulting files can be imported into a genome viewer and the analysis of methylation data can commence.\nAlternatively, the output of the methylation extractor can be transformed into a bedGraph and coverage file, depending on the parameter **BedGraph** (`--bedGraph`) setting. The **BedGraph** file will have the following structure:\n\n` <chromosome> <start position> <end position> <methylation percentage> `\n\nTwo additional columns will be written into **Coverage Output** file, which enable basically any downstream processing from the file. The structure of **Coverage Output** file look like this:\n\n` <chromosome> <start position> <end position> <methylation percentage> <count methylated> <count unmethylated> `\n\nBy default, this mode will only consider cytosines in CpG context, but it can be extended to cytosines in any sequence context by using the parameter **CX Context** (`--CX`).\n\n\n### Changes Introduced by Seven Bridges\n\n* Instead of a genome directory, the tool requires the fasta itself on **Reference genome file(s)** input.\n\n\n### Common Issues and Important Notes\n\n* **Reference genome file(s)** is mandatory for the genome-wide cytosine methylation report (if the parameter **Cytosine report** (`--cytosine_report`) is set to TRUE). Otherwise, it is not required.\n\n\n### Performance Benchmarking\n**Bismark Methylation Extractor** is not memory nor CPU demanding and it can be run on smaller instances; 8 GB of RAM and 8 CPUs will be sufficient in most of the situations. The storage requirement depends on the input file size and reports that will be generated.\nThe table below shows some approximate run times and costs. All test were done with **BedGraph** (`--bedGraph`), **Comprehensive** (`--comprehensive`) and **Cytosine report**(`--cytosine_report`) parameters set to TRUE and on the c4.2xlarge AWS instance.\n\n|Input BAM size| Duration | Cost |\n| :---: | :---: | :---: |\n| 1.9 GB | 1h 24m | $0.56 |\n| 4.7 GB | 3h 2m | $1.21 |\n| 27.8 GB | 13h 42m | $5.48 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pubmed/21493656). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Reference genome file(s)", "encodingFormat": "application/x-fasta"}, {"name": "Alignment file(s)", "encodingFormat": "application/x-bam"}, {"name": "Single-end/Paired-end"}, {"name": "Include overlap"}, {"name": "Ignore"}, {"name": "Ignore r2"}, {"name": "Comprehensive"}, {"name": "BedGraph"}, {"name": "Buffer size"}, {"name": "Multicore"}, {"name": "Zero based"}, {"name": "Cutoff"}, {"name": "Remove spaces"}, {"name": "CX context"}, {"name": "Scaffolds"}, {"name": "Ample memory"}, {"name": "Ignore 3prime r2"}, {"name": "Ignore 3prime"}, {"name": "Merge non CpG"}, {"name": "No header"}, {"name": "Gzip"}, {"name": "Mbias"}, {"name": "Cytosine report"}, {"name": "Split by chromosome"}, {"name": "Yacht"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "CHG context", "encodingFormat": "text/plain"}, {"name": "CpG context", "encodingFormat": "text/plain"}, {"name": "CHH context", "encodingFormat": "text/plain"}, {"name": "Mbias", "encodingFormat": "text/plain"}, {"name": "Splitting report", "encodingFormat": "text/plain"}, {"name": "BedGraph"}, {"name": "Coverage Output file"}, {"name": "Cytosine Methylation Report", "encodingFormat": "text/plain"}, {"name": "Yacht file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Methylation Extraction"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648560772, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-nome-filtering-0-21-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-nome-filtering-0-21-0/5", "applicationCategory": "CommandLineTool", "name": "Bismark NOMe filtering", "description": "**NOMe filtering**  is used for filtering reads in yacht file (output of **Bismark Methylation Extractor**).\n\nIt is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\n**NOMe filtering** has two required inputs:\n\n* **Yacht file** is a file with additional information about the read a methylation call belongs to. This file needs to have been generated with **Bismark Methylation Extractor** with the option **Yacht** specified.\n* **Reference genome file(s)** is(are) a reference genome file(s) in .FASTA or .FA format.\n\nThis tool has one output:\n\n* **Report file** reports cytosines in CpG context only if they are in A-CG or T-CG context, and cytosines in GC context only when the C is not in CpG context. The output file is in tab-delimited format.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**NOMe filtering**  is used for filtering reads in yacht file (output of Bismark Methylation Extractor), which contains additional information about the reads that methylation calls belonged to. It processes entire (**Single-end**) reads and then filters calls for NOMe-Seq positions (nucleosome occupancy and methylome sequencing) where accessible DNA gets methylated in a GpC context:\n \n* Filters CpGs to only output cytosines in A-CG and T-CG context;\n\n* Filters GC context to only report cytosines in GC-A, GC-C and GC-T context.\n\nBoth of these measures aim to reduce unwanted biases, i.e. the influence of G-CG (intended) and C-CG (off-target) on endogenous CpG methylation, and the influence of CpG methylation on (the NOMe-Seq specific) GC context methylation.\n\n### Changes Introduced by Seven Bridges\n\n* **NOMe\\_filtering** tool creates **genome\\_folder** which accepts provided genome reference files, instead of providing a path to genome folder (as required by the original NOMe_filtering script `--genome_path`).\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\n\n| Input size (Yacht file) | Duration | Cost | Instance (AWS) |\n| --- | --- | --- | --- |\n|4.4 GB|30 min|$0.27|c4.2xlarge (1024GB storage)|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*    \n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pubmed/21493656). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Reference genome file(s)", "encodingFormat": "application/x-fasta"}, {"name": "Yacht file", "encodingFormat": "text/plain"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Report file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/archive/0.21.0.tar.gz", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Filtering"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648561065, "dateCreated": 1574861319, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bitmapperbs-align-1-0-2-3/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bitmapperbs-align-1-0-2-3/7", "applicationCategory": "CommandLineTool", "name": "BitMapperBS Align", "description": "**BitMapperBS Align** is a high-speed and memory-efficient aligner used for processing BS-seq reads. It is designed to process WGBS (whole-genome bisufite) reads from directional protocol.\n\n**BitMapperBS Align** follows the seed-and-extend strategy. Compared with DNA sequence aligners, WGBS aligners need to extend more candidate locations due to the lower sequence complexity of bisulfite sequencing reads (C is not encoded by the three-letter FM-index). By utilising the precomputed FM-index, **BitMapperBS Align** extracts multiple overlapping and variable-length seeds from every read, and searches for the occurrence positions of these seeds via FM-index. After that, in the extension step, these occurrence positions of seeds are extended to obtain the full alignment of each read. The algorithm used in this step extends multiple candidate locations simultaneously, while the other aligners extend their candidate locations one-by-one. As a result, the time-consuming extension step of the **BitMapperBS Align** tool can be significantly accelerated. Like most existing WGBS aligners, it discards an ambiguous read if it is equally aligned to multiple best mapping locations [1].\n\nThe tool has two required inputs:\n\n* **Index folder** - a folder containing FM-indexes of the reference files, generated by the **BitMapperBS Index** tool;\n\n* **Fastq files** - input files with single-end or paired-end reads; the files can be in FASTQ, FASTQ.GZ, FQ or FQ.GZ formats; all reads should be preprocessed by the popular **Trim Galore** tool to perform the quality and adapter trimming.\n\nThe tool produces the following outputs:\n\n* **Alignment file** - file with aligned reads; it is in BAM format by default, but it can be switched to SAM using the **Output format** parameter;\n\n* **Alignment report** - file in TXT format, with statistical report.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**BitMapperBS Align** is designed to efficiently align massive WGBS reads against the large reference genome. It takes a folder containing FM-indexes of the reference (generated by the **BitMapperBS Index** tool) and single-end or paired-end reads from directional library (preprocessed by the popular **Trim Galore** tool) as the inputs. **BitMapperBS Align** is scalable with the length of reads, while the performance of the other WGBS aligners decreased when processing longer reads. Therefore, **BitMapperBS Align** is suited for longer WGBS reads produced by new high-throughput sequencing technologies, as well as for short reads.\n\n### Changes Introduced by Seven Bridges\n\n* **Alignment file** is set to be in BAM format by default. It can be switched to SAM using the **Output format** parameter.\n\n### Common Issues and Important Notes\n\n* **Input reads** must have metadata fields properly set: **Sample ID** for single-end, and **Sample ID** and **Paired-end** fields for paired-end samples; **Paired-end** field should be **1** or **2**;\n\n* The **Unmapped reads report** (`--out_unmapped`) and **Ambiguous reads report** (`--out_ambiguous`) parameters should not be set to 'True' simultaneously with **Intermediate methylation output** (`--out_methy`); Only unique mapped reads can be processed by methylation extraction for intermediate methylation output.\n\n\n### Performance Benchmarking\n\n**BitMapperBS Align** is a fast and memory-efficient aligner. Analyses with the human genome indexes as **Index input** show that running time mostly depends on the size of the **Input reads** files. Optimal memory and CPU requirements are **15GB** and **8CPU**. It is recommended to set up the maximum **Number of threads** for the chosen instance to gain better running time and cost.\n\nIn the following table you can find estimates of **BitMapperBS Align** running time and cost.\n\n                   \n| Reads size |  Single-end/Paired-end | Running time| AWS instance | Number of threads | Cost |\n|--------------------------------|--------------------------|---------------------|--------------------------|----------------------|-----------------------|---------------------------|\n| 1.4 GB | Single-end |   5min  | c4.2xlarge (on-demand) | 8 | ~ $0.04 |\n| 2 x 2.4GB| Paired-end | 7min  | c4.2xlarge (on-demand)  | 8 | ~ $0.07 |\n| 2x 50GB|  Paired-end |  42m | c4.2xlarge (on-demand) | 8 | ~ $0.37|\n| 2x 107.5GB|  Paired-end | 36min | c5.9xlarge (on-demand) |36 | ~ $0.99 |\n| 2x 210 GB| Paired-end |  3h 4min| c4.2xlarge (on-demand) | 8 | ~ $1.64 |\n| 2x 210 GB|  Paired-end |  56min | c5.9xlarge (on-demand) | 36 | ~ $1.56 |\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \u00a0 \u00a0 \u00a0\u00a0       \n\n\n\n### References\n\n[1] [BitMapperBS paper](https://www.biorxiv.org/content/10.1101/442798v1.full.pdf+html)", "input": [{"name": "Output format"}, {"name": "Name of the alignment output"}, {"name": "Number of threads"}, {"name": "Name of the statistics output"}, {"name": "Edit distance rate"}, {"name": "Minimun observed template length"}, {"name": "Maximum observed template length"}, {"name": "Pbat protocol"}, {"name": "Unmapped reads report"}, {"name": "Ambiguous reads report"}, {"name": "Phred value"}, {"name": "Maximum mismatch penalty"}, {"name": "Minimum mismatch penalty"}, {"name": "Ambiguous character penalty"}, {"name": "Gap open penalty"}, {"name": "Gap extension penalty"}, {"name": "Index folder"}, {"name": "Fastq files", "encodingFormat": "text/fastq"}, {"name": "Fast mode"}, {"name": "Sensitive mode"}, {"name": "Intermediate methylation output"}, {"name": "Bmm folder name"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Alignment file", "encodingFormat": "application/x-sam"}, {"name": "Alignment report", "encodingFormat": "text/plain"}, {"name": "Intermediate methylation output"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/chhylp123/BitMapperBS", "https://github.com/chhylp123/BitMapperBS/releases", "https://github.com/chhylp123/BitMapperBS/archive/v1.0.2.3.tar.gz", "https://github.com/chhylp123/BitMapperBS/blob/master/README.md"], "applicationSubCategory": ["Methylation", "Alignment"], "project": "SBG Public Data", "creator": "Haoyu Cheng, Yun Xu", "softwareVersion": ["v1.0"], "dateModified": 1648048381, "dateCreated": 1612360501, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bitmapperbs-index-1-0-2-3/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bitmapperbs-index-1-0-2-3/9", "applicationCategory": "CommandLineTool", "name": "BitMapperBS Index", "description": "**BitMapperBS Index** is a tool used to build indexes of the reference genome. Generated indexes will be used by the **BitMapperBS Align** tool.\n\n**BitMapperBS Index** applies a three-letter FM-index that reorganises BWT to accelerate the character count operation. C is not encoded by the three-letter FM-index since all Cs have been converted to Ts [1].\n\nThe tool has one required input:\n\n* **Input reference file**, which can be in FASTA, FA format.\n  \n  \nThe tool produces the following output:\n\n* **Index folder**, a directory with indexes of the reference.     \n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**BitMapperBS Index** takes a FASTA or FA reference file as the required input and builds a highly optimised FM-index on the concatenation of the reference genome\u2019s two original strands with all Cs converted to Ts. As a compressed full-text index, the FM-index is proposed to reduce the memory footprint of the classical suffix array [1]. A directory with the reference indexes generated by **BitMapperBS Index** is used as a required input for the alignment tool, **BitMapperBS Align**.  \n\n### Changes Introduced by Seven Bridges\n\n* The **Prefix** (`--index_folder`) parameter for setting the folder that stores the genome indexes is set by default. Originally, if this option was not set, the indexes would be stored in the same folder as the genome (input reference file).\n\n### Common Issues and Important Notes\n\n* Memory and CPU requirements are set to **14GB** and **7CPU**, which is optimal for building the index of the human reference genome. In other situations, please set **Memory in MB per job** and **Number of CPUs per job** accordingly. \n\n### Performance Benchmarking\n\n**BitMapperBS Index** builds the indexes for the **BitMapperBS Align** tool relatively fast. Both running time and need for RAM memory increase with the size of the **Input reference file**. Therefore, for larger reference files (e.g. 5.6GB), memory should be set correspondingly.\n\nIn the following table you can find estimates of **BitMapperBS Index** running time and cost. \n                   \n| Reference file size | Running time | Cost  | Instance |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| 5.6 GB |  1h 42min| ~ $2.84  | c5.9xlarge (on-demand)  |\n| 3 GB    | 58min     | ~ $1.61  |c5.9xlarge (on-demand) |\n| 2.5 GB  | 1h 3min     | ~ $0.57  | c4.2xlarge (on-demand)|\n| 1.2 GB   | 29min     | ~ $0.27   | c4.2xlarge (on-demand) |\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n\n\n### References\n\n[1] [BitMapperBS paper](https://www.biorxiv.org/content/10.1101/442798v1.full.pdf+html)", "input": [{"name": "Input reference file", "encodingFormat": "application/x-fasta"}, {"name": "Index folder name"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Index folder"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/chhylp123/BitMapperBS", "https://github.com/chhylp123/BitMapperBS/releases", "https://github.com/chhylp123/BitMapperBS/archive/v1.0.2.3.tar.gz", "https://github.com/chhylp123/BitMapperBS/blob/master/README.md"], "applicationSubCategory": ["Methylation", "FASTA Processing", "Indexing"], "project": "SBG Public Data", "creator": "Haoyu Cheng, Yun Xu", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1612360502, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bolt-lmm-2-3-5-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bolt-lmm-2-3-5-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "BOLT-LMM", "description": "**BOLT-LMM** tests the association between genotypes and phenotypes using a linear mixed model [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**BOLT-LMM** authors recommend running the tool on cohorts with at least 5000 subjects, to analyze quantitative traits and traits with balanced case-controls [2]. This wrapper can be used to run both BOLT-LMM and BOLT-RELM algorithms (not in the same task, as the options are mutually exclusive).\n\nA genetic map file (**Genetic map file** input) should be provided to **BOLT-LMM** if the input Plink BIM file does not contain genetic coordinates [2]. The genetic maps are distributed alongside **BOLT-LMM** and can be obtained from the software TAR.GZ archive [3].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--help` and `--helpFull` are omitted from the wrapper.\n* To facilitate batch processing, input parameters corresponding to output file name prefixes for different genotype formats (`--statsFile`,  \t`--statsFileDosageSnps`, `--statsFileBgenSnps`, `--statsFileImpute2Snps` and `--statsFileDosage2Snps`) will, if not provided by the user, derive values from the associated genotype input files.\n\n### Common Issues and Important Notes\n\n* If multiple Plink files (BED and BIM) should be analyzed, **Plink BED files** and **Plink BIM files** inputs should be used. Conversely, the **Plink files** input (`--bfile`) should be used if only one dataset is to be analyzed.\n* Genetic map files are available from the **BOLT-LMM** distribution archive [3].\n* For details on accepted imputed dataset formats, please see the official documentation [2].\n* Please consult the **BOLT-LMM execution log** output for task execution details and BOLT-RELM results (if this analysis was requested).\n* Please note that the tool expects at least one input file with genotypes.\n\n### Performance Benchmarking\n\nFor typical datasets (>10000 subjects), RAM required is approximately (number_of_SNPs * number_of_individuals)/4 bytes [2]. The numbers of SNPs and individuals used in this calculation do not include variants and subjects excluded from the analysis.\nThe running time of **BOLT-LMM** scales roughly with (number_of_SNPs * number_of_individuals)^1.5 [2].\n\nPlease note that due to the limited availability of public datasets on the scale **BOLT-LMM** is designed for, this wrapper was tested on a fairly simple simulated cohort (50000 participants with 990100 variants, dataset 1) in Plink format. Performance of the tool on a real-world dataset may differ.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1, 8 threads | 22 min | $0.15 + $0.05 | c4.2xlarge - 1000 GB EBS | \n| Dataset 1, 15 threads | 17 min | $0.19 + $0.94 | c5.4xlarge - 1000 GB EBS | \n \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**BOLT-LMM** was tested with cwltool version 3.1.20211107152837. The `in_plink_files`, `in_pheno_file` and `pheno_col` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [BOLT-LMM publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4342297/)\n\n[2] [BOLT-LMM manual](https://storage.googleapis.com/broad-alkesgroup-public/BOLT-LMM/BOLT-LMM_manual.html)\n\n[3] [BOLT-LMM download](https://alkesgroup.broadinstitute.org/BOLT-LMM/downloads/)", "input": [{"name": "Plink files", "encodingFormat": "text/x-bed"}, {"name": "Plink files [GZIP]", "encodingFormat": "text/x-bed"}, {"name": "Plink FAM file"}, {"name": "Plink BIM files"}, {"name": "Plink BED files", "encodingFormat": "text/x-bed"}, {"name": "Genetic map file"}, {"name": "Individuals to ignore", "encodingFormat": "text/plain"}, {"name": "SNPs to ignore", "encodingFormat": "text/plain"}, {"name": "Maximum missing rate per SNP"}, {"name": "Maximum missing rate per individual"}, {"name": "Phenotype file", "encodingFormat": "text/plain"}, {"name": "Phenotype column"}, {"name": "Use the last FAM column as phenotype"}, {"name": "Covariate file"}, {"name": "Categorical covariate columns"}, {"name": "Quantitative covariate columns"}, {"name": "Include samples with missing covariates"}, {"name": "Run variance components analysis"}, {"name": "Run LMM"}, {"name": "Run LMM infinitesmal only"}, {"name": "LMM force non-inf"}, {"name": "Model SNPs", "encodingFormat": "text/plain"}, {"name": "LD scores file"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name PLINK prefix"}, {"name": "Dosage files"}, {"name": "FIDs and IIDs for dosage files"}, {"name": "Output file prefix for dosage assoc stats"}, {"name": "File with IMPUTE2 file pairings"}, {"name": "IMPUTE2 files"}, {"name": "IMPUTE2 samples FIDs and IIDs"}, {"name": "IMPUTE2 min MAF"}, {"name": "BGEN-format genotypes file"}, {"name": "BGEN samples file", "encodingFormat": "text/plain"}, {"name": "BGEN inputs pairing"}, {"name": "BGEN minimum MAF"}, {"name": "BGEN minimum INFO"}, {"name": "Output file name prefix for BGEN assoc stats"}, {"name": "Output file name prefix for IMPUTE2 assoc stats"}, {"name": "File with 2-dosage pairings"}, {"name": "Output file name prefix for 2-dosage assoc stats"}, {"name": "Disable automatic check of genetic map scale"}, {"name": "Disable PLINK dosage ID check"}, {"name": "Disable PLINK 2-dosage ID check"}, {"name": "Disable PLINK IMPUTE2 ID check"}, {"name": "Disable PLINK BGEN ID check"}, {"name": "Maximum number of model SNPs"}, {"name": "Maximum number of categorical covariate levels"}, {"name": "Number of SNP groups to leave out"}, {"name": "Number of random SNPs for calibration"}, {"name": "H2g guess"}, {"name": "Number of MC trials for estimating h2g"}, {"name": "Number of MC trials to reestimate h2g"}, {"name": "Faster REML variance parameters estimation"}, {"name": "Initial variance parameter guesses for REML optimization"}, {"name": "Genetic distance buffer [Morgans]"}, {"name": "Physical distance buffer [bp]"}, {"name": "Prior probability SNP effect drawn"}, {"name": "Prior fraction of variance in small-effect mixture component"}, {"name": "Number of cross validation folds for splitting"}, {"name": "Maximum number of cross validation folds to compute"}, {"name": "Run full cross validation"}, {"name": "LD scores column"}, {"name": "Use LD scores estimated among chip SNPs"}, {"name": "Match SNPs to reference LD scores by position"}, {"name": "Number of autosomes"}, {"name": "Tolerance for conjugate gradient solver convergence"}, {"name": "Tolerance for variational Bayes convergence"}, {"name": "Maximum number of iterations"}, {"name": "SNPs per block"}, {"name": "Compute Bayesian LMM stats with MCMC"}, {"name": "Number of MCMC iterations to use"}, {"name": "Output additional columns in the stats file"}, {"name": "Output file name for betas for risk prediction"}, {"name": "Dosage-2 files"}], "output": [{"name": "Association results at PLINK genotypes"}, {"name": "Association results at dosage genotypes"}, {"name": "Association results at BGEN genotypes"}, {"name": "Association results at IMPUTE2 genotypes"}, {"name": "Association results at 2-dosage genotypes"}, {"name": "BOLT-LMM execution log"}, {"name": "Output file for betas for risk prediction"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["GWAS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Po-Ru Loh", "softwareVersion": ["v1.2"], "dateModified": 1648045277, "dateCreated": 1627654456, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bowtie2-aligner/18", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bowtie2-aligner/18", "applicationCategory": "CommandLineTool", "name": "Bowtie2 Aligner", "description": "Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an [FM Index](http://portal.acm.org/citation.cfm?id=796543) (based on the [Burrows-Wheeler Transform](http://en.wikipedia.org/wiki/Burrows-Wheeler_transform) or [BWT](http://en.wikipedia.org/wiki/Burrows-Wheeler_transform)) to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 gigabytes of RAM. In order to create needed index files, you should run [Bowtie2 Indexer](https://igor.sbgenomics.com/public/apps#tool/admin/sbg-public-data/bowtie2-indexer), which produces archived index files (containing 6 files with suffixes .1.bt2, .2.bt2, .3.bt2, .4.bt2, .rev.1.bt2, and .rev.2.bt2).\n\nBowtie 2 supports gapped, local, and paired-end alignment modes. Bowtie 2 outputs alignments in SAM format, enabling interoperation with a large number of other tools (e.g. [SAMtools](http://samtools.sourceforge.net/), [GATK](http://www.broadinstitute.org/gsa/wiki/index.php/The_Genome_Analysis_Toolkit)) that use SAM.\n\n###Common issues###\nNo issues have been reported.\n\n**Q&A:**\n\n***Q: What should I do if I already have Bowtie2 index files, not archived as tar bundle?***\n\n***A***: You can provide your *.bt2 files to [SBG Compressor](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/sbg-compressor-1-0/) app from our public apps and set \"TAR\" as your output format. After the task is finished, **you should assign common prefix of the index files to the `Reference genome` metadata field** and your TAR is ready for use.\n\n***Example:***\nIndexed files: chr20.1.bt2, chr20.2.bt2, chr20.3.bt2, chr20.4.bt2, chr20.rev.1.bt2, chr20.rev.2.bt2\n\nMetadata - `Reference genome`: **chr20**\n\n__Important note: In case of paired-end alignment it is crucial to set metadata 'paired-end' field to 1/2. Sequences specified as mate 1s must correspond file-for-file and read-for-read with those specified for mate 2s. Reads may be a mix of different lengths. In case of unpaired reads, the same metadata field should be set to '-'. Only one type of alignment can be performed at once, so all specified reads should be either paired or unpaired.__", "input": [{"name": "Bowtie index archive", "encodingFormat": "application/x-tar"}, {"name": "Alignment mode"}, {"name": "Skip reads"}, {"name": "Align next n reads"}, {"name": "Trim from 5'"}, {"name": "Trim from 3'"}, {"name": "Quality scale"}, {"name": "Integer qualities"}, {"name": "Allowed mismatch number"}, {"name": "Seed substring length"}, {"name": "Dynamic padding"}, {"name": "Disallow gaps"}, {"name": "Ignore qualities"}, {"name": "Don't align forward"}, {"name": "Don't align reverse complement"}, {"name": "Disable 1 mismatch alignments"}, {"name": "Preset"}, {"name": "Set match bonus"}, {"name": "Maximum mismatch penalty"}, {"name": "Ambiguous character penalty"}, {"name": "Read gap penalties"}, {"name": "Reference gap penalties"}, {"name": "Report k alignments"}, {"name": "Report all alignments"}, {"name": "Seed extension attempts"}, {"name": "Max number of re-seed"}, {"name": "Minimum fragment length"}, {"name": "Maximum fragment length"}, {"name": "Mates alignment orientation"}, {"name": "Disable unpaired alignments"}, {"name": "Disable discordant alignments"}, {"name": "Disable dovetail alignments"}, {"name": "Disable containing alignments"}, {"name": "Disable overlapping alignments"}, {"name": "Suppress header lines"}, {"name": "Suppress SQ header lines"}, {"name": "Set the read group ID"}, {"name": "Platform"}, {"name": "Sample"}, {"name": "Library"}, {"name": "Platform unit"}, {"name": "Sequencing center"}, {"name": "Median fragment length"}, {"name": "Omit SEQ and QUAL"}, {"name": "Reorder output"}, {"name": "Set seed"}, {"name": "Non deterministic"}, {"name": "Interval function"}, {"name": "Constant A [ interval function ]"}, {"name": "Coefficient B [ interval function ]"}, {"name": "Ambiguous chars function"}, {"name": "Constant A [ ambiguous chars function ]"}, {"name": "Coefficient B [ ambiguous chars function ]"}, {"name": "Alignment score function"}, {"name": "Constant A [ alignment score function ]"}, {"name": "Coefficient B [ alignment score function ]"}, {"name": "Unpaired unaligned reads"}, {"name": "Unpaired aligned reads"}, {"name": "Paired unaligned reads"}, {"name": "Paired aligned reads"}, {"name": "Read sequence", "encodingFormat": "text/fastq"}, {"name": "Suppress SAM records for unaligned reads"}, {"name": "Input FASTA files"}, {"name": "Number of threads"}], "output": [{"name": "Result SAM file", "encodingFormat": "application/x-sam"}, {"name": "Aligned reads only", "encodingFormat": "text/fastq"}, {"name": "Unaligned reads only", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/BenLangmead/bowtie2"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Ben Langmead/John Hopkins University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1533302763, "dateCreated": 1453799018, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bowtie2-indexer/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bowtie2-indexer/4", "applicationCategory": "CommandLineTool", "name": "Bowtie2 Indexer", "description": "Bowtie2 Indexer is a tool for indexing reference genomes of any size used in an alignment. It was built from `bowtie2-build` script and used for reference genome indexing aimed at assisting Bowtie2 in fast and memory-efficient alignment. It outputs an archive which consists of 6 files with suffixes .1.bt2, .2.bt2, .3.bt2, .4.bt2, .rev.1.bt2, and .rev.2.bt2. This archive constitutes the index and should be provided when aligning the reads (either with [Bowtie2 Aligner](https://igor.sbgenomics.com/public/apps#tool/admin/sbg-public-data/bowtie2-aligner) or [TopHat2](https://igor.sbgenomics.com/public/apps#tool/admin/sbg-public-data/tophat2)). \n\n###Common issues###\nNo issues have been reported.", "input": [{"name": "Reference/Index files", "encodingFormat": "application/x-tar"}, {"name": "Large index"}, {"name": "Disable default parameters"}, {"name": "Packed representation"}, {"name": "Suffixes"}, {"name": "Suffixes as fraction"}, {"name": "Difference-cover period"}, {"name": "Disable diff-cover sample"}, {"name": "Discard bitpacked files"}, {"name": "Only bitpacked files"}, {"name": "Rows to mark"}, {"name": "Ftab lookup table size"}, {"name": "Seed"}], "output": [{"name": "Bowtie index archive", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Alignment", "Indexing"], "project": "SBG Public Data", "creator": "Ben Langmead/John Hopkins University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1485268785, "dateCreated": 1453799295, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bracken-2-5/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bracken-2-5/4", "applicationCategory": "CommandLineTool", "name": "Bracken", "description": "**Bracken** (Bayesian Reestimation of Abundance with KrakEN) tool is used for abundance estimation [1]. **Bracken** uses the taxonomic assignments made by **Kraken**/**Kraken2**, a very fast read-level classifier, along with information about the genomes themselves to estimate abundance at the species level, the genus level, or above [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Bracken** takes a database generated by **Bracken Build** and the report from **Kracken2** as its input. It is a highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample. When running **Kraken2**, it is required to use the **--report** flag.\n\n### Changes Introduced by Seven Bridges\n\n* The main difference to a local execution is that input database, library and taxonomy files are expected and provided as TAR.GZ archives, containing the database top-level folder.\n* It is not allowed to specify output name as it is built from the input kraken report name, with the addition of the .bracken extension.\n\n### Common Issues and Important Notes\n\n* Bracken is compatible with both Kraken 1 and Kraken 2. However, we tested it only with Kraken 2 results [3]. \n* If you run Kraken using one of the pre-built MiniKraken databases, you can find corresponding Bracken files on the corresponding Bracken webpage [3].\n\n### Performance Benchmarking\n\nThe experiment task was performed on the default AWS on-demand c4.2xlarge instance using the standard database build with **Kraken2 Build**/**Bracken Build**. Execution time did not differ for different sample types. It took 35 minutes to finish ($0.23).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [Bracken git page](https://github.com/jenniferlu717/Bracken/)\n\n[2] [Bracken publication](https://peerj.com/articles/cs-104/)\n\n[3] [Bracken webpage](https://ccb.jhu.edu/software/bracken/)", "input": [{"name": "Bracken database", "encodingFormat": "application/x-tar"}, {"name": "Kraken kreport"}, {"name": "Read length"}, {"name": "Level"}, {"name": "Threshold"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Prefix for output file"}], "output": [{"name": "Output abundance estimation"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/jenniferlu717/Bracken", "https://github.com/jenniferlu717/Bracken/archive/v2.5.tar.gz", "https://github.com/jenniferlu717/Bracken/"], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "creator": "Jennifer Lu, Florian Breitwieser", "softwareVersion": ["v1.0"], "dateModified": 1648038176, "dateCreated": 1612269913, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bracken-build-2-5/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bracken-build-2-5/4", "applicationCategory": "CommandLineTool", "name": "Bracken Build", "description": "**Bracken Build** tool is used to prepare the reference database for **Bracken** [1]. It is a part of the **Bracken** toolkit which uses the taxonomic assignments made by **Kraken**/**Kraken2**, a very fast read-level classifier, along with information about the genomes themselves to estimate abundance at the species level, the genus level, or above.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Bracken Build** takes a database generated by **Kracken2 Build** as its input. It's output, together with **Kracken2** output, is then used in order to compute the abundance of species in DNA sequences from a metagenomics sample. \n\n### Changes Introduced by Seven Bridges\n\n* The main difference to a local execution is that input and output database, library and taxonomy files are expected and provided as TAR.GZ archives, containing the database top-level folder.\n\n### Common Issues and Important Notes\n\n* **Bracken** is compatible with both **Kraken1** and **Kraken2**. However, we tested it only with **Kraken2** results [3]. \n* If you run **Kraken(2)** using one of the pre-built MiniKraken databases, you can find corresponding **Bracken** files on the corresponding **Bracken** webpage [3]. Do not run **Bracken Build** with MiniKraken [2].\n\n### Performance Benchmarking\n     \n\n| Database  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Arachaea - 1 thread | 25 min | $0.40 + $0.03 | c4.8xlarge - 500GB |  \n| Standard - 1 thread | 18 h 18 min |$19.47 + $2.03 | r4.4xlarge - 800GB | \n| Standard - 8 threads| 8 h 17 min |$8.81 + $0.92 | r4.4xlarge - 800GB | \n| Standard - 15 threads| 6 h 50 min |$7.27 + $0.76 | r4.4xlarge - 800GB | \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [Bracken git page](https://github.com/jenniferlu717/Bracken/)\n\n[2] [Bracken publication](https://peerj.com/articles/cs-104/)\n\n[3] [Bracken webpage](https://ccb.jhu.edu/software/bracken/)", "input": [{"name": "Kraken database", "encodingFormat": "application/x-tar"}, {"name": "Threads"}, {"name": "Length of kmer"}, {"name": "Read length"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Output database", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/jenniferlu717/Bracken", "https://github.com/jenniferlu717/Bracken/archive/v2.5.tar.gz", "https://github.com/jenniferlu717/Bracken/"], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "creator": "Jennifer Lu, Florian Breitwieser", "softwareVersion": ["v1.0"], "dateModified": 1648038175, "dateCreated": 1612269912, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/breakdancer/18", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/breakdancer/18", "applicationCategory": "CommandLineTool", "name": "BreakDancer", "description": "BreakDancer-1.4.5 is a Cpp package that provides genome-wide detection of structural variants from next generation paired-end sequencing reads. BreakDancerMax predicts five types of structural variants: insertions, deletions,  inversions, inter- and intra-chromosomal translocations from next-generation short paired-end sequencing reads using read pairs that are mapped with unexpected separation distances or orientation. \nThe bam2cfg.pl in the released package is used to automatic generate a configuration file from bam files. If you have a single bam file that contains multiple libraries, make sure that the readgroup and library information are properly encoded in the bam header, and in each alignment record, otherwise bam2cfg.pl may fail to produce a correct configuration file.\n###Common Issues\nBAM INDEX FILES: When BreakDancer operates on a single chromosome (-o flag), bam index files are necessary.  Otherwise, they are not required. \nOPERATE ON A SINGLE CHROMOSOME: String used as input for this option (-o flag) must follow notation from bam files.", "input": [{"name": "Minimum mapping quality"}, {"name": "Using mapping quality instead of alternative mapping quality"}, {"name": "Minimal mean insert size"}, {"name": "Change default system from Illumina to SOLiD"}, {"name": "Cutoff in unit of standard deviation"}, {"name": "Number of observation required to estimate mean and s.d. insert size"}, {"name": "Cutoff on coefficients of variation"}, {"name": "A two column tab-delimited text file (RG, LIB) specify the RG=>LIB mapping, useful when BAM header is incomplete"}, {"name": "Number of bins in the histogram"}, {"name": "Output mapping flag distribution"}, {"name": "Plot insert size histogram for each BAM library"}, {"name": "Bams", "encodingFormat": "application/x-bam"}, {"name": "Operate on a single chromosome"}, {"name": "Minimum length of a region"}, {"name": "Cutoff in unit of standard deviation"}, {"name": "Maximum SV size"}, {"name": "Minimum alternative mapping quality"}, {"name": "Minimum number of read pairs required to establish a connection"}, {"name": "Maximum threshold of haploid sequence coverage for regions to be ignored"}, {"name": "Buffer size for building connection"}, {"name": "Only detect transchromosomal rearrangement"}, {"name": "Prefix of fastq files that SV supporting reads will be saved by library"}, {"name": "Dump SVs and supporting reads in BED format for GBrowse"}, {"name": "Analyze Illumina long insert (mate-pair) library"}, {"name": "Print out copy number and support reads per library rather than per bam"}, {"name": "Print out Allele Frequency column"}, {"name": "Output score filter"}, {"name": "Bai files"}], "output": [{"name": "Result"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/kenchen/breakdancer", "https://github.com/kenchen/breakdancer", "https://github.com/kenchen/breakdancer/wiki", "https://github.com/kenchen/breakdancer", "https://github.com/kenchen/breakdancer"], "applicationSubCategory": ["Structural Variant Calling"], "project": "SBG Public Data", "creator": "Ken Chen and Xian Fan/Washington University Genome Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038486, "dateCreated": 1476440106, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/breakseq-lite/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/breakseq-lite/7", "applicationCategory": "CommandLineTool", "name": "BreakSeq Lite", "description": "BreakSeq Lite performs junction mapping with aligned BAM files. It realignes the unaligned reads from BAM files, in order to detect structural variations(SV). It uses break point library(BPLIB) file, which contains regions flanking know break points, as the reference (FASTA).", "input": [{"name": "Input BAM files", "encodingFormat": "application/x-bam"}, {"name": "Input BPLIB (FASTA) file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Output result file"}, {"name": "Output binary file"}, {"name": "Output textual file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Hugo Y. K. Lam, Xinmeng Jasmine Mu , Adrian M. St\u00fctz, Andrea Tanzer, Philip D. Cayting, Michael Snyder, Philip M. Kim, Jan O. Korbel, and Mark B. Gerstein", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799551, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bustools-0-39-3/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bustools-0-39-3/7", "applicationCategory": "CommandLineTool", "name": "BUStools", "description": "**BUStools** is a suite of tools for working with BUS (Barcode-UMI-Set) files which facilitates rapid quantification and analysis of single-cell RNA-seq data. \nBUS format is a file format for single-cell RNA-seq data designed to facilitate the development of modular workflows for data processing. It consists of a binary representation of barcode and UMI sequences from scRNA-seq reads, along with sets of equivalence classes obtained by pseudoalignment of reads to a reference transcriptome (hence the acronym Barcode, UMI, Set) [1, 2].\n\n### Common Use Cases\n- If a whitelist file is provided on the **Barcode whitelist file** input port, the barcodes are error corrected by processing the BUS file with the **\u200bbustools correct\u200b** command, which will correct all barcodes that are at Hamming distance 1 (i.e. one substitution) away from a single barcode in the whitelist.\n- BUS file is sorted with \u200b**bustools sort** command, where duplicate reads are collapsed into a single record and their abundance saved as a new metadata column in the BUS file named multiplicity. This will create a new BUS file where the BUS records are sorted by barcode first, UMI second, and equivalence class third.\n- **bustools capture** command is used to separate BUS files into multiple files according to the capture criteria.\n- BUS files can be converted into a barcode-feature matrix with the **bustools count** command, where the feature can be TCCs (Transcript Compatibility Counts) or genes. The output is in a standard Matrix Market Exchange format.\n\n### Changes Introduced by Seven Bridges\n\n- To perform analysis on a gene level, it is required to provide transcripts to genes mappings file to the **Transcripts to genes** input port. The alternative is to generate this file by providing an annotation file in GTF format to the **GTF annotation** input port and selecting the **Create transcript to gene annotation** option.\n- BUStools suite of tools is created as a single tool in Common Workflow Language (CWL).\n- We introduced an R script that outputs the cell-gene count matrix in .Rdata format if the **Gene-level counts** (*--genecount*) option is selected.\n\n### Common Issues and Important Notes\n\n - Bustools correct\u200b  is an optional step performed if \u200bwhitelist file is provided on the **Barcode whitelist file** input port.\n - Bustools capture is an optional step performed if \u200bthe capture list file is provided on the **Capture list** input port.\n - Cell-gene count file produced as Rdata object can be used in Seurat interactive analysis for identification of cell subpopulations and discovering the marker genes.\n\n### Performance Benchmarking\n\nThe execution time of BUStools takes several minutes on the default instance. Unless specified otherwise, the default instance used to run it will be c5.2xlarge (AWS).\nThe price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n### References\n\n[1] [Modular and efficient pre-processing of single-cell RNA-seq](https://www.biorxiv.org/content/10.1101/673285v1)\n\n[2] [The barcode, UMI, set format and BUStools](https://www.biorxiv.org/content/10.1101/472571v2)", "input": [{"name": "Barcode whitelist file", "encodingFormat": "text/plain"}, {"name": "Input BUS file"}, {"name": "Number of threads"}, {"name": "Maximum memory used"}, {"name": "Name of sorted BUS file"}, {"name": "Capture complement"}, {"name": "Capture transcripts"}, {"name": "Capture UMIs"}, {"name": "Capture barcodes"}, {"name": "Capture list", "encodingFormat": "text/plain"}, {"name": "Equivalence classes"}, {"name": "Transcript names", "encodingFormat": "text/plain"}, {"name": "Transcripts to genes"}, {"name": "Prefix name for count output"}, {"name": "Gene-level counts"}, {"name": "Count multimapping"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Sorted BUS"}, {"name": "Compressed matrix"}, {"name": "Barcode names", "encodingFormat": "text/plain"}, {"name": "Equivalence class", "encodingFormat": "text/plain"}, {"name": "Gene names", "encodingFormat": "text/plain"}, {"name": "Counts table"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["Single Cell", "RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Pachter Lab", "softwareVersion": ["v1.0"], "dateModified": 1649165282, "dateCreated": 1575462926, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bwa-index-0-7-17/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bwa-index-0-7-17/2", "applicationCategory": "CommandLineTool", "name": "BWA INDEX", "description": "BWA INDEX constructs the FM-index (Full-text index in Minute space) for the reference genome.\nGenerated index files will be used with BWA MEM, BWA ALN, BWA SAMPE and BWA SAMSE tools.\n\nIf input reference file has TAR extension it is assumed that BWA indices came together with it. BWA INDEX will only pass that TAR to the output. If input is not TAR, the creation of BWA indices and its packing in TAR file (together with the reference) will be performed.\n\nTAR also contains alt reference from bwa.kit suggested by the author of the tool for HG38 reference genome.", "input": [{"name": "Bwt construction"}, {"name": "Prefix of the index to be output"}, {"name": "Block size"}, {"name": "Output index files renamed by adding 64"}, {"name": "Reference", "encodingFormat": "application/x-tar"}, {"name": "Total memory [Gb]"}, {"name": "Do not add alt contigs file to TAR bundle"}], "output": [{"name": "TARed fasta with its BWA indices", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/lh3/bwa"], "applicationSubCategory": ["Indexing", "FASTA Processing"], "project": "SBG Public Data", "creator": "Heng Li", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649159218, "dateCreated": 1558353187, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bwa-mem-bundle-0-7-13/69", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bwa-mem-bundle-0-7-13/69", "applicationCategory": "CommandLineTool", "name": "BWA MEM Bundle", "description": "**BWA MEM** is an algorithm designed for aligning sequence reads onto a large reference genome. BWA MEM is implemented as a component of BWA. The algorithm can automatically choose between performing end-to-end and local alignments. BWA MEM is capable of outputting multiple alignments, and finding chimeric reads. It can be applied to a wide range of read lengths, from 70 bp to several megabases. \n\nIn order to obtain possibilities for additional fast processing of aligned reads, two tools are embedded together into the same package with BWA MEM (0.7.13): Samblaster. (0.1.22) and Sambamba (v0.6.0). \nIf deduplication of alignments is needed, it can be done by setting the parameter 'Duplication'. **Samblaster** will be used internally to perform this action.\nBesides the standard BWA MEM SAM output file, BWA MEM package has been extended to support two additional output options: a BAM file obtained by piping through **Sambamba view** while filtering out the secondary alignments, as well as a Coordinate Sorted BAM option that additionally pipes the output through **Sambamba sort**, along with an accompanying .bai file produced by **Sambamba sort** as side effect. Sorted BAM is the default output of BWA MEM. Parameters responsible for these additional features are 'Filter out secondary alignments' and 'Output format'. Passing data from BWA MEM to Samblaster and Sambamba tools has been done through the pipes which saves processing times of two read and write of aligned reads into the hard drive. \n\nFor input reads fastq files of total size less than 10 GB we suggest using the default setting for parameter 'total memory' of 15GB, for larger files we suggest using 58 GB of memory and 32 CPU cores.\n\n**Important:**\nIn order to work BWA MEM Bundle requires fasta reference file accompanied with **bwa fasta indices** in TAR file.\nThere is the **known issue** with samblaster. It does not support processing when number of sequences in fasta is larger than 32768. If this is the case do not use deduplication option because the output BAM will be corrupted.\n\nHuman reference genome version 38 comes with ALT contigs, a collection of diverged alleles present in some humans but not the others. Making effective use of these contigs will help to reduce mapping artifacts, however, to facilitate mapping these ALT contigs to the primary assembly, GRC decided to add to each contig long flanking sequences almost identical to the primary assembly. As a result, a naive mapping against GRCh38+ALT will lead to many mapQ-zero mappings in these flanking regions. Please use post-processing steps to fix these alignments or implement [steps](https://sourceforge.net/p/bio-bwa/mailman/message/32845712/) described by the author of BWA toolkit.", "input": [{"name": "Reference Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Threads"}, {"name": "Minimum seed length"}, {"name": "Dropoff"}, {"name": "Select seeds"}, {"name": "Seed occurrence for the 3rd round"}, {"name": "Skip seeds with more than INT occurrences"}, {"name": "Drop chains fraction"}, {"name": "Discard chain length"}, {"name": "Mate rescue rounds"}, {"name": "Skip mate rescue"}, {"name": "Skip pairing"}, {"name": "Discard exact matches"}, {"name": "Score for a sequence match"}, {"name": "Mismatch penalty"}, {"name": "Smart pairing in input FASTQ file"}, {"name": "Read group header"}, {"name": "Insert string to output SAM or BAM header"}, {"name": "Ignore ALT file"}, {"name": "Verbose level"}, {"name": "Minimum alignment score for a read to be output in SAM/BAM"}, {"name": "Output alignments"}, {"name": "Append comment"}, {"name": "Output header"}, {"name": "Use soft clipping"}, {"name": "Mark shorter"}, {"name": "Gap open penalties"}, {"name": "Gap extension"}, {"name": "Clipping penalty"}, {"name": "Unpaired read penalty"}, {"name": "Sequencing technology-specific settings"}, {"name": "Output in XA"}, {"name": "Specify distribution parameters"}, {"name": "Band width"}, {"name": "Platform"}, {"name": "Sample ID"}, {"name": "Library ID"}, {"name": "Platform unit ID"}, {"name": "Data submitting center"}, {"name": "Median fragment length"}, {"name": "Output format"}, {"name": "Sambamba Sort threads"}, {"name": "Memory for BAM sorting"}, {"name": "PCR duplicate detection"}, {"name": "Total memory"}, {"name": "Filter out secondary alignments"}, {"name": "Output SAM/BAM file name"}, {"name": "Reserved number of threads on the instance"}, {"name": "Read group ID"}, {"name": "Optimize threads for HG38"}], "output": [{"name": "Aligned SAM/BAM", "encodingFormat": "application/x-bam"}, {"name": "Samblaster log"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/lh3/bwa"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Heng Li", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035173, "dateCreated": 1459166575, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bwa-mem-bundle-0-7-17/45", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bwa-mem-bundle-0-7-17/45", "applicationCategory": "CommandLineTool", "name": "BWA MEM Bundle 0.7.17", "description": "**BWA-MEM** is an algorithm designed for aligning sequence reads onto a large reference genome. BWA-MEM is implemented as a component of BWA. The algorithm can automatically choose between performing end-to-end and local alignments. BWA-MEM is capable of outputting multiple alignments, and finding chimeric reads. It can be applied to a wide range of read lengths, from 70 bp to several megabases. \n\n## Common Use Cases\nIn order to obtain possibilities for additional fast processing of aligned reads, **Biobambam2 sortmadup** (2.0.87) tool is embedded together into the same package with BWA-MEM (0.7.17).\nIf deduplication of alignments is needed, it can be done by setting the parameter 'Duplication'. Biobambam2 sortmadup will be used internally to perform this action.\n\nBesides the standard BWA-MEM SAM output file, BWA-MEM package has been extended to support additional output options enabled by Biobambam2 sortmadup: BAM file, Coordinate Sorted BAM along with accompanying .bai file, queryname sorted BAM and CRAM. Sorted BAM is the default output of BWA-MEM. Parameter responsible for output type selection is *Output format*. Passing data from BWA-MEM to Biobambam2 sortmadup tool has been done through the linux pipes which saves processing times (up to an hour of the execution time for whole genome sample) of two read and write of aligned reads into the hard drive.\n\n## Common Issues and Important Notes\nFor input reads fastq files of total size less than 10 GB we suggest using the default setting for parameter 'total memory' of 15GB, for larger files we suggest using 58 GB of memory and 32 CPU cores.\n\nIn order to work BWA-MEM Bundle requires fasta reference file accompanied with **BWA Fasta indices** in TAR file.\n\nHuman reference genome version 38 comes with ALT contigs, a collection of diverged alleles present in some humans but not the others. Making effective use of these contigs will help to reduce mapping artifacts, however, to facilitate mapping these ALT contigs to the primary assembly, GRC decided to add to each contig long flanking sequences almost identical to the primary assembly. As a result, a naive mapping against GRCh38+ALT will lead to many mapQ-zero mappings in these flanking regions. Please use post-processing steps to fix these alignments or implement [steps](https://sourceforge.net/p/bio-bwa/mailman/message/32845712/) described by the author of BWA toolkit.\n\nWhen desired output is CRAM file without deduplication of the PCR duplicates, it is necessary to provide FASTA Index file as input.\n\nIf __Read group ID__ parameter is not defined, by default it will  be set to \u20181\u2019. If the tool is scattered within a workflow it will assign the Read Group ID according to the order of the scattered folders. This ensures a unique Read Group ID when when processing multi-read group input data from one sample.", "input": [{"name": "Reference Index TAR", "encodingFormat": "application/x-tar"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Threads"}, {"name": "Minimum seed length"}, {"name": "Dropoff"}, {"name": "Select seeds"}, {"name": "Seed occurrence for the 3rd round"}, {"name": "Skip seeds with more than INT occurrences"}, {"name": "Drop chains fraction"}, {"name": "Discard chain length"}, {"name": "Mate rescue rounds"}, {"name": "Skip mate rescue"}, {"name": "Skip pairing"}, {"name": "Discard exact matches"}, {"name": "Score for a sequence match"}, {"name": "Mismatch penalty"}, {"name": "Smart pairing in input FASTQ file"}, {"name": "Read group header"}, {"name": "Insert string to output SAM or BAM header"}, {"name": "Ignore ALT file"}, {"name": "Verbose level"}, {"name": "Minimum alignment score for a read to be output in SAM/BAM"}, {"name": "Output alignments"}, {"name": "Append comment"}, {"name": "Output header"}, {"name": "Use soft clipping"}, {"name": "Mark shorter"}, {"name": "Gap open penalties"}, {"name": "Gap extension"}, {"name": "Clipping penalty"}, {"name": "Unpaired read penalty"}, {"name": "Sequencing technology-specific settings"}, {"name": "Output in XA"}, {"name": "Specify distribution parameters"}, {"name": "Band width"}, {"name": "Platform"}, {"name": "Sample ID"}, {"name": "Library ID"}, {"name": "Platform unit ID"}, {"name": "Data submitting center"}, {"name": "Median fragment length"}, {"name": "Output format"}, {"name": "Memory for BAM sorting"}, {"name": "PCR duplicate detection"}, {"name": "Total memory"}, {"name": "Filter out secondary alignments"}, {"name": "Output SAM/BAM file name"}, {"name": "Reserved number of threads on the instance"}, {"name": "Read group ID"}, {"name": "Optimize threads for HG38"}, {"name": "Split alignment smallest coordinate as primary"}, {"name": "Don't modify mapQ of supplementary alignments"}, {"name": "process INT input bases in each batch (for reproducibility)"}, {"name": "Fasta Index file for CRAM output"}], "output": [{"name": "Aligned SAM/BAM", "encodingFormat": "application/x-bam"}, {"name": "Sormadup metrics"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/lh3/bwa"], "applicationSubCategory": ["Alignment", "FASTQ Processing"], "project": "SBG Public Data", "creator": "Heng Li", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649159217, "dateCreated": 1532539861, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cancer-predisposition-sequencing-reporter-0-6-1-cwl1-1/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cancer-predisposition-sequencing-reporter-0-6-1-cwl1-1/8", "applicationCategory": "CommandLineTool", "name": "Cancer Predisposition Sequencing Reporter", "description": "**Cancer Predisposition Sequencing Reporter** for clinically significant cancer-predisposing germline variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Cancer Predisposition Sequencing Reporter** can be used to interpret germline variants in the context of cancer predisposition [1,2]. The tool takes a VCF file with germline variants (**Input variants**) obtained from WES or WGS, cross-references the set with the user-selected set of genes of interest (**Panel ID** or **Custom gene list from panel 0** inputs), annotates the variants and reports the associated information (ClinVar-classified variants, ACMG secondary findings, Variant biomarkers and GWAS hits) [2]. \n\nThe analysis is limited to a specific gene panel (**Panel ID** input). Panel 0 is a superpanel combining several cancer-predisposition gene sources [2], including all of Genomics England PanelApp genes. Panels 1-42 correspond to gene panels for specific conditions, as curated by Genomics England PanelApp. The users can also build their own custom gene panels, if these are subsets of panel 0 (**Custom gene list from panel 0**). Either a **Panel ID** or a **Custom gene list from panel 0** must be provided when running a task.\n\n**PCGR data archive** input refers to the shared reference data bundle used by both the **Personal Cancer Genome Reporter** tool and the **Cancer Predisposition Sequencing Reporter**. The reference data bundle should match the genome assembly version of the input data (**Genome assembly**).\n\n**Cancer Predisposition Sequencing Reporter** uses a configuration file in TOML format. This file can either be prepared by the user and provided as the **CPSR configuration file in TOML format** input or can be built interactively at task run time, as the tool input parameters beginning with **Conf -** relate to the corresponding fields in the configuration file. If a value for a field is not provided, the field will be populated with the value from the [default tool configuration file](https://github.com/sigven/cpsr/blob/65b26642e67cf15748ba1561a28ddf0827c403f9/cpsr.toml).\n\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--force_overwrite`, `--debug` and `--docker-uid` were omitted from the wrapper as they do not apply to task executions on Seven Bridges platforms.\n* Parameters `--version` and `--help` were omitted from the wrapper.\n* Parameter `--no-docker` was hardcoded in the wrapper and is always applied.\n* Parameters `--pcgr-dir` and `--output-dir` were hardcoded in the wrapper to point to the current working directory of the task.\n* Parameter `--sample_id` is no longer required. If a user provides a value (via the **Sample ID** input) it will be used, otherwise, the tool will attempt to read the **Sample ID** metadata field of the provided input VCF file. In the absence of the **Sample ID** metadata field, the input VCF file name will be used as prefix for the output file names.\n* The configuration file (`--conf`) input is not required with this wrapper, but is still recommended. If the user does not provide a file (**CPSR configuration file in TOML format**) to use, the wrapper will attempt to build one from the GUI input parameters. For all unspecified parameters, the values from the tool default example configuration file will be used.\n* To assist in debugging tasks, the tool standard output and error streams are captured in the debug.log file, which is accessible with other task logs. Please consult this log if any issues with the task are encountered (missing outputs or errors).\n\n### Common Issues and Important Notes\n\n* **Input variants**, **PCGR data archive** and **Genome assembly** inputs are required. \n* The **PCGR data archive** should match the **Genome assembly** of the input data.\n* If the **Input variants** file is a VCF.GZ file, the corresponding TBI index must be present in the project.\n* Either a **Panel ID** or a **Custom gene list from panel 0** must be provided when running a task.\n* By default, the tool validates the input VCF file against the VCF file format specification using the **vcf-validator** tool. However, it is possible that VCFs failing the validation step can still be successfully analyzed by **Cancer Predisposition Sequencing Reporter**. If you are unable to obtain outputs for your VCF input, please check the debug.log file to see if the input is failing the validation step. If this is the case and you believe that the reported validation issues can be safely ignored, you can disable the input VCF validation using the **Skip input VCF validation** input parameter.\n\n\n### Performance Benchmarking\n\nThe performance of the **Cancer Predisposition Sequencing Reporter** depends on the number of variants in the input VCF file. Typical tasks are expected to complete in 10-20 minutes. The number of cores assigned to VEP and Vcfanno, either through the user-provided **CPSR configuration file in TOML format** or, in the absence of this file, through the **Conf - Number of Vcfanno processors** and **Conf - Number of VEP forks** input parameters may improve tool performance. The default value for both parameters (4) should be sufficient for most applications. Disk storage requirement for the tool is <100 GB.\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| Tool example VCF | 9 min | $0.06 + $0.02 | c4.2xlarge 1024 GB EBS |\n| WES VCF.GZ file | 10 min | $0.07 + $0.02 | c4.2xlarge 1024 GB EBS |\n| WGS VCF.GZ file | 16 min | $0.11 + $0.04 | c4.2xlarge 1024 GB EBS |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**Cancer Predisposition Sequencing Reporter** was tested with cwltool version 3.1.20211107152837. The `in_pcgr_data`, `in_conf`, `in_variants`, `genome_assembly` and `panel_id` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [CPSR publication](https://www.biorxiv.org/content/10.1101/846089v2)\n\n[2] [CPSR documentation](https://github.com/sigven/cpsr)", "input": [{"name": "PCGR data archive", "encodingFormat": "application/x-tar"}, {"name": "CPSR configuration file in TOML format"}, {"name": "Query VCF file with germline variants", "encodingFormat": "application/x-vcf"}, {"name": "Sample ID"}, {"name": "Genome assembly"}, {"name": "Only annotate [basic mode]"}, {"name": "Panel ID"}, {"name": "Custom gene list from panel 0", "encodingFormat": "text/plain"}, {"name": "Skip input VCF validation"}, {"name": "Diagnostic grade only"}, {"name": "Ignore non-coding variants"}, {"name": "Include ACMG secondary findings variants"}, {"name": "GWAS findings"}, {"name": "Classify all variants"}, {"name": "Ignore ClinVar non-cancer variants"}, {"name": "MAF upper threshold [gnomAD]"}, {"name": "Conf - gnomAD population"}, {"name": "Conf - Visual theme of the report"}, {"name": "Conf - Custom tags"}, {"name": "Conf - Custom panel name"}, {"name": "Conf - Custom panel version"}, {"name": "Conf - Custom panel URL"}, {"name": "Conf - P value for GWAS hits"}, {"name": "Conf - Number of Vcfanno processors"}, {"name": "Conf - Number of VEP forks"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Conf - Omit intergenic variants in VEP"}, {"name": "Conf - Customize VEP pick order"}], "output": [{"name": "CPSR-annotated PASS variants", "encodingFormat": "application/x-vcf"}, {"name": "PASS variants in TSV format"}, {"name": "CPSR HTML report", "encodingFormat": "text/html"}, {"name": "JSON dump of HTML report content"}, {"name": "Variant tiers in TSV format"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/sigven/cpsr", "https://github.com/sigven/cpsr/tree/master/src", "https://github.com/sigven/cpsr/releases/tag/v0.6.1", "https://github.com/sigven/cpsr#cpsr-documentation"], "applicationSubCategory": ["Annotation", "Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Sigve Nakken", "softwareVersion": ["v1.1"], "dateModified": 1648219355, "dateCreated": 1648219354, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cas-offinder-2-4/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cas-offinder-2-4/5", "applicationCategory": "CommandLineTool", "name": "Cas-OFFinder", "description": "**Cas-OFFinder** is a tool that searches for potential off-target sites of CRISPR/Cas-derived RNA-guided endonucleases. The program is OpenCL based, ultrafast and versatile. **Cas-OFFinder** is not limited by the number of mismatches and allows variations in protospacer-adjacent motif (PAM) sequences recognized by Cas9, the essential protein component in RNA-guided endonucleases (RGEN) [1].\n\n**Cas-OFFinder** enables searching for potential off-target sites in any sequenced genome rapidly without limiting the PAM sequence or the number of mismatched bases. These features make **Cas-OFFinder** applicable to ZFNs, TALENs and transcription factors that are prone to off-target DNA recognition [2]. To run the analysis it is required to provide the **Input file**, with the desired the pattern including PAM site, the query sequences and maximum mismatch numbers, and **Reference files**. As a result, the tool will generate the **Output file** in TXT format containing six columns: given query sequence, chromosome, the 0-based position of the off-target site, the actual sequence from the position (mismatched bases noted in lowercase letters), the forward strand(+) or reverse strand(-) of the found sequence and the number of the mismatched bases.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* **Cas-OFFinder** can be used for searching for potential off-target sites. **Cas-OFFinder** is mainly designed for CRISPR/Cas9 derived RGENs, however, it also can be used for searching off-targets of other nucleases, e.g. TALENs(Transcription activator-like effector nucleases) or ZFNs(Zinc-finger nucleases), by specifying the pattern sequence as all 'N's [1].\n\n### Changes Introduced by Seven Bridges\n* The **Input file** with pattern and query sequences should not include the path to the reference file directory, as it is subsequently added by the tool during task execution.\n* The tool will always run using GPU, as the parameter for the running device is predefined to G value. This parameter was fixed based on the tool documentation stating that running on GPU is considerably faster than running on CPU [3].\n\n\n### Common Issues and Important Notes\n* The instance hint in the tool is set to the p2.xlarge (1024GB EBS, 4vCPUs, 61GB RAM, 1GPU) instance. The instance can be changed to a bigger one in the Execution Settings tab of a Draft task (more information can be found [here](https://docs.sevenbridges.com/docs/set-execution-hints-at-task-level)).\n* Currently, only the AWS GPU instances are supported, so at the moment the tasks can't be run on the Google instances.\n* The **Input file** must be provided as a TXT file and include the following data: the first line indicates the desired pattern including the PAM site, while the remaining lines are the query sequences and maximum mismatch numbers, separated by spaces. The length of the desired pattern and the query sequences should be the same!\n\n\n### Performance Benchmarking\n\nPerformance was  tested with two **Input files**, one with 4 query sequences and 5 maximum mismatch numbers and the other with 540 query sequences and 5 maximum mismatch numbers. For all benchmark tasks set of 93 FASTA files for hg19 was used as a reference (obtained from http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz).\\\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n                   \n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n|  4 query sequences  | 5 min   | $0.08 ($0.07 + $0.01)            |p2.xlarge      |\n|  540 query sequences | 122 min    | $2.12 ($1.83 + $0.29)               | p2.xlarge     |\n|  540 query sequences | 80 min    | $9.79 ($9.6 + $0.19)               | p2.8xlarge     |\n|  4 query sequences  | 2 min   | $0.106 ($0.102 + $0.004)           |p3.2xlarge      |\n|  540 query sequences | 6 min    | $0.32 ($0.306 + $0.014)               | p3.2xlarge     |\n|  540 query sequences | 5 min    | $1.03  ($1.02 + $0.01)              | p3.8xlarge     |\n\n\n### References\n\n[1] [Cas-OFFinder GitHub page](https://github.com/snugel/cas-offinder)\n\n[2] [Homepage](http://www.rgenome.net/cas-offinder)\n\n[3] [Publication](https://academic.oup.com/bioinformatics/article/30/10/1473/267560#3206519)", "input": [{"name": "Reference files", "encodingFormat": "application/x-fasta"}, {"name": "Input file with pattern and query sequences", "encodingFormat": "text/plain"}, {"name": "Output filename"}], "output": [{"name": "Output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/snugel/cas-offinder", "https://github.com/snugel/cas-offinder/archive/2.4.1.zip", "https://github.com/snugel/cas-offinder"], "applicationSubCategory": ["Gene Editing"], "project": "SBG Public Data", "creator": "Genome Engineering Laboratory, Seoul National University", "softwareVersion": ["v1.1"], "dateModified": 1648039728, "dateCreated": 1612360097, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/ccs-6-3-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/ccs-6-3-0/5", "applicationCategory": "CommandLineTool", "name": "CCS", "description": "**CCS** combines multiple subreads of the same SMRTbell molecule and outputs one highly accurate consensus sequence [1].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nHighly accurate consensus sequences, also called HiFi reads, have accuracy of >99.9%, and as such are on par with short reads and Sanger sequencing. These HiFi reads are enabled by evolved Single Molecule, Real-Time (SMRT) Sequencing technology and **CCS** tool [1]. \n\n### Changes Introduced by Seven Bridges\n\n* Input parameters `--help`, `--version`, `--log-level`, `--log-file` are not part of the Seven Bridges wrapper.\n* Input **Path to a chemistry model file** can only be a file (directory is not supported in the Seven Bridges wrapper).\n* Input parameters `--report-file`, `--report-json`, `--metrics-json` are not part of the Seven Bridges wrapper. Instead names for these output files are deduced from the **Unaligned BAM shard** input.\n* The input file format for **Unaligned BAM shard** in the Seven Bridges wrapper is BAM.\n\n\n### Common Issues and Important Notes\n\n* Input **Unaligned BAM shard** is required. This file contains subreads from a single movie in PacBio BAM format (.subreads.bam) [1].\n* Instance can be chosen by setting input parameters **CPUs per job** and **Memory per job [MB]**.\n* The tool was tested on the Seven Bridges platform by using [CCS data with Barcoded overhang adapter kit 8B](https://downloads.pacbcloud.com/public/dataset/RepeatExpansionDisorders_NoAmp/). The raw data in a BAM file format was run with the **CCS** tool. Users should be aware that other use-cases were not tested - it is possible that other use-cases might require more time/resources and might cost more.\n* Users should be aware of the warning from the tool helper: This program comes with ABSOLUTELY NO WARRANTY; it is intended for Research Use Only and not for use in diagnostic procedures.\n\n\n### Performance Benchmarking\n\nPerformance benchmarking was done using [CCS data with Barcoded overhang adapter kit 8B](https://downloads.pacbcloud.com/public/dataset/RepeatExpansionDisorders_NoAmp/) and by utilizing AWS on-demand instances and 1024 GB storage.\n\n| Experiment type  | Duration | Cost (Instance + Storage) | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n|  CCS, BAM - 7.9GB, threads = 1 | 396 min   | $3.54 ($2.63 + $0.91)            | c4.2.xlarge      |\n|  CCS, BAM - 7.9GB, threads = 7 | 80 min    | $0.72 ($0.53 + $0.19)               | c4.2.xlarge     |\n|  CCS, BAM - 7.9GB, threads = 14 | 42 min    | $0.66 ($0.56 + $0.10)               | c4.4.xlarge    |\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n###Portability\n\nThis app was tested with cwltool version 3.1.20211107152837. The **Unaligned BAM shard** and **Emit all ZMWs** inputs were set in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [CCS documentation page](https://ccs.how/)", "input": [{"name": "Emit all ZMWs"}, {"name": "Number of threads"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Stderr JSON log"}, {"name": "Suppress reports"}, {"name": "Unaligned BAM shard", "encodingFormat": "application/x-bam"}, {"name": "Minimum number of full-length subreads"}, {"name": "Minimum SNR of subreads to use for generating CCS"}, {"name": "Pick at maximum the top N passes for each ZMW"}, {"name": "Minimum draft length before polishing"}, {"name": "Maximum draft length before polishing"}, {"name": "Operate on a single chunk. Format i/N, where i in [1,N]"}, {"name": "Determine maximum number of chunks"}, {"name": "Path to a chemistry model file"}, {"name": "Name of chemistry or model to use, overriding default selection"}, {"name": "Generate a consensus for each strand"}, {"name": "Only output the initial draft template (faster, less accurate)"}, {"name": "Emit a representative subread"}, {"name": "Calculate mean pulse widths (PW) and interpulse durations (IPD) for every ZMW"}, {"name": "Calculate mean pulse widths (PW) and interpulse durations (IPD) for every HiFi read"}, {"name": "Minimum predicted accuracy in [0, 1]"}], "output": [{"name": "Consensus BAM file", "encodingFormat": "application/x-bam"}, {"name": "Report", "encodingFormat": "text/plain"}, {"name": "Report JSON file"}, {"name": "Metrics JSON file"}, {"name": "HiFi summary JSON"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PacificBiosciences/ccs", "https://github.com/PacificBiosciences/pbbioconda"], "applicationSubCategory": ["Long Reads", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Pacific Biosciences", "softwareVersion": ["v1.2"], "dateModified": 1649410115, "dateCreated": 1649410115, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/centrifuge-build/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/centrifuge-build/14", "applicationCategory": "CommandLineTool", "name": "Centrifuge Build", "description": "**Centrifuge Build** is a part of the Centrifuge suite, used for building an index from a set of DNA sequences [1]. It can be run as a standalone tool or as part of the **Reference Index Creation** workflow.\n\n**Centrifuge** is a novel microbial classification engine that enables rapid, accurate, and sensitive labelling of reads and quantification of species present in metagenomic samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem [1].\n\nThe **Centrifuge Build** tool requires the following input files:\n\n- **Conversion table** (`--conversion-table`) - a tab delimited file that maps sequence IDs to taxonomy IDs;\n- **Taxonomy tree** (`--taxonomy-tree`) - a file that maps taxonomy IDs to their parents and respective rank, up to the root of the tree; when using NCBI taxonomy IDs, this will be the nodes.dmp file from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz, which can be also obtained using **Centrifuge Download**;\n- **Name table** (`--name-table`) - a file that maps taxonomy IDs to a name; when using NCBI taxonomy IDs, names.dmp is the appropriate file;\n- **Reference sequences** - a file specifying the sequences to be used for building the index.\n\nIt outputs one TAR file, containing a set of 3 files with suffixes .1.cf, .2.cf, and .3.cf. These files together constitute the index, which is necessary to align reads to the reference. The original sequence FASTA files are no longer used by Centrifuge once the index is built.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Centrifuge Build** uses a set of DNA sequences (from the **Reference sequences** input file or the **List of sequences** input argument) to build an index for classifying metagenomic reads. There are some publicly available indexes at Centrifuge's website. However, the number of microbial sequences in public databases are constantly growing, therefore we advise creating new and revised index files from time to time.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n* **Centrifuge Build** has memory demands that depend on the size of the reference sequences. Due to this, the memory requirement is automatically set to a value higher than seven times the size of the sequences (this value is based on our experience). This should be changed only with good reason. \n\n### Performance Benchmarking\n\nBased on our experience running the tool, the minimal amount of RAM **Centrifuge Build** needs should be at least 6 times more than the size of the input sequences in (**Reference sequences**).\nThis tool is set to use even more RAM (7 times the size of the input sequences), in order to ensure smooth operation. Unless absolutely necessary, this memory setting should not be changed. To find out how to change the instance, please refer to the [documentation](https://docs.sevenbridges.com/docs/set-computation-instances#section-set-the-instance-type-for-a-workflow).\n\n**Centrifuge Build** also requires certain number of threads. Based on the running instance's number of available CPUs, one can set the *Number of threads* parameter to 8, 16, 32, etc. Since all benchmarking experiments were done with bacterial sequences (input files of 36.5 GB), available instances meeting the memory requirements of the **Centrifuge Build** tool are m4.16xlarge (AWS) and r4.16xlarge (AWS). If running time and cost for instances m4.16xlarge (AWS) and r4.16xlarge (AWS) are compared, it can be seen that running **Centrifuge Build** on m4.16xlarge (AWS) is more cost effective.  See the table below for details.\nTherefore, as building the bacterial index is the most demanding task memory-wise, the default instance for **Centrifuge Build** is set to be m4.16xlarge (AWS). However, when building smaller size indices, we recommend using other AWS instances. \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Instance (AWS)|Number of threads | Duration | Cost |\n| --- | --- | --- | --- |\n|m4.16xlarge| 8 | 7h 42m | $25.72 |\n|r4.16xlarge|8| 7h 44m | $ 33.99 |\n|r4.16xlarge|16| 4h 29m | $ 19.70 |\n|r4.16xlarge|32| 4h 1m| $17.67|\n\n\n### References\n\n[1] Kim D, Song L, Breitwieser FP, and Salzberg SL. [Centrifuge: rapid and sensitive classification of metagenomic sequences](http://genome.cshlp.org/content/early/2016/11/16/gr.210641.116.abstract). Genome Research 2016", "input": [{"name": "List of sequences"}, {"name": "Large index"}, {"name": "No automatic memory fitting"}, {"name": "Maximum number of suffixes"}, {"name": "Maximum number of suffixes - fraction of length"}, {"name": "Period for difference-cover sample"}, {"name": "Do not use difference-cover sample"}, {"name": "No packed reference portion"}, {"name": "Just reference portion"}, {"name": "Off-rate for marking positions"}, {"name": "Number of chars for initial BW calculation"}, {"name": "Random number generator seed"}, {"name": "Only error messages"}, {"name": "Number of threads"}, {"name": "Kmer count"}, {"name": "Conversion table"}, {"name": "Taxonomy tree"}, {"name": "Name table"}, {"name": "Size table"}, {"name": "Reference sequences"}, {"name": "Index base name"}, {"name": "Memory per job"}], "output": [{"name": "Index TAR", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/infphilo/centrifuge"], "applicationSubCategory": ["Metagenomics", "Indexing"], "project": "SBG Public Data", "creator": "John Hopkins University, Center for Computational Biology", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648560772, "dateCreated": 1509720865, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/centrifuge-classifier-1/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/centrifuge-classifier-1/16", "applicationCategory": "CommandLineTool", "name": "Centrifuge Classifier", "description": "**Centrifuge Classifier** is the main component of the Centrifuge suite, used for classification of metagenomics reads. Upon building (or downloading) and inspecting an Index (if desired), this tool can be used to process samples of interest. It can be used as a standalone tool, or as a part of **Metagenomics WGS analysis** workflow.\n\n**Centrifuge** is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species present in metagenomic samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem [1]. \n\n\nThe **Centrifuge Classifier** tool requires the following input files:\n\n- **Input reads** files with metagenomic reads - they can be in FASTQ or FASTA format, gzip-ed (extension .GZ) or bzip2-ed (extension .BZ2); in case of host-associated samples, it is presumed that files are already cleaned from  the host's genomic sequences;\n- **Reference index** in TAR format; for larger indexes the TAR.GZ format can be used, in this case we suggest the user manually set the required memory for the **Centrifuge Classifier** job by changing the value for the **Memory per job** parameter.\n\nThe results of the classification process are presented in two output files:\n\n- **Centrifuge report**, a tab delimited text file with containing results of an analysis for each taxonomic category from the reference organized into eight columns: (1) ID of a read, (2) sequence ID of the genomic sequence where the read is classified, (3) taxonomic ID of the genomic sequence from the second column, (4) the score of the classification, (5) the score for the next best classification, (6) an approximate number of base pairs of the read that match the genomic sequence, (7) the length of a read, and (8) the number of classifications for this read. The resulting line per read looks like the following:   \n     `HWUSI-EAS688_103028660:7:100:10014:18930 NC_020104.1 1269028 81 0 24 200 1`\n\n- **Classification result**, a tab delimited file with a classification summary for each genome or taxonomic unit from the reference index organized into seven columns: (1) the name of a genome, (2) taxonomic ID, (3) taxonomic rank, (4) the length of the genome sequence, (5) the number of reads classified to the provided genomic sequences, including multi-classified, (6) the number of reads uniquely classified to this particular genomic sequence, (7) the proportion of this genome normalized by its genomic length. The resulting line per genome looks like the following:   \n     `Streptococcus phage 20617 1392231 species 48800 1436 1325 0.453983`\n\n\nBased on the `--met-file` parameter value, **Centrifuge Classifier** can produce an additional output file with alignment metrics. However, it seems that this option does not work properly (see *Common Issues and Important Notes*). We suggest excluding it.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Centrifuge Classifier** takes files from the **Input reads** input node with raw reads (presumably cleaned from host genomic sequences as recommended by [Human Microbiome Project](https://hmpdacc.org/)) and one reference index file (in TAR ot TAR.GZ format) with microbial reference sequences. Based on the k-mer exact matching algorithm, Centrifuge assigns scores for each species on which k-mers from the read are aligned, and afterwards traverses upwards along the taxonomic tree to reduce the number of assignments, first by considering the genus that includes the largest number of species and then replacing those species with the genus. For more details on Centrifuge's algorithm see [1].\nThe results of the classification analysis are given for each reference sequence (in the **Classification result** file) and for each read (in the **Centrifuge report** file).\n\nThe index used in the analysis is Centrifuge's FM-index, whose size is reduced by compressing redundant genomic sequences. It can be downloaded from [Centrifuge's website](https://ccb.jhu.edu/software/centrifuge/manual.shtml) or created with the **Centrifuge Build** tool and/or the **Reference Index Creation workflow** provided by the Seven Bridges platform. In either case, it must be in the appropriate format (this can be checked with the **Centrifuge Inspect** tool). Based on our experience, an index should contain all of the organisms that are expected to be present in the sample. Providing an index with a smaller number of organisms (for example, in cases when the user is just interested in detecting one particular species) can result in miscalculated abundances of organisms within the sample.\n\n\n### Changes Introduced by Seven Bridges\n\n* **Centrifuge Classifier options** `--un`, `--un-conc`, `--al`, `--al-conc` and `--met-file` do not work properly, therefore all of them are excluded from the Seven Bridges version of the tool. In case a new release of the tool addresses these issues, an updated Seven Bridges version of the tool will be released as well.\n* The tool will automatically extract the index TAR file into the working directory and the basename from the **Reference genome** metadata field will be passed to **Centrifuge Classifier** using the `-x` argument.\n\n### Common Issues and Important Notes\n\n* If the index is in TAR.GZ format, the memory for **Centrifuge Classifier** should be set manually by changing the default value for the **Memory per job** parameter (in MB). Based on our experience, it would be enough to use twice as much memory as the size of the index file, with an additional 4GB overhead. For example, if the size of the index file is 8GB, the user should use 8 x 2 + 4 = 20GB, which amounts to 20 x 1024 = 20480MB.\n\n### Performance Benchmarking\n\n**Centrifuge Classifier** requires a significant amount of memory (based on our experience 4GB more than the size of the index files is suggested) in order to work properly. If the index is in TAR format, the tool will automatically allocate the required memory size. However, if the index is in TAR.GZ format, where the compression ratio is not always the same, we suggest the user manually set the required amount of memory necessary for the **Centrifuge Classifier** job. This can be done by changing the default **Memory per job** parameter value. This way, using expensive and memory overqualified instances, task failures would be avoided.\n\nIn the following table you can find estimates of **Centrifuge Classifier** running time and cost. All experiments are done with one sample. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Experiment type | Input size | Duration | Cost | Instance (AWS)\n| --- | --- | --- | --- | --- |\n|p_h_v index (prokaryotic, human and viral genomes)|Index 6.9 GB, reads SRS013942 sample 2 x 1 GB| 11m| $ 0.11 | c4.2xlarge|\n|Viral index |Index 118 MB, reads SRS013942 sample 2 x 1 GB| 5m| $ 0.07 | c4.2xlarge|\n|Bacterial index |Index 13GB, reads SRS013942 sample 2 x 1 GB | 16m| $ 0.25 | c4.4xlarge|\n|Bacterial index |Index 13GB, reads SRS019027 sample 2 x 3.5 GB | 27m| $ 0.42 | c4.4xlarge|\n\n\n### References\n[1] Kim D, Song L, Breitwieser FP, and Salzberg SL. [Centrifuge: rapid and sensitive classification of metagenomic sequences](http://genome.cshlp.org/content/early/2016/11/16/gr.210641.116.abstract). Genome Research 2016", "input": [{"name": "Reference index", "encodingFormat": "application/x-tar"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Trim from 5'"}, {"name": "Trim from 3'"}, {"name": "Align first n reads"}, {"name": "Skip the first n reads"}, {"name": "Quality scale"}, {"name": "Host taxids"}, {"name": "Exclude taxids"}, {"name": "Time"}, {"name": "Quiet"}, {"name": "Metrics standard error"}, {"name": "Metrics"}, {"name": "Parallel threads"}, {"name": "Memory mapping"}, {"name": "Minimum length of partial hits"}, {"name": "Minimum summed length"}, {"name": "Ignore qualities"}, {"name": "Query input files"}, {"name": "SRA accession number"}, {"name": "No forward version"}, {"name": "No reverse complement"}, {"name": "Output format"}, {"name": "Columns in tabular format"}, {"name": "QC filter"}, {"name": "Memory per job"}, {"name": "Paired reads not aligned concordantly"}, {"name": "Paired reads aligned concordantly"}, {"name": "Unpaired reads that aligned at least once"}, {"name": "Unpaired reads that didn't align"}], "output": [{"name": "Centrifuge report"}, {"name": "Classification result", "encodingFormat": "text/plain"}, {"name": "Pairs that didn't align concordantly", "encodingFormat": "text/plain"}, {"name": "Pairs that aligned concordantly at least once", "encodingFormat": "text/plain"}, {"name": "Unpaired reads that aligned at least once", "encodingFormat": "text/plain"}, {"name": "Unpaired reads that that didn't align", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/infphilo/centrifuge"], "applicationSubCategory": ["Metagenomics", "Taxonomic Profiling"], "project": "SBG Public Data", "creator": "John Hopkins University, Center for Computational Biology", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648560771, "dateCreated": 1509720864, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/centrifuge-download/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/centrifuge-download/5", "applicationCategory": "CommandLineTool", "name": "Centrifuge Download", "description": "**Centrifuge Download** is part of the Centrifuge suite, used for downloading reference sequences from NCBI, prior to index building. It can be used as a standalone tool, or as part of the **Reference Index Creation** workflow.\n\n\n**Centrifuge** is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species present in metagenomic samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem.\n\n\n**Centrifuge Download**  does not take any file inputs. Instead, it has several input parameters (see *Ports* section) that define how and what will be downloaded.The most important parameters are as follows:\n\n* **Domain to download** (`--domain`) - a list of domains that sequences can be downloaded with;\n* **Taxonomy IDs** (`-t`) - a list of taxonomy identifiers for sequences that the user wants to download;\n* **Assembly level** (`-a`) and **Refseq category** (`-c`) - used for filtering the database from which the sequences will be downloaded;\n* **Database to use** - one of four types of databases: \"*taxonomy*\", \"*refseq*\", \"*genbank*\" or \"*contaminants*\".\n\nThe output of the tool depends on the database used:\n\n- **taxonomy** - it takes taxonomy dump from NCBI and creates *nodes.dmp* and *names.dmp* files;\n- **refseq** - downloads a list of arbitrary sequences from NCBI RefSeq. The output is a file with all downloaded sequences;\n- **genbank** - downloads a list of arbitrary sequences from NCBI GenBank. The output is a file  with all downloaded sequences;\n- **contaminants** - gets contaminant sequences from UniVec and EmVec. The outputs are *UniVec.fna* and *EmVec.fna* files.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Centrifuge Download** is commonly used prior to **Centrifuge Build**, in order to download the sequences from NCBI databases, that will be used for making the index. Regardless of the type of sequences, **Centrifuge Build** needs information about the taxonomy used in the databases. Therefore, it is common practice to first run **Centrifuge Download** with **Database to use** set to \"*taxonomy*\" in order to have accurate *nodes.dmp* and *names.dmp*. Afterwards, **Centrifuge Download** is commonly run with one of the remaining **Database to use** values.\n\n### Changes Introduced by Seven Bridges\n\n* Originally, **Centrifuge Download** produces a set of .FNA files with sequences (for example, if sequences are downloaded from several domains, there will be an output file per domain). However, on the Seven Bridges platform **Centrifuge Download** automatically merges all files into one output file with all sequences.\n\n### Common Issues and Important Notes\n\n* Although none of the parameters are required in general, some combination of parameter values requires that some other parameter is set. For example, if **Database to use** is \"*refseq*\" or \"*genbank*\", then **Domain to download** is required.\n\n### Performance Benchmarking\n\nBased on our experience, depending on the selected domain, it takes between 5 minutes and one hour to download reference sequences from NCBI. Downloading fungal reference sequences takes only 5 minutes (with the cost of $0.05 on instance c4.2xlarge), while viral and bacterial sequences require approximately 40 minutes and an hour (at the cost of $0.30 and $0.50, using the same c4.2xlarge instance), respectively.", "input": [{"name": "Number of threads"}, {"name": "Folder for downloading"}, {"name": "Domain to download"}, {"name": "Assembly level"}, {"name": "Refseq category"}, {"name": "Taxonomy IDs"}, {"name": "Include RNA sequences"}, {"name": "Filter unplaced sequences"}, {"name": "Mask low-complexity regions"}, {"name": "Modify header"}, {"name": "Download GI map"}, {"name": "Database to use"}, {"name": "Base name for naming the index"}, {"name": "Memory per job"}], "output": [{"name": "Taxonomy names"}, {"name": "Taxonomy nodes"}, {"name": "EmVec contaminants"}, {"name": "UniVec contaminants"}, {"name": "SeqID_to_TaxonomyID"}, {"name": "Referent sequences"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/infphilo/centrifuge"], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "creator": "John Hopkins University, Center for Computational Biology", "softwareVersion": ["sbg:draft-2"], "dateModified": 1527589729, "dateCreated": 1509720865, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/centrifuge-inspect/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/centrifuge-inspect/13", "applicationCategory": "CommandLineTool", "name": "Centrifuge Inspect", "description": "**Centrifuge Inspect** is part of the Centrifuge suite, used for inspecting the **Centrifuge Index**, and giving information about what kind of index it is and what reference sequences were used to build it. \n\n**Centrifuge** is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species present in metagenomic samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem. \n\n\nThe **Centrifuge Inspect** tool requires the following input files:\n\n- **Reference Index**, a TAR format file. The index can be built using **Centrifuge Build**, or one can use any of the pre-built indexes, which can be downloaded from the Centrifuge website. \n\nThe results of the index inspection are presented in *result.txt*. Depending on the selected options, different inspection results are written to the output file. By default, FASTA records of the indexed nucleotide sequences will be written to the *result.txt* output file.\nFor example, if the **Summary** (`-s`/`--summary`) option is selected, the result will contain information about each sequence in the index, including reference name, index properties and sequence length. The resulting line per sequence looks like following:\n\n`Sequence-1\tNC_001422.1 Enterobacteria phage phiX174 sensu lato, complete genome\t5386`\n\n*A List of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Centrifuge Inspect** requires the input reference index file (in TAR format) with microbial reference sequences. The index used in the analysis is the Centrifuge's FM-index, whose size is reduced by compressing redundant genomic sequences. It can be downloaded from [Centrifuge's website](https://ccb.jhu.edu/software/centrifuge/manual.shtml) or created with **Centrifuge Build** and/or **Reference Index Creation workflow** found on the Seven Bridges platform.  \n\nA list of options one can use to inspect the index are as follows:\n\n* **Across** (`-a`/`--across`) - number of characters across in FASTA output (default: 60);\n* **Names** (`-n`/`--names`) - prints the reference sequence names only;\n* **Summary** (`-s`/`--summary`) - prints a summary;\n* **Reconstruct reference** (`-e`/`--bt2-ref`) - reconstructs the reference;\n* **Conversion table** (`--conversion-table`) - prints a conversion table;\n* **Taxonomy tree** (`--taxonomy-tree`) - prints the taxonomy tree;\n* **Name table** (`--name-table`) - prints names corresponding to taxonomic IDs;\n* **Size table** (`--size-table`) - prints the lengths of the sequences belonging to the same taxonomic ID.\n  \n\n### Changes Introduced by Seven Bridges\n\n**Centrifuge Inspect** option  `--large-index` does not work properly, therefore it is excluded from the Seven Bridges version of the tool. In case a new release of the tool addresses this issues, an updated Seven Bridges version of the tool will be released as well.\n\n### Common Issues and Important Notes\n\n- **cf base** must be provided when running **Centrifuge Inspect**, as it is shown in the cf filename, only without the trailing *.1.cf extension.\n- There is a known issue concerning a malfunctioning `--large-index` argument in the **Centrifuge Inspect** tool.\n\n### Performance Benchmarking\n\nThe running time of **Centrifuge Inspect** depends on the options selected. In the following table you can find estimates of **Centrifuge Classifier** running time and cost. The default AWS instance is c4.4xlarge.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n| Experiment type | Input size | Duration | Cost | Instance\n| --- | --- | --- | --- | --- |\n|Reference reconstruction|p_h_v index (prokaryotic, human and viral genomes), 6.9 GB| 1h 9m| $ 1.06 | c4.4xlarge|\n|Summary|p_h_v index, 6.9 GB| 3m| $ 0.05 | c4.4xlarge|\n|Default options|Bacterial index, 13GB| 2h 21m| $ 1.60 | r4.2xlarge|\n\n### References\n[1] Kim D, Song L, Breitwieser FP, and Salzberg SL. [Centrifuge: rapid and sensitive classification of metagenomic sequences](http://genome.cshlp.org/content/early/2016/11/16/gr.210641.116.abstract). Genome Research 2016", "input": [{"name": "Names"}, {"name": "Summary"}, {"name": "Across"}, {"name": "Reconstruct reference"}, {"name": "Conversion table"}, {"name": "Taxonomy tree"}, {"name": "Name table"}, {"name": "Size_table"}, {"name": "verbose"}, {"name": "cf base"}, {"name": "input_index"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/infphilo/centrifuge"], "applicationSubCategory": ["Metagenomics", "Indexing"], "project": "SBG Public Data", "creator": "John Hopkins University, Center for Computational Biology", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648560771, "dateCreated": 1510650884, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/centrifuge-kreport/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/centrifuge-kreport/6", "applicationCategory": "CommandLineTool", "name": "Centrifuge Kreport", "description": "**Centrifuge Kreport** is part of the Centrifuge suite, used to make a Kraken-style report from the Centrifuge output including taxonomy information. It can be run as a standalone tool, but it is also part of the **Metagenomics WGS analysis** workflow.\n\n\n**Centrifuge** is a novel microbial classification engine that enables rapid, accurate, and sensitive labelling of reads and quantification of species present in metagenomic samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem [1]. \n\n\nThe **Centrifuge Kreport** tool requires the following input files:\n\n- **Reference index** in TAR format. The index can be built using the Centrifuge Build tool, or one can use any of the pre-built indexes, which can be downloaded from the Centrifuge website. \n\n- **Centrifuge result**,  a tab delimited output file presented by **Centrifuge Classifier**. The file contains a classification summary for each genome or taxonomic unit from the reference index organized into seven columns: (1) the name of a genome, (2) taxonomic ID, (3) taxonomic rank, (4) the length of the genome sequence, (5) the number of reads classified to the provided genomic sequences, including multi-classified, (6) the number of reads uniquely classified to this particular genomic sequence, (7) the proportion of this genome normalized by its genomic length. The resulting line per genome looks like the following:   \n    `Streptococcus phage 20617 1392231 species 48800 1436 1325 0.453983`\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Centrifuge Kreport** takes inputs **Centrifuge result** and **Reference index** and converts the **Centrifuge result** (the output of the **Centrifuge Classifier**) into a Kraken-style report, which is more suitable for use with many tools available for further metagenomic analysis.\nThe output of **Centrifuge Kreport** is tab delimited, with one line per taxon. The fields of the output, from left-to-right, are: (1) Percentage of reads covered by the clade rooted at this taxon, (2) Number of reads covered by the clade rooted at this taxon, (3) Number of reads assigned directly to this taxon, (4) A rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies; all other ranks are simply '-', (5) NCBI taxonomy ID, and (6) Indented scientific name.The resulting line per genome looks like the following:\n\n`0.29\t39\t39\tS\t12402\t            Streptococcus phage EJ-1`\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\n\nThe execution time for human gene annotation takes several minutes on the default instance.\nUnless specified otherwise, the default instance used to run the **Centrifuge Kreport** tool will be c4.2xlarge (AWS). \n\n### References\n[1] Kim D, Song L, Breitwieser FP, and Salzberg SL. [Centrifuge: rapid and sensitive classification of metagenomic sequences](http://genome.cshlp.org/content/early/2016/11/16/gr.210641.116.abstract). Genome Research 2016", "input": [{"name": "Reference index"}, {"name": "Only unique"}, {"name": "Show zeros"}, {"name": "Is count table"}, {"name": "Minimum score"}, {"name": "Minimum alignment length"}, {"name": "Centrifuge result", "encodingFormat": "text/plain"}], "output": [{"name": "Output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/infphilo/centrifuge"], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "creator": "John Hopkins University, Center for Computational Biology", "softwareVersion": ["sbg:draft-2"], "dateModified": 1527589729, "dateCreated": 1509720865, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chimera-1-12-0/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chimera-1-12-0/14", "applicationCategory": "CommandLineTool", "name": "Chimera-1.12.0", "description": "Chimera is a software package for the secondary analysis of fusion products. This package facilitates the characterization of fusion products events. It allows fusion data results to be imported from the following fusion finders: ChimeraScan, bellerophontes, deFuse, FusionFinder, FusionHunter, mapSplice, tophat-fusion, FusionMap, STAR, Rsubread, and fusionCatcher.\n\nChimera generates a list of detected and filtered fusion gene files. Additionally, it can generate files required for graphical representation of fusions with Circos.\n\nRequired inputs are:\n\n**fusion_data** (Fusion Data) -  generated as the output of fusion finders ( bedpe, junction, tsv...)\n\n**fusionfinder** (The Fusion Finder Tool) - Here one has to specify tool that is used for fusion detection, the one that generated fusion_file.\n\n**organism** (The organism to be used for annotation) - this is a version of a reference genome used by Chimera tool for annotation. One can chose between hg19 and hg38. It is important that the genome reference version used for the alignment in a fusion finder is the same of the one used by Chimera for annotation because between hg38 and hg19 there are shifts in gene location.\n\n**filterlist** (FilterList type) - A function that filters out the fusion list. A fusion is discarded: \n(i) if it has less spanning reads than a set value, \n(ii) if its name is not in the given list, \n(iii) if the intronic regions are included in the fusion, \n(iv) if the partner genes are not annotated or \n(v) if  gene partners are the same,  respectively.\n\n**minsupport** (Define detected fusions by minimum supporting reads) - Parameter \"min.support\" allows to retrieve only a subset of fusions supported by a user defined minimal number of junction spanning reads. If one defines a less stringent number of supports, e.g. 2-3, more fusions supported by defined spanning reads will be detected, normally those with low overall quality. \n\n**filterfusionnames** (Filter detected fusions: by fusion partner) - Search detected fusions when fusion.names is selected in \"Filterlist type\" by gene/fusion name or its part\n\n**filterminsupport** (Filter detected fusions: by minimum supporting reads) - Minimum number of supporting reads for the fusion not the be filtered out applied when spanning.reads is selected as \"filterlist\" type.\n\nPaper:\nBeccuti M, Carrara M, Cordero F, Lazzarato F, Donatelli S, Nadalin F, Policriti A and Calogero RA (2014). \u201cChimera: a Bioconductor package for secondary analysis of fusion products.\u201d Bioinformatics, 0, pp. 3. http://doi.org/10.1093/bioinformatics/btu662.", "input": [{"name": "Fusion Data"}, {"name": "The Fusion Finder Tool"}, {"name": "Define detected fusions by minimum supporting reads"}, {"name": "FilterList type"}, {"name": "Filter detected fusions: by minimum supporting reads"}, {"name": "Filter detected fusions: by fusion partner"}, {"name": "The organism to be used for annotation"}], "output": [{"name": "Detected fusions", "encodingFormat": "text/plain"}, {"name": "Filtered fusions", "encodingFormat": "text/plain"}, {"name": "Circos links: filtered annotated fusions", "encodingFormat": "text/plain"}, {"name": "Circos names: filtered annotated fusions", "encodingFormat": "text/plain"}, {"name": "Filtered annotated fusions."}], "softwareRequirements": ["MemoryRequirement", "ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/Bioconductor-mirror/chimera/tree/release-3.3"], "applicationSubCategory": ["RNA-Seq", "Gene Fusion"], "project": "SBG Public Data", "creator": "Raffaele Calogero <raffaele.calogero@unito.it> / Dept. of Molecular Biotechnology and Health Sciences, University of Torino", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799791, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chimerascan-gtf2genepred-0-4-5/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chimerascan-gtf2genepred-0-4-5/8", "applicationCategory": "CommandLineTool", "name": "ChimeraScan Gtf2Genepred", "description": "ChimeraScan GTF2GenePred is a tool that parses standard gene annotation file format (GTF) to genePred text format acceptable by ChimeraScan Index tool. It is a part of ChimeraScan package that besides ChimeraScan GTF2Genepred contains ChimeraScan Index and ChimeraScan Run tool.\n\n#### Inputs\n**genes** - Gene feature file (GTF format) to be converted to UCSC genePred text format.\n\n### Common issues \n For this tool to work properly  attribute field _gene\\_name_ has to be present in the GTF file.", "input": [{"name": "Transcriptome reference", "encodingFormat": "application/x-gtf"}], "output": [{"name": "UCSC GenePred file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Matthew K. Iyer", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453798817, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chimerascan-index-0-4-5/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chimerascan-index-0-4-5/10", "applicationCategory": "CommandLineTool", "name": "ChimeraScan Index", "description": "The ChimeraScan Index builds a combined index using Bowtie-1.1.2 from genomic sequence (FASTA) and transcriptome references (UCSC GenePred format). The required format of transcriptome reference can be made from the GTF transcriptome reference file using the ChimeraScan Gtf2genepred tool. Output of ChimeraScan Index is used by the fusion finder ChimeraScan Run. \n\n\n##  Inputs ###\n\n**reference** - FASTA or corresponding TAR file. If FASTA file is used then combined index (TAR file) is formed and execution lasts approximately one hour. Resulting TAR file can be used in any future execution as **reference** file (if the same **reference** FASTA file and **genes** GTF file are to be used ) as it is already appropriately indexed. Usage of TAR file shortens execution to couple of minutes. \n\n**genes** - GENPRED file. It is a reference transcriptome file and it has to be compatible with the **reference**. If GenePred file is obtained from GTF file (using ChimeraScan Gtf2genepred tool) then GTF has to correspond to the provided **reference**.\nExample: human\\_hg19\\_genes\\_2015.gtf is compatible with ucsc.hg19.fasta.\n\n##  Output  ###\n\n**index** - TAR file, used further in ChimeraScan Run tool.\n\n##  Common Issues ###\nEven if **genes** file does not correspond  to the used genome build (**reference** file) tool will not necessarily break.", "input": [{"name": "Reference fasta", "encodingFormat": "application/x-tar"}, {"name": "Transcriptome reference", "encodingFormat": "text/plain"}], "output": [{"name": "Compressed index file", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/chimerascan-vrl/tree/master/chimerascan"], "applicationSubCategory": ["RNA-Seq", "Gene Fusion"], "project": "SBG Public Data", "creator": "Matthew K. Iyer", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799088, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chimerascan-make-html-0-4-5/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chimerascan-make-html-0-4-5/7", "applicationCategory": "CommandLineTool", "name": "ChimeraScan Make Html", "description": "The ChimeraScan Make HTML creates a table in the user-friendly HTML format for web browser viewing. It accepts a tab-delimited text file containing detected chimera information.", "input": [{"name": "Include Read-Throughs"}, {"name": "BEDPE file"}], "output": [{"name": "HTML table with fusions", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Matthew K. Iyer", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799094, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chimerascan-run-0-4-5/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chimerascan-run-0-4-5/14", "applicationCategory": "CommandLineTool", "name": "ChimeraScan Run", "description": "ChimeraScan detects fusion genes (chimeras) in paired-end RNA-seq datasets. This tool uses the Bowtie aligner to align paired-end reads to a combined genome-transcriptome reference. It aims to discover discordant reads, predict an optimal fusion breakpoint location, and detect chimeras.This application outputs a tabular file (*.chimeras.bedpe) that contains information about the chromosomal regions, transcript IDs, genes, and statistics for each chimera.\n\n### Inputs\n\n**reads** -  RNA-Seq FASTQ paired-end files.\n\n**index** - TAR file created by ChimeraScan Index tool.  \n\n**false_positives** - TXT supporting file containing list of likely false positives (hg19 Homo Sapiens), https://code.google.com/archive/p/chimerascan/downloads\n\n### Common issues:\n\nThe paired-end reads must be of the same length.\n\nFASTQ.GZ files provided as **reads** instead of FASTQ.\n \n\nReferences:\n\n1. Maher, C.A., et al. Transcriptome sequencing to detect gene fusions in cancer. Nature 458, 97-101 (2009).\n\n2. Maher, C.A., et al. Chimeric transcript discovery by paired-end transcriptome sequencing. Proceedings of the National Academy of Sciences of the United States of America 106, 12353-12358 (2009).", "input": [{"name": "Filter Isoform Fraction"}, {"name": "Filter Insert Size Probability"}, {"name": "Filter Unique Fragments"}, {"name": "Anchor Mismatches"}, {"name": "Anchor Length"}, {"name": "Anchor Minimum"}, {"name": "Homology Mismatches"}, {"name": "Additional Bowtie Arguments for Discordant Phase"}, {"name": "Additional Bowtie Arguments"}, {"name": "Segment Length"}, {"name": "Discordant Mismatches"}, {"name": "Mismatch Limit"}, {"name": "Ignore Multi-hits"}, {"name": "Maximum Fragment Length"}, {"name": "Minimum Fragment Length"}, {"name": "Trim 3' Bases"}, {"name": "Trim 5' Bases"}, {"name": "Insert Size Standard Deviation"}, {"name": "Mean Insert Size"}, {"name": "Library type"}, {"name": "Quality Scale"}, {"name": "False positives", "encodingFormat": "text/plain"}, {"name": "Index", "encodingFormat": "application/x-tar"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "Num Processors"}], "output": [{"name": "Detected fusion genes"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/chimerascan-vrl/tree/master/chimerascan"], "applicationSubCategory": ["RNA-Seq", "Gene Fusion"], "project": "SBG Public Data", "creator": "Matthew K. Iyer", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799646, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/circos-0-68/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/circos-0-68/14", "applicationCategory": "CommandLineTool", "name": "Circos-0.69-4", "description": "Circos is a software package for visualizing data and information. It applies the circular ideogram layout to display of relationships between genomic intervals. One timely application of this approach is creating effective figures showing how cancer genomes differ from healthy ones (e.g. http://cancer.sanger.ac.uk/cosmic).\n\nIn general Circos is ideal for exploring relationships between objects or positions, but version hosted here is adapted only for plotting **human fusion genes**. \n\nRequired inputs are GFF-style data files and Apache-like configuration files.\n\nOutput images are given in PDF.\n\nIt is used in Fusion Transcript Detection - ChimeraScan workflow as a tool for visualization of results obtained with Chimera tool and with Oncofuse. Please note that SBG Oncofuse4Circos is used for parsing Oncofuse output file for Circos.\n\n### Inputs ###\n\n**circos_links** - input file with listed chromosomes, start and end position of each gene partner as well as its gene's name. \n\n**circos_names** - input file with both fusion partners listed with origin information (chromosome, region on a given chromosome) and gene's name of each fusion partner.\n\n### Common Issues ###\n\nTo form a proper display, Circos tool requires both files with gene names and fusion links as well as setting karyotype that is used for creation of input files.", "input": [{"name": "Fusion links", "encodingFormat": "text/plain"}, {"name": "Fusion names", "encodingFormat": "text/plain"}, {"name": "Human karyotype"}], "output": [{"name": "Circos pdf"}], "softwareRequirements": ["MemoryRequirement", "ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/node/circos"], "applicationSubCategory": ["RNA-Seq", "Imaging"], "project": "SBG Public Data", "creator": "Martin Krzywinski / Canada's Michael Smith Genome Sciences Centre", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799679, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvnator-0-3-2/20", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvnator-0-3-2/20", "applicationCategory": "CommandLineTool", "name": "CNVnator", "description": "CNVnator performs CNV calling by doing read-depth(RD) analysis of the input BAM files. CNVnator tool has five major steps:  \n1. Reads extraction\n2. Histogram generation\n3. Statistics calculation\n4. RD signal partitioning\n5. CNVs identification\n\nThe tools can also merge multiple ROOT files and evaluate results of the statistics calculation step.\n\nCommon issues:\nThere aren't any known common issues.\n\nFor a more comprehensive analysis we suggest using the [CNVnator Analysis workflow](https://igor.sbgenomics.com/public/apps#workflow/sevenbridges/public-apps/cnvnator-analysis).", "input": [{"name": "Input BAM files", "encodingFormat": "application/x-bam"}, {"name": "Reference genome file", "encodingFormat": "application/x-fasta"}, {"name": "Chromosomes"}, {"name": "Uniquely mapped reads"}, {"name": "Input ROOT files"}, {"name": "Input ROOT file"}, {"name": "Histogram"}, {"name": "Reference genome files", "encodingFormat": "application/x-fasta"}, {"name": "Calculate statistics"}, {"name": "No GC correction"}, {"name": "RD signal partitioning"}, {"name": "Identifying CNVs"}, {"name": "Evaluate RD"}, {"name": "Input BAM Files", "encodingFormat": "application/x-bam"}], "output": [{"name": "Output ROOT file"}, {"name": "CNV calling results file", "encodingFormat": "text/plain"}, {"name": "Average RD output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/abyzovlab/CNVnator", "https://github.com/abyzovlab/CNVnator/releases", "https://github.com/abyzovlab/CNVnator/wiki", "https://github.com/abyzovlab/CNVnator/releases/download/v0.3.2/CNVnator_v0.3.2.zip"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Alexej Abyzov, Alexander E. Urban, Michael Snyder, and Mark Gerstein", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648048904, "dateCreated": 1453799735, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvnator2vcf/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvnator2vcf/1", "applicationCategory": "CommandLineTool", "name": "CNVnator2VCF-0-3-2", "description": "Convert the CNV file from CNVnator into VCF format using Perl script from CNVnator package. \n\nCommon issues: None.", "input": [{"name": "Result from CNVnator", "encodingFormat": "text/plain"}], "output": [{"name": "CNV calling VCF file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/abyzovlab/CNVnator/releases", "https://github.com/abyzovlab/CNVnator/blob/master/cnvnator2VCF.pl"], "applicationSubCategory": ["Converters"], "project": "SBG Public Data", "creator": "Alexej Abyzov, Alexander E. Urban, Michael Snyder, and Mark Gerstein", "softwareVersion": ["sbg:draft-2"], "dateModified": 1471539421, "dateCreated": 1465231637, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/comet-2016-01/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/comet-2016-01/8", "applicationCategory": "CommandLineTool", "name": "Comet", "description": "**Comet** is an open-source tool for tandem mass spectrometry (MS/MS) sequence database searching. It supports multiple input and output file formats.  \n\n###Required Inputs\n1. input_files: mass spectrometry data file in compressed mzML, mzXML, MGF, MS2, CMS2, or BMS2 format\n2. database_file: protein sequence database file in FASTA or FA format\n \n###Outputs\n1. output\\_pepxml\\_file: file in pepXML format that contains search results\n2. output\\_percolator\\_file: file in Percolator's tab-delimited format that contains search results\n3. output\\_txt\\_file: file in tab-delimited TXT format that contains search results\n4. output\\_sqt\\_file: file in SQT format that contains search results\n\n###Common Issues and Important Notes\n\nWhen **Comet** is run using a specific combination of parameter values, it can be computationally intensive, increasing the analysis running time from a couple of minutes (under default settings) to a couple of hours (under specific parameter settings) on a per input file basis.", "input": [{"name": "Database file", "encodingFormat": "application/x-fasta"}, {"name": "Decoy search"}, {"name": "Numer of threads"}, {"name": "Peptide mass tolerance"}, {"name": "Peptide mass units"}, {"name": "Mass type parent"}, {"name": "Mass type fragment"}, {"name": "Precursor tolerance type"}, {"name": "Isotope error"}, {"name": "Search enzyme number"}, {"name": "num_enzyme_termini"}, {"name": "Allowed missed cleavage"}, {"name": "Fragment bin tol"}, {"name": "Fragment bin offset"}, {"name": "Theoretical fragment ions"}, {"name": "Use A ions"}, {"name": "Use B ions"}, {"name": "Use C ions"}, {"name": "Use X ions"}, {"name": "Use Y ions"}, {"name": "Use Z ions"}, {"name": "Use NL ions"}, {"name": "Output sqtfile"}, {"name": "Output txtfile"}, {"name": "Output pepxmlfile"}, {"name": "Output percolator file"}, {"name": "Print expect score"}, {"name": "Num output lines"}, {"name": "Sample enzyme number"}, {"name": "Scan range"}, {"name": "Precursor charge"}, {"name": "Override charge"}, {"name": "MS level"}, {"name": "Activation method"}, {"name": "Digest mass range"}, {"name": "Num results"}, {"name": "Skip researching"}, {"name": "Max fragment charge"}, {"name": "Max precursor charge"}, {"name": "Nucleotide reading frame"}, {"name": "Clip nterm methionine"}, {"name": "Spectrum batch size"}, {"name": "Decoy prefix"}, {"name": "Output suffix"}, {"name": "Mass offsets"}, {"name": "Minimum peaks"}, {"name": "Minimum intensity"}, {"name": "remove_precursor_peak"}, {"name": "Remove precursor tolerance"}, {"name": "Clear mz range"}, {"name": "Variable mod 01"}, {"name": "Variable mod 02"}, {"name": "Variable mod 03"}, {"name": "Variable mod 04"}, {"name": "Variable mod 05"}, {"name": "Variable mod 06"}, {"name": "Variable mod 07"}, {"name": "Variable mod 08"}, {"name": "Variable mod 09"}, {"name": "Max variable mods in peptide"}, {"name": "Require variable mod"}, {"name": "Add Cterm peptide"}, {"name": "Add Nterm peptide"}, {"name": "Add Cterm protein"}, {"name": "Add Nterm protein"}, {"name": "Add G glycine"}, {"name": "Add A alanine"}, {"name": "Add S serine"}, {"name": "Add P proline"}, {"name": "Add V valine"}, {"name": "Add T threonine"}, {"name": "Add C cysteine"}, {"name": "Add L leucine"}, {"name": "Add I isoleucine"}, {"name": "Add N asparagine"}, {"name": "Add D aspartic acid"}, {"name": "Add Q glutamine"}, {"name": "Add K lysine"}, {"name": "Add E glutamic acid"}, {"name": "Add M methionine"}, {"name": "Add O ornithine"}, {"name": "Add H histidine"}, {"name": "Add F phenylalanine"}, {"name": "Add U selenocysteine"}, {"name": "Add R arginine"}, {"name": "Add Y tyrosine"}, {"name": "Add W tryptophan"}, {"name": "Add B user amino acid"}, {"name": "Add J user amino acid"}, {"name": "Add X user amino acid"}, {"name": "Add Z user amino acid"}, {"name": "Input files"}, {"name": "Name"}, {"name": "Comet enzyme info"}], "output": [{"name": "Output percolator file"}, {"name": "Output pepxml file"}, {"name": "Output txt file", "encodingFormat": "text/plain"}, {"name": "Output sqt file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510941132, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/control-freec-11-5/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/control-freec-11-5/5", "applicationCategory": "CommandLineTool", "name": "Control-FREEC", "description": "Control-FREEC analyzes copy-number variants and allelic imbalances in exome and whole-genome DNA sequencing.\n\nThis tool automatically computes, normalizes and segments copy number and beta allele frequency (BAF) profiles, then calls copy number alterations and LOH. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **chrLenFile** input is required and can be found in Public Reference Files as **Homo\\_sapiens\\_assembly38.fasta.sizes**, **ucsc.hg19.fasta.sizes** and **human\\_g1k\\_v37\\_decoy.fasta.sizes**.\n\n* The **ploidy** parameter is required. In case of doubt, different values can be set and Control-FREEC will select the one that explains the most observed CNVs.\n\n* Normal and control sample can be provided through two possible inputs:\n     * **mateFile**, a file with mapped reads\n     * **mateCopyNumberFile**, a raw copy number file created for both normal and control sample, provided through **mateFile** in a first run, and can be reused in the future runs for more efficient computation.\n\n\n\n* **A control (matched normal) sample is optional for whole genome sequencing data but mandatory for whole exome or targeted sequencing data.**\n\n* Similar to **mateCopyNumberFile**, a **Mini pileup Sample** and **Mini pileup Control** files can be created in the first run, if the **Known SNPs** file is provided. Consequently, by providing these files as inputs in future tasks, execution time will decrease significantly.\n\n* If a **mateFile** is specified, the **mateOrientation** parameter must be set.\n\n* In order to create a **BAF profile**, one of the following options must be implemented:\n    * **mateFile** + **Known SNPs** \n    * **mateCopyNumberFile** + **mateFile** + **KnownSNPs**\n    * **mateCopyNumberFile** + **miniPileup** + **KnownSNPs**\n\n### Changes Introduced by Seven Bridges\n\n* Based on the input parameters, a config file is created in order to properly run Control-FREEC.\n\n### Common Issues and Important Notes\n\n* **A control (matched normal) sample is optional for whole genome sequencing data but mandatory for whole exome or targeted sequencing data.**\n\n* A **gemMappabilityFile** can be used only in the mode without a control sample.\n\n* If a **mateFile** is specified, the **mateOrientation** parameter must be set.\n\n* Currently, there is an issue with creating a **BAF sample file** with the b37 notation. The genotypes for CNV regions are, however, created.\n\n\n### Performance Benchmarking\n\nThe instance set for this tool is the AWS c4.2xlarge instance with 8 vCPUs, 15 GiB of RAM and 1 TB of EBS (disk space).\n|     BAM size in GB    | Type |  Instance  | Duration | Cost ($) |\n|:--------------------:|:----:|:----------:|:--------:|:--------:|\n|         2x12 (Normal-Tumor)         |  WES | c4.2xlarge |  1h 52m  |    0.8   |\n|   100 (Tumor-only)   |  WGS | c4.2xlarge |  17h 43m |     7    |\n| 2x100 (Normal-Tumor) |  WGS | c4.2xlarge |   1d 8h  |    13    |\n|   100 (Tumor-only)   |  WGS | c4.8xlarge |  6h 30m |     10    |\n| 2x100 (Normal-Tumor) |  WGS | c4.8xlarge |   11h  |    18    |\n\nAn instance with more resources can be obtained by providing inputs for **Maximum threads** and **Total memory [MB]**.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n###References\n[1] [Control-FREEC: Prediction of copy number alterations and loss of heterozygosity using deep-sequencing data](http://boevalab.com/FREEC/tutorial.html#install)", "input": [{"name": "Mate file Sample", "encodingFormat": "application/x-bam"}, {"name": "Mate file Control", "encodingFormat": "application/x-bam"}, {"name": "Known SNPs", "encodingFormat": "application/x-vcf"}, {"name": "Capture regions", "encodingFormat": "text/x-bed"}, {"name": "Bed Graph output"}, {"name": "Break point threshold"}, {"name": "Break point type"}, {"name": "Contamination adjustment"}, {"name": "Degree"}, {"name": "Force GC content normalization"}, {"name": "GC content profile"}, {"name": "Minimum CNA length"}, {"name": "Maximum threads"}, {"name": "Noisy data"}, {"name": "Ploidy"}, {"name": "Print NA"}, {"name": "Sex"}, {"name": "Step"}, {"name": "Mate orientation sample"}, {"name": "Mate orientation control"}, {"name": "Minimal coverage per position"}, {"name": "Minimal quality per position"}, {"name": "Shift in quality"}, {"name": "Contamination"}, {"name": "Minimal exptected GC"}, {"name": "Maximum expected GC"}, {"name": "Minimal subclone presence"}, {"name": "Read count threshold"}, {"name": "Telocentromeric"}, {"name": "Unique match"}, {"name": "Mate copy number file sample"}, {"name": "Mate copy number file control"}, {"name": "Minimum mappability per window"}, {"name": "Intercept"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Total memory [MB]"}, {"name": "Chromosomes length file", "encodingFormat": "text/plain"}, {"name": "GEM mappability file"}, {"name": "Mini pileup Sample"}, {"name": "Coefficient of variation"}, {"name": "Window"}, {"name": "Mini pileup Control"}], "output": [{"name": "CNVs output", "encodingFormat": "text/plain"}, {"name": "Ratio", "encodingFormat": "text/plain"}, {"name": "BAF sample file", "encodingFormat": "text/plain"}, {"name": "Sample CPN"}, {"name": "Control CPN"}, {"name": "GC profile"}, {"name": "Ratio BedGraph"}, {"name": "Configuration script used for running", "encodingFormat": "text/plain"}, {"name": "Sample Pileup"}, {"name": "Control Pileup"}, {"name": "Info TXT", "encodingFormat": "text/plain"}, {"name": "CNVs with p-value", "encodingFormat": "text/plain"}, {"name": "Copy number profile"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/BoevaLab/FREEC"], "applicationSubCategory": ["Copy Number Analysis"], "project": "SBG Public Data", "creator": "Bioinformatics Laboratory of Institut Curie", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155967, "dateCreated": 1541788109, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/coverage2cytosine/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/coverage2cytosine/1", "applicationCategory": "CommandLineTool", "name": "Coverage2Cytosine", "description": "**Coverage2Cytosine** is a tool which generates a cytosine methylation report for a genome of interest using a sorted methylation input file produced by the tool **Bismark Methylation Extractor** when the **BedGraph** (`--bedGraph`) option was specified or by the tool **Bismark2BedGraph**. It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\nThe Coverage2Cytosine has two mandatory inputs.\n\n* **Coverage file** is a file in COV or COV.GZ format. It reports the position of methylated or unmethylated cytosine, methylation percentage and count of methylated and unmethylated cytosine\n\n* **Reference genome** are files of genome sequences in FASTA or FA formats.\n\nThe tool has six outputs, where with default options it generates only output **CpG report**.\n\n* The outputs **CX report** and **CpG report** contain information on every single cytosine in the genome irrespective of its context.\n\n* **GpC report** is a file which contains methylation in GpC context. The format of this output is exactly the same as for the normal cytosine report. In addition, this will write out a **GpC coverage** file.\n\n* **Merge CpG** and **Discordant CpG** are files which are created when the option **Merge CpG** (`--merge_CpG`) is set to TRUE and the integer value is provided to option **Discordance** (`--discordance <int>`). This value represents maximum allowed discordance between the top and the bottom strand methylation values expressed as the absolute difference in percent methylation.\n\n* Outputs will be compressed if the option **Gzip** is set to TRUE.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Coverage2Cytosine** is a tool which generates a cytosine methylation report of specific context for a genome of interest. The input **Coverage file** is expected to use 1-based genomic coordinates. By default, the output uses 1-based chromosomal coordinates and reports CpG positions only (for both strands individually and not merged in any way). Output coordinates may be changed to 0-based coordinates using the option **Zero Based** (`--zero_based`).\n\n### Changes Introduced by Seven Bridges\n\n* The tool requires reference genome sequences in FA or FASTA format, instead of providing the name of genome folder (`--genome_folder` <path>). The name of the folder, which is required in the command line, is taken from the current working directory.\n\n### Common Issues and Important Notes\n\n* **Merge CpG** (`--merge_CpG`) doesn't work with the options **CX context** (`--CX_context`) or **Split By Chromosome** (`--split_by_chromosome`).\n\n* **NOMe-Seq** data requires a COV.GZ file as input which has been generated in non-CG mode **CX context** (`--CX_context`) in the tools Bismark Methylation Extractor or in Bismark2BedGraph, else the GpC output file will be empty.\n\n* Default value of the input parameter **Discordance** (`--discordance`) is 0 if no value is provided when the option **Merge CpG** is selected.\n\n### Performance Benchmarking\n\n| Context | Total input size (COV, COV.GZ) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- |\n| CpG | 333 MB | c4.2xlarge | 19m | $0.17 |\n| CX | 333 MB | c4.2xlarge | 56m | $0.5 |\n| CX | 730 MB(.GZ) | c4.2xlarge | 1h 17m | $0.69 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "CX context"}, {"name": "Merge CpG"}, {"name": "Discordance"}, {"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Coverage file"}, {"name": "GC context"}, {"name": "NOMe-Seq"}, {"name": "FF"}, {"name": "Zero Based"}, {"name": "Split By Chromosome"}, {"name": "Gzip"}], "output": [{"name": "CX report", "encodingFormat": "text/plain"}, {"name": "CpG report", "encodingFormat": "text/plain"}, {"name": "GpC report", "encodingFormat": "text/plain"}, {"name": "Discordant CpG"}, {"name": "Merged CpG"}, {"name": "GpC coverage"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "File Format Conversion"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561065, "dateCreated": 1519389842, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/crossmap-0-5-4-cwl1-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/crossmap-0-5-4-cwl1-2/5", "applicationCategory": "CommandLineTool", "name": "CrossMap", "description": "**CrossMap** converts genomic coordinates between different assemblies [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**CrossMap** converts genomic coordinates from a **File to remap** based on the pairwise mappings between the starting and target assemblies given in the **Chain file**. Supported **File to map** formats include: BAM, SAM, VCF, GVCF, [BigWig](http://genome.ucsc.edu/goldenPath/help/bigWig.html), [Wiggle](http://genome.ucsc.edu/goldenPath/help/wiggle.html), MAF, BED, [GFF](http://genome.ucsc.edu/FAQ/FAQformat.html#format3) and [GTF](http://genome.ucsc.edu/FAQ/FAQformat.html#format4). For more details on input file formats and usage examples, please consult the official tool documentation [2]. The documentation page [2] also contains links to USCS and Ensembl repositories hosting commonly used chain files (**Chain file**).\n\n### Changes Introduced by Seven Bridges\n\n* This wrapper contains nine **CrossMap** commands (`bed`, `bam`, `bigwig`, `gvcf`, `gff`, `maf`, `region`, `vcf` and `wig`). The command to execute in the task is specified through the required **CrossMap command to run** input parameter.\n* The output is always saved to a file (not output to stdout).\n* **Output file name prefix** input parameter can be used to select the name for the generated outputs. If no value is provided for this parameter, the outputs will be named based on the **Sample ID** metadata field value of the **File to remap**. If this field is not populated, the base name of the **File to remap** is used instead. The names of the outputs will always contain the name of the chain file used for the conversion.\n* Only local files can be used as inputs for this wrapper (remote links are not supported).\n* **CrossMap** `viewchain` command is not included in this wrapper. It has been wrapped separately as **CrossMap Viewchain**. \n* As issues were encountered during testing with CRAM files, processing CRAMs directly is not supported with this wrapper. If you encounter issues processing CRAM data, please convert the input to BAM format and use the obtained BAM with **CrossMap**.\n\n\n### Common Issues and Important Notes\n\n* **File to remap**, **Chain file** and **CrossMap command to run** inputs are required.\n* If **File to remap** is a BAM file it should be sorted and indexed with **Samtools** [2]. \n* If **File to remap** is a BED file, it should contain at least three columns (chrom, start and end).\n* This wrapper combines nine different CrossMap commands and not all inputs are applicable to all commands. The labels of the inputs contain references (e.g. [GVCF, VCF]) to commands (**Crossmap command to run**) which accept or expect them:\n  - **Target reference sequence - [GVCF MAF VCF]** input file is required for running `gvcf`, `maf` and `vcf` CrossMap commands and should correspond to the target assembly.\n  - **Build name of the target assembly - [MAF]** input parameter is required for running the `maf` command.\n  - **Average insert size - [BAM]**, **Standard deviation of insert size - [BAM]**, **Insert size fold - [BAM]** and **Add tag to each alignment - [BAM]** input parameters can only be used with the `bam` CrossMap command.\n  - **Do not compare alleles - [GVCF VCF]** and **Compress (gzip) the output - [GVCF VCF]** input parameters can only be used with the `gvcf` and `vcf` CrossMap commands. \n  - **Minimum ratio of bases to remap - [REGION]** input parameter can only be used with the `region` CrossMap command.\n* When using the `bed` **CrossMap command to run**, if a region cannot be consecutively mapped to the target assembly, it will be split. Use the `region` CrossMap command to attempt to map large genomic regions (in BED format) whole.\n* As issues were encountered during testing with CRAM files, processing CRAMs directly is not supported with this wrapper. If you encounter issues processing CRAM data, please convert the input to BAM format and use the obtained BAM with **CrossMap**.\n* This version of CrossMap does not fully support VCFs with structural variants.\n* BIGWIG and WIGGLE input formats were not extensively tested with this wrapper.\n\n\n### Performance Benchmarking\n\n**CrossMap** performance depends on the type and size of the provided inputs.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| BED file (7 MB) | 2 min | $0.01 + $0.00 | c4.2xlarge - 1024 GB EBS | \n| GTF file (135 MB) | 2 min | $0.01 + $0.00 | c4.2xlarge - 1024 GB EBS | \n| MAF file (92 MB) | 3 min | $0.02 + $0.01 | c4.2xlarge - 1024 GB EBS | \n| WGS GVCF.GZ file (8.9 GB) | 101 min | $0.67 + $0.24 | c4.2xlarge - 1024 GB EBS |\n| WGS BAM file (48.4 GB) | 15 h 18 min | $6.09 + $1.06 | c4.2xlarge - 500 GB EBS |\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**CrossMap** was tested with cwltool version 3.1.20210628163208. The `in_file`, `in_command`, `in_chain` and `prefix` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [CrossMap publication](https://pubmed.ncbi.nlm.nih.gov/24351709/)\n\n[2] [CrossMap documentation](https://crossmap.readthedocs.io/en/latest/#)", "input": [{"name": "CrossMap command to run"}, {"name": "Chain file"}, {"name": "File to remap", "encodingFormat": "application/x-vcf"}, {"name": "Target reference sequence [GVCF MAF VCF]", "encodingFormat": "application/x-fasta"}, {"name": "Output file name prefix"}, {"name": "Average insert size - [BAM]"}, {"name": "Standard deviation of insert size - [BAM]"}, {"name": "Insert size fold - [BAM]"}, {"name": "Add tag to each alignment - [BAM]"}, {"name": "Do not compare alleles - [GVCF VCF]"}, {"name": "Compress (gzip) the output - [GVCF VCF]"}, {"name": "Build name of the target assembly - [MAF]"}, {"name": "Minimum ratio of bases to remap - [REGION]"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Remapped data", "encodingFormat": "application/x-gtf"}, {"name": "Unmapped data"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/liguowang/CrossMap"], "applicationSubCategory": ["File Format Conversion", "Utilities", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Wang Liguo", "softwareVersion": ["v1.2"], "dateModified": 1648040157, "dateCreated": 1636031576, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/crossmap-viewchain-0-5-4-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/crossmap-viewchain-0-5-4-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "CrossMap Viewchain", "description": "**CrossMap Viewchain** prints the chain file for two assemblies in a human-readable format  [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**CrossMap** prints the supplied **Chain file** as a tab-separated file listing the 'chrom', 'start' ,'end' and 'strand' of the source and target genome assemblies [2].\n\n### Changes Introduced by Seven Bridges\n\n* The output is always captured in a file. If the **Output file name prefix** is not provided, the output file is named based on the **Chain file**.\n\n\n### Common Issues and Important Notes\n\n* **Chain file** input is required.\n\n### Performance Benchmarking\n\nTypical **CrossMap Viewchain** tasks last 1-2 minutes (<$0.1) on an on-demand AWS c4.2xlarge instance.\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**CrossMap Viewchain** was tested with cwltool version 3.1.20210628163208. The `in_chain` and `prefix` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [CrossMap publication](https://pubmed.ncbi.nlm.nih.gov/24351709/)\n\n[2] [CrossMap documentation](https://crossmap.readthedocs.io/en/latest/#)", "input": [{"name": "Chain file"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}], "output": [{"name": "Human-readable chain file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/liguowang/CrossMap"], "applicationSubCategory": ["File Format Conversion", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Wang Liguo", "softwareVersion": ["v1.2"], "dateModified": 1648040157, "dateCreated": 1636031730, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cuffdiff/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cuffdiff/7", "applicationCategory": "CommandLineTool", "name": "Cuffdiff", "description": "Cuffdiff is a highly accurate tool for performing differential expression analysis for RNA-seq experiments. It accepts read alignment files from two or more samples, calculates expression at the isoform and gene level, and tests for significant expression differences. Cuffdiff outputs a downloadable collection of files that can be explored as spreadsheets. A quality control analysis of Cuffdiff results can be performed using CummeRbundQC.\n\nCommon issues:\n\n1) To use Cuffdiff for differential expression estimation on our platform, we recommend you take a look at \"RNA-seq Differential Expression\" public workflow and it's description. There is a hidden metadata field \u201csample group\u201d that needs to be set for all sample files prior to the analysis. This is achieved using SBG Group Input tool.\n\n2) The number of replicates doesn\u2019t need to be the same in each experimental condition. However, if you use fewer than 3 replicates per condition, by default Cuffdiff will not test for significant alterations in splicing, tss, or cds usage (here 3 is the default value of \"min_reps_for_js_test\").", "input": [{"name": "Sample files", "encodingFormat": "application/x-bam"}, {"name": "Compatible hits normalization"}, {"name": "Mask file", "encodingFormat": "application/x-gtf"}, {"name": "Contrast file", "encodingFormat": "text/plain"}, {"name": "Fragment bias correct", "encodingFormat": "application/x-fasta"}, {"name": "Transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Total hits normalization"}, {"name": "False discovery rate"}, {"name": "Correct for reads mapped to multiple loci"}, {"name": "Minimum alignment count"}, {"name": "Library type"}, {"name": "Dispersion method"}, {"name": "Library normalization method"}, {"name": "Mean fragment length"}, {"name": "Fragment lengths' standard deviation"}, {"name": "Maximum MLE number of iterations"}, {"name": "Maximum bundle fragments"}, {"name": "Number of draws for counting fragments"}, {"name": "Number of assignments for fragment draws"}, {"name": "Minimum replicates required for Jensen-Shannon test"}, {"name": "No effective length correction"}], "output": [{"name": "Archive of Cuffdiff output files", "encodingFormat": "application/zip"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Differential Expression", "RNA-Seq"], "project": "SBG Public Data", "creator": "Cole Trapnell/University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033583, "dateCreated": 1453799183, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cufflinks/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cufflinks/4", "applicationCategory": "CommandLineTool", "name": "Cufflinks", "description": "Cufflinks assembles transcripts and estimates their abundances in RNA-seq samples. It accepts aligned RNA-seq reads and assembles the alignments into a parsimonious set of transcripts. Cufflinks then estimates the relative abundances of these transcripts based on how many reads support each one, taking into account biases in library preparation protocols.\n\nCufflinks tool can be used before Cuffdiff tool when the annotated transcriptome is not available, or when novel transcripts need to be included.", "input": [{"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Reference annotation to estimate isoform expression", "encodingFormat": "application/x-gtf"}, {"name": "Mask file", "encodingFormat": "application/x-gtf"}, {"name": "Fragment bias correct", "encodingFormat": "application/x-fasta"}, {"name": "Reference annotation to guide RABT assembly", "encodingFormat": "application/x-gtf"}, {"name": "Correct for reads mapped to multiple loci"}, {"name": "Library type"}, {"name": "Mean fragment length"}, {"name": "Fragment lengths' standard deviation"}, {"name": "Library normalization method"}, {"name": "Maximum MLE number of iterations"}, {"name": "Compatible hits normalistaion"}, {"name": "Total hits normalization"}, {"name": "Fragment generation samples"}, {"name": "Fragment assignment samples"}, {"name": "Alignments per fragment"}, {"name": "No length correction"}, {"name": "No effective length correction"}, {"name": "Minimum isoform fraction"}, {"name": "Precursor mRNA fraction"}, {"name": "Maximum intron length"}, {"name": "Alpha value for the binomial test"}, {"name": "Small anchor fraction"}, {"name": "Minimium frags per transfrag"}, {"name": "Overhang tolerance"}, {"name": "Maximum bundle length"}, {"name": "Maximum bundle fragments"}, {"name": "Minimum intron length"}, {"name": "Trim 3' average coverage threshold"}, {"name": "Trim 3' dropoff fraction"}, {"name": "Maximum multiread fraction"}, {"name": "Overlap radius"}, {"name": "No faux reads"}, {"name": "3' overhang tolerance"}, {"name": "Intron overhang tolerance"}], "output": [{"name": "Gene-level expression", "encodingFormat": "text/plain"}, {"name": "Isoform-level expression", "encodingFormat": "text/plain"}, {"name": "Skipped loci", "encodingFormat": "application/x-gtf"}, {"name": "Transcripts", "encodingFormat": "application/x-gtf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["SAM/BAM Processing", "Differential Expression"], "project": "SBG Public Data", "creator": "Cole Trapnell/University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799028, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cuffnorm/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cuffnorm/4", "applicationCategory": "CommandLineTool", "name": "Cuffnorm", "description": "Cuffnorm normalizes the read counts across RNA-seq libraries to control for read depth and allow comparisons. It produces a number of output files that contain expression levels and normalized fragment counts at the level of transcripts and genes. It also tracks changes in the relative abundance of transcripts sharing a common transcription start site, and in the relative abundances of the primary transcripts of each gene, allowing to see changes in splicing.\n\nCommon issues:\n\n1) To use Cuffnorm for quantification in a differential expression analysis on our platform, we recommend you take a look at \"RNA-seq Differential Expression\" public workflow and it's description. There is a hidden metadata field \u201csample group\u201d that needs to be set for all sample files prior to the analysis. This is achieved using SBG Group Input tool.", "input": [{"name": "Sample files", "encodingFormat": "application/x-bam"}, {"name": "Library type"}, {"name": "Library normalization method"}, {"name": "Output format"}, {"name": "Compatible hits normalization"}, {"name": "Total hits normalization"}, {"name": "Transcripts", "encodingFormat": "application/x-gtf"}], "output": [{"name": "Archive of Cuffnorm output files", "encodingFormat": "application/zip"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Differential Expression", "Quantification"], "project": "SBG Public Data", "creator": "Cole Trapnell/University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799109, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cuffquant/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cuffquant/3", "applicationCategory": "CommandLineTool", "name": "Cuffquant", "description": "Cuffquant performs preparations on SAM/BAM files for differential expression analysis of RNA-seq data. It computes the gene and transcript expression profiles of various samples, which can be fed into Cuffnorm or Cuffdiff. \n\nNote: It is recommended to use Cuffquant for analyses involving more than a handful of libraries.", "input": [{"name": "Annotations", "encodingFormat": "application/x-gtf"}, {"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Mask file", "encodingFormat": "application/x-gtf"}, {"name": "Fragment bias correct", "encodingFormat": "application/x-fasta"}, {"name": "Correct for reads mapped to multiple loci"}, {"name": "Library type"}, {"name": "Mean fragment length"}, {"name": "Fragment lengths' standard deviation"}, {"name": "Maximum MLE number of iterations"}, {"name": "Maximum bundle fragments"}, {"name": "No effective length correction"}, {"name": "Minimum number of alignments"}, {"name": "Maximum alignments per fragment"}, {"name": "No length correction"}], "output": [{"name": "Abundances"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Cole Trapnell/University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033583, "dateCreated": 1453799306, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cummerbundqc/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cummerbundqc/3", "applicationCategory": "CommandLineTool", "name": "CummeRbundQC", "description": "CummeRbundQC assesses the quality of a differential expression analysis performed with Cuffdiff. It accepts differential expression results in the form of a cuffdiff.zip folder as input and produces a report with charts that can be viewed on Seven Bridges platform or downloaded to your local drive. \n\nCummeRbundQC is built on top of CummeRbund v. 2.8.2. CummeRbundQC incorporates the \"Global Statistics and Quality Control\" graphs described in the CummeRbund manual. These visualizations provide an overview of the relationships among the replicates and help in detecting over-dispersion in samples.", "input": [{"name": "Archive of Cuffdiff output files", "encodingFormat": "application/zip"}, {"name": "Dispersion plot threshold"}, {"name": "Density plot threshold"}, {"name": "Turn off thresholds"}], "output": [{"name": "Report archive", "encodingFormat": "application/zip"}, {"name": "Report"}], "codeRepository": [], "applicationSubCategory": ["Differential Expression", "Plotting"], "project": "SBG Public Data", "creator": "Cole Trapnell/University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799134, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cutadapt-2-9/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cutadapt-2-9/7", "applicationCategory": "CommandLineTool", "name": "Cutadapt", "description": "**Cutadapt**  is most commonly used for removing adapter sequences.\n\n**Cutadapt** finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequences from your high-throughput sequencing reads, which can be in **FASTA** (.fasta, .fa, .fna), **FASTQ** (.fastq, .fq) format, either uncompressed or **compressed as .gz**.\n\nCleaning your data in this way is often required: Reads from small-RNA sequencing contain the 3\u2019 sequencing adapter because the read is longer than the molecule that is sequenced. Amplicon reads start with a primer sequence. Poly-A tails are useful for pulling out RNA from your sample, but often you don\u2019t want them to be in your reads.\n\nCutadapt helps with these trimming tasks by finding the adapter or primer sequences in an error-tolerant way. It can also modify and filter reads in various ways. Adapter sequences can contain IUPAC wildcard characters. Also, paired-end reads are supported. If you want, you can also just demultiplex your input data, without removing adapter sequences at all.\n\nCutadapt comes with an extensive suite of automated tests and is available under the terms of the MIT license.\n\n*The list of  **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###**Common Use Cases**\n\n1. **Removing Adapters** - **Cutadapt** supports trimming of multiple types of adapters (3\u2019, 5\u2019, Anchored 3\u2019, Anchored 5\u2019, 5\u2019 or 3\u2019/both). Common Illumina adapters can be found in the following document: [Illumina adaptors](https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/experiment-design/illumina-adapter-sequences-1000000002694-12.pdf)\n\n\n2. **Modifying Reads**. Cutadapt modifies reads in the following **Read Modification Order**:\n - Unconditional base removal from the beginning or end of each read using the **Cut length** ( `--cut`) option. If the given length is positive, the bases are removed from the beginning of each read. If it is negative, the bases are removed from the end.\n - Quality trimming step, **Quality cutoff 5' end** and **Quality cutoff 3' end** options, can be used to trim low-quality ends from reads before adapter removal. Quality trimming can be done without adapter trimming.\n - Adapter trimming with **Adapter that was ligated to the 3' end** (`-a`), **Adapter that was ligated to the 5' end** (`-b`), **Adapter that was ligated to the 5' or 3' end** (`-g`) options.\n - Read shortening. To shorten each read down to a certain length, use the **Length trimming** (`--length`) option. The removed bases are those on the 3\u2019 end.\n - N-end trimming, remove flanking **N** bases from each read using the **Trim N's on ends of reads** (`--trim-n`) option.\n - Length tag modification. Some old read files contain the length of the read in the name, if you want to update this to the correct length after trimming use the **Length TAG** (`--length-tag`) option.\n - Read name suffix removal. In order to remove a suffix of each read name, use the **Remove suffix from read names** (`--strip-suffix`) option.\n - Addition of prefix and suffix to read name. Use **Prefix for read names** (`-x`/`--prefix`) or **Suffix for read names** (`-y`/`--suffix`). For both options, spaces need to be specified explicitly.\n - Replace negative quality values with zero(zero capping).\n \n3. **Filtering Reads** - options described here make it possible to filter reads by either discarding them entirely or by redirecting them to other files. When redirecting reads, the basic rule is that each read is written to at most one file.\n  - Minimum Length, **Minimum Length** (`-m/--minimum-length`). Discard processed reads that are shorter than specified length.\n  - Maximum Length, **Maximum Length** (`-M/--maximum-length`). Discard processed reads that are longer than specified length.\n  - Discard reads in which an adapter was found with the **Discard trimmed** (`--discard-trimmed`) option.\n  - Discard reads in which an adapter wasn't found with the **Discard untrimmed** (`--discard-untrimmed`) option.\n\n4. **Trimming Paired-end reads** - **Cutadapt** supports trimming of paired-end reads, trimming both reads in pair a at the same time. Adapter for the second read in a pair is specified with **Paired end : 3' adapter** (`-A`), **Paired-end : 5' adapter** (`-B`) and **Paired-end : 5'/3 adapter** (`-G`) options.\n\n5. **Multiple adapters** -  It is possible to specify more than one adapter sequence by using **Adapter that was ligated to the 3' end**(`-a`), **Adapter that was ligated to the 5' end**(`-b`) and **Adapter that was ligated to the 5' or 3' end** ( `-g`) more than once. Any combination is allowed. Each read will be searched for all given parameters, but only the best matching adapter is removed.\n\n\n###**Changes Introduced by Seven Bridges**###\n\n- Output prefix for reports (**report** and **info file**), the **Output prefix for reports** option specifies a common prefix for all report like files; the default prefix is **report**.\n- **Output tag** option specifies a unique tag that is to be added to **Trimmed output files**; default one is **cutadapted**.\n- Additional output options. Specify making of additional outputs with **Untrimmed output file**, **Too short output file**, **Too long output file**, **Info output file**, **Rest output file** and **Wildcard output file** options \n- Paired-end CMD line build. The tool automatically builds CMD line when paired-end files are introduced as inputs.\n- Interleaved CMD line build. Tthe ool automatically builds CMD line when the **interleaved** (`--interleaved`) option is checked.\n- Introduction of an adapter fasta file to CMD line. You can provide an adapter fasta file to the inputs and then just write `file` or `File` to the desired adapter position, while the tool will handle CMD line build.\n\n\n###**Common Issues and Important Notes**\n\n- If you are working with paired-end reads, you need to specify adapters for the first read in a pair and adapters for the second read in a pair.  This is also valid if you are using an adapter fasta file, you need to provide the same file for both ends.\n- Please check if the **Paired-end** metadata field is specified for each file before running cutadapt with paired-end options.\n- Only one of these **Discard trimmed reads** (`--discard-trimmed`), **Discard untrimmed reads** (`--discard-untrimmed`), **Untrimmed output file** (`--untrimmed-output`) options can be active at the same time.\n- The **Info file output**, **Rest file output**and **Wildcard output file** options write out information only from the first read in a pair.\n- ` --info-file`, `--rest-file`, `--wildcard-file`, `--untrimmed-output`, `--too-short-output` and `--too-long-output` are active only if you check their options ( **Info output file**, **Rest output file**, **Wildcard output file**, **Untrimmed output file**, **Too short output file**, **Too long output file**).\n- The following command-line arguments are **not compatible with multi-core** : **Info output file**, **Rest output file**, **Wildcard output file**, **Untrimmed output file**, **Too short output file**, **Too long output file**, **Input file format** (`--format`). If one of these options is specified, the multicore option is automatically turned off.\n- **Interleaved** (`--interleaved`) option must be specified when dealing with interleaved input files.\n\n\n###**Performance Benchmarking**\n\nBenchmarking was performed on c4.2xlarge (8 vCPU, 15 GiB Memory) and c4.8xlarge (36 vCPU, 60 GiB Memory)  AWS instances. Trimming isn't as computationally expensive as alignment, but it clearly benefits from multiple threads, so here are results on two instances at the different sides of the CPU spectrum.  \n\n| Input size [Gb] | Paired-End | Multicore | Duration [min] | Cost [$] |  Instance  |\n|:---------------:|------------|:---------:|:--------------:|:--------:|:----------:|\n|      2x17.4     |     Yes    |     On    |       15       |   0.14   | c4.2xlarge |\n|      2x17.4     |     Yes    |    Off    |       38       |   0.34   | c4.2xlarge |\n|      2x41.3     |     Yes    |     On    |       31       |   0.28   | c4.2xlarge |\n|      2x41.3     |     Yes    |    Off    |       71       |   0.66   | c4.2xlarge |\n|      2x210      |     Yes    |     On    |       107      |   3.39   | c4.8xlarge |\n|      2x210      |     Yes    |    Off    |       384      |   12.16  | c4.8xlarge |\n\n*Cost can be significantly reduced by  using **spot instances** . Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n###**References**\n\n[1] [Cutadapt Documentation.](http://cutadapt.readthedocs.io/en/stable/guide.html#read-processing)", "input": [{"name": "Adapter that was ligated to the 3' end"}, {"name": "Paired-end : 5'/3 adapter"}, {"name": "Paired end : 3' adapter"}, {"name": "Adapter that was ligated to the 5' end"}, {"name": "Adapter that was ligated to the 5' or 3' end"}, {"name": "Paired-end : 5' adapter"}, {"name": "Adapter fasta file", "encodingFormat": "application/x-fasta"}, {"name": "Number of cores"}, {"name": "Number of adapters to be removed from each read"}, {"name": "Cut length"}, {"name": "Discard trimmed reads"}, {"name": "Discard untrimmed reads"}, {"name": "Maximum allowed error rate"}, {"name": "Output file format"}, {"name": "Info output file"}, {"name": "Input FASTQ/FASTA file", "encodingFormat": "text/fastq"}, {"name": "Interleaved"}, {"name": "Length TAG"}, {"name": "Length trimming"}, {"name": "Interpret IUPAC wildcards in reads."}, {"name": "Discard reads with too many N bases"}, {"name": "Maximum length"}, {"name": "Minimum length"}, {"name": "NextSeq trim"}, {"name": "Do not allow INDELs in the alignments"}, {"name": "Do not interpret IUPAC wildcards in adapters"}, {"name": "Output prefix for reports"}, {"name": "Output tag"}, {"name": "Minimum overlap length"}, {"name": "Filter paired-end reads"}, {"name": "Prefix for read names"}, {"name": "Quality cutoff 3' end"}, {"name": "Quality cutoff 5' end"}, {"name": "Quality base"}, {"name": "Do not print a report"}, {"name": "Remove length on second read in pair"}, {"name": "Rest output file"}, {"name": "Remove suffix from read names"}, {"name": "Suffix for read names"}, {"name": "Too long output file"}, {"name": "Too short output file"}, {"name": "Trim N's on ends of reads"}, {"name": "Untrimmed output file"}, {"name": "Wildcard output file"}, {"name": "Zero cap"}, {"name": "What to do with found adapters"}, {"name": "Check reverse complement as well"}, {"name": "Maximum expected errors"}, {"name": "Discard reads that did not pass CASAVA filtering"}, {"name": "Report type"}], "output": [{"name": "Info file", "encodingFormat": "text/plain"}, {"name": "Report", "encodingFormat": "text/plain"}, {"name": "Rest File", "encodingFormat": "text/fastq"}, {"name": "Too long output file", "encodingFormat": "text/fastq"}, {"name": "Too short output file", "encodingFormat": "text/fastq"}, {"name": "Trimmed reads", "encodingFormat": "text/fastq"}, {"name": "Untrimmed output file", "encodingFormat": "text/fastq"}, {"name": "Wildcard output file", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/marcelm/cutadapt", "https://github.com/marcelm/cutadapt/archive/v2.9.zip"], "applicationSubCategory": ["FASTQ Processing", "Read Trimming", "Utilities"], "project": "SBG Public Data", "creator": "Marcel Martin", "softwareVersion": ["v1.0"], "dateModified": 1649165282, "dateCreated": 1585922852, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cutesv-1-0-9/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cutesv-1-0-9/5", "applicationCategory": "CommandLineTool", "name": "cuteSV", "description": "**cuteSV** is a structural variation (SV) discovery tool designed for long read sequences.  The tool can call SV variants in sequences obtained by long read sequencing technologies, such as Pacific Bioscience (PacBio) and Oxford Nanopore Technology (ONT). **cuteSV** uses read-alignment-based SV detection approach with tailored methods to collect the signatures of various types of SVs and employs a clustering-and-refinement method to analyze the signatures to implement sensitive SV detection [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**cuteSV** calls structural variation from sorted long read alignments (**Input BAM file** input parameter). The tool accepts sorted output from **Minimap2** or **NGMLR**. **cuteSV** authors have provided suggested input parameters values for long reads obtained with different sequencing technologies on their [GitHub page](https://github.com/tjiangHIT/cuteSV).\n\n### Changes Introduced by Seven Bridges\n\n* Positional argument `work_dir` is omitted from the inputs. `work_dir` is always set to the current working directory.\n* Parameters `--help` and `--version` were omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* Inputs **Input BAM file** and **Input reference** are required.\n* Suggested input values for some parameters are available on the **cuteSV** [GitHub page](https://github.com/tjiangHIT/cuteSV).\n* If the **Sample name or id** input is not provided by the user, the tool will use **Sample ID** from **Input BAM file** metadata. If **Sample ID** is not available in metadata, then the tool will use NULL as **Sample name or id**.\n\n### Performance Benchmarking\n\n * [CHM13 ONT data](https://pubmed.ncbi.nlm.nih.gov/29431738/) (50\u00d7 coverage whole-genome sequencing dataset of the CHM13hTERT human cell line on the Oxford Nanopore GridION) - 140 GB FQ.GZ file, downloaded from: https://s3.amazonaws.com/nanopore-human-wgs/chm13/nanopore/rel2/rel2.fastq.gz, aligned to GRCh38 with **Minimap 2**.\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| CHM13 - 8 cores | 1 h 3 min |$0.35 + $0.15 | c5.2xlarge 1000 GB EBS | \n| CHM13 - 30 cores | 28 min |$0.71 + $0.07 | c5.9xlarge 1000 GB EBS |\n| CHM13 - 8 cores with genotypes | 15 h 3 min |$5.12 + $2.14 | c5.2xlarge 1000 GB EBS | \n| CHM13 - 30 cores with genotypes | 6 h 3 min |$9.25 + $0.86 | c5.9xlarge 1000 GB EBS | \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### References\n[1] [cuteSV publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02107-y)\n\n[2] [cuteSV documentation](https://github.com/tjiangHIT/cuteSV)", "input": [{"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "Input reference", "encodingFormat": "application/x-fasta"}, {"name": "Threads"}, {"name": "Batch of genome segmentation interval"}, {"name": "Sample name or id"}, {"name": "Retain temporary folder and files"}, {"name": "Report read IDs"}, {"name": "Maximum number of split segments"}, {"name": "Minimum mapping quality"}, {"name": "Minimum read length"}, {"name": "Maximum distance of deletion signals"}, {"name": "Maximum distance of insertion signals"}, {"name": "Minimum number of reads"}, {"name": "Minimum size of SV to be reported"}, {"name": "Maximum size of SV to be reported"}, {"name": "Minimum length of SV signal to be extracted"}, {"name": "Generate genotypes"}, {"name": "Maximum round of iteration for genotyping"}, {"name": "Optional vcf file for force calling", "encodingFormat": "application/x-vcf"}, {"name": "Maximum distance to cluster reads together for insertion"}, {"name": "Maximum basepair identity for insertion"}, {"name": "Maximum distance to cluster read together for deletion"}, {"name": "Maximum basepair identity for deletion"}, {"name": "Maximum distance to cluster read together for inversion"}, {"name": "Maximum distance to cluster read together for duplication"}, {"name": "Maximum distance to cluster read together for translocation"}, {"name": "Filter breakpoints for translocation"}, {"name": "Memory per job"}, {"name": "CPUs per job"}, {"name": "Output filename"}], "output": [{"name": "Output variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/tjiangHIT/cuteSV/archive/cuteSV-v1.0.9.tar.gz", "https://github.com/tjiangHIT/cuteSV"], "applicationSubCategory": ["Variant Calling", "Long Reads"], "project": "SBG Public Data", "creator": "Jiang Tao", "softwareVersion": ["v1.1"], "dateModified": 1648039727, "dateCreated": 1612305096, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/deeptrio/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/deeptrio/6", "applicationCategory": "CommandLineTool", "name": "DeepTrio", "description": "**DeepTrio** is a tool that is intended for variant calling of trios or duos.\n\nThe main advantage of DeepTrio is that genetic inheritance is considered by a neural network for calling variants in trio samples. Also, variant candidates are generated from all samples at once, which ensures a genotype call is made for any position in the trio with a variant [1]. \n\n**DeepTrio** is built on top of the **DeepVariant** tool[2] and, like **DeepVariant**, is initially composed of three programs: **make\\_examples**, **call\\_variants**, and **postprocess\\_variants**.   \n**make\\_examples** consumes reads and the reference genome to create TensorFlow examples for evaluation with deep learning models. The tf.Example protos are written out in TFRecord format. This tool is used as the first step of both training and inference pipelines.  \nOnce the pileup images have been constructed  as \"examples\", the variant calling tool **call\\_variants** could start to perform inference - identifying and labeling genomic variants.  \nTo convert the tfrecord output of **call\\_variants** into the VCF format that is familiar to bioinformaticians, **postprocess\\_variants** tool needs to be invoked.\nAll three steps are merged into one **run\\_deeptrio** step step which makes the **DeepTrio** tool.\n\nThe main inputs of **DeepTrio** are a FASTA reference file and BAM files representing child/parents reads, together with the desired model, while the main output is a list of all variant calls in VCF and gVCF formats.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nAt the highest level, the user needs to provide six inputs:\n\n* A reference genome in FASTA format with the corresponding FAI index file.  \n* An aligned child reads file in BAM format with the corresponding BAI index file. The reads should be aligned to the reference genome described above.\n* An aligned parent reads file in BAM format with the corresponding BAI index file. The reads should be aligned to the reference genome described above.\n* A model checkpoint for DeepTrio. The default model which is used is the one provided by the **DeepTrio** team, while the user only needs to select whether to use the WGS, WES or PACBIO model via the **Model** parameter. Optionally, a custom model can be supplied to the **Call variants extra arguments** input, which will override the previously selected default model (with `--checkpoint` prefix). \n* Sample names for child and parent reads.\n* BED file can be provided as an optional input on the **Regions BED** input. Also, user may manually choose which regions to process on the **Regions** input\n\n\n### Changes introduced by Seven Bridges\n\n- The tool should have four optional parameters: **Output gVCF child**, **Output gVCF parent1**, **Output gVCF parent2**, **Output gVCF merged**. Those parameters should decide if gVCF files are going to be produced, and how they should be named. However, in this version of the tool those inputs should always have a value, otherwise VCF files are not going to be produced. This behaviour is explained in [this issue](https://github.com/google/deepvariant/issues/429).\n\n### Common Issues and Important Notes\n\n- The **Reference FASTA** (`--ref`) needs to be accompanied with its indexed file. We recommend using **Samtools Faidx CWL1.0** app to index the reference file.\n- **Input BAM - Child**, **Input BAM - Parent1** and **Input BAM - Parent2** (`--reads_child`, `--reads_parent1`, `--reads_parent2` ) BAM files need to be accompanied with their indexed files. We recommend using the **SAMtools Index** app to index the BAM file.\n- **Input BAM - Parent2** input is optional. **DeepTrio** can be used for calling variants in duos.\n- Unlike **DeepVariant**, the current version of **DeepTrio** does not have support for OpenVINO as explained in [this issue](https://github.com/google/deepvariant/issues/416).\n\n### References\n\n[1] [DeepTrio Documentation](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-details.md)\n\n[2] [DeepVariant paper](https://www.nature.com/articles/nbt.4235)", "input": [{"name": "Model type"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Input BAM - Child", "encodingFormat": "application/x-bam"}, {"name": "Input BAM - Parent1", "encodingFormat": "application/x-bam"}, {"name": "Input BAM - Parent2", "encodingFormat": "application/x-bam"}, {"name": "Intermediate results directory"}, {"name": "Logging directory"}, {"name": "Number of shards"}, {"name": "Regions"}, {"name": "Sample name child"}, {"name": "Sample name parent 1"}, {"name": "Sample name parent 2"}, {"name": "Use HP information"}, {"name": "Make examples extra arguments"}, {"name": "Call variants extra arguments"}, {"name": "Postprocess variants extra arguments"}, {"name": "Output visual report"}, {"name": "Regions BED", "encodingFormat": "text/x-bed"}, {"name": "Custom model"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job"}], "output": [{"name": "Variants child", "encodingFormat": "application/x-vcf"}, {"name": "Variants parent 1", "encodingFormat": "application/x-vcf"}, {"name": "Variants parent 2", "encodingFormat": "application/x-vcf"}, {"name": "Genomic variants child", "encodingFormat": "application/x-vcf"}, {"name": "Genomic variants parent 1", "encodingFormat": "application/x-vcf"}, {"name": "Genomic variants parent 2", "encodingFormat": "application/x-vcf"}, {"name": "Genomic variants merged", "encodingFormat": "application/x-vcf"}, {"name": "Visual report child", "encodingFormat": "text/html"}, {"name": "Visual report parent 1", "encodingFormat": "text/html"}, {"name": "Visual report parent 2", "encodingFormat": "text/html"}, {"name": "Intermediate results directory"}, {"name": "Logging directory"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-quick-start.md", "https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-details.md", "https://github.com/google/deepvariant/tree/r1.1/deeptrio", "https://github.com/google/deepvariant/archive/refs/tags/v1.1.0.zip"], "applicationSubCategory": ["Genomics", "Variant Calling", "CWL1.2"], "project": "SBG Public Data", "creator": "Google", "softwareVersion": ["v1.2"], "dateModified": 1629392177, "dateCreated": 1629392176, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/deepvariant-1-1-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/deepvariant-1-1-0/3", "applicationCategory": "CommandLineTool", "name": "DeepVariant", "description": "**DeepVariant** is an analysis pipeline that uses a deep neural network to call genetic variants from NGS DNA data [1]. \n\n### Why DeepVariant\n* **DeepVariant** is **highly accurate**. In 2016 **DeepVariant** won PrecisionFDA Truth Challenge in the best SNP Performance category [2].\n* **DeepVariant** is **robust**. **DeepVariant** maintains high accuracy even for error-prone sequencing conditions, including PCR-positive samples and low quality sequencing runs [3].\n* **DeepVariant** is **flexible**. **DeepVariant** can be easily adjusted or used out of the box for different sequencing technologies and even for non-human species [4].\n* **DeepVariant** is **easy to use**. No filtering is needed beyond setting your preferred minimum quality threshold.\n\n**DeepVariant** is initially composed of three programs: **make\\_examples**, **call\\_variants**, and **postprocess\\_variants**.   \n**make\\_examples** is the command used to extract pileup images from your BAM/CRAM/SAM files, encoding each as tf.Example (a kind of protocol buffer that TensorFlow knows about) in \"tfrecord\" files. This tool is used as the first step of both training and inference pipelines.  \nOnce we have constructed the pileup images as \"examples\", we can then invoke the variant calling tool **call\\_variants** to perform inference - identifying and labeling genomic variants.  \nTo convert the tfrecord output of **call\\_variants** into the VCF format that is familiar to bioinformaticians, we need to invoke the **postprocess\\_variants** tool.\n\nIn order to make using **DeepVariant** on the Seven Bridges platform as easy as possible, all of the previously described steps are merged into just this one **DeepVariant** tool, with slight modifications from the original behaviour, so that the user experience is as pleasant as possible. \n\nThe main inputs of **DeepVariant** are a FASTA and a BAM/CRAM/SAM file, together with the desired model, while the main output is a list of all variant calls in VCF format.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nAt the highest level, the user needs to provide three inputs:\n\n* A reference genome in FASTA format with the corresponding FAI index file.  \n* An aligned reads file in BAM/CRAM/SAM format with the corresponding BAI/CRAI index file. The reads should be aligned to the reference genome described above.\n* A model checkpoint for DeepVariant. The default model which is used is the one provided by the **DeepVariant** team, while the user only needs to select whether to use the WGS, WES, PACBIO or HYBRID PACBIO ILLUMINA model via the **Model** parameter. Optionally, a custom model can be supplied to the **Custom model** input, which will override the previously selected default model. \n* BED files can be provided as optional inputs on the **Regions BED** (`--regions`) input, the **Confident regions BED** (`--confident-regions`) input, or the **Exclude regions BED** (`--exclude-regions`) input, depending on what the BED file will be used for. \n* If a GVCF output (with non-variant calls) is desired, this is easily achievable by setting the **GVCF** boolean option to True. \n* All tips for parallel execution, as described on the **DeepVariant** github page, have been implemented in this wrapper, allowing the user to specify the desired number of threads with the **Threads** parameter. \n\n\n### Changes Introduced by Seven Bridges\n\n* As described previously, all the scripts that are necessary for a successful **DeepVariant** execution have been merged into one tool on the Seven Bridges platform. \n* The model for **DeepVariant** is selected as an enum parameter by default (WGS, WES, PACBIO and HYBRID PACBIO ILLUMINA). The user can provide a custom model, in which case the default models will be overridden. \n* The GVCF output is created simply by specifying the **GVCF** boolean option. In the original manual for **DeepVariant**, a couple of different options need to be set during different steps of the pipeline, but on the Seven Bridges platform, setting the mentioned boolean option to True will take care of all of those steps. \n* All output files (VCF and GVCF) will be prefixed by the **Sample ID** of the input BAM file. The **Sample ID** is inferred from the **Sample ID** metadata if available, or from the input BAM filename otherwise. \n* When choosing the PACBIO model, **ALT-aligned pileups** parameter will automatically be set to **diff_channels** value. The user can override this value by setting the value for this parameter manually.\n* The tool should have an optional **Use OpenVino**  parameter. In the Seven Bridges version of **DeepVariant** this parameter is always set to True, in order to decrease execution time.\n\n\n### Common Issues and Important Notes\n\n- The **Reference FASTA** (`--ref`) needs to be accompanied with its indexed file. We recommend using **Samtools Faidx CWL1.0** app to index the reference file.\n- If the **Input BAM/SAM/CRAM** (`--reads`) file is BAM or CRAM type, it needs to be accompanied with its indexed file. We recommend using the **SAMtools Index** app to index the BAM/CRAM file.\n- In the **DeepVariant 1.1.0** app, **Add HP channels** (`--add_hp_channel`) parameter needs to be set to True when running long reads data with the PACBIO model, as explained in [this issue](https://github.com/google/deepvariant/issues/458#issuecomment-844317545).\n- It is recommended to set the **Realign reads** parameter to False (`--norealign_reads`) when running the tool with long reads data.\n- When running **DeepVariant** with phased input BAM files, parameters **Sort by haplotypes** (`--sort_by_haplotypes`) and **Parse auxiliary fields** (`--parse_sam_aux_fields`) need to be set to True as explained in [this issue](https://github.com/google/deepvariant/issues/458#issuecomment-844317545).\n\n### Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **DeepVariant** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type | Input size| Input  file type | Coverage or # of reads | Sequencer | ML Model used | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:--------------:|:----------:|:----------------------:|:----------:|:------------:|:--------:|:------:|:--------------:|\n|     WGS     |  24.5 GB |    BAM   |   10X    |   Illumina | WGS | 4h 47min   | $3.74  | c4.8xlarge (36 CPUs) |\n|     WGS     |  114 GB |    BAM   |  50X    |  Illumina | WGS |  8h 13min  | $5.18  | c4.8xlarge (36 CPUs) |\n|     WGS     |  15.5 GB |   CRAM |    30X      |  Illumina | WGS |  6h 57min   | $5.43  | c4.8xlarge (36 CPUs) |\n|     WES     |  431 MB  |    BAM  |   7X    |   Illumina | WES | 10 min  | $0.11  | c4.8xlarge (36 CPUs) |\n|     WES     |  3.7 GB   |    BAM  |  70X    |   Illumina | WES | 11 min   | $0.14  | c4.8xlarge (36 CPUs) |\n|     WES     |  236 MB   |   CRAM|   23X    |   Illumina | WES | 16 min   | $0.2  | c4.8xlarge (36 CPUs) |\n| SMRT WGS     |  82.3 GB  |   BAM |     |  Sequel | PACBIO | 15h 15min  | $11.11   |  c5.9xlarge (36 CPUs) |\n| SMRT WGS  | 82.3 GB  |   BAM |     |  Sequel | HYBRID_PACBIO_ILLUMINA | 12h 19min | $8.99 |c5.9xlarge (36 CPUs) |\n\n\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [DeepVariant paper](https://www.nature.com/articles/nbt.4235)   \n[2] [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results)   \n[3] [Google AI blog](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html)   \n[4] [IRRI news post](http://news.irri.org/2018/01/international-rice-informatics.html)", "input": [{"name": "Max reads per partition"}, {"name": "Partition size"}, {"name": "Config string"}, {"name": "Realigner diagnostics"}, {"name": "KMP blocktime"}, {"name": "Model"}, {"name": "GVCF GQ binsize"}, {"name": "Input BAM/SAM/CRAM", "encodingFormat": "application/x-sam"}, {"name": "Qual filter"}, {"name": "Exclude regions BED", "encodingFormat": "text/x-bed"}, {"name": "Batch size"}, {"name": "Min count indels"}, {"name": "Downsample fraction"}, {"name": "Min mapping quality"}, {"name": "Use original quality scores"}, {"name": "Training random emit reference sites"}, {"name": "Include debug Info"}, {"name": "Execution Hardware"}, {"name": "Number of mappers"}, {"name": "Confident regions", "encodingFormat": "text/x-bed"}, {"name": "CNN homref call min GQ"}, {"name": "Pileup image height"}, {"name": "Min fraction SNPs"}, {"name": "Min fraction Indels"}, {"name": "Use TPU"}, {"name": "Realign reads"}, {"name": "Multi-allelic qual filter"}, {"name": "Exclude regions list"}, {"name": "Truth variants", "encodingFormat": "application/x-vcf"}, {"name": "Threads"}, {"name": "Model name"}, {"name": "Max batches"}, {"name": "Min count SNPs"}, {"name": "Regions list"}, {"name": "Pileup image width"}, {"name": "Parse auxiliary fields"}, {"name": "Number of readers"}, {"name": "GVCF"}, {"name": "Model"}, {"name": "Use reference for CRAM"}, {"name": "Logging every N candidates"}, {"name": "HTSlib block size"}, {"name": "Sample Name"}, {"name": "Min base quality"}, {"name": "Regions BED", "encodingFormat": "text/x-bed"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Debugging true label mode"}, {"name": "Sort by haplotypes"}, {"name": "Keep pass variants only"}, {"name": "ALT-aligned pileups"}, {"name": "Use OpenVINO"}, {"name": "Add HP channels"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job"}], "output": [{"name": "Output VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Realigner diagnostics"}, {"name": "Output gVCF file"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/google/deepvariant", "https://github.com/google/deepvariant/archive/refs/heads/r1.1.zip", "https://github.com/google/deepvariant/archive/refs/tags/v1.1.0.zip", "https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md"], "applicationSubCategory": ["Genomics", "Variant Calling", "CWL1.0"], "project": "SBG Public Data", "creator": "Google", "softwareVersion": ["v1.2"], "dateModified": 1629391837, "dateCreated": 1629391837, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly2-0-7-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly2-0-7-1/5", "applicationCategory": "CommandLineTool", "name": "Delly2", "description": "Delly is a tool for predicting structural variants, such as deletions, duplications, translocations, and inversions. It integrates short insert paired-ends, long-range mate-pairs, and split-read alignments to accurately delineate genomic rearrangements at single-nucleotide resolution.", "input": [{"name": "Bams", "encodingFormat": "application/x-bam"}, {"name": "Genome"}, {"name": "Type"}, {"name": "Exclude"}, {"name": "Genotype"}, {"name": "Exclude preset"}, {"name": "Min mapping quality"}, {"name": "MAD Cutoff"}, {"name": "Min flanking size"}, {"name": "Flanking"}, {"name": "No InDels"}, {"name": "InDel size"}, {"name": "Genotyping quality"}], "output": [{"name": "SV output", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/tobiasrausch/delly", "https://github.com/tobiasrausch/delly/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049967, "dateCreated": 1453799819, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly2-add-reference-allele-0-7-1/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly2-add-reference-allele-0-7-1/2", "applicationCategory": "CommandLineTool", "name": "Delly2 - Add Reference Allele", "description": "Add Reference Allele is a Delly script that adds reference allele to the resulting VCF file. During the basic execution, Delly does not provide this information on its own.\n\nDelly is a tool for predicting structural variants, such as deletions, duplications, translocations, and inversions. It integrates short insert paired-ends, long-range mate-pairs, and split-read alignments to accurately delineate genomic rearrangements at single-nucleotide resolution.", "input": [{"name": "Variants"}, {"name": "Reference"}], "output": [{"name": "Result", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/tobiasrausch/delly", "https://github.com/tobiasrausch/delly/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049968, "dateCreated": 1453799554, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-call-0-8-7-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-call-0-8-7-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Delly Call", "description": "**Delly Call** is a structural variants caller [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly Call** can be used to call and genotype structural variants [1,2]. The tool takes duplicate-marked, sorted and indexed BAM files (**Input alignments**) and the corresponding reference FASTA file (**Reference sequence**) and outputs a BCF file with structural variants.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the first provided **Input alignments** file.\n\n### Common Issues and Important Notes\n\n* **Input alignments** and **Reference sequence** inputs are required.\n* **Input alignments** should be duplicate-marked, sorted and indexed BAM files, one per sample.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM file (48.4 GB) | 105 min | $0.70 + $0.25 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly Call** was tested with cwltool version 3.1.20210628163208. The `in_alignments` and `in_reference` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly documentation](https://github.com/dellytools/delly)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "SV type"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Regions to exclude"}, {"name": "Output file name prefix"}, {"name": "Minimum paired-end mapping quality"}, {"name": "Minimum paired-end quality for translocation"}, {"name": "Insert size MAD cutoff"}, {"name": "Minimum clipping length"}, {"name": "Minimum PE/SR clique size"}, {"name": "Minimum reference separation"}, {"name": "Maximum read separation"}, {"name": "Input VCF/BCF file for genotyping", "encodingFormat": "application/x-vcf"}, {"name": "Minimum mapping quality for genotyping"}, {"name": "File name for the SV-reads output file"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Structural variants"}, {"name": "Optional SV-reads output file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/delly", "https://github.com/dellytools/delly/tree/master/src", "https://github.com/dellytools/delly/releases/tag/v0.8.7", "https://github.com/dellytools/delly/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1637677118, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-0-7-8-call/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-0-7-8-call/9", "applicationCategory": "CommandLineTool", "name": "Delly - Call", "description": "**Delly Call** is used for structural variation calling, either for germline calling or somatic calling.\n\n\n\n**Delly** is an integrated structural variant prediction method that can discover and genotype deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. It uses paired-ends and split-reads to sensitively and accurately delineate genomic rearrangements throughout the genome. \n\n\n\n### Common Use Cases\n\n* This tool is used for structural variation calling, either for germline calling or somatic calling.\n\n**This tool works in two modes:**\n\n* **Calling**\n\nIn the calling mode, the tool requires normal and tumor BAM files as well as the reference genome. In this mode, **Delly call** tool is not aware of somatic or germline calling. The tool simply discovers abnormally mapped paired-ends and for small InDels, clusters of soft-clips. \n\n\n* **Regenotyping**\n\nIn the regenotyping mode, the tool requires normal and tumor BAM files, reference genome and VCF file obtained from previous structural variation calling. In this mode, the tool will perform genotyping of the variants found in the provided VCF file.\n\n\n\n###Changes introduced by SBG\n\n\n* Conversion of the output BCF file to VCF file is already done internally in the app. The user can use both of these files.\n* **Delly Call** tool  automatically generates **samples.tsv** file that is used for somatic calling. \n\n\n### Common Issues and Important Notes :\n\n* SBG recommends using **Delly 0.7.7 - Call & Filter** tool version which is much faster. \n* **Delly** works well with WGS data. Paired-end mapping that **Delly** uses does not have much power when running on WES data . Small InDels (<500bp) may work using WES data.\n\n\n*  Initial SV files  (BCF, VCF output) contain somatic SVs, germline SVs and false positive SVs that are due to repeats and mis-mappings. Filtering these raw VCF and BCF files is highly recommended.\n\n* Delly primarily parallelizes on sample level. Hence, some tasks may last long.\n\n* The SV size that Delly can call depends on the sharpness of the insert size distribution. For an insert size of 200-300bp with a 20-30bp standard deviation, **Delly** starts to call reliable SVs >=300bp. **Delly** also supports calling of small InDels using soft-clipped reads only. In this mode, the smallest SV size called is 15bp.\n\n* If working with multiple different libraries/BAM files for a single sample, the user should first merge these BAMs using tools such as **Picard** and then tag each library with a unique ReadGroup.\n\n* If Delly is running too slowly, the user should  exclude telomere and centromere regions and also all unplaced contigs. Delly ships with  such an exclude list for human and mouse samples. In addition, input reads can be filtered more stringently using -q 20 and -s 15. \n\n* If the user is interested only in large SVs, they should deactivate small InDel calling (`--noindels`)\n\n.For known limitations and more about this tool, please see the [DELLY github page](https://github.com/dellytools/delly).\n\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*         \n\n**Delly - Call tool: Calling mode** \n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 61                     | 4h 48 min           | 16    | 3.8     | c4.4xlarge |\n| 134                    | 45h 36 min            | 8     | 17.93     | c4.2xlarge |\n\n\n**Delly - Call tool: Regenotyping mode** \n\n\n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 288                     | 1h             | 16    | 0.796     | c4.4xlarge |\n\n### References\n\n[1] [Delly- github](https://github.com/dellytools/delly)\n\n[2] [Delly - paper](https://academic.oup.com/bioinformatics/article/28/18/i333/245403/DELLY-structural-variant-discovery-by-integrated)", "input": [{"name": "Genome fasta file", "encodingFormat": "application/x-fasta"}, {"name": "Exclude file"}, {"name": "Map quality"}, {"name": "Insert size cutoff"}, {"name": "No small InDel calling"}, {"name": "VCF/BCF file for re-genotyping", "encodingFormat": "application/x-vcf"}, {"name": "Input normal files", "encodingFormat": "application/x-bam"}, {"name": "Input tumor files", "encodingFormat": "application/x-bam"}, {"name": "Min. mapping quality for genotyping."}, {"name": "Translocation quality"}], "output": [{"name": "Output raw calls BCF file"}, {"name": "Output raw calls VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Final samples TSV file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/dellytools/delly"], "applicationSubCategory": ["Structural Variant Calling"], "project": "SBG Public Data", "creator": "Tobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049967, "dateCreated": 1539005085, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-0-7-8-call-filter/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-0-7-8-call-filter/9", "applicationCategory": "CommandLineTool", "name": "Delly - Call & Filter", "description": "**Delly Call & Filter tool**  integrates two separate tools, **Delly Call** and **Delly Filter**, into one.\n\n\n\n**Delly** is an integrated structural variant prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. It uses paired-ends and split-reads to sensitively and accurately delineate genomic rearrangements throughout the genome.  [ 2 ]\n\n* **Delly Call tool** is not aware of somatic or germline calling. The tool simply discovers abnormally mapped  paired-ends and for small InDels, clusters of soft-clips. \n\n\n* **Delly filter tool** ships with a basic somatic filtering subcommand that uses the matched control and possibly additional control samples from unrelated individuals.\nThis tool is tumor and germline aware and thus results in a much shorter list of somatic SVs. The expected number of somatic SVs depends, of course ,on the tumor and the tumor re-arrangement patterns, but having only a handful of somatic SVs is not unusual. [ 1 ]\n\n* **Delly Call & Filter tool**  integrates two separate Delly Call and Delly Filter tools into one.\n\n\n### Common Use Cases\n\n\nThis tool is used for structural variation calling, either for germline calling or somatic calling in two cases:\n\n* If a larger panel of normals is not available for somatic calling.\n* If the number of available normals for genotyping is smaller than **20** for germline calling. \n\nIn all other cases, Delly Somatic Structural Variation (SV) calling workflow or Delly Germline Structural Variation (SV) calling workflow should be used.\n\nFor more information about Best practice somatic/germline workflows please visit [DELLY](https://github.com/dellytools/delly).\n\n* The BAM files that Delly requires on the **Controls** and **Tumor files** inputs need to be sorted and indexed. **Delly** also requires a reference genome (**Genome** input) in order to to identify split-reads. \n* The output of the workflow is a BCF format file with a CSI index.  \n\n \n\n###Changes introduced by SBG\n\n\n* Conversion of the output BCF file to VCF file is already done internally in the app. The user can use both of these files.\n\n* **Delly Call & Filter**  automatically generates **samples.tsv** file that is used for somatic calling. \n \n\n### Common Issues and Important Notes : \n* SBG recommends using **Delly 0.7.7 - Call & Filter** tool version which is much faster. \n* **Delly** works well with WGS data. Paired-end mapping that **Delly** uses does not have much power when running on WES data . Small InDels (<500bp) may work using WES data.\n\n\n*  Initial SV files  (BCF, VCF output) contain somatic SVs, germline SVs and false positive SVs that are due to repeats and mis-mappings. Filtering these raw VCF and BCF files is highly recommended.\n\n* Delly primarily parallelizes on the sample level. Hence, some tasks may last long.\n\n* The SV size that Delly can call depends on the sharpness of the insert size distribution. For an insert size of 200-300bp with a 20-30bp standard deviation, **Delly** starts to call reliable SVs >=300bp. **Delly** also supports calling of small InDels using soft-clipped reads only. In this mode, the smallest SV size called is 15bp.\n\n* If working with multiple different libraries/BAM files for a single sample, the user should first merge these BAMs using tools such as **Picard** and then tag each library with a unique ReadGroup.\n\n* If Delly is running too slowly, the user should exclude telomere and centromere regions and also all unplaced contigs. Delly ships with such an exclude list for human and mouse samples. In addition, input reads can be filtered more stringently using -q 20 and -s 15. \n\n* If the user is interested only in large SVs, they should deactivate small InDel calling (`--noindels`)\n\nFor known limitations and more about this tool, please see the [DELLY github page](https://github.com/dellytools/delly).\n\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference index** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*         \n\n**Delly - Call & Filter  tool: Calling mode** \n\n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 61                     | 4h 48 min           | 16    | 3.8    | c4.4xlarge |\n| 134                    | 45h 36 min            | 8     | 17.93     | c4.2xlarge |\n\n**Delly - Call  & Filter tool: Regenotyping mode** \n\n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 288                     | 1h             | 16    | 0.796     | c4.4xlarge |\n\n### References\n\n[1] [Delly- github](https://github.com/dellytools/delly)\n\n[2] [Delly - paper](https://academic.oup.com/bioinformatics/article/28/18/i333/245403/DELLY-structural-variant-discovery-by-integrated)", "input": [{"name": "Genome fasta file", "encodingFormat": "application/x-fasta"}, {"name": "Exclude file"}, {"name": "Map quality"}, {"name": "Insert size cutoff"}, {"name": "No small InDel calling"}, {"name": "VCF/BCF file for re-genotyping", "encodingFormat": "application/x-vcf"}, {"name": "Input normal BAM files", "encodingFormat": "application/x-bam"}, {"name": "Input tumor BAM files", "encodingFormat": "application/x-bam"}, {"name": "Min. mapping quality for genotyping."}, {"name": "Min. fractional ALT support"}, {"name": "Min. SV size"}, {"name": "Max. SV size"}, {"name": "Min. fraction of genotyped samples"}, {"name": "Filter sites for PASS"}, {"name": "Min. coverage in tumor."}, {"name": "Max. fractional ALT support in control"}, {"name": "Number of threads that run delly"}, {"name": "Min. median GQ"}, {"name": "Max. read-depth ratio"}, {"name": "Min. read-depth ratio"}, {"name": "Filter mode"}], "output": [{"name": "Output raw calls BCF file"}, {"name": "Output raw calls VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Final samples TSV file"}, {"name": "Prefiltered or Regenotyped somatic/germline vcf", "encodingFormat": "application/x-vcf"}, {"name": "Prefiltered or Regenotyped somatic/germline bcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/dellytools/delly"], "applicationSubCategory": ["Structural Variant Calling", "Variant Filtration"], "project": "SBG Public Data", "creator": "Tobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049967, "dateCreated": 1539005085, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-classify-0-8-7-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-classify-0-8-7-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Delly Classify", "description": "**Delly Classify** classifies somatic or germline copy-number variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly Classify** can be used to classify copy-number variants [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, the base name of the **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants** input is required.\n* **Samples file for somatic filtering** should be provided for somatic filtering (default **Filter mode**).\n\n### Performance Benchmarking\n\nTypical **Delly Classify** tasks last for a few minutes (<$0.1) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly Classify** was tested with cwltool version 3.1.20210628163208. The `in_variants` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly documentation](https://github.com/dellytools/delly)", "input": [{"name": "Input variants"}, {"name": "Output file name prefix"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Filter mode"}, {"name": "Minimum CNV size"}, {"name": "Maximum CNV size"}, {"name": "Filter sites for PASS"}, {"name": "Samples file for somatic filtering", "encodingFormat": "text/plain"}, {"name": "Probability - germline"}, {"name": "Minimum CN offset"}, {"name": "Baseline ploidy"}, {"name": "Minimum site quality"}, {"name": "Maximum population SD"}], "output": [{"name": "Classified CNVs"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/delly", "https://github.com/dellytools/delly/tree/master/src", "https://github.com/dellytools/delly/releases/tag/v0.8.7", "https://github.com/dellytools/delly/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040305, "dateCreated": 1637677340, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-cnv-0-8-7-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-cnv-0-8-7-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Delly CNV", "description": "**Delly CNV** calls copy-number variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly CNV** can be used to create and segment read depth profiles or call germline or somatic CNVs [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the **Input alignments** file.\n* The R plotting scripts are not included in this wrapper.\n\n### Common Issues and Important Notes\n\n* **Input alignments**, **Reference sequence** and **Mappability map** inputs are required.\n\n### Performance Benchmarking\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48.4 GB) | 77 min | $0.51 + $0.19 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly CNV** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `in_reference` and `in_mappability` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly documentation](https://github.com/dellytools/delly)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Output file name prefix"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Mappability map"}, {"name": "Minimum mapping quality"}, {"name": "Baseline ploidy"}, {"name": "File name for the coverage output file"}, {"name": "Minimum SD read-depth shift"}, {"name": "Minimum CN offset"}, {"name": "Minimum CNV size"}, {"name": "Delly SV file for breakpoint refinement"}, {"name": "Input VCF/BCF file for regenotyping", "encodingFormat": "application/x-vcf"}, {"name": "Segmentation"}, {"name": "Window size"}, {"name": "Window offset"}, {"name": "Input BED file", "encodingFormat": "text/x-bed"}, {"name": "Minimum callable window fraction"}, {"name": "Use mappable bases for window size"}, {"name": "Scanning window size"}, {"name": "Uniqueness filter for scan windows"}, {"name": "Scanning regions", "encodingFormat": "text/x-bed"}, {"name": "MAD cutoff"}, {"name": "Extreme GC fraction to exclude"}, {"name": "No scan window selection"}], "output": [{"name": "Delly CNV calls"}, {"name": "Optional coverage output file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/delly", "https://github.com/dellytools/delly/tree/master/src", "https://github.com/dellytools/delly/releases/tag/v0.8.7", "https://github.com/dellytools/delly/blob/master/README.md"], "applicationSubCategory": ["Copy Number Analysis", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040305, "dateCreated": 1637677083, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-filter-0-8-7-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-filter-0-8-7-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Delly Filter", "description": "**Delly Filter** filters structural variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly Filter** can be used to filter germline or somatic structural variants [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants** input is required.\n* **Samples file** input is required in the (default) `somatic` **Filter mode**.\n\n### Performance Benchmarking\n\nTypical **Delly Filter** tasks last a few minutes (<$0.1) on an on-demand c4.2xlarge AWS instance.\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly Filter** was tested with cwltool version 3.1.20210628163208. The `in_variants` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly documentation](https://github.com/dellytools/delly)", "input": [{"name": "Input variants"}, {"name": "Output file name prefix"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Filter mode"}, {"name": "Minimum fractional ALT support"}, {"name": "Minimum SV size"}, {"name": "Maximum SV size"}, {"name": "Minimum fraction of genotyped samples"}, {"name": "Filter sites for PASS"}, {"name": "Samples file"}, {"name": "Minimum coverage in tumor"}, {"name": "Maximum fractional ALT support in control"}, {"name": "Minimum median GQ"}, {"name": "Maximum read depth ratio for deletions"}, {"name": "Minimum read depth ratio for duplications"}], "output": [{"name": "Filtered structural variants"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/delly", "https://github.com/dellytools/delly/tree/master/src", "https://github.com/dellytools/delly/releases/tag/v0.8.7", "https://github.com/dellytools/delly/blob/master/README.md"], "applicationSubCategory": ["CWLtool Tested", "Structural Variant Calling", "Variant Filtration"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1637677372, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-0-7-8-filter/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-0-7-8-filter/6", "applicationCategory": "CommandLineTool", "name": "Delly - Filter", "description": "**Delly Filter** is a tool used for filtering **somatic** or **germline** structural variants. \n\n###Delly - Filter tool\n\n**Delly** is an integrated structural variant prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. It uses paired-ends and split-reads to sensitively and accurately delineate genomic rearrangements throughout the genome.  [ 2 ]\n\n**Delly filter tool** ships with a basic somatic filtering subcommand that uses the matched control and possibly additional control samples from unrelated individuals.\nThis tool is tumor and germline aware and thus results in a much shorter list of somatic SVs. The expected number of somatic SVs depends, of course, on the tumor and the tumor re-arrangement patterns, but having only a handful of somatic SVs is not unusual. [ 1 ]\n\n\n### Common Use Cases\n\n**Delly Filter** tool is used for filtering **somatic** or **germline** structural variants.\n\n###Changes introduced by SBG\n\n* Conversion of the output BCF file to VCF file is already done internally in the app. The user can use both of these files.\n\nFor known limitations and more about this tool, please see the [DELLY github page](https://github.com/dellytools/delly).\n\n\n### Performance Benchmarking\n\nThe execution time for standard size VCF file takes several minutes on the default instance. Unless specified otherwise, the default instance used to run the **Delly Filter** tool will be c4.2xlarge (AWS).", "input": [{"name": "Filter mode"}, {"name": "Input BCF file"}, {"name": "Min. fractional ALT support"}, {"name": "Min. SV size"}, {"name": "Max. SV size"}, {"name": "Min. fraction of genotyped samples"}, {"name": "Filter sites for PASS"}, {"name": "Min. coverage in tumor"}, {"name": "Max. fractional ALT support in control"}, {"name": "Min. median GQ for carriers and non-carriers."}, {"name": "Max. read-depth ratio of carrier vs. non-carrier for a deletion"}, {"name": "Min. read-depth ratio of carrier vs. non-carrier for a duplication."}, {"name": "Samples tsv file"}], "output": [{"name": "Output filtered bcf file"}, {"name": "Output filtered vcf file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/dellytools/delly"], "applicationSubCategory": ["Structural Variant Calling", "Variant Filtration"], "project": "SBG Public Data", "creator": "Tobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049967, "dateCreated": 1539005084, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-lr-0-8-7-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-lr-0-8-7-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Delly LR", "description": "**Delly LR** is a structural variants caller for long reads data [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly LR** can be used to call and genotype structural variants from long reads data (ONT or PacBio) [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the first provided **Input long reads alignments** file.\n\n### Common Issues and Important Notes\n\n* **Input long reads alignments** and **Reference sequence** inputs are required.\n\n### Performance Benchmarking\n\n**Delly LR** performance depends on the size of the provided inputs.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| PacBio BAM (12.7 GB) | 3 h 12 min | $1.28 + $0.45 | c4.2xlarge - 1024 GB EBS | \n| ONT BAM (261.8 GB) | 12 h 30 min | $4.98 + $1.78 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly LR** was tested with cwltool version 3.1.20210628163208. The `in_alignments` and `in_reference` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly documentation](https://github.com/dellytools/delly)", "input": [{"name": "Input long reads alignments", "encodingFormat": "application/x-bam"}, {"name": "Output file name prefix"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "SV type to compute"}, {"name": "Sequencing technology"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Regions to exclude"}, {"name": "Minimum mapping quality"}, {"name": "Minimum clipping length"}, {"name": "Minimum clique size"}, {"name": "Minimum reference separation"}, {"name": "Maximum read separation"}, {"name": "Maximum reads for consensus computation"}, {"name": "Minimum flank size"}, {"name": "Minimum flank quality"}, {"name": "Minimum mapping quality for genotyping"}, {"name": "File name for the SV-reads output"}], "output": [{"name": "Structural variants"}, {"name": "Optional SV-reads output file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/delly", "https://github.com/dellytools/delly/tree/master/src", "https://github.com/dellytools/delly/releases/tag/v0.8.7", "https://github.com/dellytools/delly/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "Long Reads", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040305, "dateCreated": 1637677172, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-merge-0-8-7-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-merge-0-8-7-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Delly Merge", "description": "**Delly Merge** merges structural variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly Merge** can be used to merge structural variants from **Input variants** BCF files [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the first provided **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants**  input is required.\n\n### Performance Benchmarking\n\nTypical **Delly Merge** tasks last 2-3 minutes (<$0.1) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly Merge** was tested with cwltool version 3.1.20210628163208. The `in_variants` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly documentation](https://github.com/dellytools/delly)", "input": [{"name": "Input variants"}, {"name": "Output file name prefix"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Maximum chunk size for merging"}, {"name": "Minimum fractional ALT support"}, {"name": "Minimum coverage"}, {"name": "Minimum SV size"}, {"name": "Maximum SV size"}, {"name": "Merge Delly CNV files"}, {"name": "Filter sites for PRECISE"}, {"name": "Filter sites for PASS"}, {"name": "Maximum breakpoint offset"}, {"name": "Minimum reciprocal overlap"}], "output": [{"name": "Merged structural variants"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/delly", "https://github.com/dellytools/delly/tree/master/src", "https://github.com/dellytools/delly/releases/tag/v0.8.7", "https://github.com/dellytools/delly/blob/master/README.md"], "applicationSubCategory": ["VCF Processing", "Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040305, "dateCreated": 1637677420, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-0-7-8-merge/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-0-7-8-merge/7", "applicationCategory": "CommandLineTool", "name": "Delly - Merge", "description": "**Delly Merge**  is a tool used for merging structural variants across BCF files and within a single BCF file. \n \n\n**Delly** is an integrated structural variant prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. It uses paired-ends and split-reads to sensitively and accurately delineate genomic rearrangements throughout the genome.  [ 1 ]\n\n\n\n\n### Common Use Cases\n\n**Delly Merge**  tool is used for merging both **somatic** or **germline** BCF files.\n\n\n###Changes introduced by SBG\n\n* Conversion of the output BCF file to VCF file is already done internally in the app. The user can use both of these files.\n\nFor known limitations and more about this tool, please see the [DELLY github page](https://github.com/dellytools/delly).\n\n\n### Performance Benchmarking\n\nThe execution time for standard size VCF file takes several minutes on the default instance. Unless specified otherwise, the default instance used to run the **Delly Merge** tool will be c4.2xlarge (AWS).\n\n\n### References\n\n[1] [Delly- github](https://github.com/dellytools/delly)\n\n[2] [Delly - paper](https://academic.oup.com/bioinformatics/article/28/18/i333/245403/DELLY-structural-variant-discovery-by-integrated)", "input": [{"name": "Min. SV size"}, {"name": "Max. SV size"}, {"name": "Filter PRECISE"}, {"name": "Filter PASS"}, {"name": "Max. breakpoint offset"}, {"name": "Min reciprocal overlap"}, {"name": "Input BCF files"}], "output": [{"name": "Output merged bcf file"}, {"name": "Output VCF with merged variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/dellytools/delly"], "applicationSubCategory": ["Structural Variant Calling", "VCF Processing"], "project": "SBG Public Data", "creator": "Tobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648049967, "dateCreated": 1539005085, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/delly-sansa-annotate-0-8-7-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/delly-sansa-annotate-0-8-7-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Delly Sansa Annotate", "description": "**Delly Sansa Annotate** annotates structural variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Delly Sansa Annotate** can be used to annotate structural variants (**Input variants**) by using data from an SV database (**Database VCF/BCF file**) or a file with gene/feature annotations (**GTF/GFF3/BED annotations file**) [1,2]. \n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, the base name of the  **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants** input is required.\n* Either a **Database VCF/BCF file** or a **GTF/GFF3/BED annotations file** should be provided.\n\n### Performance Benchmarking\n\nTypical **Delly Sansa Annotate** tasks take a few minutes (<$.0.1) on an on-demand c4.2xlarge AWS instance.\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Delly Sansa Annotate** was tested with cwltool version 3.1.20210628163208. The `in_variants` and `in_db` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Delly publication](https://doi.org/10.1093/bioinformatics/bts378)\n\n[2] [Delly Sansa documentation](https://github.com/dellytools/sansa)", "input": [{"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Output annotation file name prefix"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output query SVs file name prefix"}, {"name": "Database VCF/BCF file", "encodingFormat": "application/x-vcf"}, {"name": "Maximum breakpoint offset"}, {"name": "Minimum reciprocal overlap"}, {"name": "Matching strategy"}, {"name": "Do not require matching SV types"}, {"name": "Report SVs without match in database"}, {"name": "GTF/GFF3/BED annotations file", "encodingFormat": "application/x-gtf"}, {"name": "GTF/GFF3 attribute"}, {"name": "GTF/GFF3 feature"}, {"name": "Maximum distance"}, {"name": "Report contained genes"}], "output": [{"name": "Annotated structural variants"}, {"name": "Query SVs"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dellytools/sansa", "https://github.com/dellytools/sansa/tree/main/src", "https://github.com/dellytools/sansa/releases/tag/v0.0.8", "https://github.com/dellytools/sansa/blob/main/README.md"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Tobias Rausch", "softwareVersion": ["v1.2"], "dateModified": 1648040305, "dateCreated": 1637677458, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/denovogear-0-5-4/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/denovogear-0-5-4/5", "applicationCategory": "CommandLineTool", "name": "DeNovoGear", "description": "DeNovoGear is a tool to detect de novo mutations using sequencing data of related individuals. DeNovoGear calls de novo SNP's and INDEL's in trios. DeNovoGear can also be used to phase de novo mutations using genotype information at neighboring SNP sites. In addition, DeNovoGear can also be used to analyze a pair of samples and call SNP's and INDEL's present in one sample but not the other.\nTool can be run on multichromosome samples, but it is necessary do analyse X chromosome separately. Parameter \"analysis_type\" is used to make a distinction between autosomal chromosomes and X chromosome in the male and female offspring. PED file defines the family relations. Tool implementation allows usage in regular and parallel mode. To use it in parallel mode it is necessary to create workflow where the BCF files will be produced using SAMtools Mpileup parallel workflow in parallel mode. Next is is required to scatter the \"bcf_file\" port of the DeNovoGear node and set the parameter \"parallel\" to the value \"parallel\". Also if you want to use the parallel mode, PED file has to be generated by the SBG PED File Creator tool, or a provided PED file has to have same structure.", "input": [{"name": "Pedigree file"}, {"name": "BCF file from SAMtools mpileup"}, {"name": "Mutation rate prior for SNPs"}, {"name": "Mutation rate prior for INDELs"}, {"name": "Mutation rate prior for paired sample analysis"}, {"name": "Scaling factor for INDEL mutation rate"}, {"name": "Posterior probability threshold"}, {"name": "Read depth filter"}, {"name": "Execute DeNovoGear with per chromosome or integral input"}, {"name": "Analysis type"}], "output": [{"name": "De novo VCF", "encodingFormat": "application/x-vcf"}, {"name": "Command Line"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/denovogear/denovogear", "https://github.com/denovogear/denovogear/blob/develop/README.md"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Don Conrad, Avinash Ramu, Kael Dai, and Reed A. Cartwright.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649345081, "dateCreated": 1459789092, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/deseq2-1-26-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/deseq2-1-26-0/8", "applicationCategory": "CommandLineTool", "name": "DESeq2", "description": "**DESeq2** performs differential gene expression analysis by use of negative binomial generalized linear models. It analyzes estimated read counts from several samples, each belonging to one of two or more conditions under study, searching for systematic changes between conditions, as compared to within-condition variability. \n\nThe Bioconductor/R package **DESeq2** provides a set of functions for importing data, performing exploratory analysis and finally testing for differential expression. This CWL tool is a wrapper around the script based on the standard workflow for this type of analysis [1].\n\n**DESeq2** offers two kinds of hypothesis tests: the Wald test, where we use the estimated standard error of a log2 fold change to test if it is equal to zero, and the likelihood ratio test (LRT). The LRT examines two models for the counts, a full model with a certain number of terms and a reduced model, in which some of the terms of the full model are removed. The test determines if the increased likelihood of the data using the extra terms in the full model is more than expected if those extra terms are truly zero. The LRT is therefore useful for testing multiple terms at once, for example testing 3 or more levels of a factor at once, or all interactions between two variables [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nAs input files, please use one of the following: \n\n- **HTSeq**, **RSEM** or **StringTie** gene level abundance estimates;\n- **Salmon**, **Sailfish** or **Kallisto** transcript level abundance estimates.\n\nIf the abundance estimates provided are on a transcript-level, **tximport** will be used to summarize them for gene-level analysis. **Gene annotation** (in GTF format) needs to be supplied then.\n\nTo fit a generalized linear model for each gene, besides gene abundance estimates - some phenotype information is needed. In the simplest case, only a single independant variable is used to explain the expression levels. Experiments with more than one variable influencing the counts can be analyzed using design formula that includes the additional covariates. \n\nThere are two options for providing phenotype information:\n\n1. By indicating API keys for metadata fields that need to be included in the design. Phenotype information will then consist of variables you listed as **Covariate of interest** and **Control variables**.\n2. By including a CSV file (**Phenotype data** input) that contains a row for each sample, with Sample ID in the first column. These Sample IDs need to match those in input files metadata. Also, a single line header with variable names should be included.\n\nExample CSV content below:\n\n```\nsample_id,library,sex,condition\ntreated1,paired-end,male,treated\ntreated2,single-end,male,treated\ntreated3,paired-end,female,treated\nuntreated1,single-end,male,untreated\nuntreated2,paired-end,female,untreated\nuntreated3,paired-end,female,untreated\nuntreated4,paired-end,male,untreated\n```\n\nSupplying a CSV like this while entering \"condition\" for the value of the **Covariate of interest** parameter and \"library\" in **Control variables** will test for differential expression between treated and untreated samples, while controlling for effect of library preparation.\n\nThe information about sample belonging to the treated or the untreated group can also be kept in the metadata. To use a metadata field for splitting the samples into groups for testing, enter its metadata key for the **Covariate of interest** parameter. All the input files need to have this metadata field populated. To control for possible confounders, enter their API keys as **Control variables**.\n\n### Changes Introduced by Seven Bridges\n\nAlthough the script covers different use cases and gives the user some flexibility to tailor the analysis to his own needs, not everything is customizable.\n\nThe user does not choose the type of test that will be performed. The appropriate test is chosen automatically:\n\n- if there are more than two values (levels) to a chosen **Covariate of interest** - LRT is used. \n- if the **Covariate of interest** has only two different values - Wald test is used to test for differential expression.\n\nThe analysis report contains the list of input parameters, phenotype data table, a heatmap of input samples with cluster dendrogram, dispersion estimates plot and an MA plot showing the log2 fold changes attributable to a given gene over the mean of normalized counts and a short summary of results.\n\n### Common Issues and Important Notes\n\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and it's field be populated in all the samples (**Expression data**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end please refer to the table on [this link](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). To learn how to add custom metadata field to expression data files refer to the [following document](https://docs.sevenbridges.com/docs/format-of-a-manifest-file) (section: _Modifying metadata via the visual interface_).\n- Be careful when choosing covariates - generalized linear model fitting will fail if model matrix is not full rank!\n- If your task fails with \"none of the transcripts in the quantification files are present in the first column of tx2gene\" message in the error log, and you are certain that you are using the proper GTF file - you can try rerunning the task with **ignoreTxVersion** option selected. This can happen if you, for example, download the transcriptome FASTA from the Ensembl website and use it to build aligner index - transcript version will then be included in transcript ID in the quantification output file, while in the GTF it's kept as a separate attribute so the transcript IDs will not match.\n\n### Performance Benchmarking\n\nThe execution time for performing differential expression analysis on 6 samples (3 in each group), using transcripts from GENCODE Release 27 (GRCh38.p10) takes 5-6 minutes on the default instance; the price is negligible (~ 0.01$). Unless specified otherwise, the default instance used to run the __DESeq2__ tool will be c4.large (AWS) with 256GB storage.\n\n### References\n\n[1] [RNA-seq workflow: gene-level exploratory analysis and differential expression](https://www.bioconductor.org/help/workflows/rnaseqGene/)\n\n[2] [DESeq2 vignette](https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)", "input": [{"name": "FDR cutoff"}, {"name": "Fit type"}, {"name": "Gene annotation", "encodingFormat": "application/x-gtf"}, {"name": "Analysis title"}, {"name": "Covariate of interest"}, {"name": "Turn off the independent filtering"}, {"name": "log2 fold change shrinkage"}, {"name": "Expression data", "encodingFormat": "text/plain"}, {"name": "Phenotype data"}, {"name": "Quantification tool"}, {"name": "Control variables"}, {"name": "Pre-filtering threhold"}, {"name": "ignoreTxVersion"}, {"name": "Grouping factor for collapsing technical replicates"}, {"name": "Factor level - test"}, {"name": "Factor level - reference"}], "output": [{"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Normalized counts", "encodingFormat": "text/plain"}, {"name": "DESeq2 analysis results."}, {"name": "RData file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/mikelove/DESeq2"], "applicationSubCategory": ["Differential Expression", "RNA-Seq"], "project": "SBG Public Data", "creator": "Michael Love (HSPH Boston), Simon Anders, Wolfgang Huber (EMBL Heidelberg)", "softwareVersion": ["v1.1"], "dateModified": 1648033583, "dateCreated": 1584385077, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/dexseq-1-36-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/dexseq-1-36-0/4", "applicationCategory": "CommandLineTool", "name": "DEXSeq", "description": "The Bioconductor package DEXSeq tests for **differential exon usage** in comparative RNA-Seq experiments.\n\nDifferential exon usage (DEU) refers to changes in the relative usage of exons caused by the experimental condition. In the case of an inner exon, a change in relative exon usage is typically due to a change in the rate with which this exon is spliced into transcripts (alternative splicing). Note, however, that DEU is a more general concept than alternative splicing, since it also includes changes in the usage of alternative transcript start sites and polyadenylation sites, which can cause differential usage of exons at the 5\u2019 and 3\u2019 boundary of transcripts.\n\nThe DEXSeq method uses a generalized linear model to model the differential usage of exons in different sample groups. It assumes that the read counts in the exons follow a negative binomial distribution and controls for false discovery rate (FDR) by estimating the biological variability for each exon. For more details please check the original publication [1] or the DEXSeq vignette [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\t\n\t\t\t\n### Common Use Cases\n\nTo be able to run an analysis, the tool needs to be provided with:\n- **Flattened annotation file.** To convert a GTF file with gene models into a GFF file with collapsed gene models (flattened annotation file) the **DEXSeq prepare annotation** tool can be used.\n- **Count Files.** To get count files, the **DEXSeq count** tool can be used. This tool will count (for each SAM/BAM file) the number of reads that overlap with each of the exon counting bins defined in the flattened GFF file.\n- **Phenotype information.**\n\nThere are two options for providing phenotype information:\n\n**a)** By indicating API keys for metadata fields that need to be included in the design. For example, if the effect of **sample_type** is being analysed (for example: treated, untreated) on DEU controlling for **sex** (male, female) **sample_id**, **sample_type** and **sex** metadata fields need to be populated for all count files, and **Covariate of interest** and **Control variables** need to be set to **sample_type** and **sex** respectively. \n\n**b)** By including a CSV file (**Sample Table** input) - see the example below. Sample table must contain the **sample_id** column with values that match values from the metadata **sample_id** field of the count files. To run an analysis, parameters **Covariate of interest** and **Control variables** (optional) need to be set to the adequate column names. For example, entering **sample_type** for the value of the **Covariate of interest** parameter and **library** in **Control variables** will test for differential exon usage between treated and untreated samples, while controlling for effect of library preparation.\n\nExample CSV content:\n\n```\nsample_id,sample_type,library,sex\ntreated1,treated,paired-end,male\ntreated2,treated,single-end,male\ntreated3,treated,paired-end,female\nuntreated1,untreated,single-end,male\nuntreated2,untreated,paired-end,female\nuntreated3,untreated,paired-end,female\nuntreated4,untreated,paired-end,male\n\n```\n\nBy using the **Denominator** parameter it is possible to set a value of the sample annotation (e.g. condition) to use as a denominator in the log2 fold change. For example, if **Covariate of interest** consists of values: `treated` and `untreated`, and we want to use `untreated` as the control group, the **Denominator** parameter should be set to `untreated`. If the parameter is not specified, the function will take the annotation of the first sample. \n\nAs a result of an analysis, the tool will output:\n- CSV results table containing for each exon / exon part (featureID): gene id (groupID), seqnames (chromosomes), start, end, strand, exonBaseMean (mean of the counts across samples in each feature/exon), dispersion, stat, p-value, adjusted p-value, exon usage coefficients, log2fold_change for the tested groups and list of transcripts that contain that exon.\n- CSV results table with the per-gene adjusted p-values (a per-gene adjusted p-value is computed using the `perGeneQValue` function, which aggregates evidence from multiple tests within a gene to a single p-value for the gene and then corrects for multiple testing across genes).\n- HTML report - an overview of all the significant results with the links to the plots. NOTE: To be able to use the HTML report please download it as well as HTML report directory, unzip the directory, and be sure that both the report and the report directory are located in the same directory. The **FDR cutoff** parameter is by default set to **0.1** but it can be changed. Output HTML report can be omitted by setting the **HTML report** parameter to **FALSE**.\n- Optionally, by setting the **Output result R object** parameter to **TRUE**, result R object will be outputted, so DEXSeq functions for visualisation and further exploration of the data and results can be used locally. \n\nTesting for DEU in multiple conditions:\n\nDEXSeq tests for differences in exon usage between different conditions, meaning that if there are more then two different conditions, DEXSeq will still output one p-value for each exon, where the null model would be \"the different conditions do not have an effect on exon usage\". Thus, significant p-values indicate that the null model is rejected: at least one of the conditions has an effect on exon usage, such that the exon usage is different with respect to the other conditions. The fold changes in DEXSeq result table are result of comparing all the conditions with the one specified by the **Denominator** parameter. So, for three conditions, two fold change columns in the result table can be expected.\n\n\n### Common Issues and Important Notes\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and to be populated in all of the samples (**Count Files**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. Keep in mind that metadata keys are usually different from what is seen on the front-end. To match metadata keys to their corresponding values on the front-end, please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). Learn how to [add a custom metadata field](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#modifying-metadata-via-the-visual-interface) to expression data files.\n- **sample_id** metadata key must be populated with unique values for all the samples whether CSV is provided or not. \n- DEXSeq analysis for a large number of samples can take extremely long (please check the Performance Benchmarking table below). In that case (but also in general), filtering out genes / exonic regions with low levels of expression (thus reducing the number of tests) should help speed up the analysis.\n\n### Performance Benchmarking\n\nRuntime and task cost for different number of input files and two AWS instance types when using all the default options (by default, the tool will use a c5.9xlarge instance).\n\n| Input size |  # of input files |  Duration | Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|      12.8 MB     |     8   | 15 min   |    $0.43    |    c5.9xlarge     |\n|      12.8 MB     |     8   | 12 min   |    $0.67    |    c5.18xlarge     |\n|      12.8 MB     |    16   |   55 min   |    $1.54    |    c5.9xlarge     |\n|      12.8 MB     |    16   |  34 min   |   $1.86    |    c5.18xlarge     |\n|      12.8 MB     |    30   |   1 h 50 min   |   $3.06    |    c5.9xlarge     |   \n|      12.8 MB     |    30   |  1 h 06 min   |   $3.56    |    c5.18xlarge     |\n\nRuntime and task cost for different number of input files and two AWS instance types when the **HTML report** parameter is set to **FALSE**.\n\n| Input size |  # of input files |  Duration | Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|      12.8 MB     |     8   | 11 min   |    $0.29    |    c5.9xlarge     |\n|      12.8 MB     |     8   | 9 min   |    $0.49    |    c5.18xlarge     |\n|      12.8 MB     |    16   |   29 min   |    $0.81    |    c5.9xlarge     |\n|      12.8 MB     |    16   |  18 min   |   $1.00    |    c5.18xlarge     |\n|      12.8 MB     |    30   |  1 h 06 min   |   $1.85    |    c5.9xlarge     |   \n|      12.8 MB     |    30   |  39 min   |   $2.07    |    c5.18xlarge     |\n|      12.8 MB     |    64   |  8 h 48 min   |   $14.73    |    c5.9xlarge     |  \n|      12.8 MB     |    64   |   3 h 24 min   |   $10.88    |    c5.18xlarge     |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [DEXSeq publication](https://genome.cshlp.org/content/22/10/2008.full)\n[2] [DEXSeq vignette](https://bioconductor.org/packages/3.12/bioc/vignettes/DEXSeq/inst/doc/DEXSeq.html)", "input": [{"name": "Count Files", "encodingFormat": "text/plain"}, {"name": "Flattened annotation file"}, {"name": "Sample Table"}, {"name": "Covariate of interest"}, {"name": "Control variables"}, {"name": "Output name prefix"}, {"name": "Output result R object"}, {"name": "Denominator"}, {"name": "HTML report"}, {"name": "FDR cutoff"}], "output": [{"name": "DEXSeq result R object"}, {"name": "DEXSeq Results"}, {"name": "HTML report", "encodingFormat": "text/html"}, {"name": "HTML report directory", "encodingFormat": "application/zip"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/areyesq89/DEXSeq"], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1619097901, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/dexseq-count-1-36-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/dexseq-count-1-36-0/3", "applicationCategory": "CommandLineTool", "name": "DEXSeq count", "description": "This tool counts the number of reads that overlap with each exon counting bin defined in the flattened GFF file.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nThis tool expects two files, namely the GFF file produced by the **DEXSeq prepare annotation** tool and a BAM/SAM file with the aligned reads from a sample. The tool generates an output file with one line for each exon counting bin defined in the flattened GFF file. The lines contain the exon counting bin IDs (which are composed of gene IDs and exon bin numbers), followed by an integer number which indicates the number of reads that were aligned in such a way that they overlap with the counting bin. Note that a read that overlaps with several counting bins of the same gene is counted for each of these. \n\n**NOTE:** Bam/SAM files must be sorted by coordinate (position) or by name. If you are using STAR as the aligner, by setting the **Output sorting type** option to **SortedByCoordinate**, you will get an aligned and sorted BAM, which is what **DEXSeq count** tool needs as an input.   \nUse the tool multiple times to produce a count file from each of your BAM/SAM files.\n\n**Optional parameters:**\n\n- **Paired-end data** (`yes` / `no`) If your data is from a paired-end sequencing run the option should be set to `yes`. For single-end data  it should be set to `no`. \n- **BAM/SAM Sort Type** (`pos` / `name`) Data must be sorted by alignment position (`pos`) or by read name (`name`). It's `pos` by default. \n- **Strandedness** (`yes` / `no` / `reverse`) In case of paired-end data, `yes` means strand-specific data, i.e., reads are aligned to the same strand as the gene they originate from, `reverse` specifies the opposite case and `no` specifies unstranded data. It's `no` by default. \n- **Minimal alignment quality** All reads with a lower quality than specified (default 10) are skipped.\n\n### Changes Introduced by Seven Bridges\n\n- The tool will automatically recognise the type of input (SAM or BAM).\n\n### Common Issues and Important Notes\n\n- If you are using classic alignment methods (i.e. not pseudo-alignment approaches) it is important to align them to the genome, not to the transcriptome, and to use a splice-aware aligner (i.e., a short-read alignment tool that can deal with reads that span across introns) such as STAR or TopHat2 [2].\n- Input Bam/SAM files must be sorted by coordinate (position) or sorted by name.\n\n### Performance Benchmarking\n\nRuntime and task cost for different sizes of BAM files using default options. Flattened GFF file was created from a 1.1 GB GTF with the aggregation option set to yes (default). \n\n| Input size |  Duration | Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:------------:|\n|      402 MB     |    8 min   |     $0.08    |    c4.2xlarge     |\n|      1.6 GB     |    27 min   |     $0.24    |    c4.2xlarge     |\n|      10.6 GB     |    2 h 40 min   |     $1.43    |    c4.2xlarge     |   \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [DEXSeq paper](https://genome.cshlp.org/content/22/10/2008.full)\n[2] [DEXSeq vignette](https://bioconductor.org/packages/3.12/bioc/vignettes/DEXSeq/inst/doc/DEXSeq.html#3_counting_reads)", "input": [{"name": "Flattened GFF file"}, {"name": "Aligned sorted BAM or SAM", "encodingFormat": "application/x-bam"}, {"name": "Minimal alignment quality"}, {"name": "BAM/SAM Sort Type"}, {"name": "Strandedness"}, {"name": "Paired-end data"}], "output": [{"name": "Counts table", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/areyesq89/DEXSeq"], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1619097901, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/dexseq-dtu-1-36-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/dexseq-dtu-1-36-0/4", "applicationCategory": "CommandLineTool", "name": "DEXSeq-DTU", "description": "DEXSeq-DTU is a modified version of DEXSeq that performs differential transcript usage (DTU) analyses. \n\nThe DEXSeq package was originally designed for detecting differential exon usage, but can also be adapted to run on estimated transcript counts, in order to detect DTU. For more information about DEXSeq-DTU implementation and it's statistical framework please consult Love at al. 2018 [1] and the original DEXSeq publication [2], respectively.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nTo be able to run an analysis, the tool needs to be provided with:\n- **Counts.** Transcript-level estimated read counts. Depending on the software used for quantification (salmon, kallisto, rsem), the **Input type** parameter should be specified adequately. \n- **Gene annotation file.** GTF file in uncompressed (.gtf) or compressed (.gtf.gz) form.\n- **Phenotype information.**\n\nThere are two options for providing phenotype information:\n\n**a)** By indicating API keys for metadata fields that need to be included in the design. For example, if the effect of **sample_type** is being analysed (for example: treated, untreated) on DTU controlling for **sex** (male, female) **sample_id**, **sample_type** and **sex** metadata fields need to be populated for all count files, and **Covariate of interest** and **Control variables** need to be set to **sample_type** and **sex** respectively. \n\n**b)** By including a CSV file (**Phenotype data** input) - see the example below. Phenotype data must contain the **sample_id** column with values that match values from the metadata **sample_id** field of the count files. To run an analysis parameters **Covariate of interest** and **Control variables** (optional) need to be set to the adequate column names. For example, entering **sample_type** for the value of the **Covariate of interest** parameter and **library** in **Control variables** will test for differential transcript usage between treated and untreated samples, while controlling for effect of library preparation.\n\nExample CSV content below:\n\n```\nsample_id,sample_type,library,sex\ntreated1,treated,paired-end,male\ntreated2,treated,single-end,male\ntreated3,treated,paired-end,female\nuntreated1,untreated,single-end,male\nuntreated2,untreated,paired-end,female\nuntreated3,untreated,paired-end,female\nuntreated4,untreated,paired-end,male\n\n```\n\n- By setting the **Scaling type** parameter it can be chosen whether to use estimated counts scaled up to the library size (**scaledTPM**), scaled using the average transcript length over the samples and then the library size (**lengthScaledTPM**),  scaled using the median transcript length among isoforms of a gene, and then the library size (**dtuScaledTPM**) or to use the original estimated counts without scaling (**no**). By default, the **Scaling type** parameter is set to **dtuScaledTPM** - scaling designed for DTU analysis. \n\n- Filtering data, i.e removing transcripts that have low expression levels or are not expressed at all has positive effects on the performance and also speeds up the analysis. There are six parameters that control filtering. By default, all the filtering parameters are set to zero, meaning that transcripts with zero expression in all samples will be removed as well as genes with only one non-zero transcript. By setting the **Suggested filtering** parameter to **TRUE** you can enable filtering suggested by Love at al. 2018 [1] and the parameters will automatically be set as follows:\n**Min samples gene expression** = the total number of samples,\n**Min samples gene expression** and **Min samples feature proportion** = the sample size of the smallest group,\n**Min gene expression** = 10, **Min feature expression** = 10, **Min feature proportion** = 0.1. It is also possible to use **Suggested filtering** as a starting point and then further adjust some of the filtering parameters.\n\n- By using the **Denominator** parameter it is possible to set a value of the sample annotation (e.g. condition) to use as a denominator in the log2 fold change. For example, if **Covariate of interest** consists of values: treated and untreated, and we want to use untreated as the control group, the **Denominator** parameter should be set to untreated. If the parameter is not specified, the function will take the annotation of the first sample.\n\n- After getting the adjusted p-values on the transcript level, a per-gene adjusted p-value is computed using the `perGeneQValue` function, which aggregates evidence from multiple tests within a gene to a single p-value for the gene and then corrects for multiple testing across genes. Additionally, to extract genes that are differentially spliced and also exhibit enough evidence to identify one or more transcripts that are participating in the DTU, a two-stage test is performed. For the two-stage test, the **Overall False Discovery Rate** parameter is set to **0.05** by default but it can be changed.\n\nAs a result of an analysis, the tool will output: \n- Results table containing for each transcript: gene id, mean of the counts across samples in each transcript (transcriptBaseMean), stat, p-value, adjusted p-value, transcript usage coefficients and log2fold_change for the tested groups. \n- Two-stage test results table with the per-gene and per-transcript adjusted p-values.\n- Optionally, by setting the **Out DEXSeq-DTU result R object** parameter to **TRUE**, the result R object will be outputted, so DEXSeq functions for visualisation and further exploration of the data and results can be used locally.\n\nTesting for DTU in multiple conditions: DEXSeq-DTU tests for differences in transcript usage between different conditions, meaning that if there are more than two different conditions, DEXSeq-DTU will still output one p-value for each transcript, where the null model would be \"the different conditions do not have an effect on transcript usage\". Thus, significant p-values indicate that the null model is rejected: at least one of the conditions has an effect on transcript usage, such that the transcript usage is different with respect to the other conditions. The fold changes in DEXSeq-DTU result table are the result of comparing all the conditions with the one specified by the **Denominator** parameter. So, for three conditions, two fold change columns in the result table can be expected.\n\n\n### Changes Introduced by Seven Bridges\n- R/Bioconductor package **tximport** is used for importing count files, allowing the tool to work with direct outputs of quantifiers like salmon, kallisto and rsem. This package also allows choosing the scaling method (**Scaling type** parameter).\n- R/Bioconductor package **GenomicFeatures** is used for creating a tx2gene tab from the input GTF file.\n- For filtering  the low expressed transcripts the `dmFilter` function from the R/Bioconductor package **DRIMSeq** [3] was used.\n- Following DEXSeq-DTU analysis, a two-stage test is performed by using the R/Bioconductor package **stageR**.  \n\n\n### Common Issues and Important Notes\n- All imported count files must have a unique sample id set in the **sample_id** metadata field.\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and to be populated in all of the samples (**Count Files**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end, please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). Learn how to [add a custom metadata field](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#modifying-metadata-via-the-visual-interface) to expression data files.\n- By default, the tool will use a c5.4xlarge AWS instance. For a large number of input files, c5.9xlarge AWS instance can be used as an alternative (please check the benchmarking table below). To find more about setting the custom instance type using visual interface please check this [document](https://docs.sevenbridges.com/docs/set-execution-hints-at-task-level).\n\n\n### Performance Benchmarking\n\nRuntime and task cost for different number of input files and different instance types (by default, the tool will use a c5.4xlarge AWS instance). \n\n| Input size |  # of input files | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|       8.7 MB     |     8   |   7 min   |   $0.06    |    c4.2xlarge     |\n|       8.7 MB     |     8   |   5 min   |   $0.07    |    c5.4xlarge     |\n|       8.7 MB     |     8   |   4 min   |   $0.12    |    c5.9xlarge     |\n|       8.7 MB     |    32   |   29 min   |   $0.27    |    c4.2xlarge    |\n|       8.7 MB     |    32   |   15 min   |   $0.21    |    c5.4xlarge     |\n|       8.7 MB     |    32   |   9 min   |   $0.27    |    c5.9xlarge     |\n|       8.7 MB     |    64   |   1 h 10 min   |   $0.96    |    c5.4xlarge    |\n|       8.7 MB     |    64   |   36 min   |   $1.00    |    c5.9xlarge     |        \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [Love at al. 2018 publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6178912/)\n[2] [DEXSeq publication](https://genome.cshlp.org/content/22/10/2008.full)\n[3] [DRIMSeq publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5200948/)", "input": [{"name": "Counts"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Covariate of interest"}, {"name": "Control variables"}, {"name": "Phenotype data"}, {"name": "Suggested filtering"}, {"name": "Input type"}, {"name": "Scaling type"}, {"name": "Two-stage test"}, {"name": "Output name prefix"}, {"name": "Out DEXSeq-DTU result R object"}, {"name": "Min samples feature expression"}, {"name": "Min feature expression"}, {"name": "Min gene expression"}, {"name": "Min samples gene expression"}, {"name": "Min samples feature proportion"}, {"name": "Min feature proportion"}, {"name": "Denominator"}, {"name": "Overall False Discovery Rate"}], "output": [{"name": "DEXSeq-DTU result R object"}, {"name": "Two stage test results"}, {"name": "DEXseq-DTU results"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1619097901, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/dexseq-prepare-annotation-1-36-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/dexseq-prepare-annotation-1-36-0/4", "applicationCategory": "CommandLineTool", "name": "DEXSeq prepare annotation", "description": "This tool flattens a GTF file i.e defines exon counting bins that will be used letter on by DEXSeq count tool.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n - In the initial preprocessing step of the DEXSeq analysis [1, 2] the **dexseq_prepare_annotation.py** Python script is used to convert a GTF file with gene models into a GFF file with collapsed gene models. Using GTF files downloaded from Ensembl is recommended, as \n    files from other sources may deviate from the format expected by the script.\n\n - Optional parameter **Gene Aggregation (yes / no):** In the process of forming the counting bins, the script might come across overlapping genes. If two genes on the same strand are found with an exon of the first gene overlapping with an exon of the second gene, the \n   script\u2019s default behaviour is to combine the genes into \n   a single \u201caggregate gene\u201d which is subsequently referred to with the IDs of the individual genes, joined by a plus (\u2018+\u2019) sign. If you do not like this behaviour, you can disable aggregation by setting this option to `no`. Without aggregation, exons that overlap with other exons from different genes are simply skipped.\n\n### Changes Introduced by Seven Bridges\n\n- The tool also accepts a compressed GTF (GTF.GZ) file.\n- If GFF file is provided, the tool will export it as it is, assuming that the file is already flattened (GFF.GZ file will be unzipped first). \n\n### Common Issues and Important Notes\n - GTF files from sources other than Ensembl may deviate from the format expected by the script. Hence, if you need to use a GTF or GFF file from another source, you may need to convert it to the expected format. Make sure that the type of exon lines is exon and that the attributes providing gene ID (or gene symbol) and transcript ID are called `gene_id` and `transcript_id`, with this exact spelling. Remember to also take care that the chromosome names match those in your SAM files, and that the coordinates refer to the reference assembly that you used when aligning your reads. For more details check DEXSeq vignette, section 10.6 [2].\n\n\n### Performance Benchmarking\n\nRuntime and task cost for a 1.1 GB GTF file using default options.\n\n| Input size |  Duration | Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:------------:|\n|      1.1GB     |   5 min   |     $0.05    |    c4.2xlarge     |   \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [DEXSeq paper](https://genome.cshlp.org/content/22/10/2008.full)\n[2] [DEXSeq vignette](https://bioconductor.org/packages/3.12/bioc/vignettes/DEXSeq/inst/doc/DEXSeq.html#preparing-the-annotation)", "input": [{"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Output name"}, {"name": "Gene Agreggation"}], "output": [{"name": "Flattened GFF file"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/areyesq89/DEXSeq"], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1619097901, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/dragmap-1-2-1-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/dragmap-1-2-1-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "DRAGMAP", "description": "**DRAGMAP** is the open source Dragen mapper/aligner [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**DRAGMAP** can be used to align single or paired-end data from **Input reads** (`-1` and `-2`) or **Input BAM file** (`--bam-input`). A **Reference and hash table TAR.GZ archive** should be prepared in advance, using the **Generate a reference/hash table** (`--build-hash-table`), **Reference sequence for building hash table** (`--ht-reference`) and a **BED file for base masking** (`--ht-mask-bed`) inputs. The alignments are output in SAM (default) or BAM format (after conversion with samtools view 1.11). Output type is controlled via the **Output file type** input parameter.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`, `--help-defaults`, `--help-md`, `--ht-test-only`, `--pe-stats-interval-memory` (encountered issues when testing the default value with tool version 1.2.1) and `--version`.\n* The `--output-directory` parameter has been hardcoded in the wrapper to the task current working directory (`$PWD`) and `$PWD/reference` directory for alignment tasks  and tasks in which **Generate a reference/hash table** (`--build-hash-table`) is used, respectively.\n* The `--ref-dir` input parameter has been hardcoded to `$PWD/reference` for alignment tasks. The wrapper expects a TAR.GZ archive with the reference data and hash table (**Reference and hash table TAR.GZ archive**) which it will unpack in the designated location. A suitable archive can be created with this wrapper using the **Generate a reference/hash table** (`--build-hash-table`) input parameter.\n* If a value is not supplied for the **Output file name prefix** input parameter, the output will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the first provided **Input BAM file** (`--bam-input`) or **Input reads** (`-1` and `-2`) file.\n* To facilitate batch processing of inputs (**Input reads** (`-1` and `-2`) or **Input BAM file** (`--bam-input`)), if a value is not provided for the **Read group ID** (`--RGID`) input parameter, but **Platform unit ID** and optionally **Library ID** metadata fields are populated on the first input file, these values will be used to construct a read group ID and `--RGID` with the corresponding value will be added to the tool command line. Similarly, the wrapper will use the **Sample ID** metadata field of the inputs (**Input reads** (`-1` and `-2`) or **Input BAM file** (`--bam-input`)) to add `--RGSM` to the command line of the tool, if no value is provided for the **Read group sample** (`--RGSM`) input parameter. If these fields are not populated, the command line of the tool will not be altered.\n* **Samtools View** (v1.11) was added to the wrapper to enable optional conversion of the output SAM stream to BAM format (example command line: `samtools view --output-fmt BAM -h -o prefix-string-value.bam - `). If `BAM` is chosen as the **Output file format**, the output of **DRAGMAP** will be piped into **Samtools View** and converted. Selecting `SAM` as the output format will output the SAM file as created by **DRAGMAP**.\n\n### Common Issues and Important Notes\n\n* Either **Reference sequence archive** or **Reference sequence for building hash table** (`--ht-reference`) must be provided.\n* If paired end FASTQ files are provided as **Input reads** (`-1` and `-2`), the files must have **Paired-end** metadata field set.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS FQ.GZ (11.6 GB) -> BAM | 3 h 37 min | $5.56 + $0.50 | c5.9xlarge - 1024 GB EBS | \n| WGS FQ.GZ (11.6 GB) -> SAM | 4 h 22 min | $6.68 + $0.60 | c5.9xlarge - 1024 GB EBS | \n| WGS FQ.GZ 30x (46 GB) -> BAM | 6 h 18 min | $9.64 + $0.87 | c5.9xlarge - 1024 GB EBS | \n| WGS FQ.GZ 30x (46 GB) -> SAM | 6 h 39 min | $10.17 + $0.92 | c5.9xlarge - 1024 GB EBS | \n| 24 WGS uBAMs(~2.8 GB each) -> BAM | 4 h 29 min | $6.81 + $0.62 | c5.9xlarge - 1024 GB EBS |\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**DRAGMAP** was tested with cwltool version 3.1.20210628163208. The `in_reads` and `in_reference_archive` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [DRAGMAP documentation](https://github.com/Illumina/DRAGMAP)", "input": [{"name": "Expected paired-end orientation"}, {"name": "Expected mean of the insert size"}, {"name": "Expected PE mean read length"}, {"name": "Quartiles for the insert size"}, {"name": "Expected standard deviation of the insert size"}, {"name": "Rescue scan window maximum ceiling"}, {"name": "Rescue scan window sigmas"}, {"name": "Maximum secondary alignments to report"}, {"name": "Force unmapped if secondary cannot be output"}, {"name": "Phred likelihood cutoff for secondary alignments"}, {"name": "Pair score for secondary alignments"}, {"name": "Smith-Waterman on candidate alignments"}, {"name": "Smith Waterman implementation"}, {"name": "Ratio for controlling seed chain filtering"}, {"name": "Read group ID"}, {"name": "Read group sample"}, {"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "Generate a reference/hash table"}, {"name": "Enable sampling"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "FASTQ quality offset value"}, {"name": "Bits - reference bins for anchored seed search"}, {"name": "Cost coefficient of extended seed frequency"}, {"name": "Cost coefficient of extended seed length"}, {"name": "Cost penalty to extend a seed"}, {"name": "Cost penalty to incrementally extend a seed"}, {"name": "Index of CRC polynomial for hashing extended seeds"}, {"name": "Index of CRC polynomial for hashing primary seeds"}, {"name": "Decoys FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Testing - dump internal parameters"}, {"name": "Cost penalty for EXTEND or INTERVAL records"}, {"name": "8-byte records to reserve in extend_table.bin"}, {"name": "BED file for base masking", "encodingFormat": "text/x-bed"}, {"name": "Maximum decimation factor for seed thinning"}, {"name": "Maximum bases to extend a seed - one step"}, {"name": "Maximum extended seed length"}, {"name": "Maximum seeds populated at multi-base codes"}, {"name": "Maximum allowed seed match frequency"}, {"name": "Seed length to reach maxSeedFreq"}, {"name": "Maximum 1GB thread table chunks in memory"}, {"name": "Memory limit (hash table and reference)"}, {"name": "Methylated"}, {"name": "Minimum probability of repair success"}, {"name": "Worker threads for generating hash table"}, {"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Override hash table size check"}, {"name": "Maximum frequency for a primary seed match"}, {"name": "Random hit EXTEND record frequency"}, {"name": "Include a random hit with each HIFREQ record"}, {"name": "Number of positions per reference seed"}, {"name": "Reference sequence for building hash table", "encodingFormat": "application/x-fasta"}, {"name": "Seed extension repair strategy"}, {"name": "Initial seed length to store in hash table"}, {"name": "Size of hash table"}, {"name": "Reserved space for RNA annotated SJs"}, {"name": "Soft seed frequency cap for thinning"}, {"name": "Target seed frequency for seed extension"}, {"name": "Uncompress hash_table.cmp"}, {"name": "Write decompressed hash_table.bin"}, {"name": "Input qname suffix delimiter"}, {"name": "Interleaved paired-end reads"}, {"name": "Map only"}, {"name": "Memory-map reference data"}, {"name": "Worker threads for the mapper/aligner"}, {"name": "Output file type"}, {"name": "Output file name prefix"}, {"name": "Paired-end stats interval delay"}, {"name": "Paired-end stats interval size"}, {"name": "Paired-end stats sample size"}, {"name": "Preserve the order of mapper/aligner output"}, {"name": "Reference  and hash table TAR.GZ archive", "encodingFormat": "application/x-tar"}, {"name": "Expect uncompressed hash table"}, {"name": "File with additional command-line arguments", "encodingFormat": "text/plain"}, {"name": "Verbose"}], "output": [{"name": "DRAGMAP alignments", "encodingFormat": "application/x-sam"}, {"name": "Reference and hash table archive", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Illumina/DRAGMAP", "https://github.com/Illumina/DRAGMAP/tree/master/src", "https://github.com/Illumina/DRAGMAP/releases/tag/1.2.1", "https://github.com/Illumina/DRAGMAP#readme"], "applicationSubCategory": ["Alignment", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Illumina", "softwareVersion": ["v1.2"], "dateModified": 1648040157, "dateCreated": 1635333129, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/drimseq-1-16-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/drimseq-1-16-1/4", "applicationCategory": "CommandLineTool", "name": "DRIMSeq", "description": "DRIMSeq performs differential transcript usage (DTU) analyses using Dirichlet-multinomial generalized linear models.\n\nFor more information about the DRIMSeq statistical framework please check DRIMSeq publication [1] and vignette [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nTo be able to run an analysis, the tool needs to be provided with:\n- **Counts.** Transcript-level estimated read counts. Depending on the software used for quantification (salmon, kallisto, rsem), the **Input type** parameter should be specified adequately. \n- **Gene annotation file.** GTF file in uncompressed (.gtf) or compressed (.gtf.gz) form.\n- **Phenotype information.**\n\nThere are two options for providing phenotype information:\n\n**a)** By indicating API keys for metadata fields that need to be included in the design. For example, if effect of **sample_type** is being analysed (for example: treated, untreated) on DTU controlling for **sex** (male, female) **sample_id**, **sample_type** and **sex** metadata fields need to be populated for all count files, and **Covariate of interest** and **Control variables** need to be set to **sample_type** and **sex** respectfully. \n\n**b)** By including a CSV file (Phenotype data input) - see the example below. Phenotype data must contain the **sample_id** column with values that match values from the metadata **sample_id** field of the count files. To run an analysis, parameters **Covariate of interest** and **Control variables** (optional) need to be set to the adequate column names. For example, entering **sample_type** for the value of the **Covariate of interest** parameter and **library** in **Control variables** will test for differential transcript usage between treated and untreated samples, while controlling for effect of library preparation.\n\nExample CSV content below:\n\n```\nsample_id,sample_type,library,sex\ntreated1,treated,paired-end,male\ntreated2,treated,single-end,male\ntreated3,treated,paired-end,female\nuntreated1,untreated,single-end,male\nuntreated2,untreated,paired-end,female\nuntreated3,untreated,paired-end,female\nuntreated4,untreated,paired-end,male\n\n```\n\n- By setting the **Scaling type** parameter it can be chosen whether to use estimated counts scaled up to the library size (**scaledTPM**), scaled using the average transcript length over the samples and then the library size (**lengthScaledTPM**),  scaled using the median transcript length among isoforms of a gene, and then the library size (**dtuScaledTPM**) or to use the original estimated counts without scaling (**no**). By default, the **Scaling type** parameter is set to **dtuScaledTPM** - scaling designed for DTU analysis. \n\n- Filtering data, i.e removing transcripts that have low expression levels or are not expressed at all has positive effects on the performance and also speeds up the analysis. There are six parameters that control filtering. By default, all the filtering parameters are set to zero, meaning that transcripts with zero expression in all samples will be removed as well as genes with only one non-zero transcript. By setting the **Suggested filtering** parameter to **TRUE** you can enable filtering suggested by Love at al. 2018 [3] and the parameters will automatically be set as follows:\n**Min samples gene expression** = the total number of samples,\n**Min samples gene expression** and **Min samples feature proportion** = the sample size of the smallest group,\n**Min gene expression** = 10, **Min feature expression** = 10, **Min feature proportion** = 0.1. It is also possible to use **Suggested filtering** as a starting point and then further adjust some of the filtering parameters.\n\n- DRIMSeq analysis outputs two CSV files with likelihood ratio statistics, degrees of freedom, p-values and Benjamini-Hochberg (BH) adjusted p-values, for each gene (first file) and for each transcript (second file). Additionally, to extract genes that are differentially spliced and also exhibit enough evidence to identify one or more transcripts that are participating in the DTU, a two-stage test is performed. For the two-stage test, the **Overall False Discovery Rate** parameter is set to **0.05** by default but it can be changed. \n \n- By setting the **Out DRIMSeq result R object** parameter to **TRUE**, the result R object will also be present within the tool outputs, so DRIMSeq functions for visualisation and further exploration of the data and results can be used locally.  \n\n- Testing for DTU in multiple conditions: DRIMSeq tests for differences in transcript usage between different conditions, meaning that if there are more then two different conditions, DRIMSeq will still output one p-value for each transcript, where the null model would be \"the different conditions do not have an effect on transcript usage\". Thus, significant p-values indicate that we are rejecting the null model: at least one of the conditions has an effect on transcript usage, such that the transcript usage is different with respect to the other conditions. \n\n### Changes Introduced by Seven Bridges\n- R/Bioconductor package **tximport** is used for importing count files allowing the tool to work with direct outputs of quantifiers like salmon, kallisto and rsem. This package also allows choosing the scaling method (**Scaling type** parameter).\n- R/Bioconductor package **GenomicFeatures** is used for creating a tx2gene tab from the input GTF file.\n- Following DRIMSeq analysis, a two-stage test is performed by using the R/Bioconductor package **stageR**.  \n\n\n### Common Issues and Important Notes\n- All imported count files must have a unique sample id set in the **sample_id** metadata field.\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and to be populated in all of the samples (**Count Files**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in it's header. Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end, please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). Learn how to [add a custom metadata field](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#modifying-metadata-via-the-visual-interface) to expression data files.\n\n\n### Performance Benchmarking\n\nRuntime and task cost for different number of input files and different instances (by default, the tool will use a c5.4xlarge AWS instance).\n\n| Input size |  # of input files | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|      8.7 MB     |     8   |  22 min   |   $0.20    |    c4.2xlarge     |\n|      8.7 MB     |    8   |   10 min   |   $0.14    |    c5.4xlarge     |\n|      8.7 MB     |     8   |   7 min   |   $0.20    |    c5.9xlarge     |\n|      8.7 MB     |    32   |  26 min   |   $0.23    |    c4.2xlarge     |\n|      8.7 MB     |    32   |  12 min   |   $0.17    |    c5.4xlarge     |\n|      8.7 MB     |    32   |  8 min   |   $0.23    |    c5.9xlarge     |\n|      8.7 MB     |    64   |  29 min   |   $0.27    |    c4.2xlarge     |\n|      8.7 MB     |    64   |   13 min   |   $0.18    |    c5.4xlarge     |\n|      8.7 MB     |    64   |  9 min   |   $0.26    |    c5.9xlarge     |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [DRIMSeq publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5200948/)\n[2] [DRIMSeq vignette](https://bioconductor.org/packages/3.11/bioc/vignettes/DRIMSeq/inst/doc/DRIMSeq.pdf)\n[3] [Love at al. 2018 publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6178912/)", "input": [{"name": "Counts"}, {"name": "Phenotype data"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Control variables"}, {"name": "Covariate of interest"}, {"name": "Min samples feature expression"}, {"name": "Min feature expression"}, {"name": "Min gene expression"}, {"name": "Min samples gene expression"}, {"name": "Min samples feature proportion"}, {"name": "Min feature proportion"}, {"name": "Suggested filtering"}, {"name": "Two-stage test"}, {"name": "Output name prefix"}, {"name": "Overall False Discovery Rate"}, {"name": "Input type"}, {"name": "Scaling type"}, {"name": "Out DRIMSeq result R object"}], "output": [{"name": "DRIMSeq results - R object"}, {"name": "DRIMSeq results"}, {"name": "Two stage test results"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/gosianow/DRIMSeq"], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035484, "dateCreated": 1619097452, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/eautils-fastq-mcf-1-1-2/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/eautils-fastq-mcf-1-1-2/8", "applicationCategory": "CommandLineTool", "name": "Eautils Fastq-mcf", "description": "Detects levels of adapter presence, computes likelihoods and locations (start, end) of the adapters. Removes the adapter sequences from the FASTQ file(s).", "input": [{"name": "Minimum length match"}, {"name": "Adapter occurrence threshold"}, {"name": "Min clip length"}, {"name": "Max adapter difference"}, {"name": "Minimum remaining sequence length"}, {"name": "Maximum remaining sequence length"}, {"name": "Remove duplicate reads"}, {"name": "SKew percentage"}, {"name": "Bad read percentage threshold"}, {"name": "Quality threshold"}, {"name": "Trimming window size"}, {"name": "Remove homopolymer reads"}, {"name": "Set all default parameters to zero/do nothing"}, {"name": "Illumina PF"}, {"name": "Phred-scale"}, {"name": "Dont remove Ns"}, {"name": "Don't clip"}, {"name": "Number of reads to use for subsampling"}, {"name": "Save discarded reads"}, {"name": "Output lots of random debugging stuff"}, {"name": "Adjust cycle"}, {"name": "Adjust score"}, {"name": "Minimum mean quality score"}, {"name": "Quality greater than threshold"}, {"name": "Maxmium N-calls"}, {"name": "Minimum remaining length"}, {"name": "Mate quality minimum score"}, {"name": "Quality greater than threshold, applies to second non-barcode read only"}, {"name": "Mate maxmium N-calls in a read"}, {"name": "Mate minimum remaining length"}, {"name": "Homopolymer filter percentage"}, {"name": "Adapters", "encodingFormat": "application/x-fasta"}, {"name": "Reads", "encodingFormat": "text/fastq"}, {"name": "Keep only clipped reads"}, {"name": "Phred adjust max"}, {"name": "Output N records"}, {"name": "Complexity filter"}, {"name": "Keep clipped"}, {"name": "Max output reads"}], "output": [{"name": "Filtered reads", "encodingFormat": "text/fastq"}, {"name": "Skipped reads", "encodingFormat": "text/fastq"}, {"name": "Summary", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["FASTQ Processing"], "project": "SBG Public Data", "creator": "Erik Aronesty", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453800010, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/elprep-sfm-4-1-6/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/elprep-sfm-4-1-6/9", "applicationCategory": "CommandLineTool", "name": "ElPrep sfm", "description": "**elPrep 4.1.6 sfm** is a high-performance tool for preparing SAM/BAM files for variant calling in sequencing pipelines.\n It can be used as a replacement for SAMtools and Picard for preparation steps such as filtering, sorting, marking duplicates, calculating and applying base quality score recalibration, and so on. Its software architecture allows executing preparation pipelines by making only a single pass through the data, no matter how many preparation steps are used in the pipeline. elPrep is designed as a multithreaded application that runs entirely in memory, avoids repeated file I/O, and merges the computation of several preparation steps to speed up the execution time [1].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* **elPrep 4.1.6 sfm** can be used as a replacement of the data cleanup portion (BAM sorting, duplicate marking and base quality score recalibration and application) of variant calling pipelines.\n* It can also be used to filter out unmapped reads, or reads based on genomic regions of interest, removing optional fields, soft clipping alignments that hang off the end, removing all alignments with a mapping quality lower than the given threshold, etc.  \n* It can be used for replacing or adding read groups to the alignments in the input file.\n\n\n### Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output file name prefix**. In case **Output file name prefix** is not provided, output prefix will be the same as the **Sample ID** metadata field from  file, if the **Sample ID** metadata field exists. Otherwise, output prefix will be inferred from the **Input SAM/BAM file**. \n* elPrep uses internal formats for representing VCF, BED, or FASTA files used by specific sfm options. Automatic creation of elPrep-specific FASTA, VCF, and BED formats was added in order to make the CWL tool easier to use. Conversion to elPrep specific formats takes around a minute and these files can be saved and used in the later analysis if the **Save elPrep specific formats** option is enabled.\n\n### Common Issues and Important Notes \n\n* The **Remove duplicates** option only removes duplicates based on the FLAG entry for each read. It does not trigger marking of duplicates. If the user wants marking of duplicates to be done prior to removing duplicates both parameters should be set to true.\n* The options for configuring the base quality score recalibration will be ignored if **BQSR Base quality score recalibration** is not requested.\nSpecifying a reference file is required in order to calculate base recalibration. Otherwise, if **BQSR Base quality score recalibration** is requested without providing a **Reference FASTA/ELFASTA** for BQSR, the tool will fail with an error message indicating that there was an attempt to calculate base recalibration without specifying a reference file.\n* Sorting according to the query name is not yet implemented with **elPrep 4.1.6 sfm**.\n* VCF.GZ is not supported as an input format for **Excluded known sites for BQSR**.\n* elPrep is designed to store big amounts of data in RAM during computation, thus it consumes significantly more memory than similar tools. It is not possible to limit the memory that could be used and the tool will fail when it runs out of memory. \n* There may be small differences when comparing BAMs processed by elPrep and GATK 4/Picard. The discrepancies in BAM comparison are caused due to the fact that\u00a0many of the algorithms are non-deterministic. For example, the GATK 4/Picard mark duplicate algorithm compares reads for duplicate marking by comparing the mapping positions and quality scores. When two reads have the same mapping position, read with the worse quality score will be marked as a duplicate. It may, however occur that two reads have the exact same mapping position\u00a0and\u00a0the exact same quality score. In this case, which read is marked as the duplicate, conceptually does not matter, and in Picard and GATK 4, which one is marked will just depend on the order of the reads in the input file. Since elPrep parallelizes the processing of reads, they are not always examined in the same order of the input file. This can also have an effect on a later step of the pipeline such as BQSR, because it may see a different read marked as duplicate compared to GATK, which might impact the BQSR result.\u00a0\u00a0\n\n\n\n\n\n### Performance Benchmarking\n\n| Sample |BAM size [GB] | BQSR|Markdup|Sorting|Filters|RG | Instance| Duration [h]| Cost |RAM peak [GB]|Disk peak [GB]|\n| --- | --- | --- | --- | --- | --- |---|---|---|---|---|---|\n|C835.HCC1143.2| 10.8 | 1 | 0 | 0 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.33|$0.55 |26|40|\n|C835.HCC1143.2| 10.8 | 0 | 1 | 0 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.18|$0.3 |21.8|32|\n|C835.HCC1143.2| 10.8 | 0 | 0 | 1 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.16|$0.26 |16.2|33|\n|C835.HCC1143.2| 10.8 | 0 | 0 | 0 | 1|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.28|$0.13 |7.26|34|\n|C835.HCC1143.2| 10.8 | 1| 1 | 1 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.36|$0.47 |24.8|40|\n|HCC1143-CCLE | 23.5 | 1 | 0 | 0 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.73|$1.22 |42|63.4|\n|HCC1143-CCLE| 23.5 | 0 | 1 | 0 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.46|$0.77 |54.3|58|\n|HCC1143-CCLE| 23.5 | 0 | 0 | 1| 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|0.4|$0.67 |46|57|\n|HCC1143-CCLE| 23.5 | 0 | 0 | 0 | 1|0|c5.9xlarge (36vCPUs, 72GB RAM)|1|$1.67 |34|81|\n|HCC1143-CCLE| 23.5 | 1| 1 | 1 | 0|0|c5.9xlarge (36vCPUs, 72GB RAM)|1.3|$2.17 |55.9|57|\n|NA12878-50x| 75.7 | 0|0 | 0 | 0|1|c5.9xlarge (36vCPUs, 72GB RAM)|1.08|$1.8 |8.5|184|\n|NA12878-50x| 75.7 | 1 | 0 | 0 | 0|0|c5.24xlarge(96vCPUs, 192GB RAM)|1.88|$7.93 |105|209|\n|NA12878-50x| 75.7 | 0 | 1 | 0 | 0|0|c5.24xlarge (96vCPUs, 192GB RAM)|1.41|$5.95 |156|186|\n|NA12878-50x| 75.7 | 0 | 0 | 1| 0|0|c5.24xlarge (96vCPUs, 192GB RAM)|1.3|$5.48 |120|185|\n|NA12878-50x| 75.7 | 0 | 0 | 0 | 1|0|c5.24xlarge (96vCPUs, 192GB RAM)|1.03|$4.35 |18|178|\n|NA12878-50x|75.7 | 1| 1 |1| 0|0|c5.24xlarge (96vCPUs, 192GB RAM)|2.03|$8.57 |150|209|\n| NA12877.bam 150x|113 | 1| 1 |1| 0|0|m5.24xlarge (96vCPUs, 384GB RAM)|3.61|$17.15 |249|299|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\nFor benchmarking purposes, the BAM processing part of the **GATK Germline variant calling from FASTQ 4.1.0.0-4.1.4.0** pipeline (**Picard RemoveDupicates**, **BamSort**, **GATK4 BaseRecalibrator**, and **GATK4 ApplyBQSR**) was replaced by elPrep (BAM sorting, duplicate marking, and base quality score recalibration and application). The pipeline outputs both variants (VCF file) and processed BAM file.\n\nThe obtained variants\u00a0from the set of Genome In A Bottle (HG001-HG004) [2] and CHM [3] samples with available truth sets were compared with the ones from the original GATK4 Best Practice pipeline. Here, we provide figures of Precision, Recall, F-score, False Positives, False negatives for SNPs and INDELs for five different samples, as well as execution time and price for both original and elPrep substitute analysis.\n\n[![confluence-new-color-prec-rec-fscore.png](https://i.postimg.cc/bvqTsnXx/confluence-new-color-prec-rec-fscore.png)](https://postimg.cc/Wdfryh1z)\n[![new-color-final-false-positive-negative.png](https://i.postimg.cc/Nf5J8mzB/new-color-final-false-positive-negative.png)](https://postimg.cc/D83PnJhM)\n[![confluence-price-e3ecution.png](https://i.postimg.cc/Njkc9Lf6/confluence-price-e3ecution.png)](https://postimg.cc/pmr73Wjr)\n\n\n\n\n\n\n\n### References\n\n[1] Herzeel C, Costanza P, Decap D, Fostier J, Verachtert W (2019) [elPrep 4: A multithreaded framework for sequence analysis](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209523). PLoS ONE 14(2): e0209523. https://doi.org/10.1371/journal.pone.0209523\n\n[2] Zook, J., Catoe, D., McDaniel, J. et al. (2016) [Extensive sequencing of seven human genomes to characterize benchmark reference materials](https://www.nature.com/articles/sdata201625) Sci Data 3, 160025 (2016). https://doi.org/10.1038/sdata.2016.25\n\n[3] Heng Li et al. (2018) [A synthetic-diploid benchmark for accurate variant calling evaluation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6341484/) Nat Methods. 2018 Aug; 15(8): 595\u2013597. doi: 10.1038/s41592-018-0054-7", "input": [{"name": "Input SAM/BAM file", "encodingFormat": "application/x-bam"}, {"name": "Output file name prefix"}, {"name": "Output file format"}, {"name": "Filter unmapped reads"}, {"name": "Reference FASTA/ELFASTA for BQSR", "encodingFormat": "application/x-fasta"}, {"name": "Mapping quality filter threshold"}, {"name": "Filter non exact mapping reads"}, {"name": "Intervals overlapping with mapping positions", "encodingFormat": "text/x-bed"}, {"name": "Clean SAM"}, {"name": "File required for header replacement", "encodingFormat": "application/x-bam"}, {"name": "Replace read group"}, {"name": "Mark duplicates"}, {"name": "Mark optical duplicates"}, {"name": "Optical duplicates pixel distance"}, {"name": "Excluded known sites for BQSR", "encodingFormat": "text/x-bed"}, {"name": "Remove duplicates"}, {"name": "Remove optional fields"}, {"name": "Keep optional fields"}, {"name": "Sorting"}, {"name": "Number of threads"}, {"name": "Intermediate files output type"}, {"name": "Single end reads in input alignments"}, {"name": "Contig group size"}, {"name": "Save elPrep specific formats"}, {"name": "BQSR Base quality score recalibration"}, {"name": "BQSR Number of levels for quantizing quality scores"}, {"name": "BQSR Static quantized quality"}, {"name": "BQSR Maximum cycle value"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Filtered SAM/BAM file", "encodingFormat": "application/x-bam"}, {"name": "Metrics file", "encodingFormat": "text/plain"}, {"name": "BQSR file"}, {"name": "elPrep specific files"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ExaScience/elprep/releases", "https://github.com/ExaScience/elprep"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648035173, "dateCreated": 1612270811, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-annotate-3-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-annotate-3-4-2/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Annotate", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n__EPACTS Annotate__ is used to annotate EPACTS or VCF files for further EPACTS analyses. \n\nThe tool has several inputs:\n\n* __Input VCF__ - Input Multi-Sample Variant Call Format file that needs to be annotated\n* __Gene Prediction File__ - UCSC refGene format gene database file in plain or gzipped format\n* __Reference Fasta File__ with index (FAI)\n\n\nThe tool will output an annotated VCF file, along with its corresponding index file (TBI).\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nThe `epacts anno` script will add the `ANNO=[function]:[genename]` entry into the `INFO` field based on integrated gencodeV14 and refGene database for GRCh37, or __Gene Prediction File__ in UCSC refGene format given as input.\n\n\n###Changes Introduced by Seven Bridges\n\n* If the Output Prefix parameter is not given, it will be derived from the name of the __Input VCF or EPACTS File__.\n* EPACTS `anno` function uses the _human_g1k_v37.fasta_ file embedded in the package. To ensure the correct reference usage for files aligned to GRCh38, __Reference FASTA File__ is defined as a required input parameter.\n\n\n### Common Issues and Important Notes\n\n* __It is important to check whether the VCF file is already annotated or not in order to avoid missing or redundant annotation.__\n* If the VCF is not annotated, __EPACTS Create Marker Group File__ tool cannot be used.\n* Input VCF file must be bgzipped and tabixed before running association to allow efficient random access to the file. Use __Tabix BGZIP__ and __Tabix Index__ tools to prepare the input VCF.\n* This tool is configured to run for GRCh37 by default , and will use the _hg19_gencodeV14.txt.gz_ file to annotate variants. If using files aligned to GRCh38, appropriate annotation files need to be provided using the __Gene Prediction File__ input parameter. For more information on EPACTS support of GRCh38 visit the [GitHub issue](https://github.com/statgen/EPACTS/issues/20) on this topic. \n\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type | Input size | # of Samples | Contigs | Duration | Cost | Instance (AWS) | Attached Storage (AWS) |\n|-----------------|------------|--------------|---------|----------|-------|----------------|------------------------|\n| GWAS | 48.1 MiB | 2549                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 20 | 1,5min | $0.01 | c4.2xlarge | 1024GB |\n| GWAS | 293.4 MiB  | 2709                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 20                                                                                                                                                                                                      | 17min | $0.12 | c4.2xlarge | 1024GB |\n| GWAS | 751.2 GiB | 2709                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | all | 7h 23min |$3.1| c4.2xlarge | 1024GB |\n\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Build Version"}, {"name": "Database for GRCh37"}, {"name": "Gene Prediction File", "encodingFormat": "text/plain"}, {"name": "Keep intermediate files"}, {"name": "Output Prefix"}, {"name": "Reference Fasta File", "encodingFormat": "application/x-fasta"}, {"name": "Input VCF or EPACTS File", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2", "https://github.com/statgen/EPACTS/archive/v3.4.2.tar.gz"], "applicationSubCategory": ["GWAS", "Annotation"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-create-kinship-matrix-3-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-create-kinship-matrix-3-4-2/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Create Kinship Matrix", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n__EPACTS Create Kinship Matrix__ creates a file containing the kinship coefficients of the samples. \n\n\nThe tool has several inputs:\n* __VCF File__ - Input Multi-Sample Variant Call Format file\n* __PED File__ - Pedigree file containing phenotypes and covariates\t\n* __Reference File__ with index (FAI)\n* __Intervals File__ - List of intervals as a unit to perform association in standard BED format\n\nThe tool will output a __Kinship Matrix File__ (KINF). \n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe kinship coefficient is a simple measure of relatedness, defined as the probability that a pair of randomly sampled homologous alleles are identical by descent. It is the probability that an allele selected from one sample, and an allele selected at the same autosomal locus from another sample, are identical and from the same ancestor. \n__EPACTS Create Kinship Matrix__ creates a kinship matrix containing the kinship coefficients of the samples contained in the input __VCF File__. \nThis file is used for the EMMAX Association Analysis in the __EPACTS Single Variant Test__ or __EPACTS Groupwise Burden Test__.\nIf __PED File__ is provided, it will calculate the subset the individuals contained in the pedigree file.\n\n###Changes Introduced by Seven Bridges\n\n* EPACTS `make-kin` function uses the _human_g1k_v37.fasta_ FASTA file embedded in the package. To ensure the correct reference usage for files aligned to GRCh38, __Reference File__ is defined as a required parameter.\n\n\n### Common Issues and Important Notes\n\n* To select high-quality markers to generate kinship, it is recommended to set the Minimum MAF (e.g. 0.01) and call rate (e.g. 0.95) thresholds.\n* If only a certain subset of SNPs needs to be considered due to target regions, LD-pruning, or any other reasons, a VCF containing the subset of markers must be created beforehand and should be used as input __VCF File__.\n* Due to an error in the EPACTS pre-release 3.2.4 version, his tool contains EPACTS v3.4.2 - development branch. For more information on the error and different EPACTS versions visit the [GitHub issue](https://github.com/statgen/EPACTS/issues/23) on this topic. \n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type \t| Input size \t| # of Samples \t| Contigs \t| Duration \t| Cost  \t| Instance (AWS) \t| Attached Storage (AWS) \t|\n|-----------------\t|------------\t|--------------\t|---------\t|----------\t|-------\t|----------------\t|------------------------\t|\n| GWAS            \t| 48.1 MiB   \t| 2549         \t| 20      \t| 2min     \t| $0,01 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 293.4      \t| 2709         \t| 20      \t| 6min        \t|   $0,03    \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t|   11min        \t|   $0,055    \t| c4.2xlarge     \t| 1024GB                      \t|\n\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "BED File", "encodingFormat": "text/x-bed"}, {"name": "Chromosome"}, {"name": "Format Field"}, {"name": "Include Sex Chromosome"}, {"name": "Minimum Call Rate"}, {"name": "Minimum MAF"}, {"name": "MOSIX Nodes"}, {"name": "Output Prefix"}, {"name": "Pass Filter"}, {"name": "Pedigree File"}, {"name": "Remove Complex Regions"}, {"name": "Info Field Rule"}, {"name": "Number of CPUs"}, {"name": "Indicator of Separated VCFs"}, {"name": "Unit for Parallel Run"}, {"name": "VCF File", "encodingFormat": "application/x-vcf"}, {"name": "Reference File", "encodingFormat": "application/x-fasta"}, {"name": "Intervals File", "encodingFormat": "text/x-bed"}], "output": [{"name": "Kinship Matrix"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2", "https://github.com/statgen/EPACTS/archive/develop.zip"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-create-marker-group-file-3-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-create-marker-group-file-3-4-2/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Create Marker Group File", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n__EPACTS Create Marker Group File__ creates a file containing the list of markers per group, needed for the  __EPACTS Groupwise Burden Test__ tool.\n\nThe tool has one input - __VCF File__ - Input Multi-Sample Variant Call Format file\nAnd will output the __Group File__ (GRP). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nThe marker group file has the following format: \n\n```[GROUP_ID]  [MARKER_ID_1]   [MARKER_ID_2]  .... [MARKER_ID_N]```\n\n* `[GROUP_ID]` is a string representing the group (e.g. gene name)\n* `[MARKER_ID_K]` is a marker key in the `[CHROM]:[POS]_[REF]/[ALT]` format (note that this is _different from typical VCF `MARKER ID` field_), and it has to be sorted by _ascending order_ of genomic coordinate.\n\n\n###Changes Introduced by Seven Bridges\n\n* If the __Output File Prefix__ parameter is not given, it will be derived from the name of the __Input VCF file__.\n\n### Common Issues and Important Notes\n\n* __Input VCF File__ must be bgzipped and tabixed before running association to allow efficient random access to the file. Use __Tabix BGZIP__ and __Tabix Index__ tools to prepare the input VCF.\n* __Input VCF File__ must be annotated for the EPACTS `make-group` function to work. Otherwise, this tool will produce an empty file on the output.\n\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type \t| Input size \t| # of Samples \t| Contigs \t| Duration \t| Cost   \t| Instance (AWS) \t| Attached Storage (AWS) \t|\n|-----------------\t|------------\t|--------------\t|---------\t|----------\t|--------\t|----------------\t|------------------------\t|\n| GWAS            \t| 48.1 MiB   \t| 2549         \t| 20      \t| 45s      \t| $0,006 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 293.4 MiB     \t| 2709         \t| 20      \t|     6min    \t|   $0,04   \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t|     18min      \t|   $0,13     \t|   c4.2xlarge     \t| 1024GB               \t|\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Annotation Format"}, {"name": "Gene Name Field"}, {"name": "Nonsynonymous SNP Categories"}, {"name": "Output File Prefix"}, {"name": "Use PASS-Filtered SNPs"}, {"name": "Functional Type(s)"}, {"name": "Functional Class Field"}, {"name": "VCF File", "encodingFormat": "application/x-vcf"}, {"name": "LoF Variant Categories"}, {"name": "HIGH Impact Variant Categories"}, {"name": "Moderate Variant Categories"}], "output": [{"name": "Output Group File"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2", "https://github.com/statgen/EPACTS/archive/v3.4.2.tar.gz"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-groupwise-burden-test-3-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-groupwise-burden-test-3-4-2/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Groupwise Burden Test", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n__EPACTS Groupwise Burden Test__ is a weighted-sum method to jointly analyse a group of mutations in order to test for groupwise association with phenotype status.\n\nThe tool has several inputs:\n\n* __VCF File__ - Input Multi-Sample Variant Call Format file\n* __Group File__ - EPACTS marker group file (GRP) that defines the groups of markers to be tested for EPACTS groupwise association\n* __Kinship File__ - File containing the kinship matrix (KINF) produced by __EPACTS Create Kinship Matrix File__\n* __Input PED File__ - Pedigree file containing phenotypes and covariates\t\n* __Reference File__ - Reference FASTA file with index (FAI)\n* __REML file File__ - REML file produced by a previous run of the q.emmax test with __EPACTS Single Variant Test__\n* __Intervals List__ - List of intervals as a unit to perform association in standard BED format\n\nThe tool will output the following output files:\n* __EPACTS Associations Table__ - Results of the tests for every group, presented in a table containing the following columns: \n    * Number of phenotyped samples with non-missing genotypes\n    * Fraction of individual carrying rare variants below --max-maf (default : 0.05) threshold\n    * Number of all variants defining the group\n    * Number of variants passing the --min-maf, --min-mac, --max-maf, --min-callrate thresholds\n    * Number of singletons among variants in NUM_PASS_VARS\n    * P-value of burden tests\n    * Other columns are test-specific auxiliary columns. For example, in the VT test, the optimal MAF threshold is recorded as an auxiliary output column.\n* __Top Associations File__ - File containing the top 5,000 associations (TOP5000), in the same format as the EPACTS Associations Table\n* __Q-Q Plot__ - Q-Q plot of test statistics, stratified by minimum allele frequency (MAF)\n* __Manhattan Plot__ - Manhattan plot of test statistics\n* Additional generated files are used as inputs for __EPACTS Plot__\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n__EPACTS Groupwise Burden Test__ provides the following sets of widely used statistical tests for burden tests:\n\n| Test Name | Phenotypes | Covariates | Computational Time | Description | Implemented by |\n|:----------:|:-------------------:|:--------------------------:|:------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------:|\n| b.collapse | Binary | YES  (Joint Estimation) | Slow | Logistic Wald Test between binary phenotypes and 0/1 collapsed variables | Hyun Min Kang |\n| b.madsen | Binary | NO | Slow | Wilcoxon Rank Sum Test between binary phenotypes and weighted rare variant scores (slightly different version from the published method - it uses pooled allele frequency across cases and controls for weighting each variant) | Hyun Min Kang |\n| b.wcnt | Binary | YES  (Joint Estimation) | Slow | Logistic Wald Test between binary phenotypes and weighted rare variant scores | Hyun Min Kang |\n| q.reverse | Quantitative | YES  (Joint Estimation) | Slow | Reverse regression of phenotypes on binary collapsed variables | Hyun Min Kang |\n| q.wilcox | Quantitative | YES  (Regressed Out First) | Slow | Nonparametric Reverse regression of phenotypes on collapsed variables | Hyun Min Kang |\n| skat | Binary/Quantitative | YES  (Joint Estimation) | Slow | SKAT-O Test by Lee et al, Biostatistics (2012) | Seunggeun Lee  (adaptive by Xueling Sim and Hyun Min Kang) |\n| emmaxCMC | Binary/Quantitative | YES  (Regressed Out First) | Slow | Collapsing burden test using EMMAX | Hyun Min Kang |\n| emmaxVT | Binary/Quantitative | YES  (Regressed Out First) | Slow | Variable-threshold burden test using EMMAX | Hyun Min Kang |\n\n\n# \n####Specialized Instruction for EMMAX tests\n\nEMMAX (Efficient Mixed Model Association eXpedited - Kang et al (2010) Nat Genet 42:348-54) is an efficient implementation of mixed-model association accounting for sample structure including population structure and hidden relatedness. \nBecause EMMAX is based on a linear model, the method fits better with quantitative traits than binary traits. However, p-values for binary traits are expected to be valid in the spirit of the Armitage trend test, although the estimated effect size may not be precise.\nFor running a CMC-Style burden test select `emmaxCMC`, or select `emmaxVT` for Variable Threshold burden test. \n\n###Changes Introduced by Seven Bridges\n\n* If the __Output Prefix__ parameter is not given, it will be derived from the name of the input __VCF file__.\n* EPACTS `group` function uses the _human_g1k_v37.fasta_ reference file embedded in the package. To ensure the correct reference usage for files aligned to GRCh38, __Reference File__ is defined as a required parameter.\n* `--interval-list` parameter which sets a list of intervals as a unit to perform association, was removed due to an error in the toolkit version. For more information on this issue visit the [GitHub issue](https://github.com/statgen/EPACTS/issues/22) on this topic. \n\n### Common Issues and Important Notes\n\n* Before running __EPACTS Groupwise Burden Test__, input __VCF File__ must be annotated and the Group file (GRP) containing the list of markers per group needs to be generated with __EPACTS Create Marker Group File__.\n* When running EMMAX analysis, Kinship Matrix (KINF) must be provided on the input, otherwise the tool will fail to execute. Use __EPACTS Create Kinship Matrix__ to generate the kinship file.\n* __VCF File__ must be bgzipped and tabixed before running association to allow efficient random access to the file. Use __Tabix BGZIP__ and __Tabix Index__ tools to prepare the input __VCF File__.\n* Sample IDs in the VCF file must be consistent to those from the PED file.\n* EPACTS accepts the PED format supported by MERLIN or PLINK software to represent phenotypes.\n* Currently, EPACTS supports only bi-allelic variants, but it handles SNPs, INDELs, snd SVs.\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type \t| Input size \t| # of Samples \t| Contigs | # of CPUs  | Duration \t| Cost   \t| Instance (AWS) \t| Attached Storage (AWS) \t|\n|-----------------\t|------------\t|--------------\t|---------|---------\t|----------\t|--------\t|----------------\t|------------------------\t|\n| GWAS            \t| 48.1 MiB   \t| 2549         \t| 20      \t    \t|1  | 1min       \t| $0,008 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 293.4 MiB     \t| 2709         \t| 20      \t    \t| 1 | 6min       \t| $0,06  \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t|     1    \t|   58min    \t|   $0,46     \t| c4.2xlarge     \t| 1024GB                     \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t|      16    \t|  8min    \t|  $,08\t|  c5.4xlarge\t| 1024GB                     \t|\n\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "SKAT Beta Prior"}, {"name": "Conditional Association Analysis"}, {"name": "Covariate Column(s)"}, {"name": "FORMAT Field"}, {"name": "Marker Group File", "encodingFormat": "text/plain"}, {"name": "Inverse-Normal Transformation"}, {"name": "Kinship Matrix File"}, {"name": "Maximum MAF"}, {"name": "Minimum call rate"}, {"name": "Minimum MAC"}, {"name": "Minimum MAF"}, {"name": "Minimum RSQ_HAT"}, {"name": "Missing Value String"}, {"name": "MOSIX Nodes"}, {"name": "Skip Manhattan and QQ plots"}, {"name": "Output Prefix"}, {"name": "Pedigree File", "encodingFormat": "text/plain"}, {"name": "Phenotype Column"}, {"name": "Recessive Encoding of Burden"}, {"name": "REML File"}, {"name": "Number of CPUs"}, {"name": "Indicator of Separated VCFs"}, {"name": "SKAT Size Adjustment"}, {"name": "SKAT Use Flat Prior"}, {"name": "SKAT Optimal Mode"}, {"name": "Groupwise Test"}, {"name": "Number of Units (Groups)"}, {"name": "VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Score File"}, {"name": "Reference File", "encodingFormat": "application/x-fasta"}], "output": [{"name": "EPACTS Associations Table"}, {"name": "Supplementary Files"}, {"name": "Top 5000 Associations"}, {"name": "Plot Files"}, {"name": "Log Files"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2", "https://github.com/statgen/EPACTS/archive/v3.4.2.tar.gz"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-plot-3-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-plot-3-4-2/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Plot", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n\n__EPACTS Plot__ is used to generate PDF plots from __EPACTS Single Variant Test__  or __EPACTS Groupwise Burden Test__ results.\n\nThe tool has several inputs:\n\n* __Input EPACTS File__ - EPACTS output files created by the __EPACTS Single Variant Test__ or the __EPACTS Groupwise Burden Test__ \n* __Labels File__ - File containing [MAKER_ID] and [LABEL].\n* __Reference File__ - Reference FASTA file with index (FAI)\n\nThe tool will output:\n* Two plots\n    * __Q-Q Plot__ - Q-Q plot of test statistics, stratified by minimum allele frequency (MAF)\n    * __Manhattan Plot__ - Manhattan plot of test statistics \n* Top 5000 Associations - File containing the top 5,000 associations (TOP5000) in the same format as the __Input EPACTS File__ \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n__EPACTS Plot__  will generate a Q-Q plot of test statistics (stratified by MAF) that provides the global distribution of test statistics and shows if there is an apparent inflation of test statistics, and Manhattan plot of test statistics will display information about the genome-wide distribution of association signals.\n\n\n###Changes Introduced by Seven Bridges\n\n* If the __Output Prefix__ parameter is not given, it will be derived from the name of the input VCF file.\n* EPACTS `plot` function uses the human_g1k_v37.fasta FASTA file embedded in the package. To ensure the correct reference usage for files aligned to GRCh38, Reference FASTA File is defined as a required parameter.\n\n\n### Common Issues and Important Notes\n* Q-Q and Manhattan plots are automatically produced by the __EPACTS Single Variant Test__ and __EPACTS Groupwise Burden Test__ tools, unless the `no-plot` parameter is selected.\n\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type \t| Input size \t| # of Samples \t| Contigs \t| Duration \t| Cost   \t| Instance (AWS) \t| Attached Storage (AWS) \t|\n|-----------------\t|------------\t|--------------\t|---------\t|----------\t|--------\t|----------------\t|------------------------\t|\n| GWAS            \t| 25.3 KiB   \t| 2549         \t| 20      \t| 1min       \t| $0,007 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 50.3 KiB     \t| 2709         \t| 20      \t|    2min      \t|   $0,014     \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 555.5 KiB  \t| 2709         \t| all     \t|   2min       \t|       $0,014     \t| c4.2xlarge     \t| 1024GB   \n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Groupwise Association"}, {"name": "Ignore MAF"}, {"name": "Input EPACTS File"}, {"name": "Labels File"}, {"name": "Output Prefix"}, {"name": "Regions"}, {"name": "Significance Threshold"}, {"name": "Thinning Unit"}, {"name": "Plot Title"}, {"name": "X Axis Maximum Value"}, {"name": "Y Axis Maximum Value"}, {"name": "Reference File", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Plots"}, {"name": "Top 5000 Associations"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2", "https://github.com/statgen/EPACTS/archive/v3.4.2.tar.gz"], "applicationSubCategory": ["GWAS", "Plotting"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-single-variant-test-3-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-single-variant-test-3-4-2/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Single Variant Test", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n__EPACTS Single Variant Test__ is used in testing for association between genetic variants and complex traits.\n\nThe tool has several inputs:\n\n* __Input VCF File__ - Input Multi-Sample Variant Call Format file\n* __Kinship File__ - File containing the kinship matrix (KINF) produced by __EPACTS Create Kinship Matrix File__\n* __Input PED File__ - Pedigree file containing phenotypes and covariates\t\n* __Reference FASTA File__ with index (FAI)\n* __REML File__ - REML file produced by a previous run of the q.emmax test with __EPACTS Single Variant Test__\n\nThe tool will output the following output files:\n* __EPACTS Associations table__ - Results of the tests for every variant, presented in a table containing the following columns: \n    * Number of phenotyped samples with non-missing genotypes\n    * Total Non-reference Allele Count\n    * Fraction of non-missing genotypes\n    * Minor allele frequencies\n    * P-value of single variant test\n    * Non-reference allele frequencies for cases\n    * Non-reference allele frequencies for controls \n* __Top 5000 Associations__ - File containing the top 5,000 associations (TOP5000), in the same format as EPACTS Associations table\n* __Q-Q Plot__ - Q-Q plot of test statistics, stratified by minimum allele frequency (MAF)\n* __Manhattan Plot__ - Manhattan plot of test statistics\n* Additional generated files are used as inputs for __EPACTS Plot__ and __EPACTS Zoom Plot__ tools.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n__EPACTS Single Variant Test__ provides the following sets of widely used statistical tests for single variant tests:\n\n| Test Name     | Phenotypes     | Covariates     | Computational Time     | Description     | Implemented by     |\n|:---------:    |:------------:    |:--------------------------:    |:------------------:    |:----------------------------------------------------------------------------------------:    |:------------------------------------------:    |\n| b.wald     | Binary     | YES  (Joint)     | Slow     | Logistic Wald Test     | Hyun Min Kang  (simply used glm in R)     |\n| b.score     | Binary     | YES  (Regressed Out)     | Fast     | Logistic Score Test  (from Lin DY and Tang ZZ, AJHG 2011 89:354-67)     | Clement Ma & Hyun Min Kang     |\n| b.firth     | Binary     | YES  (Joint)     | Slow     | Firth Bias-Corrected Logistic Likelihood Ratio Test     | Clement Ma     |\n| b.spa2     | Binary     | YES      | Moderate     | Saddlepoint Approximation Method     | Shawn Lee & Rounak Dey     |\n| b.lrt     | Binary     | YES  (Joint)     | Slow     | Likelihood Ratio Test     | Clement Ma     |\n| b.glrt     | Binary     | NO     | Fast     | Genotype Likelihood Ratio Test  (use GL or PL field in VCF to perform the case-control test)     | Hyun Min Kang     |\n| q.lm     | Quantitative     | YES  (Joint)     | Slow     | Linear Wald Test     | Hyun Min Kang  (as implemented in lm in R)     |\n| q.linear     | Quantitative     | YES  (Regressed Out)     | Fast     | Linear Wald Test     | Hyun Min Kang     |\n| q.reverse     | Quantitative     | YES  (Joint)     | Slow     | Reverse regression of phenotypes on binary genotypes (dominant model)     | Hyun Min Kang     |\n| q.wilcox     | Quantitative     | YES  (Regressed Out First)     | Slow     | Nonparametric Reverse regression of phenotypes on binary genotypes (dominant model)     | Hyun Min Kang     |\n| q.emmax     | Quantitative     | YES  (Regressed Out First)     | Slow     | EMMAX  ( Kang et al (2010) Nat Genet 42:348-54 )     | Hyun Min Kang     |\n\n\n# \n####Specialized Instruction for EMMAX tests\n\nEMMAX (Efficient Mixed Model Association eXpedited - Kang et al (2010) Nat Genet 42:348-54) is an efficient implementation of mixed-model association accounting for sample structure including population structure and hidden relatedness. \nBecause EMMAX is based on a linear model, the method fits better with quantitative traits than binary traits. However, p-values for binary traits are expected to be valid in the spirit of the Armitage trend test, although the estimated effect size may not be precise.\n\n\n#####Single Variant EMMAX Association Analysis\n\nIn order to run the EMMAX analysis from sequence-based genotypes, it is recommended to run EPACTS multiple times using the following procedure:\n* Creating Kinship Matrix - From VCF, it is recommended to set the MAF (e.g. 0.01) and Call Rate (e.g. 0.95) threshold to select high-quality markers to generate kinship matrix using the __EPACTS Create Kinship Matrix__.\nThis will create a file [outprefix.kinf] after splitting and merging the genomes into multiple pieces. If only a certain subset of SNPs needs to be considered due to target regions, LD-pruning, or any other reasons, a VCF containing the subset of markers must be created beforehand and should be used as input VCF file.\n* Perform Single Variant Association - From VCF and PED file, use less stringent MAF threshold (e.g. 0.001) and call rate (e.g. 0.50) to perform single variant association. This procedure will perform single variant association analysis compatible with other types of single variant association analyses implemented in EPACTS.\n* For the subsequent run of the EMMAX test use __REML File__ output from the previous emmax test. This file contains the following metrics:\n    * Log-likelihood with variance component\n    * Log-likelihood without variance component\n    * Ratio between variance parameters\n    * Genetic variance parameter\n    * Residual variance parameter\n    * The pseudo-heritability estimates - Explained variance by the kinship matrix\n\n\n###Changes Introduced by Seven Bridges\n\n* If the __Output Prefix__ parameter is not given, it will be derived from the name of the __Input VCF file__.\n* EPACTS `single` function uses the _human_g1k_v37.fasta_ FASTA file embedded in the package. To ensure the correct reference usage for files aligned to GRCh38, __Reference FASTA__ File is defined as a required parameter.\n* `--interval-list` parameter which sets a list of intervals as a unit to perform association, was removed due to an error in the toolkit version. For more information on this issue visit the [GitHub issue](https://github.com/statgen/EPACTS/issues/22) on this topic. \n\n\n### Common Issues and Important Notes\n* __Input VCF File__ must be bgzipped and tabixed before running association to allow efficient random access to the file. Use __Tabix BGZIP__ and __Tabix Index__ tools to prepare the input VCF.\n* If the VCF file is separated by chromosome, the VCF file specified in the input argument must contain the string \"chr1\" in the chromosome 1 file, and corresponding chromosome name for other chromosomes. The files' names need to be formatted as - `[prefix]chr1[suffix].vcf.gz, [prefix]chr2[suffix].vcf.gz, ..., [prefix]chr22[suffix].vcf.gz, [prefix]chrX[suffix].vcf.gz.`\n* Sample IDs in the VCF file must be consistent with those from the PED file.\n* EPACTS accepts the PED format supported by MERLIN or PLINK software to represent phenotypes.\n* Currently, EPACTS supports only bi-allelic variants, but it handles SNPs, INDELs, snd SVs.\n* `--anno` parameter is configured for GRCh37, and by default will use the _hg19_gencodeV14.txt.gz_ file to annotate variants. If using files aligned to GRCh38 and annotations are needed, use the __EPACTS Annotate__ with the appropriate annotation files. For more information on EPACTS support of GRCh38 visit the [GitHub issue](https://github.com/statgen/EPACTS/issues/20) on this topic. \n\n\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type \t| Input size \t| # of Samples \t| Contigs | # of CPUs \t| Duration  \t| Cost  \t| Instance (AWS) \t| Attached Storage (AWS) \t|\n|-----------------\t|------------\t|--------------\t|---------|-----------\t|-----------\t|-------\t|----------------\t|------------------------\t|\n| GWAS            \t| 48.1 MiB   \t| 2549         \t| 20      \t|   1   \t| 5min      \t| $0.03 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 293.4 MiB     \t| 2709         \t| 20           \t| \t1| 31min     \t| $0.21 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t     \t|1 | 22h 12min \t| $9.3  \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t     \t| 16 | 2h 7min | $1.4   \t| c4.2xlarge     \t| 1024GB                 \t|", "input": [{"name": "Annotate Each Variant for GRCh37"}, {"name": "Single Chromosome"}, {"name": "Conditional Association Analysis"}, {"name": "Covariate Column(s)"}, {"name": "FORMAT Field"}, {"name": "Inverse-Normal Transformation"}, {"name": "Kinship"}, {"name": "Maximum MAC"}, {"name": "Maximum MAF"}, {"name": "Minimum Call Rate"}, {"name": "Minimum MAC"}, {"name": "Minimum MAF"}, {"name": "Minimum RSQ_HAT"}, {"name": "String Representing Missing Value"}, {"name": "MOSIX Nodes"}, {"name": "Skip Manhattan and QQ plots"}, {"name": "Output Prefix"}, {"name": "Pass filter"}, {"name": "Input Pedigree File"}, {"name": "Phenotype Column"}, {"name": "Reference FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Region"}, {"name": "REML file"}, {"name": "Rule"}, {"name": "Number of CPUs"}, {"name": "Indicator of Separated VCFs"}, {"name": "Single Variant Test"}, {"name": "Unit for Parallel Run"}, {"name": "Input VCF File", "encodingFormat": "application/x-vcf"}], "output": [{"name": "EPACTS Associations table"}, {"name": "Top 5000 Associations"}, {"name": "Plot Files"}, {"name": "Supplementary Files"}, {"name": "Log Files"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2", "https://github.com/statgen/EPACTS/archive/v3.4.2.tar.gz"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/epacts-zoom-plot-3-3-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/epacts-zoom-plot-3-3-0/4", "applicationCategory": "CommandLineTool", "name": "EPACTS Zoom Plot", "description": "EPACTS (Efficient and Parallelizable Association Container Toolbox) is a versatile software pipeline to perform various statistical tests for identifying genome-wide association from sequence data through a user-friendly interface, both to scientific analysts and to method developers.\n\n__EPACTS Zoom Plot__ generates a Zoom plot for significant associations.\n\nThe tool has several inputs:\n\n* __Files produced by EPACTS tests__ - EPACTS.GZ and TOP5000 output files created by the __EPACTS Single Variant Test__\n* __VCF File__ - Input Multi-Sample Variant Call Format file\n* __Gene Prediction File__ - UCSC refFlat format gene prediction files to annotate genes\n* __IDs File__ - File with a set of individual IDs to calculate Linkage Disequilibrium (LD)\n* __Map File__ - Genetic map file showing gene positions on the chromosomes, measured in centimorgans. \n\n\nThe tool will output the Zoom Plot (PDF) for the selected variant(s). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n### Common Use Cases\n\n__EPACTS Zoom Plot__ produces locus zoom plot (PDF) for EPACTS association results from previous EPACTS analyses (top associations, etc.). \n\n__EPACTS Zoom Plot__ shows association P-values on the \u2212log10 scale on the vertical axis, and the chromosomal position along the horizontal axis. To identify SNPs that may be potentially causative, Zoom plot shows the magnitude of association for each SNP and the pairwise LD pattern. It can reveal the extent of the associated region and the location and number of SNPs in strong LD with the index SNP, as well as show strongly associated variants that are weakly correlated, suggesting the presence of multiple independent association signals [1].\n\n\n###Changes Introduced by Seven Bridges\n\n* If the __Output File Prefix__ parameter is not given, it will be derived from the name of the input __VCF file__.\n\n### Common Issues and Important Notes\n\n* __Input VCF File__ must be bgzipped and tabixed before running association to allow efficient random access to the file. Use __Tabix BGZIP__ and __Tabix Index__ tools to prepare the input VCF.\n*  If __Output Prefix__ of EPACTS files produced by previous analyses  is not correctly specified, these files will not be taken into consideration.\n* _Variant Position_ of the variant to zoom on must be in CHR:POS format.\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand instances for a set of samples with different file sizes :\n| Experiment type \t| Input size \t| # of Samples \t| Contigs \t| Duration \t| Cost   \t| Instance (AWS) \t| Attached Storage (AWS) \t|\n|-----------------\t|------------\t|--------------\t|---------\t|----------\t|--------\t|----------------\t|------------------------\t|\n| GWAS            \t| 48.1 MiB   \t| 2549         \t| 20      \t| 45s      \t| $0,006 \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 293.4      \t| 2709         \t| 20      \t|   2min       \t|    $0,014    \t| c4.2xlarge     \t| 1024GB                 \t|\n| GWAS            \t| 751.2 GiB  \t| 2709         \t| all     \t|   4min       \t|   $0,028     \t|    c4.2xlarge     \t| 1024GB                  \t|\n\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n###References\n[1] [LocusZoom: regional visualization of genome-wide association scan results](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2935401/pdf/btq419.pdf)", "input": [{"name": "Cell Type"}, {"name": "Centimorgan Boundary"}, {"name": "EPACTS Associations table"}, {"name": "Gene Prediction File", "encodingFormat": "text/plain"}, {"name": "IDs File"}, {"name": "Map File"}, {"name": "Minimum r2 Threshold"}, {"name": "Output Prefix"}, {"name": "Variant Position"}, {"name": "Region Subset"}, {"name": "Rule"}, {"name": "Separated VCF"}, {"name": "Shade"}, {"name": "Significance Threshold"}, {"name": "SNP ID"}, {"name": "Plot Title"}, {"name": "Top Associations"}, {"name": "VCF File", "encodingFormat": "application/x-vcf"}, {"name": "Base Pair Window"}, {"name": "Top 5000 Associations"}], "output": [{"name": "EPACTS Zoom Plot"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/EPACTS/releases/tag/v3.4.2"], "applicationSubCategory": ["GWAS", "Plotting"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.0"], "dateModified": 1648035757, "dateCreated": 1601930339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/exomiser-12-1-0-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/exomiser-12-1-0-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "Exomiser", "description": "**Exomiser** is a tool for prioritizing variants from WES and WGS data [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Exomiser** can be used to identify candidate causative variants from WES or WGS patient VCF data (supplied as **Input variants**) and phenotype HPO terms (the **HPO terms** input). The tool supports defining the analysis either through analysis files (**Input analysis files**) or by building the analysis YAML file through the GUI wrapper. If an analysis file is provided on input, it will be used for the analysis (with the `--analysis` command line parameter invoked). If more than one analysis file is provided, the `--batch-analysis` command line parameter will be invoked and the summary file for it created by the wrapper itself during task execution. In the absence of an analysis YAML file, the wrapper will attempt to build the analysis file from input parameter values provided by the user.\n\nPlease note that **Exomiser data files** required for running the tool are not provided by Seven Bridges. The data files can be obtained from the [Exomiser FTP site](https://data.monarchinitiative.org/exomiser/latest/). As some of the data sources used by Exomiser come with their own licenses, please ensure that all data source licenses are observed when executing **Exomiser** tasks.\n\nIf providing your own analysis files, please ensure that the analysis file parameters `vcf`, `ped`, and `outputPrefix` do not contain paths (only file names and prefixes should be provided). Conversely, when using the GUI to build the analysis file, please make sure to provide values for all elements of the analysis file, as no default values are used by the wrapper.\n\nExample analysis files can be obtained from the Exomiser GitHub repo [3].\n\n### Changes Introduced by Seven Bridges\n\n* Only the YAML analysis file input format is supported for starting the analysis. The stand-alone command line parameters and the settings file input are not supported, as tool authors recommend using the YAML analysis files.\n* Most of the input parameters for the tool were added to support the building of a YAML analysis file via the Seven Bridges platform front-end.\n\n### Common Issues and Important Notes\n\n* **Input variants**, **Exomiser data files**, **Genome assembly** and **Exomiser data version** inputs are required.\n* If providing your own analysis files, please ensure that analysis file parameters `vcf`, `ped`, and `outputPrefix` do not contain paths (only file names and prefixes should be provided). Conversely, when using the GUI to build an analysis file, please make sure to provide values for all elements of the analysis file, as no defaults are used.\n* Input parameter **Exomiser data version** should match the **Exomiser data files** used.\n* In order to use CADD or REMM scores, the corresponding files must be provided as inputs (**CADD SNV file**, **CADD indel file** and **REMM file**, respectively).\n* If you encounter RAM issues, please consider changing the Java garbage collector (**Use G1GC**) and enabling caching (**Spring cache type** and **Caffeine Spring cache size** inputs). Additionally, please consider setting `analysisMode: PASS_ONLY` in the analysis file (or via the **Analysis mode** input parameter).\n\n\n### Performance Benchmarking\n\n**Exomiser** performance depends on the number of variants in the input data and selected analysis parameters.\n\nAs setting up Exomiser data sources introduces ~5 minutes of task duration overhead, multiple analysis files and samples can be provided at once, to trigger `--batch-analysis` mode. Additionally, reducing the amount of requested disk space for a task will lower execution costs. \n\n* Dataset 1: 18803 variants (WES subset, NA12878) with Exomiser WES template analysis file\n* Dataset 2: 3619471 variants (WGS high-confidence NA12878) with Exomiser WGS template analysis file\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1  | 13 min | $0.09 + $0.03 | c4.2xlarge - 1000 GB EBS | \n| Dataset 2  | 21 min | $0.14 + $0.04 | c4.2xlarge - 1000 GB EBS |\n| Datasets 1 and 2 - batch | 16 min | $0.11 + $0.04 | c4.2xlarge - 1000 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### References\n\n[1] [Exomiser publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5467691/)\n\n[2] [Exomiser documentation](https://exomiser.github.io/Exomiser/manual/7/quickstart/)\n\n[3] [Exomiser GitHub repo examples](https://github.com/exomiser/Exomiser/tree/master/exomiser-cli/src/main/resources/examples)", "input": [{"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Exomiser data files", "encodingFormat": "application/zip"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "Use G1GC"}, {"name": "Pedigree file"}, {"name": "Input analysis files"}, {"name": "Genome assembly"}, {"name": "Exomiser data version"}, {"name": "Exomiser transcript source"}, {"name": "CADD SNV file"}, {"name": "CADD indel file"}, {"name": "REMM file"}, {"name": "Local frequency file"}, {"name": "Spring cache type"}, {"name": "Caffeine Spring cache size"}, {"name": "Enable logging"}, {"name": "Proband"}, {"name": "HPO terms"}, {"name": "Inheritance modes"}, {"name": "Output file name prefix"}, {"name": "Analysis mode"}, {"name": "Frequency sources"}, {"name": "Pathogenicity sources"}, {"name": "Analysis steps"}, {"name": "Output contributing variants only"}, {"name": "Number of genes to output"}, {"name": "Output formats"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Exomiser gene outputs"}, {"name": "Exomiser variant outputs"}, {"name": "Exomiser VCF outputs", "encodingFormat": "application/x-vcf"}, {"name": "Exomiser HTML outputs", "encodingFormat": "text/html"}, {"name": "Exomiser JSON outputs"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/exomiser/Exomiser", "https://github.com/exomiser/Exomiser/releases/tag/12.1.0"], "applicationSubCategory": ["Annotation", "Variant Filtration"], "project": "SBG Public Data", "creator": "Monarch Initiative", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612278461, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/express/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/express/8", "applicationCategory": "CommandLineTool", "name": "eXpress", "description": "eXpress is a quantification tool for probabilistic estimation of abundances of a set of target sequences from sampled subsequences. Example applications include transcript-level RNA-seq quantification, allele-specific/haplotype expression analysis (from RNA-Seq), transcription factor binding quantification in ChIP-Seq, and analysis of metagenomic data. It is based on an online-EM algorithm that results in memory requirements proportional to the total size of the target sequences and time requirements that are proportional to the number of sampled fragments. eXpress is able to resolve multi-mappings of reads across gene families, and does not require a reference genome so that it can be used in conjunction with de novo assemblers such as Trinity. \n\n**Input files**:\n\n* **Target sequences** (required): A file of target sequences in multi-FASTA format.  In the case of RNA-Seq, these are the transcript sequences.\n\n* **Aligned reads** (required): File(s) with reads aligned to the target sequences in BAM or SAM format.  \n\n* **Auxiliary parameter file** (optional): Specifies an auxiliary parameter file produced by a different run of eXpress to be used as the auxiliary parameters for this round. Greatly improves speed and should be used when a subset of the targets or fragments are being used in a second estimation.\n\n* **Haplotype targets** (optional): CSV (comma-separated file) of sets of target IDs (one set per line) specifying which targets represent multiple haplotypes of a single feature (ie, transcript). Useful for allele-specific expression.\n\n**Output files:**  \n\n* **Target abundances** (results.xprs) file is always produced and contains the target abundances and other values calculated based on the input sequences and read alignments. The file has 10 tab-delimited columns, sorted by the bundle_id. \n\n* **Parameter estimates** (params.xprs) file contains the values of the other parameters (besides abundances and counts) estimated by eXpress. If multiple alignment files were provided, a separate parameter output will be produced with a unique index (ie, the second BAM file in the argument list will be named 'params.2.xprs'). \n\n* **Count variance-covariance** (varcov.xprs) file is produced only when the `calc_covars` option flag is used. The file contains the estimated variances and covariances on the counts between pairs of targets that shared multi-mapped reads, primarily to be used in differential expression analysis.\n\n* **Augmented alignment file(s)** (hits.prob.(sam/bam) or hits.samp.(sam/bam)) are additional files produced when switch `output_align` is set to `add_probabilities` or `sample_likelihoods`, respectively. First option outputs files that contain identical copies of all input alignments with an additional XP tag that contains the estimated probability that each alignment of the read (pair) is the \"correct\" one. With second option files contain a single alignment for each fragment sampled at random based on the alignment likelihoods calculated by eXpress.\n\n**Common Issues:**\n\n1. \"**SEVERE**: Cannot add both online and batch rounds\"   \nIt means you have set both batch and online additional rounds of EM algorithm, which is not permitted by eXpress.\n2. \"**SEVERE**: Input file contains no valid alignments\"   \nIt means that selected type for `stranded_reads` option doesn't correspond to aligned reads in BAM/SAM file. Usually, when one selects \"forward\" or \"reverse\" only and reads are paired-end.\n3. \"**SEVERE**: Sequence for target 'targetID' not found in MultiFasta file 'transcriptome.fasta'\"   \nThis error usually means that aligned reads are not aligned only to transcriptome FASTA provided, but to reference genome also. For example, when using TopHat as aligner, option `--transcriptome-only` should be provided, or eXpress will find reads aligned to transcripts that are not in transcriptome. See [this thread](https://groups.google.com/forum/#!topic/express-users/RWP1MjBWGAA).\n4. \"**SEVERE**: Target 'targetID' differs in length between MultiFASTA and alignment (SAM/BAM) files (targetID.size  vs. bamREF.size)\"   \nThis error could be if Windows linebreaks are used, or due to Bowtie not reporting length of contig well. See [this thread](https://groups.google.com/forum/#!category-topic/express-users/questions--support/jts9ppH9K3A).  \n5. \"**WARNING**: Target 'targetID' exists in MultiFASTA but not alignment (SAM/BAM) file\"    \nThis is a warning produced when transcriptome and alignments are not compatible, but it doesn't break the execution.\n6. \"**WARNING**: Not enough fragments observed to accurately learn bias parameters. Either disable bias correction (--no-bias-correct) or provide a file containing auxiliary parameters (--aux-param-file)\"  \nThis error occurs when not enough reads are provided for optimal statistics. Results are given but should be taken with reserve.\n7. \"**WARNING**: It is recommended that at least one additional round be used when outputting alignment probabilities or sampled alignments\"  \nDoesn't brake the execution.", "input": [{"name": "Target sequences (FASTA)", "encodingFormat": "application/x-fasta"}, {"name": "Aligned reads (BAM/SAM)", "encodingFormat": "application/x-sam"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Haplotype targets (CSV)"}, {"name": "Forget param"}, {"name": "Library size"}, {"name": "Max indel size"}, {"name": "Calculate covariance"}, {"name": "Weight of uniform prior"}, {"name": "Number of fragment to process"}, {"name": "Number of fragments for which to calculate params"}, {"name": "No bias correction"}, {"name": "No error model"}, {"name": "Auxiliary parameter file"}, {"name": "Augment alignment"}, {"name": "Reads strandedness"}, {"name": "Additional online rounds"}, {"name": "Additional batch rounds"}], "output": [{"name": "Target abundances"}, {"name": "Parameter estimates"}, {"name": "Count variance-covariance"}, {"name": "Augmented alignment file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/adarob/eXpress"], "applicationSubCategory": ["RNA", "Quantification"], "project": "SBG Public Data", "creator": "Adam Roberts, University of California, Berkeley", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270166, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/extasy/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/extasy/1", "applicationCategory": "CommandLineTool", "name": "eXtasy", "description": "eXtasy is a pipeline for ranking nonsynonymous single nucleotide variants given a specific phenotype. It takes into account the putative deleteriousness of the variant, haploinsufficiency predictions of the underlying gene and the similarity of the given gene to known genes in the given phenotype.", "input": [{"name": "VCF", "encodingFormat": "application/x-vcf"}, {"name": "Prioritisation database"}, {"name": "HPO codes"}, {"name": "HPO terms"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}, {"name": "Extasy RAW output"}], "codeRepository": [], "applicationSubCategory": ["Prioritization", "Annotation"], "project": "SBG Public Data", "creator": "Alejandro Sifrim", "softwareVersion": ["sbg:draft-2"], "dateModified": 1460640041, "dateCreated": 1460640041, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fastp-0-20-1/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fastp-0-20-1/15", "applicationCategory": "CommandLineTool", "name": "Fastp", "description": "**Fastp** is an ultra-fast FASTQ preprocessor with useful quality control and data-filtering features. It can perform quality control, adapter trimming, quality filtering, per-read quality pruning and many other operations with a single scan of the FASTQ data [1]. \nThe authors claim that **Fastp** is faster than other FASTQ pre-processing tools, such as **Trimmomatic** or **Cutadapt** .\n\n\n*The list of  **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n###Common Use Cases\n\n1. **Fastp** can apply multiple filters: \n\n+ Quality filtering is enabled by default, but you can disable it by `-Q` or `disable_quality_filtering`. Currently it supports filtering by limiting the N base number (`-n`, `--n_base_limit`), and the percentage of unqualified bases. \nTo filter reads by its percentage of unqualified bases, the following options should be provided:\n\n    + **Qualified quality (phred) score** (`-q`, `--qualified_quality_phred`) - Default 15 means phred quality >=Q15 is qualified.\n    + **Unqualified limit [%]** (`-u`, `--unqualified_percent_limit`) - How many percent of bases are allowed to be unqualified (0~100). Default value is 40%.\n\n    You can also filter reads by their average quality score with:\n\n    + **Minimum average read quality score** (`-e`, `--average_qual`) -  If one read's average quality score < avg_qual, then this read/pair is discarded. Default value 0 means no requirement.\n\n+ Length filtering is enabled by default, but you can disable it by `-L` or `--disable_length_filtering`. The minimum length requirement is specified with `-l` or `--length_required`. For some applications like small RNA sequencing, you may want to discard the long reads. You can specify `--length_limit` to discard the reads longer than length_limit. The default value 0 means no limitation.\n\n+ Low complexity filter is disabled by default, and you can enable it by `-y` or `--low_complexity_filter`. The complexity is defined as the percentage of a base that is different from its next base (base[i] != base[i+1]). The threshold for the low complexity filter can be specified by `-Y` or `--complexity_threshold`. Its range should be 0~100, and its default value is 30, which means 30% complexity is required.\n\n2. **Fastp** can apply adapter trimming, enabled by default, but you can disable it by `-A` or `--disable_adapter_trimming`. Adapter sequences can be automatically detected for both PE/SE data.\n\n+ For SE data, the adapters are evaluated by analyzing the tails of first ~1M reads. This evaluation may be inaccurate, and you can specify the **Adapter sequence** by `-a` or `--adapter_sequence` option. If **Adapter sequence** is specified, the auto detection for SE data will be disabled.\n\n+ For PE data, the adapters can be detected by per-read overlap analysis, which seeks for the overlap of each pair of reads. This method is robust and fast, so normally you don't have to input the adapter sequence even if you know it. But you can still specify the **Adapter sequence** for read1 by `--adapter_sequence`, and for read2 by `--adapter_sequence_r2`. If **Fastp** fails to find an overlap (e.g. due to low quality bases), it will use these sequences to trim adapters for read1 and read2 respectively.\n\n+ For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by the overlap analysis. However, you can specify **Detect adapter for paired-end data via tail end analysis** (`--detect_adapter_for_pe`) to enable it.\n\n+ For PE data, **Fastp** will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n\n+ The most widely used adapter is the Illumina TruSeq adapter. If your data is from the TruSeq library, you can add `--adapter_sequence`=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA, `--adapter_sequence_r2`=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT to your command lines, or enable auto detection for PE data by specifying `--detect_adapter_for_pe`.\n\n+ **Fastp** contains some built-in known adapter sequences for better auto-detection. You can also specify `--adapter_fasta` to provide a FASTA file to tell **Fastp** to trim multiple adapters in this FASTA file. The adapter sequence in this file should be at least 6bp long, otherwise it will be skipped. And you can provide whatever you want to trim, rather than regular sequencing adapters (i.e. polyA). **Fastp** first trims the auto-detected adapter or the adapter sequences given by `--adapter_sequence` | `--adapter_sequence_r2`, then trims the adapters given by `--adapter_fasta` one by one. The sequence distribution of trimmed adapters can be found at the **HTML/JSON reports**.\n\n3. **Fastp** supports per read sliding window cutting by evaluating the mean quality scores in the sliding window. From v0.19.6, **Fastp** supports 3 different operations, and you can enable one or all of them:\n\n+ **Cut front** (`-5`, `--cut_front`)-  Move a sliding window from front (5') to tail, drop the bases in the window if their mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The leading N bases are also trimmed. Use **Cut front window size** (`--cut_front_window_size`) to set the window size, and **Cut front mean quality** ( `--cut_front_mean_quality`) to set the mean quality threshold. If the window size is 1, this is similar to the **Trimmomatic** LEADING method.\n\n+ **Cut tail** (`-3`, `--cut_tail`) -  Move a sliding window from tail (3') to front, drop the bases in the window if their mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The trailing N bases are also trimmed. Use **Cut tail window size** (`--cut_tail_window_size`) to set the window size, and **Cut tail mean quality** (`--cut_tail_mean_quality`) to set the mean quality threshold. If the window size is 1, this is similar to the **Trimmomatic** TRAILING method.\n\n+ **Cut right** (`-r`, `--cut_right`) - Move a sliding window from front to tail, if meet one window with mean quality < threshold, drop the bases in the window and the right part, and then stop. Use **Cut right window size** (`--cut_right_window_size`) to set the window size, and **Cut right mean quality** (`--cut_right_mean_quality)` to set the mean quality threshold. This is similar to the **Trimmomatic** SLIDINGWINDOW method\n\nWARNING:  All these three operations will interfere with deduplication for SE data, and `--cut_front` or -`-cut_right` may also interfere with deduplication for PE data. The deduplication algorithms rely on the exact match of coordination regions of the grouped reads/pairs.\n\n\nIf `--cut_right` is enabled, then there is no need to enable `--cut_tail`, since the former is more aggressive. If `--cut_right` is enabled together with `--cut_front`, `--cut_front` will be performed before `--cut_right` to avoid dropping whole reads due to the low quality starting bases.\n\nPlease note that `--cut_front` will interfere with deduplication for both PE/SE data, and `--cut_tail` will interfere with deduplication for SE data, since the deduplication algorithms rely on the exact match of coordination regions of the grouped reads/pairs.\n\nIf you don't set window size and mean quality threshold for these functions respectively, **Fastp** will use the values from `-W`, `--cut_window_size` and `-M`, `--cut_mean_quality`.\n\n4. **Fastp** performs overlap analysis for PE data, which tries to find an overlap of each pair of reads. If a proper overlap is found, it can correct mismatched base pairs in overlapped regions of paired end reads, if one base has high quality while the other one has ultra low quality. If a base is corrected, the quality of its paired base will be assigned to it so that they will share the same quality. This function is not enabled by default, specify **Enable base correction in overlapped regions** ( `-c`, `--correction`) to enable it. This function is based on overlapping detection, which has adjustable parameters `--overlap_len_require` (default 30), `--overlap_diff_limit` (default 5) and `--overlap_diff_limit_percent` (default 20%). Please note that the reads should meet these three conditions simultaneously.\n\n5. **Fastp** supports global trimming, which means trim all reads at the front or the tail. This function is useful since sometimes you want to drop some cycles of a sequencing run. For example, the last cycle of Illumina sequencing usually has low quality, and it can be dropped using the **Number of bases to trim from end of R1 reads** ( `-t 1` , `--trim_tail1=1`) option.\n\n+ For read1 or SE data, the front/tail trimming settings are provided using `-f`, `--trim_front1` and `-t`, `--trim_tail1`.\n\n+ For read2 of PE data, the front/tail trimming settings are provided using `-F`, `--trim_front2` and `-T`, `--trim_tail2`. But if these options are not specified, they will be the same as read1 options, which means `trim_front2 = trim_front1` and `trim_tail2 = trim_tail1`.\n\n+ If you want to trim the reads to maximum length, you can specify `-b`, `--max_len1` for read1, and `-B`, `--max_len2` for read2. If `--max_len1` is specified but `--max_len2` is not, `--max_len2` will be the same as `--max_len1`. For example, if `--max_len1` is specified and read1 is longer than `--max_len1`, **Fastp** will trim read1 at its tail to make it as long as `--max_len1`.\n\nPlease note that the trimming for `--max_len` limitation will be applied at the last step. Here are **Fastp**'s processing steps that may affect the read lengths, in this order:\n\na. UMI preprocessing (`--umi`)\n\nb. Global trimming at front (`--trim_front`)\n\nc. Global trimming at tail (`--trim_tail`)\n\nd. Quality pruning at 5' (`--cut_front`)\n\ne. Quality pruning by sliding window (`--cut_right`)\n\nf. Quality pruning at 3' (`--cut_tail`)\n\ng. Trim polyG (`--trim_poly_g`, enabled by default for NovaSeq/NextSeq data)\n\nh. Trim adapter by overlap analysis (enabled by default for PE data)\n\ni. Trim adapter by adapter sequence (`--adapter_sequence`, `--adapter_sequence_r2`. For PE data, this step is skipped if previous step succeeded)\n\nj. Trim polyX (`--trim_poly_x`)\n\nk. Trim to max length (`--max_len`)\n\n\n6. For Illumina NextSeq/NovaSeq data, polyG can happen in read tails since G means no signal in the Illumina two-color systems. **Fastp** can detect the polyG in read tails and trim them. This feature is enabled for NextSeq/NovaSeq data by default, and you can specify **Force polyG tail trimming** ( `-g`, `--trim_poly_g`) to enable it for any data, or specify **Disable polyG tail trimming** ( `-G`, `--disable_trim_poly_g`) to disable it. NextSeq/NovaSeq data is detected by the machine ID in the FASTQ records. A minimum length can be set with **Minimum length to detect polyG read tail** ( `--poly_g_min_len`) for **Fastp** to detect polyG. This value is 10 by default.\n\n7. PolyX tail trimming is similar to polyG tail trimming, but is disabled by default. Use **Enable polyX trimming in 3' reads** ( `-x`,  `--trim_poly_x`) to enable it. **Minimum length to detect polyX in read tail** ( `--poly_x_min_len` ) can be set with for **Fastp** to detect polyX. This value is 10 by default. When polyG tail trimming and polyX tail trimming are both enabled, **Fastp** will perform polyG trimming first, then perform polyX trimming. This setting is useful for trimming the tails having polyX (i.e. polyA) before polyG. PolyG is usually caused by sequencing artifacts, while polyA can be commonly found from the tails of mRNA-Seq reads.\n\n8. Unique molecular identifier (UMI) is useful for duplication elimination and error correction based on generating consensus of reads originated from the same DNA fragment. It's usually used in deep sequencing applications like ctDNA sequencing. For Illumina platforms, UMIs can commonly be integrated in two different places: index or head of read. You can activate this with the **Enable UMI preprocessing**( `-U`, `--umi`)  option, and specify **UMI location** ( `--umi_loc`)  which can be one of the following:\n\n+ index1 - the first index is used as the UMI. If the data is PE, this UMI will be used for both read1/read2.\n\n+ index2 - the second index is used as the UMI. PE data only, this UMI will be used for both read1/read2.\n\n+ read1 - the head of read1 is used as the UMI. If the data is PE, this UMI will be used for both read1/read2.\n\n+ read2 - the head of read2 is used as the UMI. PE data only, this UMI will be used for both read1/read2.\n\n+ per_index, index1_index2 is used as the UMI for both read1/read2.\n\n+ per_read defines umi1 as the head of read1, and umi2 as the head of read2. umi1_umi2 is used as the UMI for both read1/read2.\n\nIf **UMI location** is specified with read1, read2 or per_read, the length of the UMI should be specified with **Length of UMI in read1/read2** ( `--umi_len`).\n\n**Fastp** will extract the UMIs, and append them to the first part of read names, so the UMIs will also be presented in SAM/BAM records. If the UMI is in the reads, then it will be shifted from the read so that the read will become shorter. If the UMI is in the index, it will be kept.\n\nA prefix can be specified with **UMI prefix** ( `--umi_prefix`). If a prefix is specified, an underline will be used to connect it and the UMI. For example, UMI=AATTCCGG, prefix=UMI, then the final string presented in the name will be `UMI_AATTCCGG`.\n\nIf the **UMI location** is read1/read2/per_read, **Fastp** can skip some bases after the UMI to trim the UMI separator and A/T tailing. Specify **Number of bases to skip following UMI** (`--umi_skip`) to enable the number of bases to skip. By default it is not enabled.\n\n\n9. For parallel processing of FASTQ files (i.e. alignment in parallel), **Fastp** supports splitting the output into multiple files. The splitting can work with two different modes: by limiting file number or by limiting lines of each file. These two modes cannot be enabled together. The file names of these split files will have a sequential number prefix, adding to the original file name specified by `--out1` or `--out2`, and the width of the prefix is controlled by the `-d` or `--split_prefix_digits` option. For example, `--split_prefix_digits=4`, `--out1=out.fq`, `--split=3`, then the output files will be `0001.out.fq,0002.out.fq,0003.out.fq`\n\n+ Splitting by limiting file number - Use **Number of files to split output FASTQ to** ( `-s`,  `--split`) to specify how many files you want to have. **Fastp** evaluates the read number of a FASTQ by reading its first ~1M reads. This evaluation is not accurate so the file sizes of the last several files can be a little different (a bit bigger or smaller). For best performance, it is suggested to specify the file number to be a multiple of the thread number.\n\n+ Splitting by limiting the lines of each file - Use **Number of lines in split output FASTQ files** ( `-S`,  `--split_by_lines`) to limit the lines of each file. The last files may have smaller sizes since usually the input file cannot be perfectly divided. The actual file lines may be a little greater than the value specified by `--split_by_lines` since **Fastp** reads and writes data by blocks (a block = 1000 reads).\n\n10. Overrepresented sequence analysis is disabled by default, you can specify **Enable overrepresented sequence analysis** ( `-p`, `--overrepresentation_analysis`) to enable it. For consideration of speed and memory, **Fastp** only counts sequences with length of 10bp, 20bp, 40bp, 100bp or (cycles - 2 ). By default, **Fastp** uses 1/20 reads for sequence counting, and you can change these settings by specifying the **Read sampling for overrepresentation analysis** ( `-P`,  `--overrepresentation_sampling`) option. For example, if you set 100, only 1/100 reads will be used for counting, and if you set 1, all reads will be used but it will be extremely slow. The default value of 20 is a balance of speed and accuracy. **Fastp** not only gives the counts of overrepresented sequences, but also provides the information on how they distribute over cycles. A figure is provided for each detected overrepresented sequence, from which you can know where this sequence is mostly found.\n\n11. For PE input, **Fastp** supports stitching them by specifying the **Merge overlapping paired-end reads** ( `-m`, `--merge`) option. In this merging mode:\n\n+ `--merged_out` should be given to specify the file to store merged reads. The merged reads are also filtered.\n\n+ `--out1` and `--out2` will be the reads that cannot be merged successfully, but both pass all the filters.\n\n+ `--unpaired1` will be the reads that cannot be merged, `read1` passes filters but `read2` doesn't.\n\n+ `--unpaired2` will be the reads that cannot be merged, `read2` passes filters but `read1` doesn't.\n\n+ `--include_unmerged` can be enabled to make reads of `--out1`, `--out2`, `--unpaired1` and `--unpaired2` redirected to `--merged_out`. So you will get a single output file. This option is disabled by default.\n\n\n`--failed_out` can still be given to store the reads (either merged or unmerged) that failed to pass filters.\n\nIn the output file, a tag like `merged_xxx_yyy` will be added to each read name to indicate how many base pairs are from read1 and from read2, respectively. For example, `@NB551106:9:H5Y5GBGX2:1:22306:18653:13119 1:N:0:GATCAG merged_150_15` means that 150bp are from read1, and 15bp are from read2. **Fastp** prefers the bases in read1 since they usually have higher quality than read2.\n\nSame as the base correction feature, this function is also based on overlapping detection, which has adjustable parameters **Minimum length for detecting overlapped regions** ( `--overlap_len_require`) (default 30), **Maximum number of mismatched bases for detecting PE overlaps** ( `--overlap_diff_limit`) (default 5) and **Maximum % of mismatched bases for detecting PE overlaps** ( `--overlap_diff_limit_percent`) (default 20%). Please note that the reads should meet these three conditions simultaneously.\n\n###Changes Introduced by Seven Bridges\n\n* **Interleaved FASTQ file output** parameter is added. If set to True, there will be one interleaved output file at **Trimmed reads** output port will contain one interleaved output file containing both read 1 and read 2.\n\nInput parameters omitted from this tool:\n* `--stdin` - Omitted\n* `--stdout` - Omitted\n* `--dont_overwrite` - Unnecessary for common use on Seven Bridges platforms.\n* `--cut_by_quality5` -  Deprecated in this tool version\n* `--cut_by_quality3` - Deprecated in this tool version\n* `--cut_by_quality_aggressive` - Deprecated in this tool version\n* `--discard_unmerged` - Deprecated in this tool version\n* `--help` - Omitted\n\n\n###Common Issues and Important Notes\n\n+ If you are working with paired-end reads, you need to specify an adapters for both reads in a pair. This is also valid if you are using adapter fasta file, you need to provide the same file for both ends.\n\n+ Please check if the **Paired-end** metadata field is specified for each file before running **Fastp** with paired-end inputs.\n\n+ Note: The tool installation was done using commit 424900e376a02033a32b623bc5c836897f6b7e5a (https://github.com/OpenGene/fastp/commit/424900e376a02033a32b623bc5c836897f6b7e5a) instead of the official release 0.20.1\n\n###Performance Benchmarking\n\nBenchmarking was performed on c4.2xlarge (8 vCPUs, 15 GB Memory) and c5.4xlarge (16 vCPUs, 32 GB Memory) AWS instances. Trimming clearly benefits from multiple threads. Below is a table describing the runtimes and task costs for a couple of samples with different file sizes.\n\n\n| Input size (GB) | # of threads | Duration | Cost ($) | Instance (AWS) |\n|:---------------:|:------------:|:--------------:|:--------:|:--------------:|\n| 2 x 18.5 | 8 | 22 min | 0.15 | c4.2xlarge |\n|2 x 18.5 | 16 | 14 min | 0.16 | c5.4xlarge |\n| 2 x 39.7 | 8 | 48 min | 0.32 | c4.2xlarge |\n| 2 x 39.7 | 16 | 31 min | 0.35 | c5.4xlarge |\n| 2 x 209.8 | 8 | 4 h 20 min | 1.72 | c4.2xlarge |\n| 2 x 209.8 | 16 | 2 h 29 min | 1.70 | c5.4xlarge |\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [the Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*          \n\n\n###References\n\n[1] [Fastp documentation](https://github.com/OpenGene/fastp)", "input": [{"name": "Input FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Output file name for unpaired reads (PE 1)"}, {"name": "Output file name for separate unpaired reads (PE 2)"}, {"name": "Output file name for reads that failed filters"}, {"name": "Merge overlapping paired-end reads"}, {"name": "Output file name for merged reads"}, {"name": "Include unmerged reads in merged mode output"}, {"name": "Input uses phred64 scoring"}, {"name": "Compression level for GZIP output"}, {"name": "Interleaved FASTQ file input"}, {"name": "Number of reads/pairs to process"}, {"name": "Output verbose log information"}, {"name": "Disable adapter trimming"}, {"name": "Adapter sequence"}, {"name": "Adapter sequence for R2 data"}, {"name": "Adapter FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Detect adapter for paired-end data via tail end analysis"}, {"name": "Number of bases to trim from start of R1 reads"}, {"name": "Number of bases to trim from end of R1 reads"}, {"name": "Maximum length of R1 reads"}, {"name": "Number of bases to trim from start of R2 reads"}, {"name": "Number of bases to trim from end of R2 reads"}, {"name": "Maximum length of R2 reads"}, {"name": "Force polyG tail trimming"}, {"name": "Minimum length to detect polyG read tail"}, {"name": "Disable polyG tail trimming"}, {"name": "Enable polyX trimming in 3' reads"}, {"name": "Minimum length to detect polyX in read tail"}, {"name": "Cut front"}, {"name": "Cut tail"}, {"name": "Cut right"}, {"name": "Cut window size"}, {"name": "Cut mean quality"}, {"name": "Cut front window size"}, {"name": "Cut front mean quality"}, {"name": "Cut tail window size"}, {"name": "Cut tail mean quality"}, {"name": "Cut right window size"}, {"name": "Cut right mean quality"}, {"name": "Disable quality filtering"}, {"name": "Qualified quality (phred) score"}, {"name": "Unqualified limit [%]"}, {"name": "Maximum allowed number of N bases"}, {"name": "Minimum average read quality score"}, {"name": "Disable read length filtering"}, {"name": "Minimum read length allowed"}, {"name": "Maximum read length allowed"}, {"name": "Enable low complexity read filtering"}, {"name": "Read complexity threshold"}, {"name": "File with index1 barcodes for filtering"}, {"name": "File with index2 barcodes for filtering"}, {"name": "Filter by index threshold"}, {"name": "Enable base correction in overlapped regions"}, {"name": "Minimum length for detecting overlapped regions"}, {"name": "Maximum number of mismatched bases for detecting PE overlaps"}, {"name": "Maximum % of mismatched bases for detecting PE overlaps"}, {"name": "Enable UMI preprocessing"}, {"name": "UMI location"}, {"name": "Length of UMI in read1/read2"}, {"name": "UMI prefix"}, {"name": "Number of bases to skip following UMI"}, {"name": "Enable overrepresented sequence analysis"}, {"name": "Read sampling for overrepresentation analysis"}, {"name": "Report title (with quotes)"}, {"name": "Number of worker threads"}, {"name": "Number of files to split output FASTQ to"}, {"name": "Number of lines in split output FASTQ files"}, {"name": "Number of digits in output number prefixes"}, {"name": "Memory per job"}, {"name": "Interleaved FASTQ file output"}], "output": [{"name": "HTML report", "encodingFormat": "text/html"}, {"name": "JSON report"}, {"name": "Trimmed reads"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/OpenGene/fastp", "https://github.com/OpenGene/fastp", "https://github.com/OpenGene/fastp", "https://github.com/OpenGene/fastp"], "applicationSubCategory": ["FASTQ Processing", "Quality Control", "Read Trimming", "Filtering", "Utilities"], "project": "SBG Public Data", "creator": "Shifu Chen", "softwareVersion": ["v1.1"], "dateModified": 1648468513, "dateCreated": 1618325866, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fastqc-0-11-4/20", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fastqc-0-11-4/20", "applicationCategory": "CommandLineTool", "name": "FastQC", "description": "FastQC reads a set of sequence files and produces a quality control (QC) report from each one. These reports consist of a number of different modules, each of which will help identify a different type of potential problem in your data. \n\nSince it's necessary to convert the tool report in order to show them on Seven Bridges platform, it's recommended to use [FastQC Analysis workflow instead](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/fastqc-analysis/). \n\nFastQC is a tool which takes a FASTQ file and runs a series of tests on it to generate a comprehensive QC report.  This report will tell you if there is anything unusual about your sequence.  Each test is flagged as a pass, warning, or fail depending on how far it departs from what you would expect from a normal large dataset with no significant biases.  It is important to stress that warnings or even failures do not necessarily mean that there is a problem with your data, only that it is unusual.  It is possible that the biological nature of your sample means that you would expect this particular bias in your results.", "input": [{"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Kmers"}, {"name": "Limits", "encodingFormat": "text/plain"}, {"name": "Adapters", "encodingFormat": "text/plain"}, {"name": "Contaminants", "encodingFormat": "text/plain"}, {"name": "Format"}, {"name": "Nogroup"}, {"name": "Nano"}, {"name": "Casava"}, {"name": "Threads"}, {"name": "Quiet"}, {"name": "Number of CPUs."}, {"name": "Amount of memory allocated per job execution."}], "output": [{"name": "Report zip", "encodingFormat": "application/zip"}, {"name": "Report HTMLs", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Quality Control", "FASTQ Processing"], "project": "SBG Public Data", "creator": "Babraham Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799142, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fastqc-0-11-9/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fastqc-0-11-9/6", "applicationCategory": "CommandLineTool", "name": "FastQC CWL 1.0", "description": "**FastQC** reads a set of sequence files and produces a quality control report from each one. These reports consist of a number of different modules, each of which will help identify a different type of potential problem in your data [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n\n### Common Use Cases\n\n**FastQC** is a tool which takes a FASTQ file and runs a series of tests on it to generate a comprehensive QC report.  This report will tell you if there is anything unusual about your sequence.  Each test is flagged as a pass, warning, or fail depending on how far it departs from what you would expect from a normal large dataset with no significant biases.  It is important to stress that warnings or even failures do not necessarily mean that there is a problem with your data, only that it is unusual.  It is possible that the biological nature of your sample means that you would expect this particular bias in your results.\n\n- In order to search the library for specific adapter sequences, a TXT file with the adapter sequences needs to be provided on the **Adapters** (`--adapters/-a`) input port. The lines in the file must follow the name [tab] sequence format.\n- In order to search the overrepresented sequences for specific contaminants, a TXT file with the contaminant sequences needs to be provided on the **Contaminants** (`--contaminants/-c`) input port. The lines in the file must follow the name [tab] sequence format.\n- In order to determine the warn/error limits for the various modules or remove some modules from the output, a TXT file with sets of criteria needs to be provided on the **Limits** (`--limits/-l`) input port. The lines in the file must follow the parameter [tab] warn/error [tab] value format.\n\n\n### Changes introduced by Seven Bridges\n\nNo modifications to the original tool representation have been made.\n\n\n### Common Issues and Important Notes\n\nUser can manually set CPU/Memory requirements by providing values on the **Number of CPUs** and **Memory per job [MB]** input ports. If neither of these two is provided and number of threads has been specified on the **Threads** (`--threads/-t`) input port, both CPU and memory per job will be determined by the provided number of threads; if neither number of CPUs/memory per job nor number of threads have been provided as inputs, the CPU and memory requirements will be determined according to the number of files provided on the **Input file** input port.\n\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the size of the input FASTQ files. The following table showcases the metrics for the task running on the c4.2xlarge on-demand AWS instance. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\nFastq file 1 size(.gz) | Fastq file 2 size(.gz) | Duration | Cost | Instance type (AWS) |\n|---------------|-----------------|-----------|--------|-----|\n| 700 MB | 680 MB | 2 min. | $0.01 | c4.2xlarge |\n| 12.6 GB | 12.6 GB | 25 min. | $0.11 | c4.2xlarge |\n| 23.8 GB | 26.8 GB | 1 hour |$0.27 | c4.2xlarge |\n| 47.9 GB | 48.9 GB | 1 hour 40 min. | $0.44 | c4.2xlarge |\n\n### References\n\n[1] [FastQC GitHub](https://github.com/s-andrews/FastQC/blob/master/fastqc)", "input": [{"name": "Adapters", "encodingFormat": "text/plain"}, {"name": "Casava"}, {"name": "Contaminants", "encodingFormat": "text/plain"}, {"name": "Number of CPUs"}, {"name": "Format"}, {"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Kmers"}, {"name": "Limits", "encodingFormat": "text/plain"}, {"name": "Memory per job [MB]"}, {"name": "Min length"}, {"name": "Nano"}, {"name": "No filter"}, {"name": "Nogroup"}, {"name": "Quiet"}, {"name": "Threads"}], "output": [{"name": "HTML reports", "encodingFormat": "text/html"}, {"name": "Report zip", "encodingFormat": "application/zip"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["FASTQ Processing", "Quality Control", "Utilities"], "project": "SBG Public Data", "creator": "Babraham Institute", "softwareVersion": ["v1.0"], "dateModified": 1649156263, "dateCreated": 1582563129, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fastqvalidator-0-1-1/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fastqvalidator-0-1-1/11", "applicationCategory": "CommandLineTool", "name": "fastQValidator", "description": "__fastQValidator__ checks format correctness of paired-end and single-end FASTQ files [1].\n\nIf two paired-end FASTQ files are given as input, __fastQValidator__ have the option of checking if FASTQ files have the corresponding order of reads. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n- To successfully run __fastQValidator__, just supply it with one single-end, or pair of paired-end FASTQ files. As an output, TXT report for every given FASTQ file will be generated. Also, __fastQValidator__ returns input FASTQ files as an output.\n\n### Changes Introduced by Seven Bridges\n\n*  __SBG FASTQ Checker__ tool is added to __fastQValidator__ tool. If the **Run FastQChecker** parameter is set to True - this tool runs before  __fastQValidator__, and checks if FASTQ files have the corresponding order of reads. If files don't have the corresponding order of reads, __fastQValidator__ does not run, and the task fails.\n* Additional parameter **Step** is added. This parameter is used by __SBG FASTQ Checker__ tool, and defines a step for comparing read names of the corresponding reads. This parameter represents frequency of checking pairs of read names.  \n* __fastQValidator__ is able to check not just one, but two paired-end FASTQ files.\n\n### Common Issues and Important Notes\n\n* __fastQValidator__ can't run with multi-fastq files.\n* Parameter **Validate consecutive reads** which ensures that consecutive reads have the same sequence identifier cannot be used if **Unique sequence identifier check** parameter is specified.\n\n### Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of __fastQValidator__ for a couple of different samples, executed on the AWS cloud instances:\n\n|  Input size | Input file type | Duration |  Cost |  Instance (AWS) |\n|:--------------:|:----------:|:----------:|:----------:|:-----------:|\n|     998 MB |    FASTQ   |    3 min   | $0.02   | c4.2xlarge (8 CPUs) |\n|    4.5 GB |    FASTQ   |   11 min   | $0.06   | c4.2xlarge (8 CPUs) |\n|       17 GB |   FASTQ |     43 min    | $0.22  | c4.2xlarge (8 CPUs) |\n|      679 MB  |    FASTQ.GZ  |    7 min   | $0.04   | c4.2xlarge (8CPUs) |\n|     12 GB   |    FASTQ.GZ  |     28 min    |  $0.33   | c4.8xlarge (36 CPUs) |\n|     200 GB   |    FASTQ.GZ  |     6h 34min    |  $15.31   | r4.8xlarge (32 CPUs) |\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1] [fastQValidator documentation](https://genome.sph.umich.edu/wiki/FastQValidator#Optional_Parameters)", "input": [{"name": "FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Minimum read length"}, {"name": "Max errors"}, {"name": "Print maximum number of errors"}, {"name": "Ignore errors"}, {"name": "Base composition"}, {"name": "Phred quality"}, {"name": "Unique sequence identifier check"}, {"name": "Check eof"}, {"name": "Validate consequtive reads"}, {"name": "Step"}, {"name": "Run FastQChecker"}], "output": [{"name": "FQValidator output files", "encodingFormat": "text/plain"}, {"name": "FASTQ files", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/fastQValidator", "https://github.com/statgen/fastQValidator/tree/master/src"], "applicationSubCategory": ["FASTQ Processing", "Utilities"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648048649, "dateCreated": 1618325230, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/filter-non-conversion/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/filter-non-conversion/1", "applicationCategory": "CommandLineTool", "name": "Filter Non Conversion", "description": "**Filter Non Conversion** is used for filtering incomplete bisulfite conversion in non-CG context from Bismark BAM files. It is a part of the *Bismark* toolkit which represents a set of tools for the time-efficient analysis of Bisulfite-Seq (BS-Seq) data. Bismark performs alignments of bisulfite-treated reads to a reference genome and cytosine methylation calls at the same time.\n\n**Filter Non Conversion** has one input:\n\n* **Input BAM file** is a file (BAM) produced by the tools **Bismark** or **Bismark Deduplicate**. Bam file should be sorted by name. This file is mandatory.\n\nThe tool creates three output files:\n\n* **Filtered BAM** is a file in BAM format which contains reads that passed filter.\n\n* **Removed sequences** is a file in BAM format, too. That represents sequences which are removed.\n\n* **Report** is the text file of how many sequences have been analysed and removed.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Filter Non Conversion** examines the methylation calls of reads, or read pairs for paired-end sequencing, and filters out reads that exceed a certain **Threshold** (`--threshold`) of methylated calls in non-CG context (the default is 3). This is extended to the possibility of setting a percentage **Percentage cutoff** (`--percentage_cutoff`) of methylation in non-CG context for any given read.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n* This kind of filtering is not advisable and will introduce biases if analysis considers the organisms with any appreciable levels of non-CG methylation (e.g. most plants [2]).\n\n* **Filter Non Conversion** in paired-end analysis expects Read 1 and Read 2 to follow each other in consecutive lines. The file should be sorted by read name.\n\n### Performance Benchmarking\n\n| Size of BAM file| Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- |\n| 1.9 GB | c4.2xlarge | 10m | $0.09 |\n| 27 GB | c4.2xlarge | 5h 14m | $2.8 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.\n\n[2] Niederhuth C. E., Schmitz R. J. , [Putting DNA methylation in context: from genomes to gene expression in plants](https://doi.org/10.1016/j.bbagrm.2016.08.009). Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms, Volume 1860, Issue 1, 2017, Pages 149-156, ISSN 1874-9399.", "input": [{"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "Single-end"}, {"name": "Threshold"}, {"name": "Consecutive"}, {"name": "Percentage cutoff"}, {"name": "Minimum count"}], "output": [{"name": "Filtered BAM", "encodingFormat": "application/x-bam"}, {"name": "Removed sequences", "encodingFormat": "application/x-bam"}, {"name": "Report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Filtering"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561065, "dateCreated": 1519389842, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/filter-vep-94-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/filter-vep-94-2/3", "applicationCategory": "CommandLineTool", "name": "Filter VEP", "description": "**Filter VEP** tool is a perl script named filter_vep and is part of **ensembl-vep distribution** and can be used to filter VEP output.\n\nThe only required input for this tool is **Input file** (`--input_file`), which should originate from a **Variant Effect Predictor** run (either a VCF file or a tab-delimited VEP output file).\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Filter VEP** tool is intended for filtering **Variant Effect Predictor** output (annotated VCF files or tab-delimited format) using customizable filters, as described in [documentation](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html).\n\n### Changes Introduced by Seven Bridges\n\n- The `--ontology` parameter was not included in the Seven Bridges version of the tool since it requires a database connection.\n\n### Common Issues and Important Notes\n\n* **Input file** (`--input_file`) which should originate from a **Variant Effect Predictor** run (either a VCF file or a tab-delimited VEP output file).\n* Please note that the script expects VEP annotations to be added under CSQ entry in the INFO field. If a custom name for the VEP annotation field was used, annotation subfields will not be accessible for filtering unless **VCF INFO field with VEP annotations** (`--vcf_info_field`) input parameter is set appropriately.\n* Multiple filters are treated as if joined with logical ANDs (all filters must pass for a line to be printed) [1]. Each filter should be given as an unquoted string.\n* **List allowed fields from the input file** (`--list`) parameter will only print available filtering fields (standard VCF fields and annotation fields present in the INFO column) and should not be used with the parameter **Filters to apply** (`--filter`).\n* If providing custom filters, please make sure that all column names (fields to filter on) and filter expressions are correct, as currently, the tool does not fail if a filter is impossible to apply, producing unfiltered outputs.\n\n### Performance Benchmarking\n\nFiltering VEP-annotated NA12878 genome (GRCh38, ~220 Mb as VCF.GZ file) took 12 minutes ( at a price of $0.05) using three filters.\n\n### References\n\n[1] [Documentation](http://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Input file format"}, {"name": "Output file name"}, {"name": "Overwrite existing output file"}, {"name": "Filters to apply"}, {"name": "List allowed fields from the input file"}, {"name": "Only count matches"}, {"name": "Output only matched blocks"}, {"name": "VCF INFO field with VEP annotations"}], "output": [{"name": "Output file"}, {"name": "File listing available filter fields", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-vep"], "applicationSubCategory": ["VCF Processing", "Variant Filtration"], "project": "SBG Public Data", "creator": "Ensembl", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648040158, "dateCreated": 1539879968, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/flexbar/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/flexbar/7", "applicationCategory": "CommandLineTool", "name": "Flexbar 2.5", "description": "The program Flexbar preprocesses high-throughput sequencing data efficiently. It demultiplexes barcoded runs and removes adapter sequences. Moreover, trimming and filtering features are provided. Flexbar increases read mapping rates and improves genome as well as transcriptome assemblies. It supports next-generation sequencing data in fasta and fastq format, e.g. from Illumina and the Roche 454 platform.", "input": [{"name": "Input Read file", "encodingFormat": "text/fastq"}, {"name": "Adapters", "encodingFormat": "application/x-fasta"}, {"name": "Barcodes", "encodingFormat": "application/x-fasta"}, {"name": "Separate barcode reads", "encodingFormat": "text/fastq"}, {"name": "Prefix for output"}, {"name": "Barcode trim-end Mode"}, {"name": "Barcode tail length"}, {"name": "Barcode min overlap"}, {"name": "Barcode threshold"}, {"name": "Barcode keep"}, {"name": "Barcode match"}, {"name": "Barcode mismatch"}, {"name": "Barcode gap"}, {"name": "Adapter sequence"}, {"name": "Adapter trim-end Mode"}, {"name": "Adapter tail length"}, {"name": "Adapter min overlap"}, {"name": "Adapter threshold"}, {"name": "Adapter match"}, {"name": "Adapter mismatch"}, {"name": "Adapter gap"}, {"name": "Max uncalled"}, {"name": "Pre trim left"}, {"name": "Pre trim right"}, {"name": "Pre trim phred"}, {"name": "Post trim length"}, {"name": "Min read length"}, {"name": "Log level"}, {"name": "Single reads"}, {"name": "Length dist"}, {"name": "Fasta output"}, {"name": "Number tags"}, {"name": "Remove tags"}, {"name": "Barcode unassigned reads"}, {"name": "Adapter reverse complement"}, {"name": "Adapter relaxed"}, {"name": "Random tags"}, {"name": "Threads"}], "output": [{"name": "Processed reads", "encodingFormat": "text/fastq"}, {"name": "Length distribution (optional)"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/seqan/flexbar/wiki", "https://github.com/seqan/flexbar/wiki/Manual"], "applicationSubCategory": ["FASTQ Processing", "Read Trimming"], "project": "SBG Public Data", "creator": "Johannes T. R\u00f6hr/Freie Universit\u00e4t Berlin", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1459789092, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/fbd/80", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/fbd/80", "applicationCategory": "CommandLineTool", "name": "Freebayes", "description": "FreeBayes is a Bayesian genetic variant detector designed to find small polymorphisms, specifically SNPs (single-nucleotide polymorphisms), indels (insertions and deletions), MNPs (multi-nucleotide polymorphisms), and complex events (composite insertion and substitution events) smaller than the length of a short-read sequencing alignment.\n\nFreeBayes incorporates a number of features in order to reduce the complexity of variant detection for researchers and developers:\n\n1. Indel realignment is accomplished internally using a read-independent method, and issues resulting from discordant alignments are dramatically reduced through the direct detection of haplotypes.\n2. The need for base quality recalibration is avoided through the direct detection of haplotypes. Sequencing platform errors tend to cluster (e.g. at the ends of reads), and generate unique, non-repeating haplotypes at a given locus.\n3. Variant quality recalibration is avoided by incorporating a number of metrics, such as read placement bias and allele balance, directly into the Bayesian model.\n\n###Common Issues\nFASTA INDEX FILE is not required. If it is not provided (as secondary file), it is generated. When it is provided, the tool runs faster.\nBAM INDEX FILES are not required, but when not provided (as separate input), random access is disabled.\nVARIANT INPUT INDEX FILE is required (as secondary file). It should be generated using Tabix BGZIP and Tabix Index file.\nREGION parameter should match data present in variant input file, both chromosome and positions.", "input": [{"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Bam files", "encodingFormat": "application/x-bam"}, {"name": "Region", "encodingFormat": "text/x-bed"}, {"name": "VCF Name"}, {"name": "Samples", "encodingFormat": "text/plain"}, {"name": "Populations"}, {"name": "CNV map", "encodingFormat": "text/x-bed"}, {"name": "Region"}, {"name": "Variant input", "encodingFormat": "application/x-vcf"}, {"name": "Only use input alleles"}, {"name": "Haplotype basis alleles", "encodingFormat": "application/x-vcf"}, {"name": "Report all haplotype alleles"}, {"name": "Report monomorphic"}, {"name": "Pvar"}, {"name": "Theta"}, {"name": "Ploidy"}, {"name": "Pooled discrete"}, {"name": "Pooled continuous"}, {"name": "Use reference allele"}, {"name": "Reference quality"}, {"name": "No snps"}, {"name": "No indels"}, {"name": "No mnps"}, {"name": "No complex"}, {"name": "Use best n alleles"}, {"name": "Max complex gap"}, {"name": "Haplotype length"}, {"name": "Min repeat size"}, {"name": "Min repeat entropy"}, {"name": "No partial observations"}, {"name": "Dont left align indels"}, {"name": "Use duplicate reads"}, {"name": "Min mapping quality"}, {"name": "Min base quality"}, {"name": "Min supporting allele qsum"}, {"name": "Min supporting mapping qsum"}, {"name": "Mismatch base quality threshold"}, {"name": "Read mismatch limit"}, {"name": "Read max mismatch fraction"}, {"name": "Read snp limit"}, {"name": "Read indel limit"}, {"name": "Standard filters"}, {"name": "Min alternate fraction"}, {"name": "Min alternate count"}, {"name": "Min alternate qsum"}, {"name": "Min alternate total"}, {"name": "Min coverage"}, {"name": "Max coverage"}, {"name": "No population priors"}, {"name": "Hwe priors off"}, {"name": "Binomial obs priors off"}, {"name": "Allele balance priors off"}, {"name": "Observation bias"}, {"name": "Base quality cap"}, {"name": "Prob contamination"}, {"name": "Legacy gls"}, {"name": "Contamination estimates"}, {"name": "Report genotype likelihood max"}, {"name": "Genotyping max iterations"}, {"name": "Genotyping max banddepth"}, {"name": "Posterior integration limits"}, {"name": "Exclude unobserved genotypes"}, {"name": "Genotype variant threshold"}, {"name": "Use mapping quality"}, {"name": "Harmonic indel quality"}, {"name": "Read dependence factor"}, {"name": "Genotype qualities"}, {"name": "Bam index files"}, {"name": "GVCF"}, {"name": "GVCF chunk"}], "output": [{"name": "Variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/ekg/freebayes", "https://github.com/ekg/freebayes", "https://github.com/ekg/freebayes", "https://github.com/ekg/freebayes"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Erik Garrison", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038485, "dateCreated": 1457032402, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-applybqsr-4-2-0-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-applybqsr-4-2-0-0/5", "applicationCategory": "CommandLineTool", "name": "GATK ApplyBQSR", "description": "The **GATK ApplyBQSR** tool recalibrates the base quality scores of an input BAM or CRAM file containing reads.\n\nThis tool performs the second pass in a two-stage process called Base Quality Score Recalibration (BQSR). Specifically, it recalibrates the base qualities of the input reads based on the recalibration table produced by the **GATK BaseRecalibrator** tool [1]. The goal of this procedure is to correct for systematic bias that affect the assignment of base quality scores by the sequencer. The first pass consists of calculating error empirically and finding patterns in how error varies with basecall features over all bases. The relevant observations are written to a recalibration table. The second pass consists of applying numerical corrections to each individual basecall based on the patterns identified in the first step (recorded in the recalibration table) and write out the recalibrated data to a new BAM or CRAM file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n###Common Use Cases\n\n* The **GATK ApplyBQSR** tool requires a BAM or CRAM file on its **Input alignments** (`--input`) input and the covariates table (= recalibration file) generated by the **BaseRecalibrator** tool on its **BQSR recal file** input (`--bqsr-recal-file`). The reference sequence is required on the **Reference** (`--reference`) input of the tool when using CRAM format. The tool generates a new alignments file which contains recalibrated read data on its **Output recalibrated alignments** output.\n\n* Usage example\n\n```\n gatk --java-options \"-Xmx2048M\" ApplyBQSR \\\n   --R reference.fasta \\\n   --I input.bam \\\n   --bqsr-recal-file recalibration.table \\\n   --O output.bam\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from the **Input alignments**. If the **Sample ID** value is not set, the name will be inherited from the **Input alignments** file name. In case there are multiple files on the **Input alignments** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. Moreover,  **.recalibrated** will be added before the extension of the output file name.\n\n* The user has a possibility to specify the output file format using the **Output file format** argument. Otherwise, the output file format will be the same as the format of the input file.\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals file** and **Include intervals string** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals file** and **Exclude intervals string** options.\n\n* The following GATK parameters were excluded from the tool wrapper: `--add-output-vcf-command-line`, `--arguments_file`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-variant-index`, `--create-output-variant-md5`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--lenient`, `--QUIET`, `--sites-only-vcf-output`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048 MB.\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the **Memory per job** in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100 MB. \n* Note: GATK tools that take in mapped read data expect a BAM file as the primary format [2]. More on GATK requirements for mapped sequence data formats can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats).\n* Note: **Input alignments** should have corresponding index files in the same folder. \n* Note: **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n* Note: This tool replaces the use of PrintReads for the application of base quality score recalibration as practiced in earlier versions of GATK (2.x and 3.x) [1].\n* Note: You should only run **ApplyBQSR** with the covariates table created from the input BAM or CRAM file [1].\n* Note: Original qualities can be retained in the output file under the \"OQ\" tag if desired. See the **Emit original quals** (`--emit-original-quals`) argument for details [1].\n* Note: This **Read Filter** (`--read-filter`) is automatically applied to the data by the Engine before processing by **ApplyBQSR** [1]: **WellformedReadFilter**\n* Note: If the **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", the **Library** (`--library`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", the **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformUnitReadFilter\", the **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", the **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", the **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", the **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", the **Read name** (`--read-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", the **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", the **Sample** (`--sample`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"IntervalOverlapReadFilter\", the **Keep intervals** (`--keep-intervals`) option must be set to some value.\n* Note: The following options are valid only if the appropriate **Read filter** (`--read-filter`) is specified: **Ambig filter bases** (`--ambig-filter-bases`), **Ambig filter frac** (`--ambig-filter-frac`), **Max fragment length** (`--max-fragment-length`), **Min fragment length** (`--min-fragment-length`), **Keep intervals** (`--keep-intervals`), **Library** (`--library`), **Maximum mapping quality** (`--maximum-mapping-quality`), **Minimum mapping quality** (`--minimum-mapping-quality`),  **Mate too distant length** (`--mate-too-distant-length`), **Do not require soft clips** (`--dont-require-soft-clips-both-ends`), **Filter too short** (`--filter-too-short`), **Platform filter name** (`--platform-filter-name`), **Black listed lanes** (`--black-listed-lanes`), **Read group black list** (`--read-group-black-list`), **Keep read group** (`--keep-read-group`), **Max read length** (`--max-read-length`), **Min read length** (`--min-read-length`), **Read name** (`--read-name`), **Keep reverse strand only** (`--keep-reverse-strand-only`), **Sample** (`--sample`), **Invert soft clip ratio filter** (`--invert-soft-clip-ratio-filter`), **Soft clipped leading trailing ratio** (`--soft-clipped-leading-trailing-ratio`), **Soft clipped ratio threshold** (`--soft-clipped-ratio-threshold`) . See the description of each parameter for information on the associated **Read filter**.\n* Note: The wrapper has not been tested for the SAM file type on the **Input alignments** input port.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK ApplyBQSR** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost (on-demand) | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  2.1 GB |   9min   | ~0.09$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  6.4 GB |   23min   | ~0.21$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 10.7 GB |  33min  | ~0.30$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 21.2 GB |  1h 9min  | ~0.62$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n###Portability\n\n**GATK ApplyBQSR** is tested with cwltool version: \"3.0.20201203173111\"\n\n###References\n\n[1] [GATK ApplyBQSR](https://gatk.broadinstitute.org/hc/en-us/articles/360056968652-ApplyBQSR)\n\n[2] [GATK Mapped sequence data formats](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats)", "input": [{"name": "Add output SAM program record"}, {"name": "BQSR recal file"}, {"name": "Create output BAM/CRAM index"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default read filters"}, {"name": "Emit original quals"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "Exclude intervals string"}, {"name": "Global Qscore prior"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Preserve qscores less than"}, {"name": "Quantize quals"}, {"name": "Read filter"}, {"name": "Read validation stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Round down quantized"}, {"name": "Sequence dictionary"}, {"name": "Static quantized quals"}, {"name": "Use original qualities"}, {"name": "CPU per job"}, {"name": "Output name prefix"}, {"name": "Output file format"}, {"name": "Disable BAM index caching"}, {"name": "Read index"}, {"name": "Seconds between progress updates"}, {"name": "Disable read filter"}, {"name": "Ambig filter bases"}, {"name": "Ambig filter frac"}, {"name": "Max fragment length"}, {"name": "Min fragment length"}, {"name": "Keep intervals"}, {"name": "Library"}, {"name": "Maximum mapping quality"}, {"name": "Minimum mapping quality"}, {"name": "Mate too distant length"}, {"name": "Dont require soft clips both ends"}, {"name": "Filter too short"}, {"name": "Platform filter name"}, {"name": "Black listed lanes"}, {"name": "Read group black list"}, {"name": "Keep read group"}, {"name": "Max read length"}, {"name": "Min read length"}, {"name": "Read name"}, {"name": "Keep reverse strand only"}, {"name": "Sample"}, {"name": "Invert soft clip ratio filter"}, {"name": "Soft clipped leading trailing ratio"}, {"name": "Soft clipped ratio threshold"}, {"name": "Create output bam md5"}], "output": [{"name": "Output recalibrated alignments", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648047600, "dateCreated": 1634729367, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-applybqsr-4-1-0-0/21", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-applybqsr-4-1-0-0/21", "applicationCategory": "CommandLineTool", "name": "GATK ApplyBQSR CWL1.0", "description": "The **GATK ApplyBQSR** tool recalibrates the base quality scores of an input BAM or CRAM file containing reads.\n\nThis tool performs the second pass in a two-stage process called Base Quality Score Recalibration (BQSR). Specifically, it recalibrates the base qualities of the input reads based on the recalibration table produced by the **GATK BaseRecalibrator** tool. The goal of this procedure is to correct systematic biases that affect the assignment of base quality scores by the sequencer. The first pass consists of calculating the error empirically and finding patterns in how the error varies with basecall features over all bases. The relevant observations are written to the recalibration table. The second pass consists of applying numerical corrections to each individual basecall, based on the patterns identified in the first step (recorded in the recalibration table), and writing out the recalibrated data to a new BAM or CRAM file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK ApplyBQSR** tool requires a BAM or CRAM file on its **Input alignments** (`--input`) input and the covariates table (= recalibration file) generated by the **BaseRecalibrator** tool on its **BQSR recal file** input (`--bqsr-recal-file`). If the input alignments file is in the CRAM format, the reference sequence is required on the **Reference** (`--reference`) input of the tool. The tool generates a new alignments file which contains recalibrated read data on its **Output recalibrated alignments** output.\n\n* Usage example\n\n```\n gatk --java-options \"-Xmx2048M\" ApplyBQSR \\\n   --reference reference.fasta \\\n   --input input.bam \\\n   --bqsr-recal-file recalibration.table \\\n   --output output.bam\n\n```\n\n* Original qualities can be retained in the output file under the \"OQ\" tag if desired. See the **Emit original quals** (`--emit-original-quals`) argument for details [1].\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from the input alignments file. If the **Sample ID** value is not set, the name will be inherited from the input alignments file name. In case there are multiple files on the **Input alignments** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. Moreover,  **recalibrated** will be added before the extension of the output file name.\n\n* The user has a possibility to specify the output file format using the **Output file format** argument. Otherwise, the output file format will be the same as the format of the input file.\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals string** and **Include intervals file** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals string** and **Exclude intervals file** options.\n\n* The following GATK parameters were excluded from the tool wrapper: `--add-output-vcf-command-line`, `--arguments_file`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-bam-md5`, `--create-output-variant-index`, `--create-output-variant-md5`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--lenient`, `--QUIET`, `--sites-only-vcf-output`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048MB.\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the Memory per job in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100MB. \n* Note: GATK tools that take in mapped read data expect a BAM file as the primary format [2]. More on GATK requirements for mapped sequence data formats can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats).\n* Note: **Input alignments** should have corresponding index files in the same folder. \n* Note: **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n* Note: This tool replaces the use of PrintReads for the application of base quality score recalibration as practiced in earlier versions of GATK (2.x and 3.x) [1].\n* Note: You should only run **ApplyBQSR** with the covariates table created from the input BAM or CRAM file [1].\n* Note: This **Read Filter** (`--read-filter`) is automatically applied to the data by the Engine before processing by **ApplyBQSR** [1]: **WellformedReadFilter**\n* Note: If the **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", the **Library** (`--library`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", the **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to\"PlatformUnitReadFilter\", the **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", the **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", the **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", the **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", the **Read name** (`--read-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", the **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", the **Sample** (`--sample`) option must be set to some value.\n* Note: The following options are valid only if an appropriate **Read filter** (`--read-filter`) is specified: **Ambig filter bases** (`--ambig-filter-bases`), **Ambig filter frac** (`--ambig-filter-frac`), **Max fragment length** (`--max-fragment-length`), **Maximum mapping quality** (`--maximum-mapping-quality`), **Minimum mapping quality** (`--minimum-mapping-quality`),  **Do not require soft clips** (`--dont-require-soft-clips-both-ends`), **Filter too short** (`--filter-too-short`), **Min read length** (`--min-read-length`). See the description of each parameter for information on the associated **Read filter**.\n* Note: The wrapper has not been tested for the SAM file type on the **Input alignments** input port.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK ApplyBQSR** for a couple of different samples, executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost (on-demand) | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  2.2 GB |   8min   | ~0.07$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  6.6 GB |   23min   | ~0.21$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 11 GB |  37min  | ~0.33$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 22 GB |  1h 16min  | ~0.68$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK ApplyBQSR](https://gatk.broadinstitute.org/hc/en-us/articles/360036725911-ApplyBQSR)\n\n[2] [GATK Mapped sequence data formats](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats)", "input": [{"name": "Add output SAM program record"}, {"name": "Ambig filter bases"}, {"name": "Ambig filter frac"}, {"name": "Black listed lanes"}, {"name": "BQSR recal file"}, {"name": "Create output BAM/CRAM index"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default read filters"}, {"name": "Dont require soft clips both ends"}, {"name": "Emit original quals"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "Exclude intervals string"}, {"name": "Filter too short"}, {"name": "Global Qscore prior"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Keep read group"}, {"name": "Keep reverse strand only"}, {"name": "Library"}, {"name": "Max fragment length"}, {"name": "Max read length"}, {"name": "Maximum mapping quality"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Min read length"}, {"name": "Minimum mapping quality"}, {"name": "Platform filter name"}, {"name": "Preserve qscores less than"}, {"name": "Quantize quals"}, {"name": "Read filter"}, {"name": "Read group black list"}, {"name": "Read name"}, {"name": "Read validation stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Round down quantized"}, {"name": "Sample"}, {"name": "Sequence dictionary"}, {"name": "Static quantized quals"}, {"name": "Use original qualities"}, {"name": "CPU per job"}, {"name": "Output name prefix"}, {"name": "Output file format"}, {"name": "Disable BAM index caching"}, {"name": "Read index"}, {"name": "Seconds between progress updates"}, {"name": "Disable read filter"}], "output": [{"name": "Output recalibrated alignments", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648047601, "dateCreated": 1617275900, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-applyrecalibration/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-applyrecalibration/10", "applicationCategory": "CommandLineTool", "name": "GATK ApplyRecalibration", "description": "Overview\n\nThis tool performs the second pass in a two-stage process called VQSR; the first pass is performed by the VariantRecalibrator tool. In brief, the first pass consists of creating a Gaussian mixture model by looking at the distribution of annotation values over a high quality subset of the input call set, and then scoring all input variants according to the model. The second pass consists of filtering variants based on score cutoffs identified in the first pass.\n\nUsing the tranche file and recalibration table generated by the previous step, the ApplyRecalibration tool looks at each variant's VQSLOD value and decides which tranche it falls in. Variants in tranches that fall below the specified truth sensitivity filter level have their FILTER field annotated with the corresponding tranche level. This will result in a call set that is filtered to the desired level but retains the information necessary to increase sensitivity if needed.\n\nTo be clear, please note that by \"filtered\", we mean that variants failing the requested tranche cutoff are marked as filtered in the output VCF; they are not discarded.\n\nVQSR is probably the hardest part of the Best Practices to get right, so be sure to read the method documentation, parameter recommendations and tutorial to really understand what these tools and how to use them for best results on your own data.\n\nInput\nThe raw input variants to be filtered.\nThe recalibration table file that was generated by the VariantRecalibrator tool.\nThe tranches file that was generated by the VariantRecalibrator tool.\n\nOutput\nA recalibrated VCF file in which each variant of the requested type is annotated with its VQSLOD and marked as filtered if the score is below the desired quality level.\n\nUsage example for filtering SNPs\n\n java -Xmx3g -jar GenomeAnalysisTK.jar \\\n   -T ApplyRecalibration \\\n   -R reference.fasta \\\n   -input NA12878.HiSeq.WGS.bwa.cleaned.raw.subset.b37.vcf \\\n   --ts_filter_level 99.0 \\\n   -tranchesFile path/to/output.tranches \\\n   -recalFile path/to/output.recal \\\n   -mode SNP \\\n   -o path/to/output.recalibrated.filtered.vcf\n \nCaveats\n\nThe tranche values used in the example above is only a general example. You should determine the level of sensitivity that is appropriate for your specific project. Remember that higher sensitivity (more power to detect variants, yay!) comes at the cost of specificity (more false negatives, boo!). You have to choose at what point you want to set the tradeoff.\nIn order to create the tranche reporting plots (which are only generated for SNPs, not indels!) Rscript needs to be in your environment PATH (this is the scripting version of R, not the interactive version). See http://www.r-project.org for more info on how to download and install R.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Input", "encodingFormat": "application/x-vcf"}, {"name": "Recal File"}, {"name": "Tranches File"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Ignore Filter"}, {"name": "Mode"}, {"name": "Ts Filter Level"}, {"name": "Memory overhead per job"}], "output": [{"name": "VCF"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799804, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-4-0-applyvqsr/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-4-0-applyvqsr/17", "applicationCategory": "CommandLineTool", "name": "GATK ApplyVQSR", "description": "Apply a score cutoff to filter variants based on a recalibration table.\n\n###**Overview**  \n\nThis tool performs the second pass in a two-stage process called Variant Quality Score Recalibration (VQSR). Specifically, it applies filtering to the input variants based on the recalibration table produced in the first step by VariantRecalibrator and a target sensitivity value, which the tool matches internally to a VQSLOD score cutoff based on the model's estimated sensitivity to a set of true variants.\n\nThe filter determination is not just a pass/fail process. The tool evaluates for each variant which \"tranche\", or slice of the dataset, it falls into in terms of sensitivity to the truthset. Variants in tranches that fall below the specified truth sensitivity filter level have their FILTER field annotated with the corresponding tranche level. This results in a callset that is filtered to the desired level but retains the information necessary to increase sensitivity if needed.\n\nTo be clear, please note that by \"filtered\", we mean that variants failing the requested tranche cutoff are marked as filtered in the output VCF; they are not discarded unless the option to do so is specified.\n\n####**Summary of the VQSR procedure** \n\nThe purpose of variant recalibration is to assign a well-calibrated probability to each variant call in a call set. These probabilities can then be used to filter the variants with a greater level of accuracy and flexibility than can typically be achieved by traditional hard-filter (filtering on individual annotation value thresholds). The first pass consists of building a model that describes how variant annotation values co-vary with the truthfulness of variant calls in a training set, and then scoring all input variants according to the model. The second pass simply consists of specifying a target sensitivity value (which corresponds to an empirical VQSLOD cutoff) and applying filters to each variant call according to their ranking. The result is a VCF file in which variants have been assigned a score and filter status.\n\nVQSR is probably the hardest part of the Best Practices to get right, so be sure to read the method documentation, parameter recommendations and tutorial to really understand what these tools do and how to use them for best results on your own data.\n\n###**Inputs**  \n- The raw input variants to be filtered.\n- The recalibration table file that was generated by the VariantRecalibrator tool.\n- The tranches file that was generated by the VariantRecalibrator tool. \n\n###**Outputs**  \n- A recalibrated VCF file in which each variant of the requested type is annotated with its VQSLOD and marked as filtered if the score is below the desired quality level. \n\n###**Usage example**\n\n####**Applying rcelibration/filtering to SNPs**\n\n    ./gatk-launch ApplyVQSR \\\n      -R reference.fasta \\\n      -V input.vcf \\\n      -O output.vcf \\\n      --ts_filter_level 99.0 \\\n      -tranchesFile output.tranches \\\n      --recalFile output.recal \\\n      -mode SNP\n\n####**Allele-specific version of the SNP filtering (beta)**\n\n     ./gatk-launch ApplyVQSR \\\n       -R reference.fasta \\\n       -V input.vcf \\\n       -O output.vcf \\\n       -AS \\\n       --ts_filter_level 99.0 \\\n       -tranchesFile output.AS.tranches \\\n       --recalFile output.AS.recal \\\n       -mode SNP\n\nNote that the tranches and recalibration files must have been produced by an allele-specific run of VariantRecalibrator. Also note that the AS_culprit, AS_FilterStatus, and AS_VQSLOD fields will have placeholder values (NA or NaN) for alleles of a type that have not yet been processed by ApplyRecalibration. The spanning deletion allele (*) will not be recalibrated because it represents missing data. Its VQSLOD will remain NaN, and its culprit and FilterStatus will be NA.\n\nEach allele will be annotated by its corresponding entry in the AS_FilterStatus INFO field annotation. Allele-specific VQSLOD and culprit are also carried through from VariantRecalibrator, and stored in the AS_VQSLOD and AS_culprit INFO fields, respectively. The site-level filter is set to the most lenient of any of the allele filters. That is, if one allele passes, the whole site will be PASS. If no alleles pass, the site-level filter will be set to the lowest sensitivity tranche among all the alleles.\n\n###**Caveats**\n\n- The tranche values used in the example above are only meant to be a general example. You should determine the level of sensitivity that is appropriate for your specific project. Remember that higher sensitivity (more power to detect variants, yay!) comes at the cost of specificity (more false negatives, boo!). You have to choose at what point you want to set the tradeoff.\n- In order to create the tranche reporting plots (which are only generated for SNPs, not indels!) the Rscript executable needs to be in your environment PATH (this is the scripting version of R, not the interactive version).\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Recal File"}, {"name": "Variants", "encodingFormat": "application/x-vcf"}, {"name": "Add Output Sam Program Record"}, {"name": "Cloud Index Prefetch Buffer"}, {"name": "Cloud Prefetch Buffer"}, {"name": "Create Output Bam Index"}, {"name": "Create Output Bam Md5"}, {"name": "Create Output Variant Index"}, {"name": "Create Output Variant Md5"}, {"name": "Disable Bam Index Caching"}, {"name": "Disable Read Filter"}, {"name": "Disable Sequence Dictionary Validation"}, {"name": "Disable Tool Default Read Filters"}, {"name": "Exclude Filtered"}, {"name": "Ignore All Filters"}, {"name": "Ignore Filter"}, {"name": "Input"}, {"name": "Interval Exclusion Padding"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Lenient"}, {"name": "Mode"}, {"name": "Quiet"}, {"name": "Read Filter"}, {"name": "Read Index"}, {"name": "Read Validation Stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Seconds Between Progress Updates"}, {"name": "Tranches File"}, {"name": "Truth Sensitivity Filter Level"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Use Allele Specific Annotations"}, {"name": "Verbosity"}, {"name": "Lod Cutoff"}, {"name": "Ambig Filter Frac"}, {"name": "Max Fragment Length"}, {"name": "Library"}, {"name": "Maximum Mapping Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Dont Require Soft Clips Both Ends"}, {"name": "Filter Too Short"}, {"name": "Platform Filter Name"}, {"name": "Black Listed Lanes"}, {"name": "Read Group Black List"}, {"name": "Keep Read Group"}, {"name": "Max Read Length"}, {"name": "Min Read Length"}, {"name": "Read Name"}, {"name": "Keep Reverse"}, {"name": "Sample"}, {"name": "Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Exclude Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Intervals String"}, {"name": "Exclude Intervals String"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}, {"name": "Cpu Per Job"}, {"name": "Ambig Filter Bases"}], "output": [{"name": "VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1501857800, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-baserecalibrator-4-2-0-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-baserecalibrator-4-2-0-0/4", "applicationCategory": "CommandLineTool", "name": "GATK BaseRecalibrator", "description": "**GATK BaseRecalibrator** generates a recalibration table based on various covariates for input mapped read data [1]. It performs the first pass of the Base Quality Score Recalibration (BQSR) by assessing base quality scores of the input data.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n###Common Use Cases\n\n* The **GATK BaseRecalibrator** tool requires input mapped read data whose quality scores need to be assessed on its **Input alignments** (`--input`) input, a database of known polymorphic sites to skip over on its **Known sites** (`--known-sites`) input, and a reference file on its **Reference** (`--reference`) input. On its **Output recalibration report** output, the tool generates a GATK report file with many tables: the list of arguments, the quantized qualities table, the recalibration table by read group, the recalibration table by quality score,\nthe recalibration table for all the optional covariates [1].\n\n* Usage example:\n\n```\ngatk --java-options \"-Xmx2048M\" BaseRecalibrator \\\n   --input my_reads.bam \\\n   --reference reference.fasta \\\n   --known-sites sites_of_variation.vcf \\\n   --known-sites another/optional/setOfSitesToMask.vcf \\\n   --output recal_data.table\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from **Input alignments**. If the **Sample ID** value is not set, the name will be inherited from the **Input alignments** file name. In case there are multiple files on the **Input alignments** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. Moreover,  **.recal_data** will be added before the extension of the output file name which is **CSV** by default.\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals file** and **Include intervals string** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals file** and **Exclude intervals string** options.\n\n* The following GATK parameters were excluded from the tool wrapper: `--add-output-sam-program-record`, `--add-output-vcf-command-line`, `--arguments_file`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-bam-index`, `--create-output-bam-md5`, `--create-output-variant-index`, `--create-output-variant-md5`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--lenient`, `--QUIET`, `--sites-only-vcf-output`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048 MB.\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the **Memory per job** in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100 MB. \n* Note: GATK tools that take in mapped read data expect a BAM file as the primary format [2]. More on GATK requirements for mapped sequence data formats can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats).\n* Note: **Known sites**, **Input alignments** should have corresponding index files in the same folder. \n* Note: **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n* Note: These **Read Filters** (`--read-filter`) are automatically applied to the data by the Engine before processing by **BaseRecalibrator** [1]: **NotSecondaryAlignmentReadFilter**, **PassesVendorQualityCheckReadFilter**, **MappedReadFilter**, **MappingQualityAvailableReadFilter**, **NotDuplicateReadFilter**, **MappingQualityNotZeroReadFilter**, **WellformedReadFilter**\n* Note: If the **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", the **Library** (`--library`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", the **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformUnitReadFilter\", the **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", the **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", the **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", the **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", the **Read name** (`--read-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", the **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", the **Sample** (`--sample`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"IntervalOverlapReadFilter\", the **Keep intervals** (`--keep-intervals`) option must be set to some value.\n* Note: The following options are valid only if the appropriate **Read filter** (`--read-filter`) is specified: **Ambig filter bases** (`--ambig-filter-bases`), **Ambig filter frac** (`--ambig-filter-frac`), **Max fragment length** (`--max-fragment-length`), **Min fragment length** (`--min-fragment-length`), **Keep intervals** (`--keep-intervals`), **Library** (`--library`), **Maximum mapping quality** (`--maximum-mapping-quality`), **Minimum mapping quality** (`--minimum-mapping-quality`),  **Mate too distant length** (`--mate-too-distant-length`), **Do not require soft clips** (`--dont-require-soft-clips-both-ends`), **Filter too short** (`--filter-too-short`), **Platform filter name** (`--platform-filter-name`), **Black listed lanes** (`--black-listed-lanes`), **Read group black list** (`--read-group-black-list`), **Keep read group** (`--keep-read-group`), **Max read length** (`--max-read-length`), **Min read length** (`--min-read-length`), **Read name** (`--read-name`), **Keep reverse strand only** (`--keep-reverse-strand-only`), **Sample** (`--sample`), **Invert soft clip ratio filter** (`--invert-soft-clip-ratio-filter`), **Soft clipped leading trailing ratio** (`--soft-clipped-leading-trailing-ratio`), **Soft clipped ratio threshold** (`--soft-clipped-ratio-threshold`) . See the description of each parameter for information on the associated **Read filter**.\n* Note: The wrapper has not been tested for the SAM file type on the **Input alignments** input port, nor for the BCF file type on the **Known sites** input port.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK BaseRecalibrator** for a couple of different samples, executed on AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost (on-demand) | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  2.1 GB |   13min   | ~0.11$ | c4.2xlarge | \n|     RNA-Seq     |  6.4 GB |   22min   | ~0.20$ | c4.2xlarge | \n|     RNA-Seq     | 10.7 GB |  31min  | ~0.28$ | c4.2xlarge | \n|     RNA-Seq     | 21.2 GB |  50min  | ~0.45$ | c4.2xlarge |\n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n###Portability\n\n**GATK BaseRecalibrator** is tested with cwltool version: \"3.0.20201203173111\"\n\n###References\n\n[1] [GATK BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/articles/360056969412-BaseRecalibrator)\n\n[2] [GATK Mapped sequence data formats](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats)", "input": [{"name": "Binary tag name"}, {"name": "BQSR BAQ gap open penalty"}, {"name": "Default base qualities"}, {"name": "Deletions default quality"}, {"name": "Disable read filter"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default read filters"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "Exclude intervals string"}, {"name": "Indels context size"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Insertions default quality"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Known sites", "encodingFormat": "text/x-bed"}, {"name": "Low quality tail"}, {"name": "Maximum cycle value"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Mismatches context size"}, {"name": "Mismatches default quality"}, {"name": "Preserve qscores less than"}, {"name": "Quantizing levels"}, {"name": "Read filter"}, {"name": "Read validation stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Sequence dictionary"}, {"name": "Use original qualities"}, {"name": "Output name prefix"}, {"name": "CPU per job"}, {"name": "Disable BAM index caching"}, {"name": "Seconds between progress updates"}, {"name": "Read index"}, {"name": "Ambig filter bases"}, {"name": "Ambig filter frac"}, {"name": "Max fragment length"}, {"name": "Min fragment length"}, {"name": "Keep intervals"}, {"name": "Library"}, {"name": "Maximum mapping quality"}, {"name": "Minimum mapping quality"}, {"name": "Mate too distant length"}, {"name": "Dont require soft clips both ends"}, {"name": "Filter too short"}, {"name": "Platform filter name"}, {"name": "Black listed lanes"}, {"name": "Read group black list"}, {"name": "Keep read group"}, {"name": "Max read length"}, {"name": "Min read length"}, {"name": "Read name"}, {"name": "Keep reverse strand only"}, {"name": "Sample"}, {"name": "Invert soft clip ratio filter"}, {"name": "Soft clipped leading trailing ratio"}, {"name": "Soft clipped ratio threshold"}], "output": [{"name": "Output recalibration report"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648047600, "dateCreated": 1634729322, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-baserecalibrator-4-1-0-0/23", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-baserecalibrator-4-1-0-0/23", "applicationCategory": "CommandLineTool", "name": "GATK BaseRecalibrator CWL1.0", "description": "**GATK BaseRecalibrator** generates a recalibration table based on various covariates for input mapped read data [1]. It performs the first pass of the Base Quality Score Recalibration (BQSR) by assessing base quality scores of the input data.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK BaseRecalibrator** tool requires the input mapped read data whose quality scores need to be assessed on its **Input alignments** (`--input`) input, the database of known polymorphic sites to skip over on its **Known sites** (`--known-sites`) input and a reference file on its **Reference** (`--reference`) input. On its **Output recalibration report** output, the tool generates a GATK report file with many tables: the list of arguments, the quantized qualities table, the recalibration table by read group, the recalibration table by quality score,\nthe recalibration table for all the optional covariates [1].\n\n* Usage example:\n\n```\ngatk --java-options \"-Xmx2048M\" BaseRecalibrator \\\n   --input my_reads.bam \\\n   --reference reference.fasta \\\n   --known-sites sites_of_variation.vcf \\\n   --known-sites another/optional/setOfSitesToMask.vcf \\\n   --output recal_data.table\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from the input alignment file. If the **Sample ID** value is not set, the name will be inherited from the input alignment file name. In case there are multiple files on the **Input alignments** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. Moreover,  **recal_data** will be added before the extension of the output file name which is **CSV** by default.\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals string** and **Include intervals file** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals string** and **Exclude intervals file** options.\n\n* The following GATK parameters were excluded from the tool wrapper: `--add-output-sam-program-record`, `--add-output-vcf-command-line`, `--arguments_file`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-bam-index`, `--create-output-bam-md5`, `--create-output-variant-index`, `--create-output-variant-md5`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--lenient`, `--QUIET`, `--sites-only-vcf-output`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048MB.\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the Memory per job in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100MB. \n* Note: GATK tools that take in mapped read data expect a BAM file as the primary format [2]. More on GATK requirements for mapped sequence data formats can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats).\n* Note: **Known sites**, **Input alignments** should have corresponding index files in the same folder. \n* Note: **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n* Note: These **Read Filters** (`--read-filter`) are automatically applied to the data by the Engine before processing by **BaseRecalibrator** [1]: **NotSecondaryAlignmentReadFilter**, **PassesVendorQualityCheckReadFilter**, **MappedReadFilter**, **MappingQualityAvailableReadFilter**, **NotDuplicateReadFilter**, **MappingQualityNotZeroReadFilter**, **WellformedReadFilter**\n* Note: If the **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", the **Library** (`--library`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", the **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to\"PlatformUnitReadFilter\", the **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", the **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", the **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", the **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", the **Read name** (`--read-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", the **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", the **Sample** (`--sample`) option must be set to some value.\n* Note: The following options are valid only if the appropriate **Read filter** (`--read-filter`) is specified: **Ambig filter bases** (`--ambig-filter-bases`), **Ambig filter frac** (`--ambig-filter-frac`), **Max fragment length** (`--max-fragment-length`), **Maximum mapping quality** (`--maximum-mapping-quality`), **Minimum mapping quality** (`--minimum-mapping-quality`),  **Do not require soft clips** (`--dont-require-soft-clips-both-ends`), **Filter too short** (`--filter-too-short`), **Min read length** (`--min-read-length`). See the description of each parameter for information on the associated **Read filter**.\n* Note: The wrapper has not been tested for the SAM file type on the **Input alignments** input port, nor for the BCF file type on the **Known sites** input port.\n\n###Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GATK BaseRecalibrator** for a couple of different samples, executed on AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost (on-demand) | Instance (AWS) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|\n|     RNA-Seq     |  2.2 GB |   9min   | ~0.08$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     |  6.6 GB |   19min   | ~0.17$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 11 GB |  27min  | ~0.24$ | c4.2xlarge (8 CPUs) | \n|     RNA-Seq     | 22 GB |  46min  | ~0.41$ | c4.2xlarge (8 CPUs) |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [GATK BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/articles/360036726891-BaseRecalibrator)\n\n[2] [GATK Mapped sequence data formats](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats)", "input": [{"name": "Ambig filter bases"}, {"name": "Ambig filter frac"}, {"name": "Binary tag name"}, {"name": "Black listed lanes"}, {"name": "BQSR BAQ gap open penalty"}, {"name": "Default base qualities"}, {"name": "Deletions default quality"}, {"name": "Disable read filter"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default read filters"}, {"name": "Dont require soft clips both ends"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "Exclude intervals string"}, {"name": "Filter too short"}, {"name": "Indels context size"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Insertions default quality"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Keep read group"}, {"name": "Keep reverse strand only"}, {"name": "Known sites", "encodingFormat": "text/x-bed"}, {"name": "Library"}, {"name": "Low quality tail"}, {"name": "Max fragment length"}, {"name": "Max read length"}, {"name": "Maximum cycle value"}, {"name": "Maximum mapping quality"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Min read length"}, {"name": "Minimum mapping quality"}, {"name": "Mismatches context size"}, {"name": "Mismatches default quality"}, {"name": "Platform filter name"}, {"name": "Preserve qscores less than"}, {"name": "Quantizing levels"}, {"name": "Read filter"}, {"name": "Read group black list"}, {"name": "Read name"}, {"name": "Read validation stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Sample"}, {"name": "Sequence dictionary"}, {"name": "Use original qualities"}, {"name": "Output name prefix"}, {"name": "CPU per job"}, {"name": "Disable BAM index caching"}, {"name": "Seconds between progress updates"}, {"name": "Read index"}], "output": [{"name": "Output recalibration report"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648047601, "dateCreated": 1617275900, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-bedtointervallist/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-bedtointervallist/4", "applicationCategory": "CommandLineTool", "name": "GATK BedToIntervalList", "description": "Converts a BED file to a Picard Interval List.\n\n###**Overview**  \n\nThis tool provides easy conversion from BED to the Picard interval_list format which is required by many Picard processing tools. Note that the coordinate system of BED files is such that the first base or position in a sequence is numbered \"0\", while in interval list files it is numbered \"1\". BED files contain sequence data displayed in a flexible format that includes nine optional fields, in addition to three required fields within the annotation tracks. The required fields of a BED file include:\n\n- chrom - The name of the chromosome (e.g. chr20) or scaffold (e.g. scaffold10671)\n- chromStart - The starting position of the feature in the chromosome or scaffold. The first base in a chromosome is numbered \"0\"\n- chromEnd - The ending position of the feature in the chromosome or scaffold.  The chromEnd base is not included in the display of the feature. For example, the first 100 bases of a chromosome are defined as chromStart=0, chromEnd=100, and span the bases numbered 0-99\n\nIn each annotation track, the number of fields per line must be consistent throughout a data set. For additional information regarding BED files and the annotation field options, please see: [FAQformat](http://genome.ucsc.edu/FAQ/FAQformat.html#format1). Interval list files contain sequence data distributed into intervals. The interval list file format is relatively simple and reflects the SAM alignment format to a degree. A SAM style header must be present in the file that lists the sequence records against which the intervals are described. After the header, the file then contains records, one per line in plain text format with the following values tab-separated:\n\n- Sequence name (SN) - The name of the sequence in the file for identification purposes, can be chromosome number e.g. chr20\n- Start position - Interval start position (starts at +1)\n- End position - Interval end position (1-based, end inclusive)\n- Strand - Indicates +/- strand for the interval (either + or -)\n- Interval name - (Each interval should have a unique name)\n\nThis tool requires a sequence dictionary, provided with the SEQUENCE_DICTIONARY or SD argument. The value given to this argument can be any of the following:\n\n- A file with .dict extension generated using Picard's CreateSequenceDictionaryTool\n- A reference.fa or reference.fasta file with a reference.dict in the same directory\n- Another IntervalList with @SQ lines in the header from which to generate a dictionary\n- A VCF that contains #contig lines from which to generate a sequence dictionary\n- A SAM or BAM file with @SQ lines in the header from which to generate a dictionary\n\n###**Input**  \n\nThe input BED file.\nThe sequence dictionary, or BAM/VCF/IntervalList from which a dictionary can be extracted.\n\n###**Output**  \n\nThe output Picard Interval List.\n\n###**Usage examples**   \n\n    ./gatk-launch BedToIntervalList \\\n    \t-I=input.bed \\\n    \t-O=list.interval_list \\\n    \t-SD=reference_sequence.dict\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Input", "encodingFormat": "text/x-bed"}, {"name": "Compression Level"}, {"name": "Create Index"}, {"name": "Create Md5 File"}, {"name": "Ga4 Gh Client Secrets"}, {"name": "Max Records In Ram"}, {"name": "Quiet"}, {"name": "Reference Sequence", "encodingFormat": "application/x-fasta"}, {"name": "Sort"}, {"name": "Unique"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Validation Stringency"}, {"name": "Verbosity"}], "output": [{"name": "Interval List"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1517583110, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-3-7-catvariants/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-3-7-catvariants/4", "applicationCategory": "CommandLineTool", "name": "GATK CatVariants", "description": "**CatVariants**\nConcatenate VCF files of non-overlapping genome intervals, all with the same set of samples\n\n**Category Variant Evaluation and Manipulation Tools**\n\n**Overview**\n\nThe main purpose of this tool is to speed up the gather function when using scatter-gather parallelization. This tool concatenates the scattered output VCF files. It assumes that:\n\nAll the input VCFs (or BCFs) contain the same samples in the same order.\nThe variants in each input file are from non-overlapping (scattered) intervals.\nWhen the input files are already sorted based on the intervals start positions, use -assumeSorted.\n\n**Input**\n\nTwo or more variant sets to combine. They should be of non-overlapping genome intervals and with the same samples (sorted in the same order). If the files are ordered according to the appearance of intervals in the ref genome, then one can use the -assumeSorted flag.\n\n**Output**\n\nA combined VCF or BCF. The output file should have the same extension as the input(s). <\\p>\n\nImportant note\nThis is a command-line utility that bypasses the GATK engine. As a result, the command-line you must use to invoke it is a little different from other GATK tools (see example below), and it does not accept any of the classic \"CommandLineGATK\" arguments.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Variant index parameter"}, {"name": "Variant index type"}, {"name": "Assume sorted"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Variant files"}, {"name": "CatVariants Output file name"}], "output": [{"name": "Diagnosed VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["SAM/BAM-Processing", "SAM/BAM-Analysis"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1501857800, "dateCreated": 1501857800, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-collectalignmentsummarymetrics/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-collectalignmentsummarymetrics/1", "applicationCategory": "CommandLineTool", "name": "GATK CollectAlignmentSummaryMetrics", "description": "Produces from a SAM/BAM/CRAM file containing summary alignment metrics.\n\n###**Overview**  \n\nCollectAlignmentSummaryMetrics assesses the quality of alignment by analyzing a SAM or BAM file. It compares it with the reference file (FASTA) and provides alignment statistics, such as the number of input reads and the percent of reads that are mapped. It produces a file which contains summary alignment metrics from a SAM or BAM file.\n\nNote: This tool requires the exact same FASTA file as the one to which raw reads were aligned.\n\n###**Inputs**  \nA BAM or SAM file.\n\n###**Outputs**  \nAn alignment summary file. \n\n###**Usage example**\n\n    java -Xmx4g -jar gatk.jar \\  \n         CollectAlignmentSummaryMetrics \\  \n         --reference reference.fasta \\\n         --input input.bam \\\n         --output summary_metrics.txt\n\n###**Common issues**\n\n1) BAM file - Sort order should be coordinate based.\n2) Reference sequence file - Note that while this argument is not required, without it only a small subset of the metrics will be calculated. If reference sequence file is used, sequence index and dictionary are required. This tool requires the exact same FASTA file as the one to which raw reads were aligned.\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Input SAM or BAM file"}, {"name": "List of adapter sequences to use when processing the alignment metrics"}, {"name": "If true (default), then the sort order in the header file will be ignored"}, {"name": "Compression level for all compressed files created (e"}, {"name": "Whether to create a BAM index when writing a coordinate-sorted BAM file"}, {"name": "Whether to create an MD5 digest for any BAM or FASTQ files created"}, {"name": "Paired-end reads that do not have this expected orientation will be considered chimeric"}, {"name": "Google Genomics API client_secrets"}, {"name": "Whether the SAM or BAM file consists of bisulfite sequenced reads"}, {"name": "Paired-end reads above this insert size will be considered chimeric along with\n                              inter-chromosomal pairs"}, {"name": "When writing files that need to be sorted, this will specify the number of records stored\n                              in RAM before spilling to disk"}, {"name": "The level(s) at which to accumulate metrics"}, {"name": "Whether to suppress job-summary info on System"}, {"name": "Reference sequence file"}, {"name": "Stop after processing N reads, mainly for debugging"}, {"name": "Use the JDK Deflater instead of the Intel Deflater for writing compressed output  Default\n                              value: false"}, {"name": "Use the JDK Inflater instead of the Intel Inflater for reading compressed input  Default\n                              value: false"}, {"name": "Validation stringency for all SAM files read by this program"}, {"name": "Control verbosity of logging"}], "output": [{"name": "Summary metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521481019, "dateCreated": 1521481019, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-collecthsmetrics/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-collecthsmetrics/8", "applicationCategory": "CommandLineTool", "name": "GATK CollectHsMetrics", "description": "Collects hybrid-selection (HS) metrics for a SAM or BAM file.\n\n###**Overview**  \n\nCollects hybrid-selection (HS) metrics for a SAM or BAM file.  This tool takes a SAM/BAM file input and collects metrics that are specific for sequence datasets generated through hybrid-selection. Hybrid-selection (HS) is the most commonly used technique to capture exon-specific sequences for targeted sequencing experiments such as exome sequencing; for more information, please see the corresponding [GATK Dictionary entry](http://www.broadinstitute.org/gatk/guide/article?id=6331).\n\nThis tool requires an aligned SAM or BAM file as well as bait and target interval files in Picard interval list format. You should use the bait and interval files that correspond to the capture kit that was used to generate the capture libraries for sequencing, which can generally be obtained from the kit manufacturer. If the baits and target intervals are provided in BED format, you can convert them to the Picard interval list format using Picard's [BedToInterval tool](http://broadinstitute.github.io/picard/command-line-overview.html#BedToIntervalList). If a reference sequence is provided, this program will calculate both AT_DROPOUT and GC_DROPOUT metrics. Dropout metrics are an attempt to measure the reduced representation of reads, in regions that deviate from 50% G/C content. This reduction in the number of aligned reads is due to the increased numbers of errors associated with sequencing regions with excessive or deficient numbers of G/C bases, ultimately leading to poor mapping efficiencies and lowcoverage in the affected regions. If you are interested in getting G/C content and mean sequence depth information for every target interval, use the PER_TARGET_COVERAGE option.\n\nNote: Metrics labeled as percentages are actually expressed as fractions!\n\nPlease see [CollectHsMetrics](http://broadinstitute.github.io/picard/picard-metric-definitions.html#HsMetrics) for detailed descriptions of the output metrics produced by this tool.\n\n###**Input**  \n\nAn aligned SAM or BAM file.\nAn interval list file that contains the locations of the baits used.\n\n###**Output**  \n\nA metrics file.\n\n###**Usage examples**   \n\n    ./gatk-launch CollectHsMetrics \\\n     \t -I=input.bam \\\n     \t -O=hs_metrics.txt \\\n     \t -R=reference_sequence.fasta \\\n     \t -BAIT_INTERVALS=bait.interval_list \\\n     \t -TARGET_INTERVALS=target.interval_list\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Bait Intervals"}, {"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Target Intervals"}, {"name": "Bait Set Name"}, {"name": "Clip Overlapping Reads"}, {"name": "Compression Level"}, {"name": "Coverage Cap"}, {"name": "Create Index"}, {"name": "Create Md5 File"}, {"name": "Ga4 Gh Client Secrets"}, {"name": "Max Records In Ram"}, {"name": "Metric Accumulation Level"}, {"name": "Minimum Base Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Near Distance"}, {"name": "Per Base Coverage"}, {"name": "Per Target Coverage"}, {"name": "Quiet"}, {"name": "Reference Sequence", "encodingFormat": "application/x-fasta"}, {"name": "Sample Size"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Validation Stringency"}, {"name": "Verbosity"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}], "output": [{"name": "Output metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1517583110, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-combinevariants/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-combinevariants/14", "applicationCategory": "CommandLineTool", "name": "GATK CombineVariants", "description": "Overview\n\nCombineVariants reads in variants records from separate ROD (Reference-Ordered Data) sources and combines them into a single VCF. Any (unique) name can be used to bind your ROD and any number of sources can be input. This tool aims to fulfill two main possible use cases, reflected by the two combination options (MERGE and UNION), for merging records at the variant level (the first 8 fields of the VCF) or at the genotype level (the rest).\n\nMERGE: combines multiple variant records present at the same site in the different input sources into a single variant record in the output. If sample names overlap, then they are \"uniquified\" by default, which means a suffix is appended to make them unique. Note that in version 3.3, the automatic uniquifying was disabled (unintentionally), and required setting `-genotypeMergeOptions UNIQUIFY` manually.\nUNION: assumes that each ROD source represents the same set of samples (although this is not enforced). It uses the priority list (if provided) to emit a single record instance at every position represented in the input RODs.\nCombineVariants will emit a record for every site that was present in any of your input VCF files, and will annotate (in the set attribute in the INFO field) whether the record had a PASS or FILTER status in each input ROD . In effect, CombineVariants always produces a union of the input VCFs. However, any part of the Venn of the merged VCFs can be extracted using JEXL expressions on the set attribute using SelectVariants. If you want to extract just the records in common between two VCFs, you would first run CombineVariants on the two files to generate a single VCF and then run SelectVariants to extract the common records with `-select 'set == \"Intersection\"'`, as worked out in the detailed example in the documentation guide.\n\nInput\nTwo or more variant sets to combine.\n\nOutput\nA combined VCF.\n\nUsage examples\n\nMerge two separate callsets\n java -jar GenomeAnalysisTK.jar \\\n   -T CombineVariants \\\n   -R reference.fasta \\\n   --variant input1.vcf \\\n   --variant input2.vcf \\\n   -o output.vcf \\\n   -genotypeMergeOptions UNIQUIFY\n \nGet the union of calls made on the same samples\n java -jar GenomeAnalysisTK.jar \\\n   -T CombineVariants \\\n   -R reference.fasta \\\n   --variant:foo input1.vcf \\\n   --variant:bar input2.vcf \\\n   -o output.vcf \\\n   -genotypeMergeOptions PRIORITIZE \\\n   -priority foo,bar\n \nCaveats\n\nThis tool is not intended to manipulate GVCFS! To combine GVCF files output by HaplotypeCaller, use CombineGVCFs.\nTo join intermediate VCFs produced by running jobs in parallel by interval (e.g. by chromosome), use CatVariants.\n\nAdditional notes\n\nUsing this tool's multi-threaded parallelism capability is particularly useful when converting from VCF to BCF2, which can be time-consuming. In this case each thread spends CPU time doing the conversion, and the GATK engine is smart enough to merge the partial BCF2 blocks together efficiently. However, since this merge runs in only one thread, you can quickly reach diminishing returns with the number of parallel threads. In our hands, `-nt 4` works well but `-nt 8` tends to be be too much.\nSince GATK 2.1, when merging multiple VCF records at a site, the combined VCF record has the QUAL of the first VCF record with a non-MISSING QUAL value. The previous behavior was to take the max QUAL, which could result in strange downstream confusion.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude Intervals"}, {"name": "Intervals", "encodingFormat": "text/plain"}, {"name": "Gatk key"}, {"name": "Variants", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Assume Identical Samples"}, {"name": "Filtered Are Uncalled"}, {"name": "Filteredrecordsmergetype"}, {"name": "Genotypemergeoption"}, {"name": "Merge Info With Max Ac"}, {"name": "Minimal Vcf"}, {"name": "Minimum N"}, {"name": "Print Complex Merges"}, {"name": "Set Key"}, {"name": "Suppress Command Line Header"}, {"name": "Memory overhead per job"}], "output": [{"name": "Output Combined VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799527, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-diagnosetargets/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-diagnosetargets/9", "applicationCategory": "CommandLineTool", "name": "GATK DiagnoseTargets", "description": "Overview\nThis tool is useful for diagnosing regions with bad coverage, mapping, or read mate pairs. It analyzes each sample independently and aggregates results over intervals of interest.\n\nInput\nA reference file\none or more input BAMs\nOne or more intervals\n\nOutput\nA modified VCF detailing each interval by sample and information for each interval according to the thresholds used. Interval information includes GC Content, average interval depth, callable status among others. If you use the --missing option, you can get as a second output a intervals file with the loci that have missing data. This file can then be used as input to QualifyMissingIntervals for full qualification and interpretation of why the data is missing.\n\nUsage example\n    java -jar GenomeAnalysisTK.jar\n              -T DiagnoseTargets \\\n              -R reference.fasta \\\n              -I sample1.bam \\\n              -I sample2.bam \\\n              -I sample3.bam \\\n              -L intervals.interval_list \\\n              -o output.vcf\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Bad mate status threshold"}, {"name": "Coverage status threshold"}, {"name": "Excessive coverage status threshold"}, {"name": "Maximum coverage"}, {"name": "Maximum insert size"}, {"name": "Minimum base quality"}, {"name": "Minimum coverage"}, {"name": "Minimum mapping quality"}, {"name": "Quality status threshold"}, {"name": "Voting status threshold"}, {"name": "Memory overhead per job"}], "output": [{"name": "Diagnosed VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1453799727, "dateCreated": 1453799715, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-gatherbqsrreports-4-2-0-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-gatherbqsrreports-4-2-0-0/4", "applicationCategory": "CommandLineTool", "name": "GATK GatherBQSRReports", "description": "**GATK GatherBQSRReports** gathers scattered BQSR recalibration reports into a single file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases \n\n* This tool is intended to be used to combine recalibration tables from runs of **GATK BaseRecalibrator** parallelized per interval.\n\n* Usage example:\n```\n   gatk --java-options \"-Xmx2048M\" GatherBQSRReports \\\n   --input input1.csv \\\n   --input input2.csv \\\n   --output output.csv\n```\n\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from **Input BQSR reports**. If the **Sample ID** value is not set, the name will be inherited from the **Input BQSR reports** file name. In case there are multiple files on the **Input BQSR reports** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. Moreover, **.recal_data** will be added before the extension of the output file name which is **CSV** by default.\n\n* The following GATK parameters were excluded from the tool wrapper: `--arguments_file`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--QUIET`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048 MB.\n\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the **Memory per job** in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100 MB. \n\n\n###Performance Benchmarking\n\nThis tool is fast, with a running time of a few minutes. The experiment task was performed on the default AWS on-demand c4.2xlarge instance on 50 CSV files (size of each ~350KB) and took 3 minutes to finish ($0.03).\n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n###Portability\n\n**GATK GatherBQSRReports** is tested with cwltool version: \"3.0.20201203173111\"\n\n###References\n\n[1] [GATK GatherBQSRReports](https://gatk.broadinstitute.org/hc/en-us/articles/360056968612-GatherBQSRReports)", "input": [{"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Memory overhead per job"}, {"name": "Output name prefix"}, {"name": "Input BQSR reports"}], "output": [{"name": "Gathered BQSR report"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip"], "applicationSubCategory": ["SAM/BAM Processing", "Utilities", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648047600, "dateCreated": 1634729278, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-gatherbqsrreports-4-1-0-0/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-gatherbqsrreports-4-1-0-0/15", "applicationCategory": "CommandLineTool", "name": "GATK GatherBQSRReports CWL1.0", "description": "**GATK GatherBQSRReports** gathers scattered BQSR recalibration reports into a single file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n\n### Common Use Cases \n\n* This tool is intended to be used to combine recalibration tables from runs of **GATK BaseRecalibrator** parallelized per-interval.\n\n* Usage example:\n```\n   gatk --java-options \"-Xmx2048M\" GatherBQSRReports \\\n   --input input1.csv \\\n   --input input2.csv \\\n   --output output.csv\n\n```\n\n\n###Changes Introduced by Seven Bridges\n\n* The output file will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from **Input BQSR reports**. If the **Sample ID** value is not set, the name will be inherited from the **Input BQSR reports** file name. In case there are multiple files on the **Input BQSR reports** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. Moreover, **.recal_data** will be added before the extension of the output file name.\n\n* The following GATK parameters were excluded from the tool wrapper: `--arguments_file`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--QUIET`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048MB.\n\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the Memory per job in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100MB. \n\n\n###Performance Benchmarking\n\nThis tool is fast, with a running time of a few minutes. The experiment task was performed on the default AWS on-demand c4.2xlarge instance on 50 CSV files (size of each ~350KB) and took 2 minutes to finish ($0.02).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n###References\n\n[1] [GATK GatherBQSRReports](https://gatk.broadinstitute.org/hc/en-us/articles/360036359192-GatherBQSRReports)", "input": [{"name": "Input BQSR reports"}, {"name": "Output name prefix"}, {"name": "Memory per job"}, {"name": "Memory overhead per job"}, {"name": "CPU per job"}], "output": [{"name": "Gathered BQSR report"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["SAM/BAM Processing", "Utilities"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648047601, "dateCreated": 1617275900, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-4-0-genotypegvcfs/20", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-4-0-genotypegvcfs/20", "applicationCategory": "CommandLineTool", "name": "GATK GenotypeGVCFs", "description": "Perform joint genotyping on one or more samples pre-called with HaplotypeCaller.\n\n###**Overview**  \nThis tool is designed to perform joint genotyping on multiple samples pre-called with HaplotypeCaller to produce a multi-sample callset in a highly scalable manner. However it can also be run on a single sample at a time to produce a single-sample callset. In any case, the input samples must possess genotype likelihoods produced by HaplotypeCaller with `-ERC GVCF` or `-ERC BP_RESOLUTION`.re-genotype the newly merged record, and then re-annotate it.\n\n###**Input**  \nOne or more GVCFs produced by in HaplotypeCaller with the `-ERC GVCF` or `-ERC BP_RESOLUTION` settings, containing the samples to joint-genotype. \n\n###**Output**  \nA final VCF in which all samples have been jointly genotyped. \n\n###**Usage example**   \n\n####**Perform joint genotyping on a set of GVCFs enumerated in the command line**\n\n     gatk-launch --javaOptions \"-Xmx4g\" GenotypeGVCFs \\\n       -R reference.fasta \\\n       -V input1.g.vcf \\\n       -V input2.g.vcf \\\n       -V input3.g.vcf \\\n       -O output.vcf\n\n####**Perform joint genotyping on a set of GVCFs listed in a text file, one per line**\n\n     gatk-launch --javaOptions \"-Xmx4g\" GenotypeGVCFs \\\n       -R reference.fasta \\\n       -V input_gvcfs.list \\\n       -O output.vcf\n \n###**Caveat**  \nOnly GVCF files produced by HaplotypeCaller (or CombineGVCFs) can be used as input for this tool. Some other programs produce files that they call GVCFs but those lack some important information (accurate genotype likelihoods for every position) that GenotypeGVCFs requires for its operation. \n\n###**Special note on ploidy**  \nThis tool is able to handle any ploidy (or mix of ploidies) intelligently; there is no need to specify ploidy for non-diploid organisms.  \n\n###**Additional Notes**\n- By default, the tool works only with VCF resource files. To use VCF.GZ resource files, the tool wrapper needs to be modified.\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference"}, {"name": "Variants"}, {"name": "Add Output Sam Program Record"}, {"name": "Annotate Nda"}, {"name": "Cloud Index Prefetch Buffer"}, {"name": "Cloud Prefetch Buffer"}, {"name": "Create Output Bam Index"}, {"name": "Create Output Bam Md5"}, {"name": "Create Output Variant Index"}, {"name": "Create Output Variant Md5"}, {"name": "Dbsnp"}, {"name": "Disable Bam Index Caching"}, {"name": "Disable Read Filter"}, {"name": "Disable Sequence Dictionary Validation"}, {"name": "Disable Tool Default Read Filters"}, {"name": "Heterozygosity"}, {"name": "Heterozygosity Stdev"}, {"name": "Indel Heterozygosity"}, {"name": "Interval Exclusion Padding"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Lenient"}, {"name": "Quiet"}, {"name": "Read Filter"}, {"name": "Read Index"}, {"name": "Read Validation Stringency"}, {"name": "Sample Ploidy"}, {"name": "Seconds Between Progress Updates"}, {"name": "Standard Min Confidence Threshold For Calling"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Use New Af Calculator"}, {"name": "Annotation"}, {"name": "Annotations To Exclude"}, {"name": "Group"}, {"name": "Input Prior"}, {"name": "Max Alternate Alleles"}, {"name": "Max Genotype Count"}, {"name": "Ambig Filter Frac"}, {"name": "Max Fragment Length"}, {"name": "Library"}, {"name": "Maximum Mapping Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Dont Require Soft Clips Both Ends"}, {"name": "Filter Too Short"}, {"name": "Pl Filter Name"}, {"name": "Black Listed Lanes"}, {"name": "Black List"}, {"name": "Keep Read Group"}, {"name": "Max Read Length"}, {"name": "Min Read Length"}, {"name": "Read Name"}, {"name": "Keep Reverse"}, {"name": "Sample"}, {"name": "Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Exclude Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Intervals String"}, {"name": "Exclude Intervals String"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}, {"name": "Ambig Filter Bases"}], "output": [{"name": "Output VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1501857799, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-haplotypecaller-4-2-0-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-haplotypecaller-4-2-0-0/5", "applicationCategory": "CommandLineTool", "name": "GATK HaplotypeCaller", "description": "**GATK HaplotypeCaller** calls germline SNPs and indels from input BAM file(s) via local re-assembly of haplotypes [1].\n\n**GATK HaplotypeCaller** is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region. In other words, whenever the program encounters a region showing signs of variation, it discards the existing mapping information and completely reassembles the reads in that region. This allows the **GATK HaplotypeCaller** to be more accurate when calling regions that are traditionally difficult to call, for example when they contain different types of variants close to each other. It also makes the **GATK HaplotypeCaller** much better at calling indels than position-based callers like UnifiedGenotyper [1].\n\nIn the GVCF workflow used for scalable variant calling in DNA sequence data, **GATK HaplotypeCaller** runs per-sample to generate an intermediate GVCF (not to be used in final analysis), which can then be used in GenotypeGVCFs for joint genotyping of multiple samples in a very efficient way. The GVCF workflow enables rapid incremental processing of samples as they roll off the sequencer, as well as scaling to very large cohort sizes [1].\n\nIn addition, **HaplotypeCaller** is able to handle non-diploid organisms as well as pooled experiment data. Note however that the algorithms used to calculate variant likelihoods are not well suited to extreme allele frequencies (relative to ploidy) so its use is not recommended for somatic (cancer) variant discovery. For that purpose, use **Mutect2** instead [1].\n\nFinally, **GATK HaplotypeCaller** is also able to correctly handle splice junctions that make RNAseq a challenge for most variant callers, on the condition that the input read data has previously been processed according to [GATK RNAseq short variant discovery (SNPs + Indels)](https://gatk.broadinstitute.org/hc/en-us/articles/360035531192?id=4067) [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n- Call variants individually on each sample in GVCF mode\n\n```\n gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.g.vcf.gz \\\n   -ERC GVCF\n```\n\n\n- Call variants individually on each sample in GVCF mode with allele-specific annotations. [Here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890551?id=9622) you can read more details about allele-specific annotation and filtering.\n\n```\ngatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.g.vcf.gz \\\n   -ERC GVCF \\\n   -G Standard \\\n   -G AS_Standard\n```\n\n\n- Call variants with bamout to show realigned reads.\n\n```\n gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.vcf.gz \\\n   -bamout bamout.bam\n```\n\n### Changes Introduced by Seven Bridges\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals file** and **Include intervals string** options.\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals file** and **Exclude intervals string** options.\n* **VCF output** will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from **Input alignments**. If the **Sample ID** value is not set, the name will be inherited from the **Input alignments** file name. In case there are multiple files on the **Input alignments** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case. \n* The user can specify the output file format using the **Output VCF extension** argument. Otherwise, the output will be in the compressed VCF file format.\n* The following parameters were excluded from the tool wrapper: `--arguments_file`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--QUIET`, `--recover-dangling-heads` (deprecated), `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n### Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 4000 MB.\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the **Memory per job** in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100 MB. \n* Note: GATK tools that take in mapped read data expect a BAM file as the primary format [2]. More on GATK requirements for mapped sequence data formats can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats).\n* Note: **Alleles**, **Comparison VCF**, **dbSNP**, **Input alignments**, **Population callset** should have corresponding index files in the same folder. \n* Note: **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n* Note: When working with PCR-free data, be sure to set **PCR indel model** (`--pcr_indel_model`) to NONE [1].\n* Note: When running **Emit ref confidence** ( `--emit-ref-confidence`) in GVCF or in BP_RESOLUTION mode, the confidence threshold is automatically set to 0. This cannot be overridden by the command line. The threshold can be set manually to the desired level when using **GenotypeGVCFs** [1].\n* Note: It is recommended to use a list of intervals to speed up the analysis. See [this document](https://gatk.broadinstitute.org/hc/en-us/articles/360035889551?id=4133) for details [1].\n* Note: **HaplotypeCaller** is able to handle many non-diploid use cases; the desired ploidy can be specified using the `-ploidy` argument. Note however that very high ploidies (such as are encountered in large pooled experiments) may cause performance challenges including excessive slowness [1].\n* Note: These **Read Filters** (`--read-filter`) are automatically applied to the data by the Engine before processing by **HaplotypeCaller** [1]: **NotSecondaryAlignmentReadFilter**, **GoodCigarReadFilter**, **NonZeroReferenceLengthAlignmentReadFilter**, **PassesVendorQualityCheckReadFilter**, **MappedReadFilter**, **MappingQualityAvailableReadFilter**, **NotDuplicateReadFilter**, **MappingQualityReadFilter**, **WellformedReadFilter**\n* Note: If the **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", the **Library** (`--library`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", the **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"PlatformUnitReadFilter\", the **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", the **Read group black list** (`--read-group-black-list`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", the **Keep read group** (`--keep-read-group`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", the **Max read length** (`--max-read-length`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", the **Read name** (`--read-name`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", the **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", the **Sample** (`--sample`) option must be set to some value.\n* Note: If the **Read filter** (`--read-filter`) option is set to \"IntervalOverlapReadFilter\", the **Keep intervals** (`--keep-intervals`) option must be set to some value.\n* Note: The following options are valid only if the appropriate **Read filter** (`--read-filter`) is specified: **Ambig filter bases** (`--ambig-filter-bases`), **Ambig filter frac** (`--ambig-filter-frac`), **Max fragment length** (`--max-fragment-length`), **Min fragment length** (`--min-fragment-length`), **Keep intervals** (`--keep-intervals`), **Library** (`--library`), **Maximum mapping quality** (`--maximum-mapping-quality`), **Minimum mapping quality** (`--minimum-mapping-quality`),  **Mate too distant length** (`--mate-too-distant-length`), **Do not require soft clips** (`--dont-require-soft-clips-both-ends`), **Filter too short** (`--filter-too-short`), **Platform filter name** (`--platform-filter-name`), **Black listed lanes** (`--black-listed-lanes`), **Read group black list** (`--read-group-black-list`), **Keep read group** (`--keep-read-group`), **Max read length** (`--max-read-length`), **Min read length** (`--min-read-length`), **Read name** (`--read-name`), **Keep reverse strand only** (`--keep-reverse-strand-only`), **Sample** (`--sample`), **Invert soft clip ratio filter** (`--invert-soft-clip-ratio-filter`), **Soft clipped leading trailing ratio** (`--soft-clipped-leading-trailing-ratio`), **Soft clipped ratio threshold** (`--soft-clipped-ratio-threshold`) . See the description of each parameter for information on the associated **Read filter**.\n* Note: Allele-specific annotations are not yet supported in the VCF mode.\n* Note: The wrapper has not been tested for the SAM file type on the **Input alignments** input port.\n* Note: DRAGEN-GATK features have not been tested. Once the full DRAGEN-GATK pipeline is released, parameters related to DRAGEN-GATK mode will be tested. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes.\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration | Cost (on-demand) | AWS instance type |\n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|:------:|\n|     RNA-Seq     | 2.5 GB |     Yes    |     16M     |     101     |   52min   | 0.47$ | c4.2xlarge |\n|     RNA-Seq     | 7.5 GB |     Yes    |     50M     |     101     |   1h46min   | 0.95$ | c4.2xlarge |\n|     RNA-Seq     | 12.5 GB |     Yes    |     82M    |     101     |  2h40min  | 1.43$ | c4.2xlarge |\n|     RNA-Seq     | 24.5 GB |     Yes    |     164M    |     101     |  4h55min  | 2.64$ | c4.2xlarge |\n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n###Portability\n\n**GATK HaplotypeCaller** is tested with cwltool version: \"3.0.20201203173111\"\n\n### References\n[1] [GATK HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/360056969012-HaplotypeCaller)\n\n[2] [GATK Mapped sequence data formats](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats)", "input": [{"name": "Active probability threshold"}, {"name": "Adaptive pruning"}, {"name": "Adaptive pruning initial error rate"}, {"name": "Add output SAM program record"}, {"name": "Add output VCF command line"}, {"name": "Annotate all sites with PLs"}, {"name": "Alleles", "encodingFormat": "application/x-vcf"}, {"name": "Allow non unique kmers in ref"}, {"name": "Annotate with num discovered alleles"}, {"name": "Annotation"}, {"name": "Annotation group"}, {"name": "Annotations to exclude"}, {"name": "Assembly region output"}, {"name": "Assembly region padding"}, {"name": "BAM output"}, {"name": "BAM writer type"}, {"name": "Base quality score threshold"}, {"name": "Comparison VCF", "encodingFormat": "application/x-vcf"}, {"name": "Contamination fraction per sample"}, {"name": "Contamination fraction to filter"}, {"name": "Create output BAM index"}, {"name": "Create output variant index"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Debug assembly"}, {"name": "Disable BAM index caching"}, {"name": "Disable optimizations"}, {"name": "Disable read filter"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default annotations"}, {"name": "Disable tool default read filters"}, {"name": "Do not run physical phasing"}, {"name": "Dont increase kmer sizes for cycles"}, {"name": "Do not use soft clipped bases"}, {"name": "Emit ref confidence"}, {"name": "Enable all annotations"}, {"name": "Exclude intervals string"}, {"name": "Founder ID"}, {"name": "Force-call filtered alleles"}, {"name": "Graph output"}, {"name": "GVCF GQ bands"}, {"name": "Heterozygosity"}, {"name": "Heterozygosity stdev"}, {"name": "Indel heterozygosity"}, {"name": "Indel size to eliminate in ref model"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Kmer size"}, {"name": "Lenient"}, {"name": "Max alternate alleles"}, {"name": "Max assembly region size"}, {"name": "Max genotype count"}, {"name": "Max MNP distance"}, {"name": "Max num haplotypes in population"}, {"name": "Max prob propagation distance"}, {"name": "Max reads per alignment start"}, {"name": "Max unpruned variants"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Min assembly region size"}, {"name": "Min base quality score"}, {"name": "Min dangling branch length"}, {"name": "Min pruning"}, {"name": "Native pairHMM threads"}, {"name": "Native pairHMM use double precision"}, {"name": "Num pruning samples"}, {"name": "Num reference samples if no call"}, {"name": "Output name prefix"}, {"name": "Output mode"}, {"name": "Pair HMM gap continuation penalty"}, {"name": "Pair HMM implementation"}, {"name": "PCR indel model"}, {"name": "Pedigree"}, {"name": "Phred scaled global read mismapping rate"}, {"name": "Population callset", "encodingFormat": "application/x-vcf"}, {"name": "Pruning lod threshold"}, {"name": "Read filter"}, {"name": "Read validation stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Sample name"}, {"name": "Sample ploidy"}, {"name": "Sequence dictionary"}, {"name": "Sites only VCF output"}, {"name": "Smith waterman"}, {"name": "Standard min confidence threshold for calling"}, {"name": "Use filtered reads for annotations"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "CPU per job"}, {"name": "Output VCF extension"}, {"name": "Seconds between progress updates"}, {"name": "Read index"}, {"name": "Do not use DRAGstr pair hmm scores"}, {"name": "DRAGEN mode"}, {"name": "DRAGstr het hom ratio"}, {"name": "DRAGstr parameters", "encodingFormat": "text/plain"}, {"name": "Enable dynamic read disqualification for genotyping"}, {"name": "Genotype assignment method"}, {"name": "Use posteriors to calculate qual"}, {"name": "Allele informative reads overlap margin"}, {"name": "Apply BQD"}, {"name": "Apply FRD"}, {"name": "Disable cap base qualities to map quality"}, {"name": "Disable spanning event genotyping"}, {"name": "Disable symmetric HMM normalizing"}, {"name": "Do not correct overlapping quality"}, {"name": "Dont use dragstr priors"}, {"name": "Expected mismatch rate for read disqualification"}, {"name": "Floor blocks"}, {"name": "Force active"}, {"name": "Linked De Bruijn graph"}, {"name": "Mapping quality threshold for genotyping"}, {"name": "Max effective depth adjustment for FRD"}, {"name": "Pruning seeding lod threshold"}, {"name": "Recover all dangling branches"}, {"name": "Soft clip low quality ends"}, {"name": "Transform DRAGEN mapping quality"}, {"name": "Ambig filter bases"}, {"name": "Ambig filter frac"}, {"name": "Max fragment length"}, {"name": "Min fragment length"}, {"name": "Keep intervals"}, {"name": "Library"}, {"name": "Maximum mapping quality"}, {"name": "Minimum mapping quality"}, {"name": "Mate too distant length"}, {"name": "Dont require soft clips both ends"}, {"name": "Filter too short"}, {"name": "Platform filter name"}, {"name": "Black listed lanes"}, {"name": "Read group black list"}, {"name": "Keep read group"}, {"name": "Max read length"}, {"name": "Min read length"}, {"name": "Read name"}, {"name": "Keep reverse strand only"}, {"name": "Sample"}, {"name": "Invert soft clip ratio filter"}, {"name": "Soft clipped leading trailing ratio"}, {"name": "Soft clipped ratio threshold"}, {"name": "Create output variant md5"}, {"name": "Create output bam md5"}, {"name": "Use new qual calculator"}, {"name": "Allow old rms mapping quality annotation data"}], "output": [{"name": "VCF output", "encodingFormat": "application/x-vcf"}, {"name": "BAM output", "encodingFormat": "application/x-bam"}, {"name": "Graph output", "encodingFormat": "text/plain"}, {"name": "Assembly region"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip"], "applicationSubCategory": ["Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648047601, "dateCreated": 1634729407, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-haplotypecaller-4-1-0-0/22", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-haplotypecaller-4-1-0-0/22", "applicationCategory": "CommandLineTool", "name": "GATK HaplotypeCaller CWL1.0", "description": "**GATK HaplotypeCaller** calls germline SNPs and indels from input BAM file(s) via local re-assembly of haplotypes [1].\n\n**GATK HaplotypeCaller** is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region. In other words, whenever the program encounters a region showing signs of variation, it discards the existing mapping information and completely reassembles the reads in that region. Reassembled reads are realigned to the reference. This allows **GATK HaplotypeCaller** to be more accurate when calling regions that are traditionally difficult to call, for example when they contain different types of variants close to each other. It also makes **GATK HaplotypeCaller** much better at calling indels than position-based callers like UnifiedGenotyper [1].\n\nIn the GVCF workflow used for scalable variant calling in DNA sequence data, **GATK HaplotypeCaller** runs per-sample to generate an intermediate GVCF (not to be used in final analysis), which can then be used in GenotypeGVCFs for joint genotyping of multiple samples in a very efficient way. The GVCF workflow enables rapid incremental processing of samples as they roll off the sequencer, as well as scaling to very large cohort sizes [1].\n\nIn addition, **HaplotypeCaller** is able to handle non-diploid organisms as well as pooled experiment data. Note however that the algorithms used to calculate variant likelihoods are not well suited to extreme allele frequencies (relative to ploidy) so its use is not recommended for somatic (cancer) variant discovery. For that purpose, use **Mutect2** instead [1].\n\nFinally, **GATK HaplotypeCaller** is also able to correctly handle splice junctions that make RNAseq a challenge for most variant callers, on the condition that the input read data has previously been processed according to [GATK RNAseq short variant discovery (SNPs + Indels)](https://gatk.broadinstitute.org/hc/en-us/articles/360035531192?id=4067) [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- Call variants individually on each sample in GVCF mode\n\n```\n gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.g.vcf.gz \\\n   -ERC GVCF\n```\n\n\n- Call variants individually on each sample in GVCF mode with allele-specific annotations. [Here](https://software.broadinstitute.org/gatk/documentation/article?id=9622) you can read more details about allele-specific annotation and filtering.\n\n```\ngatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.g.vcf.gz \\\n   -ERC GVCF \\\n   -G Standard \\\n   -G AS_Standard\n```\n\n\n- Call variants with bamout to show realigned reads. After performing a local reassembly and realignment, the reads' mapping positions are different than in the original file. This option could be used to visualize what rearrangements **HaplotypeCaller** has made.\n\n```\n gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n   -R Homo_sapiens_assembly38.fasta \\\n   -I input.bam \\\n   -O output.vcf.gz \\\n   -bamout bamout.bam\n```\n\n### Changes Introduced by Seven Bridges\n\n- **Include intervals** (`--intervals`) option is divided into **Include intervals string** and **Include intervals file** options.\n- **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals string** and **Exclude intervals file** options.\n- **VCF output** will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on **Sample ID** metadata value from the **Input alignments** file. If **Sample ID** value is not set, the name will be inherited from the **Input alignments** file name. In case there are multiple files on the **Input alignments** input, the files will be sorted by name and output file name will be generated based on the first file in the sorted file list, following the rules defined in the previous case.\n- The user can specify the output file format using the **Output VCF extension** argument. Otherwise, the output will be in the compressed VCF file format.\n- The following parameters were excluded from the tool wrapper: `--arguments_file`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-bam-md5`, `--create-output-variant-md5`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--QUIET`, `--recover-dangling-heads` (deprecated), `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--use-new-qual-calculator` (deprecated), `--verbosity`, `--version`\n\n### Common Issues and Important Notes\n\n-  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 4000MB.\n- **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the Memory per job in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100MB. \n- Note: GATK tools that take in mapped read data expect a BAM file as the primary format [2]. More on GATK requirements for mapped sequence data formats can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats).\n- Note: **Alleles**, **Comparison VCF**, **dbSNP**, **Input alignments**, **Population callset** should have corresponding index files in the same folder. \n- Note: **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n- Note: When working with PCR-free data, be sure to set **PCR indel model** (`--pcr_indel_model`) to NONE [1].\n- Note: When running **Emit ref confidence** ( `--emit-ref-confidence`) in GVCF or in BP_RESOLUTION mode, the confidence threshold is automatically set to 0. This cannot be overridden by the command line. The threshold can be set manually to the desired level when using **GenotypeGVCFs** [1].\n- Note: It is recommended to use a list of intervals to speed up the analysis. See [this document](https://software.broadinstitute.org/gatk/documentation/article?id=4133) for details [1].\n- Note: **HaplotypeCaller** is able to handle many non-diploid use cases; the desired ploidy can be specified using the `-ploidy` argument. Note however that very high ploidies (such as are encountered in large pooled experiments) may cause performance challenges including excessive slowness [1].\n- Note: These **Read Filters** (`--read-filter`) are automatically applied to the data by the Engine before processing by **HaplotypeCaller** [1]: **NotSecondaryAlignmentReadFilter**, **GoodCigarReadFilter**, **NonZeroReferenceLengthAlignmentReadFilter**, **PassesVendorQualityCheckReadFilter**, **MappedReadFilter**, **MappingQualityAvailableReadFilter**, **NotDuplicateReadFilter**, **MappingQualityReadFilter**, **WellformedReadFilter**\n- If the **Read filter** (`--read-filter`) option is set to \"LibraryReadFilter\", the **Library** (`--library`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"PlatformReadFilter\", the **Platform filter name** (`--platform-filter-name`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"PlatformUnitReadFilter\", the **Black listed lanes** (`--black-listed-lanes`) option must be set to some value. \n- If the **Read filter** (`--read-filter`) option is set to \"ReadGroupBlackListReadFilter\", the **Read group black list** (`--read-group-black-list`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"ReadGroupReadFilter\", the **Keep read group** (`--keep-read-group`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"ReadLengthReadFilter\", the **Max read length** (`--max-read-length`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"ReadNameReadFilter\", the **Read name** (`--read-name`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"ReadStrandFilter\", the **Keep reverse strand only** (`--keep-reverse-strand-only`) option must be set to some value.\n- If the **Read filter** (`--read-filter`) option is set to \"SampleReadFilter\", the **Sample** (`--sample`) option must be set to some value.\n- The following options are valid only if an appropriate **Read filter** (`--read-filter`) is specified: **Ambig filter bases** (`--ambig-filter-bases`), **Ambig filter frac** (`--ambig-filter-frac`), **Max fragment length** (`--max-fragment-length`), **Maximum mapping quality** (`--maximum-mapping-quality`), **Minimum mapping quality** (`--minimum-mapping-quality`),  **Do not require soft clips** (`--dont-require-soft-clips-both-ends`), **Filter too short** (`--filter-too-short`), **Min read length** (`--min-read-length`). See the description of each parameter for information on the associated **Read filter**.\n- Note: Allele-specific annotations are not yet supported in the VCF mode\n- Note: The wrapper has not been tested for the SAM file type on the **Input alignments** input port.\n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes.\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration | Cost (on-demand) | AWS instance type |\n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|:------:|\n|     RNA-Seq     | 2.6 GB |     Yes    |     16M     |     101     |   50min   | 0.45$ | c4.2xlarge |\n|     RNA-Seq     | 7.7 GB |     Yes    |     50M     |     101     |   1h31min   | 0.82$ | c4.2xlarge |\n|     RNA-Seq     | 12.7 GB |     Yes    |     82M    |     101     |  2h19min  | 1.26$ | c4.2xlarge |\n|     RNA-Seq     | 25 GB |     Yes    |     164M    |     101     |  4h5min  | 2.21$ | c4.2xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1] [GATK HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/360036359552-HaplotypeCaller)\n\n[2] [GATK Mapped sequence data formats](https://gatk.broadinstitute.org/hc/en-us/articles/360035890791-SAM-or-BAM-or-CRAM-Mapped-sequence-data-formats)", "input": [{"name": "Active probability threshold"}, {"name": "Activity profile output"}, {"name": "Adaptive pruning"}, {"name": "Adaptive pruning initial error rate"}, {"name": "Add output SAM program record"}, {"name": "Add output VCF command line"}, {"name": "Annotate all sites with PLs"}, {"name": "Alleles", "encodingFormat": "application/x-vcf"}, {"name": "Allow non unique kmers in ref"}, {"name": "Annotate with num discovered alleles"}, {"name": "Annotation"}, {"name": "Annotation group"}, {"name": "Annotations to exclude"}, {"name": "Assembly region output"}, {"name": "Assembly region padding"}, {"name": "BAM output"}, {"name": "BAM writer type"}, {"name": "Base quality score threshold"}, {"name": "Comparison VCF", "encodingFormat": "application/x-vcf"}, {"name": "Consensus"}, {"name": "Contamination fraction per sample"}, {"name": "Contamination fraction to filter"}, {"name": "Correct overlapping quality"}, {"name": "Create output BAM index"}, {"name": "Create output variant index"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Debug"}, {"name": "Disable BAM index caching"}, {"name": "Disable optimizations"}, {"name": "Disable read filter"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default annotations"}, {"name": "Disable tool default read filters"}, {"name": "Do not run physical phasing"}, {"name": "Dont increase kmer sizes for cycles"}, {"name": "Dont trim active regions"}, {"name": "Do not use soft clipped bases"}, {"name": "Emit ref confidence"}, {"name": "Enable all annotations"}, {"name": "Exclude intervals string"}, {"name": "Founder ID"}, {"name": "Genotype filtered alleles"}, {"name": "Genotyping mode"}, {"name": "Graph output"}, {"name": "GVCF GQ bands"}, {"name": "Heterozygosity"}, {"name": "Heterozygosity stdev"}, {"name": "Indel heterozygosity"}, {"name": "Indel size to eliminate in ref model"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Input prior"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Kmer size"}, {"name": "Lenient"}, {"name": "Max alternate alleles"}, {"name": "Max assembly region size"}, {"name": "Max genotype count"}, {"name": "Max MNP distance"}, {"name": "Max num haplotypes in population"}, {"name": "Max prob propagation distance"}, {"name": "Max reads per alignment start"}, {"name": "Max unpruned variants"}, {"name": "Maximum mapping quality"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Min assembly region size"}, {"name": "Min base quality score"}, {"name": "Min dangling branch length"}, {"name": "Min pruning"}, {"name": "Minimum mapping quality"}, {"name": "Native pairHMM threads"}, {"name": "Native pairHMM use double precision"}, {"name": "Num pruning samples"}, {"name": "Num reference samples if no call"}, {"name": "Output name prefix"}, {"name": "Output mode"}, {"name": "Pair HMM gap continuation penalty"}, {"name": "Pair HMM implementation"}, {"name": "PCR indel model"}, {"name": "Pedigree"}, {"name": "Phred scaled global read mismapping rate"}, {"name": "Population callset", "encodingFormat": "application/x-vcf"}, {"name": "Pruning lod threshold"}, {"name": "Read filter"}, {"name": "Read validation stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Sample name"}, {"name": "Sample ploidy"}, {"name": "Sequence dictionary"}, {"name": "Sites only VCF output"}, {"name": "Smith waterman"}, {"name": "Standard min confidence threshold for calling"}, {"name": "Use alleles trigger"}, {"name": "Use filtered reads for annotations"}, {"name": "Use old qual calculator"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "CPU per job"}, {"name": "Ambig filter bases"}, {"name": "Ambig filter frac"}, {"name": "Max fragment length"}, {"name": "Library"}, {"name": "Do not require soft clips"}, {"name": "Filter too short"}, {"name": "Platform filter name"}, {"name": "Black listed lanes"}, {"name": "Read group black list"}, {"name": "Keep read group"}, {"name": "Max read length"}, {"name": "Min read length"}, {"name": "Read name"}, {"name": "Keep reverse strand only"}, {"name": "Sample"}, {"name": "Output VCF extension"}, {"name": "Seconds between progress updates"}, {"name": "Read index"}], "output": [{"name": "VCF output", "encodingFormat": "application/x-vcf"}, {"name": "BAM output", "encodingFormat": "application/x-bam"}, {"name": "Graph output", "encodingFormat": "text/plain"}, {"name": "Raw activity profile"}, {"name": "Assembly region"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648047601, "dateCreated": 1617275898, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-indelrealigner/20", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-indelrealigner/20", "applicationCategory": "CommandLineTool", "name": "GATK IndelRealigner", "description": "Overview\n\nThe local realignment process is designed to consume one or more BAM files and to locally realign reads such that the number of mismatching bases is minimized across all the reads. In general, a large percent of regions requiring local realignment are due to the presence of an insertion or deletion (indels) in the individual's genome with respect to the reference genome. Such alignment artifacts result in many bases mismatching the reference near the misalignment, which are easily mistaken as SNPs. Moreover, since read mapping algorithms operate on each read independently, it is impossible to place reads on the reference genome such at mismatches are minimized across all reads. Consequently, even when some reads are correctly mapped with indels, reads covering the indel near just the start or end of the read are often incorrectly mapped with respect the true indel, also requiring realignment. Local realignment serves to transform regions with misalignments due to indels into clean reads containing a consensus indel suitable for standard variant discovery approaches. Unlike most mappers, this walker uses the full alignment context to determine whether an appropriate alternate reference (i.e. indel) exists. Following local realignment, the GATK tool Unified Genotyper can be used to sensitively and specifically identify indels.\n\nThere are 2 steps to the realignment process:\n\n1. Determining (small) suspicious intervals which are likely in need of realignment (see the RealignerTargetCreator tool)\n2. Running the realigner over those intervals (IndelRealigner)\nFor more details, see the indel realignment method documentation.\n\nInput\nOne or more aligned BAM files and optionally one or more lists of known indels.\n\nOutput\nA realigned version of your input BAM file(s).\n\nUsage example:\n java -jar GenomeAnalysisTK.jar \\\n   -T IndelRealigner \\\n   -R reference.fasta \\\n   -I input.bam \\\n   --known indels.vcf \\\n   -targetIntervals intervalListFromRTC.intervals \\\n   -o realignedBam.bam\n \nCaveats\n\nThe input BAM(s), reference, and known indel file(s) should be the same ones to be used for the IndelRealigner step.\nBecause reads produced from the 454 technology inherently contain false indels, the realigner will not work with them (or with reads from similar technologies).\nThis tool also ignores MQ0 reads and reads with consecutive indel operators in the CIGAR string.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Target Intervals"}, {"name": "Known Alleles", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Consensus Determination Model"}, {"name": "Lod Threshold For Cleaning"}, {"name": "Entropy Threshold"}, {"name": "Max Consensuses"}, {"name": "Max Isize For Movement"}, {"name": "Max Positional Move Allowed"}, {"name": "Max Reads For Consensuses"}, {"name": "Max Reads For Realignment"}, {"name": "Max Reads In Memory"}, {"name": "No Original Alignment Tags"}, {"name": "Memory overhead per job"}], "output": [{"name": "Realigned BAM", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1478778230, "dateCreated": 1453798788, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-indelrealigner-scatter/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-indelrealigner-scatter/12", "applicationCategory": "CommandLineTool", "name": "GATK IndelRealigner Scatter", "description": "Overview\n\nThe local realignment process is designed to consume one or more BAM files and to locally realign reads such that the number of mismatching bases is minimized across all the reads. In general, a large percent of regions requiring local realignment are due to the presence of an insertion or deletion (indels) in the individual's genome with respect to the reference genome. Such alignment artifacts result in many bases mismatching the reference near the misalignment, which are easily mistaken as SNPs. Moreover, since read mapping algorithms operate on each read independently, it is impossible to place reads on the reference genome such at mismatches are minimized across all reads. Consequently, even when some reads are correctly mapped with indels, reads covering the indel near just the start or end of the read are often incorrectly mapped with respect the true indel, also requiring realignment. Local realignment serves to transform regions with misalignments due to indels into clean reads containing a consensus indel suitable for standard variant discovery approaches. Unlike most mappers, this walker uses the full alignment context to determine whether an appropriate alternate reference (i.e. indel) exists. Following local realignment, the GATK tool Unified Genotyper can be used to sensitively and specifically identify indels.\n\nThere are 2 steps to the realignment process:\n\n1. Determining (small) suspicious intervals which are likely in need of realignment (see the RealignerTargetCreator tool)\n2. Running the realigner over those intervals (IndelRealigner)\nFor more details, see the indel realignment method documentation.\n\nInput\nOne or more aligned BAM files and optionally one or more lists of known indels.\n\nOutput\nA realigned version of your input BAM file(s).\n\nUsage example:\n java -jar GenomeAnalysisTK.jar \\\n   -T IndelRealigner \\\n   -R reference.fasta \\\n   -I input.bam \\\n   --known indels.vcf \\\n   -targetIntervals intervalListFromRTC.intervals \\\n   -o realignedBam.bam\n \nCaveats\n\nThe input BAM(s), reference, and known indel file(s) should be the same ones to be used for the IndelRealigner step.\nBecause reads produced from the 454 technology inherently contain false indels, the realigner will not work with them (or with reads from similar technologies).\nThis tool also ignores MQ0 reads and reads with consecutive indel operators in the CIGAR string.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Target Intervals"}, {"name": "Known Alleles", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Consensus Determination Model"}, {"name": "Lod Threshold For Cleaning"}, {"name": "Entropy Threshold"}, {"name": "Max Consensuses"}, {"name": "Max Isize For Movement"}, {"name": "Max Positional Move Allowed"}, {"name": "Max Reads For Consensuses"}, {"name": "Max Reads For Realignment"}, {"name": "Max Reads In Memory"}, {"name": "No Original Alignment Tags"}, {"name": "Memory overhead per job"}], "output": [{"name": "Realigned BAM", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1468402326, "dateCreated": 1453799783, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-4-0-index-feature-file/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-4-0-index-feature-file/14", "applicationCategory": "CommandLineTool", "name": "GATK IndexFeatureFile", "description": "Creates indices for Feature-containing files, such as VCF and BED files.\n\n###**Overview** \nCreates indices for Feature-containing files, such as VCF and BED files.\n\n###**Inputs**  \nA feature file to be indexed.  \n\n###**Outputs**  \nAn index file.\n\n###**Usage example**\n\n    java -Xmx4g -jar gatk.jar \\  \n         IndexFeatureFile \\  \n         --feature_file feature_file.vcf \\ \n         --output feature_file.vcf.idx", "input": [{"name": "Feature File"}, {"name": "Quiet"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Verbosity"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}], "output": [{"name": "Output file with index"}, {"name": "Index file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1501857799, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-leftalignvariants/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-leftalignvariants/8", "applicationCategory": "CommandLineTool", "name": "GATK LeftAlignVariants", "description": "LeftAlignVariants GATKv2-Lite is a tool that left-aligns the indels inside a VCF file. This will happen for indels placed at multiple position that still represent the same haplotype. This tool can handle only bi-allelic, simple indels.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Variants", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Memory overhead per job"}], "output": [{"name": "Left Aligned VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1453799114, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-mergevcfs-4-2-0-0-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-mergevcfs-4-2-0-0-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "GATK MergeVcfs", "description": "**GATK MergeVcfs**  can be used to combine multiple variant files [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**GATK MergeVcfs** can be used to combine multiple VCF files into a single variant file. If samples are present in the input variant files, they should match between all files [1]. Similarly, the files must have compatible headers and sorted (contig and position) variant records [1].\n\n### Changes Introduced by Seven Bridges\n\n* The following command line parameters have been omitted from the wrapper:  `--CREATE_MD5_FILE`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--TMP_DIR`,`--version`, `--VALIDATION_STRINGENCY` and `--showHidden`.\n* **Output file name suffix** input parameter was added to control the format of the created outputs.\n* **Use a LIST file of provided inputs** parameter was added to control whether **Input variants** files should be added to the command line directly (via `--INPUT`) or collated as a list of file paths (`--INPUT inputs.list`), which is useful for handling a large number of inputs.\n* If an output file name prefix is not provided by the user via the **Output file name prefix** input parameter, outputs will be named based on the **Sample ID** metadata field of the first of the **Input variants** files received, or, in the absence of this field, based on the root of the first file name.\n* As issues were encountered when testing the tool with BCF inputs (BCF 2.2 not supported) and creating BCF outputs from VCF inputs, BCF support was not included in this wrapper. If needed, the functionality can be restored on request.\n\n### Common Issues and Important Notes\n\n* **Input variants** and **Output file name suffix** inputs are required.\n\n### Performance Benchmarking\n\nThe tool was tested with 50 WGS NA12878 GVCF.GZ shards aligned to GRCh38.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 50 GVCF.GZ shards | 26 min |$0.17 + $0.07 | c4.2xlarge  - 1024 GB EBS | \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**GATK MergeVcfs** was tested with cwltool version 3.1.20210628163208.\n\n### References\n\n[1] [MergeVcfs documentation](https://gatk.broadinstitute.org/hc/en-us/articles/360037226612-MergeVcfs-Picard-)", "input": [{"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Output file name prefix"}, {"name": "Output file name suffix"}, {"name": "Use a LIST file of provided inputs"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Arguments files", "encodingFormat": "text/plain"}, {"name": "Comments for the merged header"}, {"name": "Compression level"}, {"name": "Create index"}, {"name": "Maximum number of records in RAM"}, {"name": "Quiet"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Sequence dictionary"}, {"name": "Use JDK deflater"}, {"name": "Use JDK inflater"}, {"name": "Verbosity"}], "output": [{"name": "Merged variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/tag/4.2.0.0"], "applicationSubCategory": ["VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648040157, "dateCreated": 1634729623, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk_printreads/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk_printreads/2", "applicationCategory": "CommandLineTool", "name": "GATK PrintReads", "description": "###**Overview**  \n\nPrintReads is a generic utility tool for manipulating sequencing data in SAM/BAM format. It can dynamically merge the contents of multiple input BAM files, resulting in merged output sorted in coordinate order. It can also optionally filter reads based on various read properties such as read group tags using the `--read_filter/-rf` command line argument (see documentation on read filters for more information).\n\nNote that when PrintReads is used as part of the Base Quality Score Recalibration workflow, it takes the `--BQSR` engine argument, which is listed under Inherited Arguments > CommandLineGATK below. \n\n###**Input**  \nOne or more bam files.\n\n###**Output**  \nA single processed bam file.\n\n###**Usage examples**  \n\n    // Prints all reads that have a mapping quality above zero  \n    java -jar GenomeAnalysisTK.jar \\  \n         -T PrintReads \\  \n         -R reference.fasta \\  \n         -I input1.bam \\  \n         -I input2.bam \\  \n         -o output.bam \\  \n         --read_filter MappingQualityZero  \n\n    // Prints the first 2000 reads in the BAM file  \n    java -jar GenomeAnalysisTK.jar \\  \n         -T PrintReads \\  \n         -R reference.fasta \\  \n         -I input.bam \\  \n         -o output.bam \\  \n         -n 2000  \n\n    // Downsamples BAM file to 25%   \n    java -jar GenomeAnalysisTK.jar \\  \n         -T PrintReads \\  \n         -R reference.fasta \\  \n         -I input.bam \\  \n         -o output.bam \\  \n         -dfrac 0.25  \n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reads", "encodingFormat": "application/x-bam"}, {"name": "Add Output Sam Program Record"}, {"name": "Arguments File"}, {"name": "Cloud Index Prefetch Buffer"}, {"name": "Cloud Prefetch Buffer"}, {"name": "Create Output Bam Index"}, {"name": "Create Output Bam Md5"}, {"name": "Create Output Variant Index"}, {"name": "Create Output Variant Md5"}, {"name": "Disable Bam Index Caching"}, {"name": "Disable Read Filter"}, {"name": "Disable Sequence Dictionary Validation"}, {"name": "Disable Tool Default Read Filters"}, {"name": "Help"}, {"name": "Interval Exclusion Padding"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Lenient"}, {"name": "Quiet"}, {"name": "Read Filter"}, {"name": "Read Index"}, {"name": "Read Validation Stringency"}, {"name": "Reference"}, {"name": "Seconds Between Progress Updates"}, {"name": "Tmp Dir"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Verbosity"}, {"name": "Version"}, {"name": "Show Hidden"}, {"name": "Ambig Filter Frac"}, {"name": "Max Fragment Length"}, {"name": "Library"}, {"name": "Maximum Mapping Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Dont Require Soft Clips Both Ends"}, {"name": "Filter Too Short"}, {"name": "Pl Filter Name"}, {"name": "Black Listed Lanes"}, {"name": "Black List"}, {"name": "Keep Read Group"}, {"name": "Max Read Length"}, {"name": "Min Read Length"}, {"name": "Read Name"}, {"name": "Keep Reverse"}, {"name": "Sample"}, {"name": "Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Exclude Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Intervals String"}, {"name": "Exclude Intervals String"}], "output": [{"name": "Recalibrated BAM", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649159218, "dateCreated": 1545047556, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-printreads-scatter/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-printreads-scatter/11", "applicationCategory": "CommandLineTool", "name": "GATK PrintReads Scatter", "description": "Overview\n\nPrintReads is a generic utility tool for manipulating sequencing data in SAM/BAM format. It can dynamically merge the contents of multiple input BAM files, resulting in merged output sorted in coordinate order. It can also optionally filter reads based on various read properties such as read group tags using the `--read_filter/-rf` command line argument (see documentation on read filters for more information).\n\nNote that when PrintReads is used as part of the Base Quality Score Recalibration workflow, it takes the `--BQSR` engine argument, which is listed under Inherited Arguments > CommandLineGATK below.\n\nInput\nOne or more bam files.\n\nOutput\nA single processed bam file.\n\nUsage examples:\n\n // Prints all reads that have a mapping quality above zero\n java -jar GenomeAnalysisTK.jar \\\n   -T PrintReads \\\n   -R reference.fasta \\\n   -I input1.bam \\\n   -I input2.bam \\\n   -o output.bam \\\n   --read_filter MappingQualityZero\n\n // Prints the first 2000 reads in the BAM file\n java -jar GenomeAnalysisTK.jar \\\n   -T PrintReads \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o output.bam \\\n   -n 2000\n\n // Downsamples BAM file to 25%\n java -jar GenomeAnalysisTK.jar \\\n   -T PrintReads \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o output.bam \\\n   -dfrac 0.25\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "BQSR"}, {"name": "Sample File"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Number"}, {"name": "Platform"}, {"name": "Read Group"}, {"name": "Sample Name"}, {"name": "Simplify"}, {"name": "Memory overhead per job"}], "output": [{"name": "Recalibrated BAM", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151252, "dateCreated": 1453799616, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-realignertargetcreator/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-realignertargetcreator/13", "applicationCategory": "CommandLineTool", "name": "GATK RealignerTargetCreator", "description": "Overview\n\nThe local realignment process is designed to consume one or more BAM files and to locally realign reads such that the number of mismatching bases is minimized across all the reads. In general, a large percent of regions requiring local realignment are due to the presence of an insertion or deletion (indels) in the individual's genome with respect to the reference genome. Such alignment artifacts result in many bases mismatching the reference near the misalignment, which are easily mistaken as SNPs. Moreover, since read mapping algorithms operate on each read independently, it is impossible to place reads on the reference genome such that mismatches are minimized across all reads. Consequently, even when some reads are correctly mapped with indels, reads covering the indel near just the start or end of the read are often incorrectly mapped with respect the true indel, also requiring realignment. Local realignment serves to transform regions with misalignments due to indels into clean reads containing a consensus indel suitable for standard variant discovery approaches. Unlike most mappers, this tool uses the full alignment context to determine whether an appropriate alternate reference (i.e. indel) exists.\n\nThere are 2 steps to the realignment process:\nDetermining (small) suspicious intervals which are likely in need of realignment (RealignerTargetCreator)\nRunning the realigner over those intervals (see the IndelRealigner tool)\nFor more details, see the indel realignment method documentation.\n\nInputs\nOne or more aligned BAM files and optionally, one or more lists of known indels.\n\nOutput\nA list of target intervals to pass to the IndelRealigner.\n\nUsage example:\n java -jar GenomeAnalysisTK.jar \\\n   -T RealignerTargetCreator \\\n   -R reference.fasta \\\n   -I input.bam \\\n   --known indels.vcf \\\n   -o forIndelRealigner.intervals\n \nNotes\n\nThe input BAM(s), reference, and known indel file(s) should be the same ones to be used for the IndelRealigner step.\nWhen multiple potential indels are found by the tool in the same general region, the tool will choose the most likely one for realignment to the exclusion of the others. This is a known limitation of the tool.\nBecause reads produced from the 454 technology inherently contain false indels, the realigner will not work with them (or with reads from similar technologies).\nThis tool also ignores MQ0 reads and reads with consecutive indel operators in the CIGAR string.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Known indels", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Maximum interval size"}, {"name": "Minimum reads at locus"}, {"name": "Mismatch fraction"}, {"name": "Window size"}, {"name": "Memory overhead per job"}], "output": [{"name": "Intervals"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Analysis"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270166, "dateCreated": 1453799670, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-realignertargetcreator-scatter/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-realignertargetcreator-scatter/15", "applicationCategory": "CommandLineTool", "name": "GATK RealignerTargetCreator Scatter", "description": "Overview\n\nThe local realignment process is designed to consume one or more BAM files and to locally realign reads such that the number of mismatching bases is minimized across all the reads. In general, a large percent of regions requiring local realignment are due to the presence of an insertion or deletion (indels) in the individual's genome with respect to the reference genome. Such alignment artifacts result in many bases mismatching the reference near the misalignment, which are easily mistaken as SNPs. Moreover, since read mapping algorithms operate on each read independently, it is impossible to place reads on the reference genome such that mismatches are minimized across all reads. Consequently, even when some reads are correctly mapped with indels, reads covering the indel near just the start or end of the read are often incorrectly mapped with respect the true indel, also requiring realignment. Local realignment serves to transform regions with misalignments due to indels into clean reads containing a consensus indel suitable for standard variant discovery approaches. Unlike most mappers, this tool uses the full alignment context to determine whether an appropriate alternate reference (i.e. indel) exists.\n\nThere are 2 steps to the realignment process:\nDetermining (small) suspicious intervals which are likely in need of realignment (RealignerTargetCreator)\nRunning the realigner over those intervals (see the IndelRealigner tool)\nFor more details, see the indel realignment method documentation.\n\nInputs\nOne or more aligned BAM files and optionally, one or more lists of known indels.\n\nOutput\nA list of target intervals to pass to the IndelRealigner.\n\nUsage example:\n java -jar GenomeAnalysisTK.jar \\\n   -T RealignerTargetCreator \\\n   -R reference.fasta \\\n   -I input.bam \\\n   --known indels.vcf \\\n   -o forIndelRealigner.intervals\n \nNotes\n\nThe input BAM(s), reference, and known indel file(s) should be the same ones to be used for the IndelRealigner step.\nWhen multiple potential indels are found by the tool in the same general region, the tool will choose the most likely one for realignment to the exclusion of the others. This is a known limitation of the tool.\nBecause reads produced from the 454 technology inherently contain false indels, the realigner will not work with them (or with reads from similar technologies).\nThis tool also ignores MQ0 reads and reads with consecutive indel operators in the CIGAR string.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Known indels", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Maximum interval size"}, {"name": "Minimum reads at locus"}, {"name": "Mismatch fraction"}, {"name": "Window size"}, {"name": "Memory overhead per job"}], "output": [{"name": "Intervals"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Analysis"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1471539421, "dateCreated": 1453799061, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-4-0-selectvariants/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-4-0-selectvariants/16", "applicationCategory": "CommandLineTool", "name": "GATK SelectVariants", "description": "Select a subset of variants from a VCF file.\n\n###**Overview**  \n\nThis tool allows you to select a subset of variants based on various criteria in order to facilitate certain analyses such as comparing and contrasting cases vs. controls, extracting variant or non-variant loci that meet certain requirements, or troubleshooting some unexpected results, to name but a few.\n\nThere are many different options for selecting subsets of variants from a larger callset:\n\nExtract one or more samples from a callset based on either a complete sample name or a pattern match.\nSpecify criteria for inclusion that place thresholds on annotation values, e.g. \"DP > 1000\" (depth of coverage greater than 1000x), \"AF < 0.25\" (sites with allele frequency less than 0.25). These criteria are written as \"JEXL expressions\", which are documented in the article about using JEXL expressions.\nProvide concordance or discordance tracks in order to include or exclude variants that are also present in other given callsets.\nSelect variants based on criteria like their type (e.g. INDELs only), evidence of mendelian violation, filtering status, allelicity, and so on.\nThere are also several options for recording the original values of certain annotations that are recalculated when a subsetting the new callset, trimming alleles, and so on.\n\n###**Input**  \n\nA variant call set in VCF format from which to select a subset.  \n\n###**Output**  \n\nA new VCF file containing the selected subset of variants.  \n\n###**Usage examples**   \n\n    ./gatk-launch SelectVariants \\\n     \t-R reference.fasta \\\n     \t-V input.vcf \\\n     \t-selectType SNP \\\n     \t-O output.vcf\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Variant"}, {"name": "Add Output Sam Program Record"}, {"name": "Cloud Index Prefetch Buffer"}, {"name": "Cloud Prefetch Buffer"}, {"name": "Concordance"}, {"name": "Create Output Bam Index"}, {"name": "Create Output Bam Md5"}, {"name": "Create Output Variant Index"}, {"name": "Create Output Variant Md5"}, {"name": "Disable Bam Index Caching"}, {"name": "Disable Read Filter"}, {"name": "Disable Sequence Dictionary Validation"}, {"name": "Disable Tool Default Read Filters"}, {"name": "Discordance"}, {"name": "Exclude Sample Expressions"}, {"name": "Exclude Sample File"}, {"name": "Exclude Sample Name"}, {"name": "Exclude Filtered"}, {"name": "Exclude I Ds"}, {"name": "Exclude Non Variants"}, {"name": "Input"}, {"name": "Interval Exclusion Padding"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Invert Mendelian Violation"}, {"name": "Invert Select"}, {"name": "Keep I Ds"}, {"name": "Keep Original Ac"}, {"name": "Keep Original Dp"}, {"name": "Lenient"}, {"name": "Max Filtered Genotypes"}, {"name": "Max Fraction Filtered Genotypes"}, {"name": "Max Indel Size"}, {"name": "Max Nocal Lfraction"}, {"name": "Max Nocal Lnumber"}, {"name": "Mendelian Violation"}, {"name": "Mendelian Violation Qual Threshold"}, {"name": "Min Filtered Genotypes"}, {"name": "Min Fraction Filtered Genotypes"}, {"name": "Min Indel Size"}, {"name": "Pedigree"}, {"name": "Preserve Alleles"}, {"name": "Quiet"}, {"name": "Read Filter"}, {"name": "Read Index"}, {"name": "Read Validation Stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Remove Fraction Genotypes"}, {"name": "Remove Unused Alternates"}, {"name": "Restrict Alleles To"}, {"name": "Sample Expressions"}, {"name": "Sample File"}, {"name": "Sample Name"}, {"name": "Seconds Between Progress Updates"}, {"name": "Select Random Fraction"}, {"name": "Select Expressions"}, {"name": "Select Type To Exclude"}, {"name": "Select Type To Include"}, {"name": "Set Filtered Gt To Nocall"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Verbosity"}, {"name": "Ambig Filter Frac"}, {"name": "Max Fragment Length"}, {"name": "Library"}, {"name": "Maximum Mapping Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Dont Require Soft Clips Both Ends"}, {"name": "Filter Too Short"}, {"name": "Pl Filter Name"}, {"name": "Black Listed Lanes"}, {"name": "Black List"}, {"name": "Keep Read Group"}, {"name": "Max Read Length"}, {"name": "Min Read Length"}, {"name": "Read Name"}, {"name": "Keep Reverse"}, {"name": "Sample"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}, {"name": "Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Exclude Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Intervals String"}, {"name": "Exclude Intervals String"}, {"name": "Ambig Filter Bases"}, {"name": "Gcs Max Retries"}, {"name": "Interval Merging Rule"}], "output": [{"name": "Select Variants VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1509554701, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-unifiedgenotyper/34", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-unifiedgenotyper/34", "applicationCategory": "CommandLineTool", "name": "GATK UnifiedGenotyper", "description": "Overview\n\nThis tool uses a Bayesian genotype likelihood model to estimate simultaneously the most likely genotypes and allele frequency in a population of N samples, emitting a genotype for each sample. The system can either emit just the variant sites or complete genotypes (which includes homozygous reference calls) satisfying some phred-scaled confidence value.\n\nInput\nThe read data from which to make variant calls.\n\nOutput\nA raw, unfiltered, highly sensitive callset in VCF format.\n\nUsage examples:\n\n//Multi-sample SNP calling\n java -jar GenomeAnalysisTK.jar \\\n   -T UnifiedGenotyper \\\n   -R reference.fasta \\\n   -I sample1.bam [-I sample2.bam ...] \\\n   --dbsnp dbSNP.vcf \\\n   -o snps.raw.vcf \\\n   -stand_call_conf [50.0] \\\n   -stand_emit_conf 10.0 \\\n   [-L targets.interval_list]\n \n//Generate calls at all sites\n java -jar GenomeAnalysisTK.jar \\\n   -T UnifiedGenotyper \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o raw_variants.vcf \\\n   --output_mode EMIT_ALL_SITES\n \nCaveats\n\nThe caller can be very aggressive in calling variants in order to be very sensitive, so the raw output will contain many false positives. We use extensive post-calling filters to eliminate most of these FPs. See the documentation on filtering (especially by Variant Quality Score Recalibration) for more details.\nThis tool has been deprecated in favor of HaplotypeCaller, a much more sophisticated variant caller that produces much better calls, especially on indels, and includes features that allow it to scale to much larger cohort sizes.\nSpecial note on ploidy\n\nThis tool is able to handle almost any ploidy (except very high ploidies in large pooled experiments); the ploidy can be specified using the -ploidy argument for non-diploid organisms.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals"}, {"name": "Gatk key"}, {"name": "BQSR"}, {"name": "DbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Alleles", "encodingFormat": "application/x-vcf"}, {"name": "Comp"}, {"name": "Reference Sample Calls"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Annotate Nda"}, {"name": "Annotation"}, {"name": "Compute Slod"}, {"name": "Contamination"}, {"name": "Exclude Annotation"}, {"name": "Genotype Likelihoods Model"}, {"name": "Genotyping Mode"}, {"name": "Group"}, {"name": "Heterozygosity"}, {"name": "Ignore Lane Info"}, {"name": "Indel Heterozygosity"}, {"name": "Max Deletion Fraction"}, {"name": "Min Base Quality Score"}, {"name": "Min Indel Cnt"}, {"name": "Min Indel Frac"}, {"name": "Output Mode"}, {"name": "Pair Hmm Implementation"}, {"name": "Pcr Error Rate"}, {"name": "Stand Call Conf"}, {"name": "Stand Emit Conf"}, {"name": "Indel Gap Continuation Penalty"}, {"name": "Indel Gap Open Penalty"}, {"name": "Max Alternate Alleles"}, {"name": "P Nonref Model"}, {"name": "Memory overhead per job"}], "output": [{"name": "VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Variant-Calling"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1486477252, "dateCreated": 1453798870, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-unifiedgenotyper-scatter/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-unifiedgenotyper-scatter/8", "applicationCategory": "CommandLineTool", "name": "GATK UnifiedGenotyper Scatter", "description": "Overview\n\nThis tool uses a Bayesian genotype likelihood model to estimate simultaneously the most likely genotypes and allele frequency in a population of N samples, emitting a genotype for each sample. The system can either emit just the variant sites or complete genotypes (which includes homozygous reference calls) satisfying some phred-scaled confidence value.\n\nInput\nThe read data from which to make variant calls.\n\nOutput\nA raw, unfiltered, highly sensitive callset in VCF format.\n\nUsage examples:\n\n//Multi-sample SNP calling\n java -jar GenomeAnalysisTK.jar \\\n   -T UnifiedGenotyper \\\n   -R reference.fasta \\\n   -I sample1.bam [-I sample2.bam ...] \\\n   --dbsnp dbSNP.vcf \\\n   -o snps.raw.vcf \\\n   -stand_call_conf [50.0] \\\n   -stand_emit_conf 10.0 \\\n   [-L targets.interval_list]\n \n//Generate calls at all sites\n java -jar GenomeAnalysisTK.jar \\\n   -T UnifiedGenotyper \\\n   -R reference.fasta \\\n   -I input.bam \\\n   -o raw_variants.vcf \\\n   --output_mode EMIT_ALL_SITES\n \nCaveats\n\nThe caller can be very aggressive in calling variants in order to be very sensitive, so the raw output will contain many false positives. We use extensive post-calling filters to eliminate most of these FPs. See the documentation on filtering (especially by Variant Quality Score Recalibration) for more details.\nThis tool has been deprecated in favor of HaplotypeCaller, a much more sophisticated variant caller that produces much better calls, especially on indels, and includes features that allow it to scale to much larger cohort sizes.\nSpecial note on ploidy\n\nThis tool is able to handle almost any ploidy (except very high ploidies in large pooled experiments); the ploidy can be specified using the -ploidy argument for non-diploid organisms.\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Read sequences", "encodingFormat": "application/x-bam"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "BQSR"}, {"name": "DbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Alleles", "encodingFormat": "application/x-vcf"}, {"name": "Comp"}, {"name": "Reference Sample Calls"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Annotate Nda"}, {"name": "Annotation"}, {"name": "Compute Slod"}, {"name": "Contamination"}, {"name": "Exclude Annotation"}, {"name": "Genotype Likelihoods Model"}, {"name": "Genotyping Mode"}, {"name": "Group"}, {"name": "Heterozygosity"}, {"name": "Ignore Lane Info"}, {"name": "Indel Heterozygosity"}, {"name": "Max Deletion Fraction"}, {"name": "Min Base Quality Score"}, {"name": "Min Indel Cnt"}, {"name": "Min Indel Frac"}, {"name": "Output Mode"}, {"name": "Pair Hmm Implementation"}, {"name": "Pcr Error Rate"}, {"name": "Stand Call Conf"}, {"name": "Stand Emit Conf"}, {"name": "Indel Gap Continuation Penalty"}, {"name": "Indel Gap Open Penalty"}, {"name": "Max Alternate Alleles"}, {"name": "P Nonref Model"}, {"name": "Memory overhead per job"}], "output": [{"name": "Raw VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151401, "dateCreated": 1453799191, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-variantannotator/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-variantannotator/2", "applicationCategory": "CommandLineTool", "name": "GATK VariantAnnotator", "description": "###**Overview**  \n\nTool for adding annotations to VCF files.\n\n###**Inputs**  \n\nA variant set to annotate and optionally one or more BAM files.\n\n###**Output**  \n\nAn annotated VCF.\n\n###**Usage example**  \n\n    java -jar gatk.jar \\  \n         VariantAnnotator \\ \n         --variant variant.vcf \\\n         --input input.bam \\\n         --reference reference.fasta \\\n         --dbsnp dbsnp.vcf", "input": [{"name": "Variant", "encodingFormat": "application/x-vcf"}, {"name": "Add Output Sam Program Record"}, {"name": "Add Output Vcf Command Line"}, {"name": "Annotation"}, {"name": "Annotation Group"}, {"name": "Annotations To Exclude"}, {"name": "Create Output Bam Index"}, {"name": "Create Output Bam Md5"}, {"name": "Create Output Variant Index"}, {"name": "Create Output Variant Md5"}, {"name": "Dbsnp", "encodingFormat": "application/x-vcf"}, {"name": "Disable Bam Index Caching"}, {"name": "Disable Read Filter"}, {"name": "Disable Sequence Dictionary Validation"}, {"name": "Exclude Intervals"}, {"name": "Expression"}, {"name": "Founder Id"}, {"name": "Gcs Max Retries"}, {"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Interval Exclusion Padding"}, {"name": "Interval Merging Rule"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Intervals"}, {"name": "Lenient"}, {"name": "Pedigree"}, {"name": "Read Filter"}, {"name": "Read Index"}, {"name": "Read Validation Stringency"}, {"name": "Reference"}, {"name": "Resource"}, {"name": "Resource Allele Concordance"}, {"name": "Sites Only Vcf Output"}, {"name": "Comp"}, {"name": "Disable Tool Default Annotations"}, {"name": "Disable Tool Default Read Filters"}, {"name": "Enable All Annotations"}, {"name": "Showhidden"}, {"name": "Ambig Filter Bases"}, {"name": "Ambig Filter Frac"}, {"name": "Max Fragment Length"}, {"name": "Library"}, {"name": "Maximum Mapping Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Dont Require Soft Clips Both Ends"}, {"name": "Filter Too Short"}, {"name": "Platform Filter Name"}, {"name": "Black Listed Lanes"}, {"name": "Read Group Black List"}, {"name": "Keep Read Group"}, {"name": "Max Read Length"}, {"name": "Min Read Length"}, {"name": "Read Name"}, {"name": "Keep Reverse Strand Only"}, {"name": "Sample"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}], "output": [{"name": "Annotated Variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1530120366, "dateCreated": 1530120300, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-varianteval/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-varianteval/1", "applicationCategory": "CommandLineTool", "name": "GATK VariantEval", "description": "Overview\nGiven a variant callset, it is common to calculate various quality control metrics. These metrics include the number of raw or filtered SNP counts; ratio of transition mutations to transversions; concordance of a particular sample's calls to a genotyping chip; number of singletons per sample; etc. Furthermore, it is often useful to stratify these metrics by various criteria like functional class (missense, nonsense, silent), whether the site is CpG site, the amino acid degeneracy of the site, etc. VariantEval facilitates these calculations in two ways: by providing several built-in evaluation and stratification modules, and by providing a framework that permits the easy development of new evaluation and stratification modules.\n\nInput\nOne or more variant sets to evaluate plus any number of comparison sets.\n\nOutput\nEvaluation tables detailing the results of the eval modules which were applied. \n \nUsage example\njava -jar GenomeAnalysisTK.jar \\\n   -T VariantEval \\\n   -R reference.fasta \\\n   -o output.eval.grp \\\n   --eval:set1 set1.vcf \\\n   --eval:set2 set2.vcf \\\n   [--comp comp.vcf]\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Evaluation File(s)", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Memory overhead per job"}, {"name": "Comparison File(s)", "encodingFormat": "application/x-vcf"}, {"name": "dbSNP File", "encodingFormat": "application/x-vcf"}, {"name": "Gold Standard"}, {"name": "Select Stratification(s)"}, {"name": "Select Name(s)"}, {"name": "Sample Genotypes"}, {"name": "ROD Bindings"}, {"name": "Stratification Module"}, {"name": "No Standard Stratifications"}, {"name": "Evaluation Module"}, {"name": "No Standard Modules"}, {"name": "Minimum Phasing Quality"}, {"name": "Minimum Genotype Quality Score"}, {"name": "Per Sample Ploidy"}, {"name": "Ancestral Alignments File", "encodingFormat": "application/x-fasta"}, {"name": "Strict Alelle Match"}, {"name": "AC not Greater than Zero"}, {"name": "Merge Evaluations"}, {"name": "Stratification Intervals", "encodingFormat": "text/plain"}, {"name": "Known CNVs File", "encodingFormat": "text/plain"}], "output": [{"name": "Evaluated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "Broad", "softwareVersion": ["sbg:draft-2"], "dateModified": 1453799483, "dateCreated": 1453799482, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-variantfiltration-4-2-0-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-variantfiltration-4-2-0-0/4", "applicationCategory": "CommandLineTool", "name": "GATK VariantFiltration", "description": "**GATK VariantFiltration** is used for filtering variants in a VCF file based on INFO and/or FORMAT annotations [1]. \n\nThis tool is designed for hard-filtering variant calls based on certain criteria. Records are hard-filtered by changing the value in the FILTER field to something other than PASS. Filtered records will be preserved in the output unless their removal is requested in the command line [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n###Common Use Cases\n\n* The **GATK VariantFiltration** tool requires a VCF file on its **Input variants** (`--variant`) input. The tool generates a filtered VCF file on its **Output filtered variants** output in which passing variants are annotated as PASS and failing variants are annotated with the name(s) of the filter(s) they failed [1].\n\n* Usage example:\n```\n   gatk --java-options \"-Xmx2048M\" VariantFiltration \\\n   --reference reference.fasta \\\n   --variant input.vcf.gz \\\n   --output output.vcf.gz \\\n   --filter-name \"my_filter1\" \\\n   --filter-expression \"AB < 0.2\" \\\n   --filter-name \"my_filter2\" \\\n   --filter-expression \"MQ0 > 50\"\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals file** and **Include intervals string** options.\n\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals file** and **Exclude intervals string** options.\n\n* **Output filtered variants** will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from the **Input variants** file. If the **Sample ID** value is not set, the name will be inherited from the **Input variants** file name. Moreover, **.filtered** will be added before the extension of the output file name.\n\n* The user can specify the output file format using the **Output VCF extension** argument. Otherwise, the output will be in the compressed VCF file format.\n\n* The following parameters were excluded from the tool wrapper: `--arguments_file`, `--add-output-sam-program-record`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-bam-index`, `--create-output-bam-md5`, `--disable-read-filter`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--input`, `--QUIET`, `--read-filter`, `--read-index`, `--read-validation-stringency`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048 MB.\n\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the **Memory per job** in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100 MB. \n\n* Values for **Filter expression**, **Filter name**, **Genotype filter expression** and **Genotype filter name** have to be enclosed in double (\" \") or single (' ') quotation marks. \n\n* **Input mask**, **Input variants** should have corresponding index files in the same folder. \n\n* **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n\n* There must be a 1-to-1 mapping between filter expressions (**Filter expression**) and filter names (**Filter name**) [1].\n\n* There must be a 1-to-1 mapping between genotype filter expressions (**Genotype filter expression**) and genotype filter names (**Genotype filter name**) [1].\n\n* Compound expressions (ones that specify multiple conditions connected by &&, AND, ||, or OR, and reference multiple attributes) require special consideration. By default, variants that are missing one or more of the attributes referenced in a compound expression are treated as PASS for the entire expression, even if the variant would satisfy the filter criteria for another part of the expression. This can lead to unexpected results if any of the attributes referenced in a compound expression are present for some variants, but missing for others [1].\n\nIt is strongly recommended that such expressions be provided as individual arguments, each referencing a single attribute and specifying a single criteria. This ensures that all of the individual expressions are applied to each variant, even if a given variant is missing values for some of the expression conditions [1].\n\nAs an example, multiple individual expressions provided like this [1]:\n\n```\n   gatk VariantFiltration \\\n   --reference reference.fasta \\\n   --variant input.vcf.gz \\\n   --output output.vcf.gz \\\n   --filter-name \"my_filter1\" \\\n   --filter-expression \"AB < 0.2\" \\\n   --filter-name \"my_filter2\" \\\n   --filter-expression \"MQ0 > 50\"\n \n```\n\nare preferable to a single compound expression such as this [1]:\n\n```\n    gatk VariantFiltration \\\n    --reference reference.fasta \\\n    --variant input.vcf.gz \\\n    --output output.vcf.gz \\\n    --filter-name \"my_filter\" \\\n    --filter-expression \"AB < 0.2 || MQ0 > 50\"\n  \n```\nSee this [article about using JEXL expressions](https://gatk.broadinstitute.org/hc/en-us/articles/360035891011-JEXL-filtering-expressions) for more information [1].\n\n###Performance Benchmarking\n\nThis tool is fast, with a running time of a few minutes. The experiment task was performed on the default AWS on-demand c4.2xlarge instance on WGS VCF.GZ file (size ~241MB) and took 3 minutes to finish ($0.03).\n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n###Portability\n\n**GATK VariantFiltration** is tested with cwltool version: \"3.0.20201203173111\"\n\n###References\n\n[1] [GATK VariantFiltration](https://gatk.broadinstitute.org/hc/en-us/articles/360057440031-VariantFiltration)", "input": [{"name": "Add output vcf command line"}, {"name": "Cluster size"}, {"name": "Cluster window size"}, {"name": "Create output variant index"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default read filters"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "Exclude intervals string"}, {"name": "Filter expression"}, {"name": "Filter name"}, {"name": "Filter not in mask"}, {"name": "Genotype filter expression"}, {"name": "Genotype filter name"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Invalidate previous filters"}, {"name": "Invert filter expression"}, {"name": "Invert genotype filter expression"}, {"name": "Lenient"}, {"name": "Input mask", "encodingFormat": "text/x-bed"}, {"name": "Mask extension"}, {"name": "Mask name"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Missing values evaluate as failing"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Sequence dictionary"}, {"name": "Set filtered genotype to no call"}, {"name": "Sites only VCF output"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Output name prefix"}, {"name": "Output VCF extension"}, {"name": "CPU per job"}, {"name": "Seconds between progress updates"}, {"name": "Disable BAM index caching"}, {"name": "Apply allele specific filters"}, {"name": "Create output variant md5"}], "output": [{"name": "Output filtered variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip"], "applicationSubCategory": ["Variant Filtration", "VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648047601, "dateCreated": 1634729491, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-variantfiltration/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-variantfiltration/10", "applicationCategory": "CommandLineTool", "name": "GATK VariantFiltration ", "description": "Overview\nThis tool is designed for hard-filtering variant calls based on certain criteria. Records are hard-filtered by changing the value in the FILTER field to something other than PASS. Filtered records will be preserved in the output unless their removal is requested in the command line.\n\nInput\nA variant set to filter.\n\nOutput\nA filtered VCF.\n\nUsage example\n java -jar GenomeAnalysisTK.jar \\\n   -T VariantFiltration \\\n   -R reference.fasta \\\n   -o output.vcf \\\n   --variant input.vcf \\\n   --filterExpression \"AB < 0.2 || MQ0 > 50\" \\\n   --filterName \"Nov09filters\" \\\n   --mask mask.vcf \\\n   --maskName InDel\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Variants", "encodingFormat": "application/x-vcf"}, {"name": "Mask", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Threads per job"}, {"name": "Cluster Size"}, {"name": "Cluster Window Size"}, {"name": "Invalidate Previous Filters"}, {"name": "Mask Extension"}, {"name": "Mask Name"}, {"name": "Missing Values In Expressions Should Evaluate As Failing"}, {"name": "FIlter name"}, {"name": "Filter expression"}, {"name": "Genotype filter name"}, {"name": "Genotype filter expression"}, {"name": "Memory overhead per job"}], "output": [{"name": "Filtered VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Broad", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151401, "dateCreated": 1453799699, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-variantfiltration-4-1-0-0/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-variantfiltration-4-1-0-0/17", "applicationCategory": "CommandLineTool", "name": "GATK VariantFiltration CWL1.0", "description": "The **GATK VariantFiltration** is used for filtering variants in a VCF file based on INFO and/or FORMAT annotations [1]. \n\nThis tool is designed for hard-filtering variant calls based on certain criteria. Records are hard-filtered by changing the value in the FILTER field to something other than PASS. Filtered records will be preserved in the output unless their removal is requested in the command line [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* The **GATK VariantFiltration** tool requires a VCF file on its **Input variants** (`--variant`) input. The tool generates a filtered VCF file on its **Output filtered variants** output.\n\n* Usage example:\n```\n   gatk --java-options \"-Xmx2048M\" VariantFiltration \\\n   --reference reference.fasta \\\n   --variant input.vcf.gz \\\n   --output output.vcf.gz \\\n   --filter-name \"my_filter1\" \\\n   --filter-expression \"AB < 0.2\" \\\n   --filter-name \"my_filter2\" \\\n   --filter-expression \"MQ0 > 50\"\n\n```\n\n###Changes Introduced by Seven Bridges\n\n* **Include intervals** (`--intervals`) option is divided into **Include intervals file** and **Include intervals string** options.\n* **Exclude intervals** (`--exclude-intervals`) option is divided into **Exclude intervals file** and **Exclude intervals string** options.\n\n* **Output filtered variants** will be prefixed using the **Output name prefix** parameter. If this value is not set, the output name will be generated based on the **Sample ID** metadata value from the **Input variants** file. If the **Sample ID** value is not set, the name will be inherited from the **Input variants** file name. Moreover, **.filtered** will be added before the extension of the output file name.\n\n* The user can specify the output file format using the **Output VCF extension** argument. Otherwise, the output will be in the compressed VCF file format.\n\n* The following parameters were excluded from the tool wrapper: `--arguments_file`, `--add-output-sam-program-record`, `--cloud-index-prefetch-buffer`, `--cloud-prefetch-buffer`, `--create-output-bam-index`, `--create-output-bam-md5`, `--create-output-variant-md5`, `--gatk-config-file`, `--gcs-max-retries`, `--gcs-project-for-requester-pays`, `--help`, `--input`, `--QUIET`, `--read-filter`, `--read-index`, `--read-validation-stringency`, `--showHidden`, `--tmp-dir`, `--use-jdk-deflater`, `--use-jdk-inflater`, `--verbosity`, `--version`\n\n###Common Issues and Important Notes\n\n*  **Memory per job** (`mem_per_job`) input allows a user to set the desired memory requirement when running a tool or adding it to a workflow. This input should be defined in MB. It is propagated to the Memory requirements part and \u201c-Xmx\u201d parameter of the tool. The default value is 2048MB.\n\n* **Memory overhead per job** (`mem_overhead_per_job`) input allows a user to set the desired overhead memory when running a tool or adding it to a workflow. This input should be defined in MB. This amount will be added to the Memory per job in the Memory requirements section but it will not be added to the \u201c-Xmx\u201d parameter. The default value is 100MB. \n\n* Values for **Filter expression**, **Filter name**, **Genotype filter expression** and **Genotype filter name** have to be enclosed in quotation marks (\" \") or apostrophe (' '). \n\n* **Input mask**, **Input variants** should have corresponding index files in the same folder. \n\n* **Reference** FASTA file should have corresponding .fai (FASTA index) and .dict (FASTA dictionary) files in the same folder. \n\n* Composing filtering expressions can range from very simple to extremely complicated depending on what you're trying to do. Compound expressions (ones that specify multiple conditions connected by &&, AND, ||, or OR, and reference multiple attributes) require special consideration. By default, variants that are missing one or more of the attributes referenced in a compound expression are treated as PASS for the entire expression, even if the variant would satisfy the filter criteria for another part of the expression. This can lead to unexpected results if any of the attributes referenced in a compound expression are present for some variants, but missing for others [1].\n\nIt is strongly recommended to provide such expressions as individual arguments, each referencing a single attribute and specifying a single criteria. This ensures that all of the individual expressions are applied to each variant, even if a given variant is missing values for some of the expression conditions [1].\n\nAs an example, multiple individual expressions provided like this [1]:\n\n```\n   gatk VariantFiltration \\\n   --reference reference.fasta \\\n   --variant input.vcf.gz \\\n   --output output.vcf.gz \\\n   --filter-name \"my_filter1\" \\\n   --filter-expression \"AB < 0.2\" \\\n   --filter-name \"my_filter2\" \\\n   --filter-expression \"MQ0 > 50\"\n \n```\n\nare preferable to a single compound expression such as this [1]:\n\n```\n    gatk VariantFiltration \\\n    --reference reference.fasta \\\n    --variant input.vcf.gz \\\n    --output output.vcf.gz \\\n    --filter-name \"my_filter\" \\\n    --filter-expression \"AB < 0.2 || MQ0 > 50\"\n  \n```\n\n###Performance Benchmarking\n\nThis tool is fast, with a running time of a few minutes. The experiment task was performed on the default AWS on-demand c4.2xlarge instance on WGS VCF.GZ file (size ~241MB) and took 3 minutes to finish ($0.03).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n###References\n\n[1] [GATK VariantFiltration](https://gatk.broadinstitute.org/hc/en-us/articles/360036726871-VariantFiltration)", "input": [{"name": "Add output vcf command line"}, {"name": "Cluster size"}, {"name": "Cluster window size"}, {"name": "Create output variant index"}, {"name": "Disable sequence dictionary validation"}, {"name": "Disable tool default read filters"}, {"name": "Exclude intervals file", "encodingFormat": "text/x-bed"}, {"name": "Exclude intervals string"}, {"name": "Filter expression"}, {"name": "Filter name"}, {"name": "Filter not in mask"}, {"name": "Genotype filter expression"}, {"name": "Genotype filter name"}, {"name": "Interval exclusion padding"}, {"name": "Interval merging rule"}, {"name": "Interval padding"}, {"name": "Interval set rule"}, {"name": "Include intervals file", "encodingFormat": "text/x-bed"}, {"name": "Include intervals string"}, {"name": "Invalidate previous filters"}, {"name": "Invert filter expression"}, {"name": "Invert genotype filter expression"}, {"name": "Lenient"}, {"name": "Input mask", "encodingFormat": "text/x-bed"}, {"name": "Mask extension"}, {"name": "Mask name"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Missing values evaluate as failing"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Sequence dictionary"}, {"name": "Set filtered genotype to no call"}, {"name": "Sites only VCF output"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Output name prefix"}, {"name": "Output VCF extension"}, {"name": "CPU per job"}, {"name": "Seconds between progress updates"}, {"name": "Disable BAM index caching"}, {"name": "Disable read filter"}], "output": [{"name": "Output filtered variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/gatk", "https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip"], "applicationSubCategory": ["Variant Filtration", "VCF Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648047600, "dateCreated": 1617275900, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-4-0-variantrecalibrator/19", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-4-0-variantrecalibrator/19", "applicationCategory": "CommandLineTool", "name": "GATK VariantRecalibrator", "description": "Build a recalibration model to score variant quality for filtering purposes.\n\n###**Overview**  \n\nThis tool performs the first pass in a two-stage process called Variant Quality Score Recalibration (VQSR). Specifically, it builds the model that will be used in the second step to actually filter variants. This model attempts to describe the relationship between variant annotations (such as QD, MQ and ReadPosRankSum, for example) and the probability that a variant is a true genetic variant versus a sequencing or data processing artifact. It is developed adaptively based on \"true sites\" provided as input, typically HapMap sites and those sites found to be polymorphic on the Omni 2.5M SNP chip array (in humans). This adaptive error model can then be applied to both known and novel variation discovered in the call set of interest to evaluate the probability that each call is real. The result is a score called the VQSLOD that gets added to the INFO field of each variant. This score is the log odds of being a true variant versus being false under the trained Gaussian mixture model.\n\n####**Summary of the VQSR procedure**\n\nThe purpose of variant recalibration is to assign a well-calibrated probability to each variant call in a call set. These probabilities can then be used to filter the variants with a greater level of accuracy and flexibility than can typically be achieved by traditional hard-filter (filtering on individual annotation value thresholds). The first pass consists of building a model that describes how variant annotation values co-vary with the truthfulness of variant calls in a training set, and then scoring all input variants according to the model. The second pass simply consists of specifying a target sensitivity value (which corresponds to an empirical VQSLOD cutoff) and applying filters to each variant call according to their ranking. The result is a VCF file in which variants have been assigned a score and filter status.\n\nVQSR is probably the hardest part of the Best Practices to get right, so be sure to read the method documentation, parameter recommendations and tutorial to really understand what these tools do and how to use them for best results on your own data. \n\n###**Inputs**  \n- The input variants to be recalibrated. These variant calls must be annotated with the annotations that will be used for modeling. If the calls come from multiple samples, they must have been obtained by joint calling the samples, either directly (running HaplotypeCaller on all samples together) or via the GVCF workflow (HaplotypeCaller with -ERC GVCF per-sample then GenotypeGVCFs on the resulting gVCFs) which is more scalable.\n- Known, truth, and training sets to be used by the algorithm. See the method documentation linked above for more details. \n\n###**Outputs**  \n- A recalibration table file that will be used by the ApplyVQSR tool.\n- A tranches file that shows various metrics of the recalibration callset for slices of the data. \n  \n###**Usage example**  \n\n####**Recalibrating SNPs in exome data**\n\n    ./gatk-launch VariantRecalibrator \\\n       -R reference.fasta \\\n       -V input.vcf \\\n       --resource hapmap,known=false,training=true,truth=true,prior=15.0 hapmap_3.3.b37.sites.vcf \\\n       --resource omni,known=false,training=true,truth=false,prior=12.0 1000G_omni2.5.b37.sites.vcf \\\n       --resource 1000G,known=false,training=true,truth=false,prior=10.0 1000G_phase1.snps.high_confidence.vcf \\\n       --resource dbsnp,known=true,training=false,truth=false,prior=2.0 dbsnp_135.b37.vcf \\\n       -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an InbreedingCoeff \\\n       -mode SNP \\\n       --recalFile output.recal \\\n       -tranchesFile output.tranches \\\n       --rscriptFile output.plots.R\n\n####**Allele-specific version of the SNP recalibration (beta)**\n\n     ./gatk-launch VariantRecalibrator \\\n       -R reference.fasta \\\n       -V input.vcf \\\n       -AS \\\n       --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.b37.sites.vcf \\\n       --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.b37.sites.vcf \\\n       --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.vcf \\\n       --resource dbsnp,known=true,training=false,truth=false,prior=2.0 dbsnp_135.b37.vcf \\\n       -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an InbreedingCoeff \\\n       -mode SNP \\\n       --recalFile output.AS.recal \\\n       --tranchesFile output.AS.tranches \\\n       --rscriptFile output.plots.AS.R\n\nNote that to use the allele-specific (AS) mode, the input VCF must have been produced using allele-specific annotations in HaplotypeCaller. Note also that each allele will have a separate line in the output recalibration file with its own VQSLOD and `culprit`, which will be transferred to the final VCF by the ApplyRecalibration tool.\n\n###**Caveats**  \n\n- The values used in the example above are only meant to show how the command lines are composed. They are not meant to be taken as specific recommendations of values to use in your own work, and they may be different from the values cited elsewhere in our documentation. For the latest and greatest recommendations on how to set parameter values for your own analyses, please read the Best Practices section of the documentation, especially the FAQ document on VQSR parameters.\n- Whole genomes and exomes take slightly different parameters, so make sure you adapt your commands accordingly! See the documents linked above for details.\n- If you work with small datasets (e.g. targeted capture experiments or small number of exomes), you will run into problems. Read the docs linked above for advice on how to deal with those issues.\n- In order to create the model reporting plots, the Rscript executable needs to be in your environment PATH (this is the scripting version of R, not the interactive version). See http://www.r-project.org for more information on how to download and install R.\n\n###**Additional notes**\n\n- This tool only accepts a single input variant file unlike earlier version of GATK, which accepted multiple input variant files.\n- SNPs and indels must be recalibrated in separate runs, but it is not necessary to separate them into different files. See the tutorial linked above for an example workflow. Note that mixed records are treated as indels.\n- By default, the tool works only with VCF resource files. To use VCF.GZ resource files, the tool wrapper needs to be modified.\n\n###**IMPORTANT NOTICE**  \n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding *.fai* (fasta index) and *.dict* (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the '***SBG FASTA Indices***' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Resource"}, {"name": "Use Annotation"}, {"name": "Variants"}, {"name": "Add Output Sam Program Record"}, {"name": "Aggregate"}, {"name": "Cloud Index Prefetch Buffer"}, {"name": "Cloud Prefetch Buffer"}, {"name": "Create Output Bam Index"}, {"name": "Create Output Bam Md5"}, {"name": "Create Output Variant Index"}, {"name": "Create Output Variant Md5"}, {"name": "Disable Bam Index Caching"}, {"name": "Disable Read Filter"}, {"name": "Disable Sequence Dictionary Validation"}, {"name": "Disable Tool Default Read Filters"}, {"name": "Ignore All Filters"}, {"name": "Ignore Filter"}, {"name": "Interval Exclusion Padding"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Lenient"}, {"name": "Mode"}, {"name": "Output Model"}, {"name": "Quiet"}, {"name": "Read Filter"}, {"name": "Read Index"}, {"name": "Read Validation Stringency"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Seconds Between Progress Updates"}, {"name": "Target Titv"}, {"name": "T Stranche"}, {"name": "Use Jdk Deflater"}, {"name": "Use Jdk Inflater"}, {"name": "Use Allele Specific Annotations"}, {"name": "Verbosity"}, {"name": "Bad Lod Cutoff"}, {"name": "Dirichlet"}, {"name": "Max Attempts"}, {"name": "Max Gaussians"}, {"name": "Max Iterations"}, {"name": "Max Negative Gaussians"}, {"name": "Max Num Training Data"}, {"name": "Min Num Bad Variants"}, {"name": "Mq Cap For Logit Jitter Transform"}, {"name": "Num K Means"}, {"name": "Prior Counts"}, {"name": "Shrinkage"}, {"name": "Std Threshold"}, {"name": "Trust All Polymorphic"}, {"name": "Ambig Filter Frac"}, {"name": "Max Fragment Length"}, {"name": "Library"}, {"name": "Maximum Mapping Quality"}, {"name": "Minimum Mapping Quality"}, {"name": "Dont Require Soft Clips Both Ends"}, {"name": "Filter Too Short"}, {"name": "Pl Filter Name"}, {"name": "Black Listed Lanes"}, {"name": "Black List"}, {"name": "Keep Read Group"}, {"name": "Max Read Length"}, {"name": "Min Read Length"}, {"name": "Read Name"}, {"name": "Keep Reverse"}, {"name": "Sample"}, {"name": "Memory Per Job"}, {"name": "Memory Overhead Per Job"}, {"name": "Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Exclude Intervals File", "encodingFormat": "text/x-bed"}, {"name": "Intervals String"}, {"name": "Exclude Intervals String"}, {"name": "Cpu Per Job"}, {"name": "Ambig Filter Bases"}, {"name": "Input Model"}, {"name": "Scatter Tranches"}, {"name": "Sample Every  Nth Variant"}], "output": [{"name": "Recalibration File"}, {"name": "Tranches File"}, {"name": "Rscript File"}, {"name": "Output Model"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["GATK-4"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1521477248, "dateCreated": 1501857799, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-variantvalidationassessor/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-variantvalidationassessor/4", "applicationCategory": "CommandLineTool", "name": "GATK VariantValidationAssessor", "description": "Overview\nThis tool is intended for vetting/assessing validation data (containing genotypes). The tool produces a VCF that is annotated with information pertaining to plate quality control and by default is soft-filtered by high no-call rate or low Hardy-Weinberg probability. If you have .ped files, please first convert them to VCF format.\n\nInput\nA validation VCF to annotate.\n\nOutput\nAn annotated VCF. Additionally, a table like the following will be output:\n\n     Total number of samples assayed:                  185\n     Total number of records processed:                152\n     Number of Hardy-Weinberg violations:              34 (22%)\n     Number of no-call violations:                     12 (7%)\n     Number of homozygous variant violations:          0 (0%)\n     Number of records passing all filters:            106 (69%)\n     Number of passing records that are polymorphic:   98 (92%)\n \nUsage example\n java -jar GenomeAnalysisTK.jar \\\n   -T VariantValidationAssessor \\\n   -R reference.fasta \\\n   -V input.vcf \\\n   -o output.vcf\n\n(IMPORTANT) Reference \".fasta\" Secondary Files\n\nTools in GATK that require a fasta reference file also look for the reference file's corresponding .fai (fasta index) and .dict (fasta dictionary) files. The fasta index file allows random access to reference bases and the dictionary file is a dictionary of the contig names and sizes contained within the fasta reference. These two secondary files are essential for GATK to work properly. To append these two files to your fasta reference please use the 'SBG FASTA Indices' tool within your GATK based workflow before using any of the GATK tools.", "input": [{"name": "Reference Genome", "encodingFormat": "application/x-fasta"}, {"name": "Exclude Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Intervals", "encodingFormat": "application/x-vcf"}, {"name": "Gatk key"}, {"name": "Variants", "encodingFormat": "application/x-vcf"}, {"name": "Disable Randomization"}, {"name": "Allow Potentially Misencoded Quals"}, {"name": "BAQ Calculation Type"}, {"name": "BAQ Gap Open Penalty"}, {"name": "Default Base Qualities"}, {"name": "Disable Indel Quals"}, {"name": "Downsample to Coverage"}, {"name": "Downsample to Fraction"}, {"name": "Downsampling Type"}, {"name": "Emit Original Quals"}, {"name": "Fix Misencoded Quals"}, {"name": "Interval Merging"}, {"name": "Interval Padding"}, {"name": "Interval Set Rule"}, {"name": "Keep Program Records"}, {"name": "Max Runtime"}, {"name": "Max Runtime Units"}, {"name": "Non Deterministic Random Seed"}, {"name": "Pedigree String"}, {"name": "Pedigree Validation Type"}, {"name": "Phone Home"}, {"name": "Preserve Qscores Less Than"}, {"name": "Read Filter"}, {"name": "Read Group Black List"}, {"name": "Remove Program Records"}, {"name": "Tag"}, {"name": "Unsafe"}, {"name": "Use Legacy Downsampler"}, {"name": "Use Original Qualities"}, {"name": "Validation Strictness"}, {"name": "Intervals"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Maximum HW violation"}, {"name": "Maximum homozygous variant rate"}, {"name": "Maximum no-call rate"}, {"name": "Memory overhead per job"}], "output": [{"name": "Assessed VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadgsa/gatk-protected"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "Broad", "softwareVersion": ["sbg:draft-2"], "dateModified": 1453798960, "dateCreated": 1453798911, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/assoc-plots-r/24", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/assoc-plots-r/24", "applicationCategory": "CommandLineTool", "name": "GENESIS Association results plotting", "description": "### Description\n\nThe UW-GAC GENESIS Association Result Plotting standalone app creates Manhattan and QQ plots from GENESIS association test results with additional filtering and stratification options available. This app is run automatically with default options set by the GENESIS Association Testing Workflows. Users can fine-tune the Manhattan and QQ plots by running this app separately, after one of the association testing workflows. The available options are:\n - Create QQ plots by chromosome.\n - Include a user-specified subset of the results in the plots.\n - Filter results to only those with MAC or MAF greater than a specified threshold.\n - Calculate genomic inflation lambda at various quantiles.\n - Specify the significance type and level.\n - Create QQ plots stratified by MAC or MAF.\n - Specify a maximum p-value to display on the plots.\n\n### Common use cases\n\nThe UW-GAC GENESIS Association Result Plotting standalone app creates Manhattan and QQ plots from GENESIS association test results with additional filtering and stratification options available.\n\n### Changes introduced by Seven Bridges\n\nNo changes introduced by Seven Bridges.", "input": [{"name": "Results from association testing"}, {"name": "Association Type"}, {"name": "Chromosomes"}, {"name": "Plots prefix"}, {"name": "Disable Thin"}, {"name": "Known hits file"}, {"name": "Number of points in each bin after thinning"}, {"name": "Thin N binsNumber of bins to use for thinning"}, {"name": "Plot MAC threshold"}, {"name": "Truncate pval threshold"}, {"name": "Plot qq by chromosome"}, {"name": "Plot include file"}, {"name": "Significance type"}, {"name": "Significance line"}, {"name": "QQ MAC bins"}, {"name": "QQ MAF bins"}, {"name": "Lambda quantiles"}, {"name": "Lambda outfile name"}, {"name": "Plot max p"}, {"name": "Plot MAF threshold"}], "output": [{"name": "Assoc plots"}, {"name": "Config files"}, {"name": "File to store lambda calculated at different quantiles", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1635438674, "dateCreated": 1617987590, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gffcompare-0-11-6/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gffcompare-0-11-6/5", "applicationCategory": "CommandLineTool", "name": "GffCompare", "description": "__GffCompare__ [1] is a part of the GFF utility toolkit and can be used to:\n\n-  compare and evaluate the accuracy of RNA-Seq transcript assemblers (__Cufflinks__, __Stringtie__).\n-  collapse (merge) duplicate transcripts from multiple GTF/GFF3 files (e.g. resulted from assembly of different samples)\n- classify transcripts from one or multiple GTF/GFF3 files as they relate to reference transcripts provided in an annotation file (also in GTF/GFF3 format)\n\nThe original form of this program is also distributed as part of the Cufflinks suite, under the name \"__CuffCompare__\". Most of the options and parameters of __CuffCompare__ are supported by __GffCompare__, while new features will likely be added to __GffCompare__ in the future.\n\n*A list of __all inputs and parameters__ with corresponding descriptions can be found at the bottom of this page.*\n\n*__Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.__*\n\n### Common Use Cases\n\n* This tool should be used after __StringTie__ and __StringTie Merge__ transcript assembling and merging. The GffCompare program then compares the genes and transcripts with the annotation and reports statistics on this comparison. For more details refer to the StringTie protocol paper [2].\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\n\n* The execution time for comparing 8 assembled transcripts takes several minutes on the default instance; the price is negligible (~ 0.01$). Unless specified otherwise, the default instance used to run the __GffCompare__ tool will be c4.large (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [GffCompare manual page](http://ccb.jhu.edu/software/stringtie/gff.shtml#gffcompare)\n\n[2] [HISAT, StringTie, Ballgown protocol paper](http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html)", "input": [{"name": "Don't generate .tmap and .refmap files"}, {"name": "Reference transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Reference transcripts only"}, {"name": "Prefix name for consensus transcripts"}, {"name": "Output prefix name"}, {"name": "Maximum transcript start sites distance"}, {"name": "Maximum exon distance"}, {"name": "Don't discard matching reference transfrags"}, {"name": "Keep alternate TSS"}, {"name": "Input transcripts only"}, {"name": "Genome sequence"}, {"name": "Discard sticking out transfrags"}, {"name": "Discard single exon transfrags and reference transcripts"}, {"name": "Discard single exon reference transcripts"}, {"name": "Discard 'contained' transfrags"}, {"name": "Assembled transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Memory per job [MB]"}, {"name": "CPU per job"}], "output": [{"name": "GffCompare stats files", "encodingFormat": "application/x-gtf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/gpertea/gffcompare"], "applicationSubCategory": ["Quantification", "RNA-Seq"], "project": "SBG Public Data", "creator": "Johns Hopkins University, Center for Computational Biology", "softwareVersion": ["v1.0"], "dateModified": 1648049435, "dateCreated": 1612220557, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-2-12-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-2-12-2/4", "applicationCategory": "CommandLineTool", "name": "GRIDSS", "description": "**GRIDSS** is a module software suite containing tools useful for the detection of genomic rearrangements [1]. \n\n**GRIDSS** includes a genome-wide break-end assembler, as well as a structural variation caller for Illumina sequencing data. **GRIDSS** calls variants based on alignment-guided positional de Bruijn graph genome-wide break-end assembly, split read, and read pair evidence [1].\n\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nThis tool is used for structural variation calling, either for germline calling or somatic calling. It is used both for whole exome and for whole genome variant calling. It supports the following analyses:\n\n- Single sample variant calling\n- Joint variant calling (e.g. tumor/normal or trio calling)\n\nBefore running **GRIDSS**, it is strongly recommended to run **GRIDSS SetupReference**, which is a one-off setup generating additional files to accompany the **Reference genome** for **GRIDSS** execution. Running **GRIDSS** without the additional files present in the project beforehand, will lead to those files being regenerated with each **GRIDSS** execution, and therefore significantly increase task runtime.\n\n\n### Changes Introduced by Seven Bridges\n\n- The following input parameters were excluded from this wrapper: `--jar`, `--keepTempFiles`,`--jobindex` and `--jobnodes` (parallelization on the Seven Bridges Platform is not done in the way intended by the tool).\n- The `--steps` parameter has been hardcoded in the wrapper to `all`.\n- The `--output` and `--assembly` parameters used for naming **GRIDSS SV VCF** and **GRIDSS assembly BAM** outputs, respectively, are set automatically. The outputs will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the first provided **Input tumor BAM files** or **Input normal BAM files** file.\n- If a value(s) is not provided for the **Labels** (`--labels`) input parameter, it will be set automatically based on the **Sample ID** metadata field value of each input BAM file or, in the absence of it, base name of each input BAM file.\n\n\n### Common Issues and Important Notes\n\n- If **Labels** values are specified, they must be specified for all input BAM files, and also the order in which they are provided must match the order of the input files. It is strongly recommended to set **Sample ID** metadata field values for input BAM files, and leave the **Labels** parameter empty, since **Labels** are used in the output VCF for the input files and in this way mismatching will be avoided. \n- By default, **GRIDSS assembly BAM** output is not indexed. If you would like to generate a BAI index file, specify `CREATE_INDEX=true` in the **Picard options** parameter field.\n- **Reference genome** supplied to GRIDSS must match the reference genome used to align input BAM files.\n- To run the tool, at least one BAM file must be provided on the input, either **Input normal BAM files** or **Input tumor BAM files**, accompanied with its index file.\n- Default values for the parameters **JVM heap** (`--jvmheap`) and **Other JVM heap** (`--otherjvmheap`) have been set in the wrapper to 30G and 4G, respectively. Based on the extensive testing we have performed on files of different sizes, those values have shown to be the most optimal for **GRIDSS** execution, and are also the ones the toolkit author recommends. Changing these values without a thorough understanding of the system is not recommended.\n\n\n### Performance Benchmarking\n\n| Experiment type |  Input size | Duration |  Cost |  Instance (AWS on-demand) |\n|:---------------:|:-----------:|:--------:|:-----:|:----------:|\n|     WGS     |  2 x 40 GB |   1h 16min   | $1.15 | m5.4xlarge |\n|     WGS     |  2 x 110 GB |   4h 27min   | $4.05 | m5.4xlarge |\n|     WGS (no alts)     |  2 x 190 GB |   4h 35min   | $4.18 | m5.4xlarge |\n|     WGS     | 2 x 220 GB |   9h 7min  | $8.31 | m5.4xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n **GRIDSS** was tested with cwltool version 3.1.20211020155521. The `in_normal_alignments`,`in_reference`, `jvmheap`, `threads` and `mem_per_job` inputs were provided in the job.yaml/job.json file and used for testing. Parameters `jvmheap`, `threads` and `mem_per_job` should be set based on the environment in which the program will be executed.\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Input normal BAM files", "encodingFormat": "application/x-bam"}, {"name": "Input tumor BAM files", "encodingFormat": "application/x-bam"}, {"name": "Threads"}, {"name": "Blacklist", "encodingFormat": "text/x-bed"}, {"name": "Maximum coverage"}, {"name": "Labels"}, {"name": "Configuration file", "encodingFormat": "text/plain"}, {"name": "External aligner"}, {"name": "Picard options"}, {"name": "Use proper pair"}, {"name": "Concordant read pair distribution"}, {"name": "No JNI"}, {"name": "Skip soft clip realignment"}, {"name": "Memory per job"}, {"name": "CPUs per job"}, {"name": "JVM heap"}, {"name": "Other JVM heap"}], "output": [{"name": "GRIDSS SV VCF", "encodingFormat": "application/x-vcf"}, {"name": "GRIDSS assembly BAM", "encodingFormat": "application/x-bam"}, {"name": "SV calling metrics"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915263, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-annotate-vcf-kraken2-2-12-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-annotate-vcf-kraken2-2-12-2/5", "applicationCategory": "CommandLineTool", "name": "GRIDSS Annotate VCF Kraken2", "description": "**GRIDSS Annotate VCF Kraken2** adds Kraken2 classifications to single breakend and breakpoint inserted sequences [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nKraken2 annotation can be useful to determine if single breakend variant calls are the result of viral integration.\n\nTo run the tool, **GRIDSS VCF** and **Kraken2 database** must be provided on the input.\n\nPre-built Kraken2 database can be downloaded from the [Kraken2 AWS Public Dataset Program](https://benlangmead.github.io/aws-indexes/k2). Alternatively, the VIRUSBReakend database can also be used, which is available for download [here](https://melbourne.figshare.com/articles/dataset/virusbreakenddb_20210401_tar_gz/14782299).\n\n\n\n### Changes Introduced by Seven Bridges\n\n- The `--jar` and `--kraken2` parameters are omitted from the inputs, as they are set automatically by the tool.\n- The `--output` parameter, used for naming the **Annotated VCF** output, is omitted from the inputs, and is set based on the **Sample ID** metadata field value of **GRIDSS VCF** or, in the absence of it, base name of the **GRIDSS VCF** file.\n\n### Common Issues and Important Notes\n\n- The tool is optimized for Kraken2 standard database. If a larger database is used, **Memory per job** parameter should be set to the size of the **Kraken2 database** used + 4GB, to avoid failing of the task due to lack of memory. \n\n\n### Performance Benchmarking\n\nTypical **GRIDSS Annotate VCF Kraken2** task executions take 5-15 minutes ($0.05 - $0.15) on a r5.2xlarge AWS on-demand instance. The performance of the tool depends on the size of the database used. \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n**GRIDSS Annotate VCF Kraken2** was tested with cwltool version 3.1.20211020155521. The `in_variants` and `kraken2_db` inputs were provided in the job.yaml/job.json file and used for testing.\n\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "GRIDSS VCF", "encodingFormat": "application/x-vcf"}, {"name": "Number of threads"}, {"name": "Kraken2 database"}, {"name": "Additional Kraken2 arguments"}, {"name": "Minimum length of inserted sequence to annotate"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested", "Annotation"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915262, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-annotate-vcf-repeatmasker-2-12-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-annotate-vcf-repeatmasker-2-12-2/5", "applicationCategory": "CommandLineTool", "name": "GRIDSS Annotate VCF RepeatMasker", "description": "**GRIDSS Annotate VCF RepeatMasker** adds RepeatMasker classifications to inserted sequences [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nThis tool is used to annotate single breakends and sequences inserted in breakpoints with RepeatMasker annotations.\n\n\n### Changes Introduced by Seven Bridges\n\n- The `--jar`, `--workingdir` and `--rm` parameters are omitted from the inputs, as they are set automatically by the tool.\n- The `--output` parameter, used for naming the **Annotated VCF** output, is omitted from the inputs, and is set based on the **Sample ID** metadata field value of **GRIDSS VCF** or, in the absence of it, base name of the **GRIDSS VCF** file.\n\n### Common Issues and Important Notes\n\n- **GRIDSS VCF** input is required.\n\n### Performance Benchmarking\n\nTypical **GRIDSS Annotate VCF RepeatMasker** task executions take 5-15 minutes ($0.05 - $0.20) on a c5.4xlarge AWS on-demand instance. The performance of the tool depends on the number of SV events being processed.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n**GRIDSS Annotate VCF RepeatMasker** was tested with cwltool version 3.1.20211020155521. The `in_variants` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "Threads"}, {"name": "Additional RepeatMasker arguments"}, {"name": "Minimum length of inserted sequence to annotate"}, {"name": "GRIDSS VCF", "encodingFormat": "application/x-vcf"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915262, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-extract-overlapping-fragments-2-12-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-extract-overlapping-fragments-2-12-2/4", "applicationCategory": "CommandLineTool", "name": "GRIDSS Extract Overlapping Fragments", "description": "**GRIDSS Extract Overlapping Fragments** is used to extract reads of interest for targeted GRIDSS variant calling. It extracts all alignments for read pairs with at least one alignment overlapping set of regions of interest. The tool correctly handles supplementary alignments [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nTo perform targeted GRIDSS variant calling, all fragments overlapping the region of interest should be extracted using **GRIDSS Extract Overlapping Fragments**, and then **GRIDSS** should be run on the resulting subset BAM. **GRIDSS Extract Overlapping Fragments** is almost identical to filtering using `samtools view` except that it extracts alignment records for any fragment overlapping a region of interest (i.e. mate reads and supplementary alignments). That is, all records with read names matching the read name of an alignment overlapping any region of interest.\n\n**GRIDSS** can also be used to validate the calls from other callers. This can be done by converting calls made by the other caller to a BED file containing the start and end positions of all SVs. The intervals should be expanded by at least 10kbp, since too small windows will have a negative impact on GRIDSS QUAL scores (since they're empirically weighted, taking only regions with soft clipped reads will cause GRIDSS to massively downweight soft clips when scoring). The resulting BED file should then be provided to **GRIDSS Extract Overlapping Fragments**, as input to **BED regions of interest**. The resulting BAM file should then be processed as outlined above.  \n\n### Changes Introduced by Seven Bridges\n\n- The `--jar` and `--workingdir` parameters are omitted from the inputs, as they are set automatically.\n- The `--output` parameter, used for naming **GRIDSS targeted BAM** output, is omitted from the inputs, and is set based on the **Sample ID** metadata field value of the **Input BAM file** or, in the absence of it, base name of the **Input BAM file** file.\n\n### Common Issues and Important Notes\n\n- The **Input BAM file** input and either **BED regions of interest** or **SV VCF** must be provided. **BED regions of interest** and **SV VCF** are mutually exclusive, therefore only one must be specified.\n- The tool is set to use 76000 MB RAM by default. The amount of memory can be adjusted through **Memory per job** input parameter, if the task fails due to lack of RAM memory.\n\n### Performance Benchmarking\n\n| Experiment type |  Input size | Duration |  Cost |  Instance (AWS on-demand) |\n|:---------------:|:-----------:|:--------:|:-----:|:----------:|\n|     WGS     |  40 GB |   12min   | $0.24 | r5.4xlarge |\n|     WGS     | 110 GB |   55min   | $1.07 | r5.4xlarge |\n|     WGS     | 170 GB |   1h 15min  | $1.44 | r5.4xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n**GRIDSS Extract Overlapping Fragments** was tested with cwltool version 3.1.20211020155521. The `in_alignments` and `in_variants` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "BED regions of interest", "encodingFormat": "text/x-bed"}, {"name": "SV VCF", "encodingFormat": "application/x-vcf"}, {"name": "Target margin"}, {"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "Threads"}, {"name": "Number of reads to process to generate metrics"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "GRIDSS targeted BAM", "encodingFormat": "application/x-bam"}, {"name": "GRIDSS metrics"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915263, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-generateponbedpe-2-12-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-generateponbedpe-2-12-2/4", "applicationCategory": "CommandLineTool", "name": "GRIDSS GeneratePonBedpe", "description": "**GRIDSS GeneratePonBedpe** aggregates variants from multiple VCFs and counts the number of samples supporting each [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**GRIDSS GeneratePonBedpe** can be used to either generate a new panel of normals (PON) based on the VCFs provided on the input or to add new samples from provided VCFs to existing PONs e.g. HMFTools generated PONs.\nOnly the first sample per VCF is processed which is useful for generating a PON from a cohort of cancer samples with matched normals. Output is a BEDPE (breakpoint) and BED (single breakend) file suitable for use by **GRIPSS** and **GRIDSS Somatic Filter**.\n\n### Changes Introduced by Seven Bridges\n\nNo changes were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n- **Reference genome**, **Input GRIDSS VCFs** and **Normal ordinal** inputs must be provided.\n- **Reference genome** supplied must match the reference genome to which the samples in the **Input GRIDSS VCFs** were aligned i.e. chromosome annotation must match (1 vs chr1). The same rule applies if **Existing GRIDSS breakpoint PON** and **Existing GRIDSS single breakend PON** are provided on the input.\n\n### Performance Benchmarking\n\nTypical **GRIDSS GeneratePonBedpe** task executions for 10 VCFs (~600k variants each) provided on the input take up to 10 minutes (~$0.08) on a c4.2xlarge AWS on-demand instance. The performance of the tool depends on the amount of VCFs provided as well as the amount of variants present in each VCF.\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n**GRIDSS GeneratePonBedpe** was tested with cwltool version 3.1.20211020155521. The `in_reference`, `in_variants` and `normal_ordinal` inputs were provided in the job.yaml/job.json file and used for testing.\n\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Input GRIDSS VCFs", "encodingFormat": "application/x-vcf"}, {"name": "Existing GRIDSS breakpoint PON"}, {"name": "Existing GRIDSS single breakend PON", "encodingFormat": "text/x-bed"}, {"name": "Normal ordinal"}, {"name": "Minimum breakpoint quality"}, {"name": "Minimum breakend quality"}, {"name": "Include imprecise calls"}, {"name": "Threads"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "GRIDSS PON breakpoint"}, {"name": "GRIDSS PON single breakend", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ShellCommandRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915263, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-setupreference-2-12-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-setupreference-2-12-2/5", "applicationCategory": "CommandLineTool", "name": "GRIDSS SetupReference", "description": "**GRIDSS SetupReference** is used for generating additional files for the reference needed for running GRIDSS [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**GRIDSS SetupReference** is a wrapper around `gridss --steps setupreference` that generates all the secondary files, to accompany **Reference genome**, necessary for running **GRIDSS**. It is ported as a separate tool to reduce **GRIDSS** runtime by skipping this step at each **GRIDSS** execution.\n\n### Changes Introduced by Seven Bridges\n\n- The `--jvmheap` parameter is hardcoded to 10G, which is sufficient for running **GRIDSS SetupReference** on any genome. \n\n### Common Issues and Important Notes\n\n- **Reference genome** input is required. No secondary files are required, everything needed for **GRIDSS** execution will be generated by running this tool.\n\n### Performance Benchmarking\n\nTypical **GRIDSS SetupReference** task executions for human genome (~3GB) take ~1h 30min ($0.80) on a c4.2xlarge AWS on-demand instance. \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n**GRIDSS SetupReference** was tested with cwltool version 3.1.20211020155521. The `in_reference` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "GRIDSS reference files"}], "softwareRequirements": ["ShellCommandRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915263, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-somatic-filter-2-12-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-somatic-filter-2-12-2/5", "applicationCategory": "CommandLineTool", "name": "GRIDSS Somatic Filter", "description": "**GRIDSS Somatic Filter** filters somatic calls from a VCF generated by GRIDSS joint tumor/normal variant calling [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**GRIDSS Somatic Filter** performs a number of filtering steps. A description for each VCF filter can be found in the VCF header of the **High confidence somatic SV VCF** (High confidence somatic call set) and **Somatic SV VCF** (Low confidence somatic call set) outputs [2].\n\n**GRIDSS Somatic Filter** tool uses a number of configuration settings, which are described in the [GRIDSS Somatic Filtering documentation](https://github.com/PapenfussLab/gridss/wiki/Somatic-Filtering). To override **GRIDSS Somatic Filter** default configuration settings, gridss_somatic.R script should be provided on the **Configuration file** input [2].\n\n**Breakpoint PON** and **Breakend PON** can be generated using **GRIDSS GeneratePonBedpe** or, alternatively, if you are using GRCh37 or 38 reference genome, a PON based on Dutch samples can be downloaded from [HMFTools website](https://nextcloud.hartwigmedicalfoundation.nl/s/LTiKTd8XxBqwaiC?path=%2FHMFTools-Resources%2FGRIDSS) [1].\n\n### Changes Introduced by Seven Bridges\n\n- The `--opts` and `--ref` parameters have been excluded from the wrapper, as they are not applicable on the Seven Bridges Platform.\n- The `--pondir`, `--plotdir` and `--scriptdir` parameters have been omitted from the inputs, as they are set automatically by the wrapper.\n- The `--output` and `--fulloutput` parameters used for naming **High confidence somatic SV VCF** and **Somatic SV VCF** outputs, respectively, are set automatically. The outputs will be named based on the **Sample ID** metadata field value or, in the absence of it, base name of the **GRIDSS VCF** file.\n\n### Common Issues and Important Notes\n\n- To run the tool, **GRIDSS VCF**, **Breakpoint PON** and **Breakend PON** must be provided.\n- The **Breakpoint PON** and **Breakend PON** inputs must be named gridss_pon_breakpoint.bedpe and gridss_pon_single_breakend.bed, respectively. \n- Chromosome notation of the reference genome used to align samples present in the **GRIDSS VCF** and the PON must match, otherwise nothing will get filtered (e.g. 1 vs chr1).\n\n### Performance Benchmarking\n\nTypical **GRIDSS Somatic Filter** task executions take up to ~10 minutes (~$0.10) on a c4.2xlarge AWS on-demand instance. The performance of the tool depends on the size of the **GRIDSS VCF**.\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**GRIDSS Somatic Filter** was tested with cwltool version 3.1.20211020155521. The `in_variants`, `breakpoint_pon` and `breakend_pon` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)\n\n[2] [GRIDSS Somatic Filter documentation](https://github.com/PapenfussLab/gridss/wiki/Somatic-Filtering)", "input": [{"name": "GRIDSS VCF", "encodingFormat": "application/x-vcf"}, {"name": "Breakpoint PON"}, {"name": "Breakend PON", "encodingFormat": "text/x-bed"}, {"name": "Normal ordinal"}, {"name": "Tumor ordinal"}, {"name": "Configuration file"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "High confidence somatic SV VCF", "encodingFormat": "application/x-vcf"}, {"name": "Somatic SV VCF", "encodingFormat": "application/x-vcf"}, {"name": "GRIDSS output plots"}], "softwareRequirements": ["ShellCommandRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/wiki/Somatic-Filtering"], "applicationSubCategory": ["Structural Variant Calling", "Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915263, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gridss-virusbreakend-2-12-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gridss-virusbreakend-2-12-2/6", "applicationCategory": "CommandLineTool", "name": "GRIDSS VIRUSBreakend", "description": "**GRIDSS VIRUSBreakend** is a high-speed viral integration detection tool. It is designed to be incorporated in the WGS pipelines with minimal additional cost [1].\n\nThis tool is part of GRIDSS - the Genomic Rearrangement IDentification Software Suite [3]. It is based on a single breakend-based approach that can reliably detect viral presence and integrations anywhere in the host genome. By identifying and assembling single breakend variants in the virus genome followed by taxonomic classification and alignment of the breakend contigs, **VIRUSBreakend** is able to reliably identify viral integrations in regions inaccessible to current integration detection approaches [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**VIRUSBreakend** uses a multistage approach to identifying viral insertions. \nStarting with a host-aligned SAM/BAM/CRAM file, **VIRUSBreakend** identifies viral reads of interest through Kraken2 taxonomic classification of all unaligned or partially aligned sequences using a custom Kraken2 database. If the read is at least partially classified as a virus, the full read pair is considered for further analysis. This approach allows read pairs in which either read contains any viral sequence to be efficiently identified. The custom Kraken2 database consists of the default human, bacteria, viral and UniVec_Core databases as well as all NCBI viral neighbour genome assemblies [2]. \nA viral reference FASTA is then created that includes a viral genome for each host-infecting genus containing a taxon to which Kraken2 assigned at least 50 reads. Viral reads are compared to all viral reference genomes associated with the identified taxon or its descendents and the genome with the most matching viral read kmers is chosen as the viral reference genome for that genus. Viral read pairs are realigned to the updated reference and structural variants called using **GRIDSS** and filtered to single breakends. \nThe assembled single breakend sequence is used to identify the host integration site by aligning to the host reference. Identified breakpoints fall into two categories: sites in which the host mapping is unambiguous, and ambiguous sites in which the integration site cannot be unambiguously determined (such as integrations into alpha satellite repeats). To facilitate downstream analysis, integration sites are annotated with the RepeatMasker repeat type and class of the single breakend sequence [2].\n\nThe **VIRUSBreakend** database can be downloaded from [here](https://melbourne.figshare.com/articles/dataset/virusbreakenddb_20210401_tar_gz/14782299). It uses a standard Kraken2 database augmented with NCBI viral neighbour genomes and associated metadata files.\n\nSome reference genomes include viral sequence (hs37d5, hs38DH, and GRCh38_full_analysis_set_plus_decoy_hla all include EBV). For **VIRUSBreakend** to report EBV infection, reads need to be aligned to the EBV sequence (or chrEBV) [1]. \n\nBy default, **VIRUSBreakend** considers any read mapped if it maps to chrEBV or any viral reference sequence in the **VIRUSBreakend** database. When checking for matches, **VIRUSBreakend** uses the exact name, the Kraken2 stripped name, and also the version stripped name. For example, the Kraken2 database contains `kraken:taxid|10376|NC_007605.1` so **VIRUSBreakend** will exclude `kraken:taxid|10376|NC_007605.1`, `NC_007605.1`, and `NC_007605` (thus picking up the `NC_007605` EBV reference in hs37d5.fa) [1].\n\nA custom list can be supplied with the **Viral references** (`--viralreferences`) command line parameter.\n\n### Changes Introduced by Seven Bridges\n\n- The `--jar` parameter has been hardcoded in the wrapper.\n- The `--force` parameter has been excluded from the wrapper.\n- The `--workingdir` parameter has been excluded from the wrapper, as it is set automatically by the tool.\n- The `--output` parameter used for naming the **Output VCF** output, is omitted from the inputs, and is set based on the **Sample ID** metadata field value of **Input alignments** or, in the absence of it, base name of the **Input alignments** file.\n\n\n### Common Issues and Important Notes\n\n- **VIRUSBreakend** does not officially support non-human hosts, and was therefore not tested on genomes other than human.\n- The tool is optimized for the VIRUSBreakend database mentioned above. If a larger database is used, **Memory per job** parameter should be set to the size of the **VIRUSBreakend database** used + 4 GB, to avoid failing of the task due to lack of memory. \n\n\n### Performance Benchmarking\n\nBelow is a table describing runtimes and task costs of **GRIDSS VIRUSBreakend** for a couple of samples of different sizes and a VIRUSBreakend database (~50GB), executed on the AWS cloud instances:\n\n| Experiment type |  Input size | Duration |  Cost |  Instance (AWS on-demand) |\n|:---------------:|:-----------:|:--------:|:-----:|:----------:|\n|     WGS     |  40GB |   13min   | $0.14 | r5.2xlarge |\n|     WGS     |  110GB |   27min   | $0.30 | r5.2xlarge |\n|     WGS     | 210GB |   50min  | $0.54 | r.5.2xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\nThe tool could not be tested locally with cwltool due to the size of the VIRUSBreakend database. Instead, `cwltool --validate` option was used to ensure the tool's CWL code satisfies the CWL1.2 specification.\n\n\n### References\n\n[1] [VIRUSBreakend documentation](https://github.com/PapenfussLab/gridss/blob/master/VIRUSBreakend_Readme.md)\n\n[2] [VIRUSBreakend publication](https://academic.oup.com/bioinformatics/article/37/19/3115/6273577)\n\n[3] [GRIDSS Github repository](https://github.com/PapenfussLab/gridss)", "input": [{"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Threads"}, {"name": "Viral references", "encodingFormat": "text/plain"}, {"name": "NBCI host filter"}, {"name": "VIRUSBreakend database"}, {"name": "Additional Kraken2 arguments"}, {"name": "Additional GRIDSS arguments"}, {"name": "Additional RepeatMasker arguments"}, {"name": "Minimum number of viral reads perform integration detection"}, {"name": "Minimum viral coverage"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "Output VCF", "encodingFormat": "application/x-vcf"}, {"name": "Summary TSV"}, {"name": "Output metrics and reports"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PapenfussLab/gridss", "https://github.com/PapenfussLab/gridss/tree/master/src/main/java/gridss", "https://github.com/PapenfussLab/gridss/releases/tag/v2.12.2", "https://github.com/PapenfussLab/gridss/blob/master/VIRUSBreakend_Readme.md"], "applicationSubCategory": ["Structural Variant Calling", "Annotation"], "project": "SBG Public Data", "creator": "Daniel Cameron", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915263, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gripss-1-11/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gripss-1-11/6", "applicationCategory": "CommandLineTool", "name": "GRIPSS", "description": "**GRIPSS** applies a set of filtering and post processing steps on GRIDSS paired tumor-normal output. It produces a high confidence set of somatic SV for a tumor sample [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**GRIPSS** performs a number of filtering steps and outputs a low confidence set of somatic SVs for a tumor sample. To get a high confidence set of somatic SVs, **GRIPSS HardFilter** should be run on the **GRIPSS VCF** output.\n\n**Breakpoint PON** and **Breakend PON** can be generated using **GRIDSS GeneratePonBedpe** or, alternatively, if you are using GRCh37 or 38 reference, a PON based on Dutch samples can be downloaded from [HMFTools resources](https://nextcloud.hartwigmedicalfoundation.nl/s/LTiKTd8XxBqwaiC?path=%2FHMFTools-Resources%2FGRIDSS) [1].\n\n**Breakpoint hotspots** can be downloaded from [HMFTools resources](https://nextcloud.hartwigmedicalfoundation.nl/s/LTiKTd8XxBqwaiC?path=%2FHMFTools-Resources%2FKnown-Fusions) as well, for both GRCh37 and 38.\n\n\n### Changes Introduced by Seven Bridges\n\n- The `-output_vcf` parameter is set automatically based on the **Tumor sample name** (`-tumor`) input.\n\n### Common Issues and Important Notes\n\n- To run the tool, the following inputs must be provided: **GRIDSS VCF**, **Reference genome**, **Breakend PON**, **Breakpoint PON**, **Breakpoint hotspot** and **Tumor sample name** must be provided. If the tool is to be run in the tumor-normal mode, **Reference sample name** must be provided as well.\n- The **Reference sample name** input is optional and if not provided, the tool will run in tumor-only mode in which case all filters that require the normal sample are de-activated. This includes `minNormalCoverage`, `minRelativeCoverage`, `maxNormalSupport`, `shortSRNormalSupport`, `discordantPairSupport`.\n- **Reference genome** must be provided along with FAI and DICT files.\n- Chromosome notation of the reference genome used to align samples present in the **GRIDSS VCF** and the PON must match (e.g. 1 vs chr1).\n\n### Performance Benchmarking\n\nTypical **GRIPSS** task executions take a few minutes (~$0.01-$0.05) on a c5.2xlarge AWS on-demand instance. The performance of the tool depends on the size of the **GRIDSS VCF** input.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n **GRIPSS** was tested with cwltool version 3.1.20211020155521. The `in_variants`,`in_reference`, `breakend_pon`, `breakpoint_pon`, `breakpoint_hotspot`, `tumor` and `reference` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [GRIPSS Github repository](https://github.com/hartwigmedical/hmftools/tree/master/gripss)", "input": [{"name": "GRIDSS VCF", "encodingFormat": "application/x-vcf"}, {"name": "Breakend PON", "encodingFormat": "text/x-bed"}, {"name": "Breakpoint PON"}, {"name": "Breakpoint hotspot"}, {"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Tumor sample name"}, {"name": "Reference sample name"}, {"name": "Hard maximum normal absolute support"}, {"name": "Hard maximum normal relative support"}, {"name": "Hard minimum tumor qual"}, {"name": "Maximum homology length short inversion"}, {"name": "Maximum inexact homology length short DEL"}, {"name": "Maximum short strand bias"}, {"name": "Minimum length"}, {"name": "Minimum normal coverage"}, {"name": "Minimum qual break end"}, {"name": "Minimum qual break point"}, {"name": "Minumum qual rescue mobile element insertions"}, {"name": "Minimum tumor allelic frequency"}, {"name": "Soft maximum normal relative support"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "GRIPSS VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/hartwigmedical/hmftools/tree/master/gripss", "https://github.com/hartwigmedical/hmftools/tree/master/gripss/src/main/kotlin/com/hartwig/hmftools/gripss", "https://github.com/hartwigmedical/hmftools/releases/tag/gripss-v1.11", "https://github.com/hartwigmedical/hmftools/blob/master/gripss/README.md"], "applicationSubCategory": ["Structural Variant Calling", "Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Hartwig Medical Foundation Tools", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915264, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gripss-hard-filter-2-12-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gripss-hard-filter-2-12-2/4", "applicationCategory": "CommandLineTool", "name": "GRIPSS Hard Filter", "description": "**GRIPSS Hard Filter** applies a set of filtering and post processing steps on GRIDSS paired tumor-normal output. It produces a high confidence set of somatic SV for a tumor sample [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**GRIPSS Hard Filter** performs a number of filtering steps and outputs a high confidence set of somatic SVs for a tumor sample. \n\n### Changes Introduced by Seven Bridges\n\n- The `-output_vcf` parameter is set automatically based on the prefix of the **GRIPSS VCF** input.\n\n### Common Issues and Important Notes\n\n- To run the tool, the **GRIPSS VCF** must be provided.\n\n### Performance Benchmarking\n\nTypical **GRIPSS Hard Filter** task executions takes a few minutes (~$0.01-$0.05) on a c4.2xlarge AWS on-demand instance. The performance of the tool depends on the size of the **GRIDSS VCF** input.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### Portability\n\n**GRIPSS Hard Filter** was tested with cwltool version 3.1.20211020155521. The `in_variants` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [GRIPSS Github repository](https://github.com/hartwigmedical/hmftools/tree/master/gripss)", "input": [{"name": "GRIPSS VCF", "encodingFormat": "application/x-vcf"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "GRIPSS filtered VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "NetworkAccess", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/hartwigmedical/hmftools/tree/master/gripss", "https://github.com/hartwigmedical/hmftools/tree/master/gripss/src/main/kotlin/com/hartwig/hmftools/gripss", "https://github.com/hartwigmedical/hmftools/releases/tag/gripss-v1.11", "https://github.com/hartwigmedical/hmftools/blob/master/gripss/README.md"], "applicationSubCategory": ["Structural Variant Calling", "Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Hartwig Medical Foundation Tools", "softwareVersion": ["v1.2"], "dateModified": 1648049968, "dateCreated": 1643915264, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-2-2-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-2-2-1/4", "applicationCategory": "CommandLineTool", "name": "HISAT2", "description": "**HISAT2** is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes as well as to a single reference genome [1]. \n\n__HISAT2__ (hierarchical indexing for spliced alignment of transcripts 2) aligns both DNA and RNA sequences using a graph Ferragina Manzini (GFM) index. In addition to using one global GFM index that represents the general population,\u00a0__HISAT2__\u00a0uses a large set of small GFM indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp, with 55,000 indexes needed to cover the human population). These small indexes (called local indexes) combined with several alignment strategies enable effective alignment of sequencing reads. This new indexing scheme is called Hierarchical Graph FM (HGFM) index [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n- Though **HISAT2** can be used for all types of next-generation sequencing reads, it is mostly used for aligning RNA-Seq data.\n- The __Downstream transcriptome assembly__ (`--downstream-transcriptome-assembly/--dta`) parameter tells __HISAT2__ to report alignments tailored for transcript assemblers including **StringTie**. With this option, __HISAT2__ requires longer anchor lengths for de novo discovery of splice sites. This leads to fewer alignments with short-anchors, which helps transcript assemblers significantly improve computation and memory usage.\n- The __Downstream transcriptome assembly__ - __Cufflinks__ (`--dta-cufflinks`) parameter tells __HISAT2__ to report alignments tailored specifically for **Cufflinks**. In addition to what __HISAT2__ does with the above option (__Downstream transcriptome assembly__), it also looks for novel splice sites with three signals (GT/AG, GC/AG, AT/AC). Nonetheless, all user-provided splice sites are used irrespective of their signals. __HISAT2__ produces an optional field, `XS:A:[+-]`, for every spliced alignment.\n\n\n### Changes Introduced by Seven Bridges\n\n* In order to facilitate and accelerate further RNA-Seq analysis, an additional toolkit __Sambamba (0.7.1)__ is integrated into the same Seven Bridges tool representation alongside __HISAT2 (2.2.1)__. Besides the standard __HISAT2__ SAM output, HISAT2 has been extended to provide two additional output file options: \n     * an **unsorted BAM** file created by piping standard __HISAT2__ output to __Sambamba view__ and \n     * a **coordinate-sorted BAM** file and its Index (BAI) file created by additional piping of the output through __Sambamba sort__ and __Sambamba index__. \n To select the desired output, use the __Output type__ parameter. The default output is a **coordinate-sorted BAM** file.\n* If the __Read group ID__ parameter is not defined, by default it will  be set to \u20181\u2019, unless the __No @RG line__ option is explicitly specified. If the tool is scattered within a workflow, it will assign the __Read group ID__ according to the order of the scattered folders. This also ensures a unique __Read group ID__ when processing multi-read group input data from one sample.\n* All output files will be prefixed by the input sample ID (inferred from the __Sample ID__ metadata field if existent, or from filename otherwise), unless the __Output prefix__ option is explicitly specified.\n\n### Common Issues and Important Notes\n\n- To run __HISAT2,__ properly indexed reference files are required. These files can be either created using the __HISAT2 Build__ tool or downloaded from the [HISAT2 home page](https://daehwankimlab.github.io/hisat2/download/).\n- Some __HISAT2__ options (i.e. __Maximum number of ambiguous characters__ (`--n-ceil`) or __Long introns with canonical splice sites penalty__ (`--pen-canintronlen`)) specify a function rather than an individual number or setting. In these cases, the user specifies three parameters: (a) a function type F, (b) a constant term B, and (c) a coefficient A. The available function types are constant (C), linear (L), square-root (S), and natural log (G). The parameters are specified as F,B,A - that is, the function type, the constant term, and the coefficient are separated by commas with no whitespace. The constant term and coefficient may be negative and/or floating-point numbers [1].\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Example 1: if the function specification is L,-0.4,-0.6, then the function defined is:\n`f(x) = -0.4 + -0.6 * x`\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Example 2: if the function specification is G,1,5.4, then the function defined is:\n`f(x) = 1.0 + 5.4 * ln(x)`\n\n- In a single run, __Reads__ should be either paired or unpaired. __Reads__ may be a mix of different lengths.\n- For paired-end __Reads__, the __Paired-end__ metadata field has to be set to 1 or 2. Paired-end __Reads__ must correspond to each other file-for-file and read-for-read.\n\n### Performance Benchmarking\n\nFor the human reference genome, __HISAT2__ requires about 9 GB of RAM to run properly. In the following table you can find estimates of __HISAT2__ running time and cost. All samples are aligned onto the hg38 human reference index built using reference transcriptome (i.e. using __Splice sites__ (`--ss`) and __Exon__ (`--exon`) options). \n\n*The cost of running __HISAT2__ can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Experiment type | Input size | Paired-end | # of reads | Read length | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|---------------|\n| RNA-Seq         | 2 x 21.5 GB     | Yes        | 95M            |  101       | 1h 50min.   | $0.99           | c4.2xlarge      |\n| RNA-Seq               | 2 x 10.8 GB       | Yes        | 47.5M            | 101         | 57min.    | $0.51            | c4.2xlarge      |\n| RNA-Seq               | 2 x 2.2 GB       | Yes        | 9.5M            | 101         |  16min.   | $0.14            | c4.2xlarge      |\n| RNA-Seq               | 3.9 GB         | No         | 17.6M           | 51         | 12min.    | $0.11            | c4.2xlarge      |\n\n### References\n\n[1] [HISAT2 manual page](https://daehwankimlab.github.io/hisat2/manual/)", "input": [{"name": "Indexed reference", "encodingFormat": "application/x-tar"}, {"name": "Reads", "encodingFormat": "application/x-fasta"}, {"name": "Skip first <n> reads"}, {"name": "Align first <n> reads"}, {"name": "Trim 5' end"}, {"name": "Trim 3' end"}, {"name": "Phred +64 encoding"}, {"name": "Solexa qualities"}, {"name": "Integer qualities"}, {"name": "Maximum number of ambiguous characters"}, {"name": "Ignore quality"}, {"name": "No forward"}, {"name": "No reverse complement"}, {"name": "Mismatch penalties"}, {"name": "Soft-clipping penalties"}, {"name": "No soft-clipping"}, {"name": "Penalty for missing positions"}, {"name": "Read gap penalty"}, {"name": "Reference gap penalty"}, {"name": "Minimum-score function"}, {"name": "Canonical splice site penalty"}, {"name": "Non canonical splice site penalty"}, {"name": "Long introns with canonical splice sites penalty"}, {"name": "Long introns with noncanonical splice sites penalty"}, {"name": "Minimum intron length"}, {"name": "Maximum intron length"}, {"name": "Known splice sites", "encodingFormat": "text/plain"}, {"name": "Output novel splice sites"}, {"name": "Novel splice site input file", "encodingFormat": "text/plain"}, {"name": "No temp splice sites"}, {"name": "No splice alignment"}, {"name": "RNA strandness"}, {"name": "Transcriptome mapping only"}, {"name": "Downstream transcriptome assembly"}, {"name": "Downstream transcriptome assembly - Cufflinks"}, {"name": "No template length adjustment"}, {"name": "Max distinct alignments"}, {"name": "Maximum number of seeds"}, {"name": "Secondary alignment"}, {"name": "Minimum fragment length"}, {"name": "Maximum fragment length"}, {"name": "Mate orientations"}, {"name": "No mixed alignments"}, {"name": "No discordant alignments"}, {"name": "Write unpaired unaligned reads"}, {"name": "Write unpaired aligned reads"}, {"name": "Write unaligned concordantly reads"}, {"name": "Write aligned concordantly reads"}, {"name": "Metrics file"}, {"name": "Summary file"}, {"name": "Dismiss unaligned reads"}, {"name": "No header"}, {"name": "No @SQ lines"}, {"name": "Read group ID"}, {"name": "Remove 'chr' string"}, {"name": "Add 'chr' string"}, {"name": "Omit secondary sequence"}, {"name": "Reorder reads"}, {"name": "Number of threads"}, {"name": "Seed"}, {"name": "Non deterministic seed"}, {"name": "Output type"}, {"name": "No @RG line"}, {"name": "Read group sequencing center"}, {"name": "Read group library ID"}, {"name": "Read group predicted median insert size"}, {"name": "Read group platform"}, {"name": "Read group platform unit ID"}, {"name": "Read group sample ID"}, {"name": "Read group additional fields"}, {"name": "Output prefix"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Output alignment", "encodingFormat": "application/x-bam"}, {"name": "Novel splice sites", "encodingFormat": "text/plain"}, {"name": "Metrics file", "encodingFormat": "text/plain"}, {"name": "Summary file", "encodingFormat": "text/plain"}, {"name": "Unpaired unaligned reads", "encodingFormat": "text/plain"}, {"name": "Unpaired aligned reads", "encodingFormat": "text/plain"}, {"name": "Unaligned concordantly reads", "encodingFormat": "text/plain"}, {"name": "Aligned concordantly reads", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["v1.0"], "dateModified": 1648049240, "dateCreated": 1612220425, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-build-2-2-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-build-2-2-1/4", "applicationCategory": "CommandLineTool", "name": "HISAT2 Build", "description": "__HISAT2 Build__\u00a0tool builds a HISAT2 index necessary for the __HISAT2__ alignment method.  To create an index, __HISAT2 Build__ uses a genome reference file(s) in FASTA format and outputs a set of 8 files with suffixes .1.ht2, .2.ht2, .3.ht2, .4.ht2, .5.ht2, .6.ht2, .7.ht2, and .8.ht2. In the case of a large index these suffixes will have an ht2l termination. These files together constitute the index: they are all that is needed to align reads to that reference [1].\n\nThe __HISAT2__ indexing scheme is called Hierarchical Graph Ferragina Manzini (HGFM) index. In addition to using one global GFM index that represents the general population,\u00a0__HISAT2__\u00a0uses a large set of small GFM indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp, with 55,000 indexes needed to cover the human population). These small indexes (called local indexes) combined with several alignment strategies enable effective alignment of sequencing reads [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* This tool is used to create an index for the __HISAT2__ splice aware aligner.\n* __HISAT2 Build__ can take advantage of known splice sites, exons, SNPs and haplotypes to create an index that will enable the __HISAT2__ aligner to create more accurate alignments. These files can be created using __HISAT2 ExtractExons__, __HISAT2 ExtractSpliceSites__ and __HISAT2 Extract SNPsHaplotypes__. Pre-built index files using these options can be also found on [HISAT2 home page](https://daehwankimlab.github.io/hisat2/download/).\n* __HISAT2 Build__ can generate either small or large indexes. The wrapper will decide which one to generate based on the length of the input genome. If the reference does not exceed 4 billion characters but a large index is preferred, the user can specify `--large-index` to force __HISAT2 Build__  to build a large index instead [1].\n\n### Changes Introduced by Seven Bridges\n\n* The directory containing the index files will be outputted as a TAR bundle (the __Index files__ output). This bundle can then be provided to the __HISAT2__ aligner, which will automatically take care of untarring it and preparing it to run successfully without further issues.\n* __HISAT2 Build__ can accept a TAR bundle containing an already indexed reference instead of a reference FASTA file(s), to skip indexing and reduce processing time if this tool is a part of a workflow.\n* Output file will be prefixed by the input reference file name, unless the **Output file prefix** option is explicitly specified.\n\n### Common Issues and Important Notes\n\n* __Exon__ (`--exon`) and __Splice sites__ (`--ss`) files have to be used together. If exactly one of these options is used, __HISAT2 Build__ will fail. Input files for these two options can be generated from  __HISAT2 ExtractExons__  and __HISAT2 ExtractSpliceSites__ tools.\n* The **Max bucket size** (`--bmax`), **Max bucket size (as divisior)** (`--bmaxdivn`), and **Diff-cover period** (`--dcv`) options are governing the trade off between running time and memory usage and should only be set by advanced users.  __HISAT2 Build__ automatically sets these parameters to their optimal values, that yield the best running time without exhausting memory. This behavior can be disabled using the\u00a0-a/--noauto\u00a0option [1].\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of __HISAT2 Build__ duration and cost. Two different instances were used for benchmarking because __HISAT2 Build__ requires significantly more memory (~ 200GB for human reference [1]) when using some of the following options: __Splice sites file__ (`--ss`), __Exon file__ (`--exon`) and/or __SNP file__ (`--snp`), __Haplotype file__ (`--haplotype`)  opposed to situations where these options are not used. The Seven Bridges version of the tool will dynamically choose an appropriate instance based on the provided inputs. The results shown here are obtained by indexing the human reference genome.  Execution time and cost can vary for other genomes.\n\n*Cost can be significantly reduced by **spot instances** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n| Settings                      | Duration | Cost | Instance (AWS)|\n|-------------------------------|--------------|------------------|---------------|\n| Default (no additional files) | 18min.       | $0.25            | c5.4xlarge    |\n| Using `--ss` and `--exon` options | 51min.    | $1.84           | r5.8xlarge    |\n| Using `--ss`, `--exon` ,`--snp` and `--haplotype` options  | 1h 12min.    | $2.60           | r5.8xlarge    |\n\n### References\n\n[1] [HISAT2 manual page](https://daehwankimlab.github.io/hisat2/manual/)", "input": [{"name": "Reference or Index files", "encodingFormat": "application/x-tar"}, {"name": "SNP file"}, {"name": "Haplotype file"}, {"name": "Splice sites file", "encodingFormat": "text/plain"}, {"name": "Exon file", "encodingFormat": "text/plain"}, {"name": "Seed"}, {"name": "Force large index"}, {"name": "Disable automatic memory fitting"}, {"name": "Max bucket size"}, {"name": "Max bucket size (as divisior)"}, {"name": "Diff-cover period"}, {"name": "No diff-cover"}, {"name": "Disable building packed reference"}, {"name": "Build only packed portion"}, {"name": "Global marking rate"}, {"name": "Number of chars in lookup"}, {"name": "Local marking rate"}, {"name": "Number of chars in lookup (local index)"}, {"name": "Output file prefix"}, {"name": "Number of threads"}], "output": [{"name": "Index files", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["v1.0"], "dateModified": 1648049240, "dateCreated": 1612220425, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-extractexons-2-2-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-extractexons-2-2-1/4", "applicationCategory": "CommandLineTool", "name": "HISAT2 ExtractExons", "description": "__HISAT2 ExtractExons__ extracts a list of exons from a GTF file in the **HISAT2**'s own format which is a tab-delimited file with the following columns: (1) chromosome name; (2) zero-offset based left genomic position of an exon; and (3) zero-offset based right genomic position of an exon.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* This tool can be used as a preprocessing step for __HISAT2 Build__. It is used to create a file containing exons that can be further fed to __HISAT2 Build__ in order to create an index that can improve alignment accuracy.\n\n### Changes Introduced by Seven Bridges\n\n* Output file will be prefixed by the input gene annotation filename, unless the __Output file prefix__ option is explicitly specified.\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges Platform have been detected.\n\n### Performance Benchmarking\n\nThe execution time for human gene annotation takes several minutes on the default instance; the price is negligible (~ $0.02) using on-demand AWS instances. \n\n*Cost can be significantly reduced by **spot instances** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Output file prefix"}], "output": [{"name": "Extracted exons", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["v1.0"], "dateModified": 1648049240, "dateCreated": 1612220425, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-extractsnpshaplotypes-2-2-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-extractsnpshaplotypes-2-2-1/5", "applicationCategory": "CommandLineTool", "name": "HISAT2 ExtractSNPsHaplotypes", "description": "__HISAT2 Extract SNPsHaplotypes__ extracts SNPs and Haplotypes from a dbSNP file or VCF file in the **HISAT2**'s own format as follows:\n\nSNP file (5 tab-delimited columns):\n(1) SNP ID; (2) SNP/Variation type (single, deletion, or insertion); (3) chromosome name; (4) zero-offset based genomic position of a SNP; and (5) an alternative base (single), with the length of the SNP (deletion), or insertion sequence (insertion)\n\nHaplotype file (5 tab-delimited columns):\n(1) Haplotype ID; (2) chromosome name; (3) zero-offset based left coordinate of haplotype; (4) zero-offset based right coordinate of haplotype; and (5) a comma-separated list of SNP IDs in the haplotype.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* This tool can be used as a preprocessing step for **HISAT2 Build**. It is used to create files containing SNPs and haplotypes that can be forwarded to **HISAT2 Build** in order to create an index that can improve alignment accuracy. Though, the haplotype file is not required, haplotype information can keep the index construction from exploding and reduce the index size substantially [1]. \n\n### Changes Introduced by Seven Bridges\n\n* Both scripts `hisat2_extract_snps_haplotypes_UCSC.py` and `hisat2_extract_snps_haplotypes_VCF.py` used for extracting SNPs and haplotypes from dbSNP file and VCF file respectively are consolidated into one and the same tool on the Seven Bridges Platform. The tool will decide which script to use depending on the file extension provided on the input.\n* Output files will be prefixed by the input reference file name, unless the __Output prefix__ option is explicitly specified.\n\n### Common Issues and Important Notes\n\n* __HISAT2 ExtractSNPsHaplotypes__ may fail when using a VCF file which is extracted from GATK due to GATK's notation for deleted alleles.\n\n### Performance Benchmarking\n\nThe execution time for extracting SNPs and haplotypes for 1000 genomes high confidence phase 1 SNPs (hg38) VCF file takes under 15 minutes on the default instance; the price is negligible (~ 0.04$). Unless specified otherwise, the default instance used to run the __HISAT2 ExtractSNPsHaplotypes__ tool will be c4.xlarge (AWS).\n\n*Cost can be significantly reduced by **spot instances** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [HISAT2 manual page](https://daehwankimlab.github.io/hisat2/manual/)", "input": [{"name": "Inter gap"}, {"name": "Intra gap"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "SNP file", "encodingFormat": "application/x-vcf"}, {"name": "Output prefix"}], "output": [{"name": "Extracted haplotypes"}, {"name": "Extracted SNPs"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["v1.0"], "dateModified": 1648049241, "dateCreated": 1612220425, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-extractsplicesites-2-2-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-extractsplicesites-2-2-1/4", "applicationCategory": "CommandLineTool", "name": "HISAT2 ExtractSpliceSites", "description": "__HISAT2 ExtractSpliceSites__ extracts a list of splice sites from a GTF file in the **HISAT2**'s own format as follows (4 tab-delimited columns):   \n(1) chromosome name; (2) zero-offset based genomic position of the flanking base on the left side of an intron; (3) zero-offset based genomic position of the flanking base on the right; (4) strand.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* This tool can be used as a preprocessing step for __HISAT2 Build__. It is used to create a file containing splice sites that can be forwarded to __HISAT2 Build__ in order to create an index that can improve alignment accuracy.\n\n### Changes Introduced by Seven Bridges\n\n* Output file will be prefixed by the input gene annotation filename, unless the __Output file prefix__ option is explicitly specified.\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges Platform have been detected. \n\n### Performance Benchmarking\n\nThe execution time for human gene annotation takes several minutes on the default instance; the price is negligible (~ $0.02) using on-demand AWS instances.\n\n*Cost can be significantly reduced by **spot instances** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Output file prefix"}], "output": [{"name": "Extracted splice sites", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["v1.0"], "dateModified": 1648049241, "dateCreated": 1612220425, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hisat2-inspect-2-2-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hisat2-inspect-2-2-1/4", "applicationCategory": "CommandLineTool", "name": "HISAT2 Inspect", "description": "__HISAT2 Inspect__ extracts information from a **HISAT2** index about what kind of\nindex it is and what reference sequences were used to build it. When run without\nany options, the tool will output a FASTA file containing the sequences of the\noriginal references (with all non-`A`/`C`/`G`/`T` characters converted to `N`s).\n It can also be used to extract just the reference sequence names using the\n**Print reference names** (`--names`/`-n`) option or a more verbose summary using the **Summary info** (`--summary`/`-s`)\noption [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* __HISAT2 Inspect__ can be used to extract information from a **HISAT2** index when it is unclear what reference sequences were used to build it (e.g. when index files are downloaded from an external source but not built on the Seven Bridges Platform).\n\n### Changes Introduced by Seven Bridges\n\n* The tool can accept TAR bundle HISAT2 index archive file, as well as GZ and BZ2 compressed TAR file, without the user having to specify anything.\n* Output file will be prefixed by the input HISAT2 index archive name, unless the __Output file prefix__ option is explicitly specified.\n\n### Common Issues and Important Notes\n\n* The options **Print exons** (`--exon`), **Print SNPs** (`--snp`), **Print splice sites** (`--ss`), **Print splice sites - all** (`--ss_all`), **Summary info** (`--summary`), **Print reference names** (`--names`) are mutually exclusive and have to be used one by one. In order to get results for  `--exon`/`--snp`/`--ss`/`--ss_all` the index has to be created using the respective options. All 8 files produced by **HISAT2 Build** (basename.[1-8].ht2(l)) have to be provided.\n\n### Performance Benchmarking\n\nThe execution time for inspecting a human indexed reference takes under 8 minutes on the default instance, c4.2xlarge (AWS), costing around $0.05. \n\n*Cost can be significantly reduced by **spot instances** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [HISAT2 manual page](https://daehwankimlab.github.io/hisat2/manual/)", "input": [{"name": "Index files", "encodingFormat": "application/x-tar"}, {"name": "Across"}, {"name": "Print reference names"}, {"name": "Summary info"}, {"name": "Print SNPs"}, {"name": "Print splice sites"}, {"name": "Print splice sites - all"}, {"name": "Print exons"}, {"name": "Output file prefix"}], "output": [{"name": "Output file", "encodingFormat": "application/x-gtf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Johns Hopkins University", "softwareVersion": ["v1.0"], "dateModified": 1648049241, "dateCreated": 1612220426, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/histoqc-2-1/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/histoqc-2-1/8", "applicationCategory": "CommandLineTool", "name": "HistoQC", "description": "**HistoQC** is an open-source quality control tool for digital pathology slides. \n\nIt performs fast quality control to not only identify and delineate artefacts but also discover cohort-level outliers (eg, slides stained darker or lighter than others in the cohort). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n### Common Use Cases\n- A challenge in digital pathology is the presence of artefacts and batch effects, unintentionally introduced during both routine slide preparation (eg, staining, tissue folding) and digitization (eg, blurriness, variations in contrast and hue). Manual review of glass and digital slides is laborious, qualitative, and subject to intra- and inter-reader variability. Therefore, there is a critical need for a reproducible automated approach of precisely localizing artefacts to identify slides that need to be reproduced or regions that should be avoided during computational analysis. **HistoQC** employs a combination of image metrics (eg, color histograms, brightness, contrast), features (eg, edge detectors), and supervised classifiers (eg, pen detection) to identify artefact-free regions on digitized slides.[1] \n- **HistoQC** consists of a pipeline of modules sequentially applied to an image. These modules act on the image to produce metrics and produce output images after applying thresholds or running classifiers. \nPipelines are defined within ``config`` files given on input. The user can select one of the provided pipelines offered by the tools authors - \n  - config.ini\n  - config_clinical.ini\n  - config_first.ini\n  - config_ihc.ini\n  - config_light.ini\n  - config_v2.1.ini\n \n  The order of events in the pipeline is important as the regions considered for computation may be affected. \n  Through various experiments, **HistoQC** developers have come to the following suggested workflow. Depending on your task and the expected homogeneity of the dataset, this approach may be rather extreme, so it is suggested to modify the approach accordingly for each dataset.\n\n  1. Run **HistoQC** on all images using a minimal pipeline, such as the one contained in ``config_first.ini``. This allows for discovery of images which are scanned at different magnifications, e.g., 20x and 40x images. Additionally, it performs basic tissue detection (white threshold) and subsequently computes histograms and differences to target templates. \nUsing this information, split the cohort into sub-cohorts based on these values since various modules are likely to function differently at different magnifications. Ideally, one wants to create sub-cohorts which have the same magnifications, contain the same number of internal storage format levels (\"levels\"), and share similar appearance properties (lightness, stain intensity, etc). This can be done easily using the web user interface and the parallel coordinates graph. Clicking and dragging on any of the axes allows for the creation of filters which update both the table above and the images below. Dragging an existing filter up and down dynamically adjusts the filters. When looking at a filtered view of the table one can click \"Save Filtered\" and save just that particular subset of images.\n  2. Once the sub-cohorts are built, you can rerun the pipeline using an expanded set of models which have higher computational load.  An example of the full pipeline is in ``config.ini``, designed to work with H&E images at 40x. \n\n\n- Interactive user interface for viewing results is available by selecting the **Generate UI** parameter. This produces the output directory containing results and the user interface in the form of a ZIP archive. When the archive is downloaded locally and unzipped, it is run by opening the ``index.html`` file inside the ``UserInterface`` directory, and with it loading the ``results.tsv`` file located in the ``Data`` directory.\n\n### Changes Introduced by Seven Bridges\n- For easier usage, after generating the interactive user interface, the **QC output directory** is copied into the ``Data`` directory of the user interface. This ensures that all the images are available when running the UI, and only the ``index.html`` file needs to be run.\n\n### Common Issues and Important Notes\n- For advanced usage see HistoQC Wiki.[2]\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n| Number of images | Average image size | Pipeline            | Number of parallel processes | Price | Execution time | Instance (AWS) |\n|------------------|--------------------|---------------------|------------------------------|-------|----------------|----------------|\n| 100              | 100 MB             | config.ini          | 1                            | $0.3  | 44min          | c4.2xlarge     |\n| 100              | 100 MB             | config.ini          | 10                           | $0.12 | 18min          | c4.2xlarge     |\n| 595              | 100 MB             | config.ini          | 1                            | $1.6  | 4h 1min        | c4.2xlarge     |\n| 595              | 100 MB             | config.ini          | 20                           | $0.67 | 1h 35min       | c4.2xlarge     |\n| 595              | 100 MB             | config_light.ini    | 1                            | $0.8  | 1h, 57min      | c4.2xlarge     |\n| 595              | 100 MB             | config_clinical.ini | 1                            | $1.4  | 3h, 26min      | c4.2xlarge     |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n[1] Janowczyk A, Zuo R, Gilmore H, Feldman M, Madabhushi A. HistoQC: An Open-Source Quality Control Tool for Digital Pathology Slides. JCO Clin Cancer Inform. 2019 Apr;3:1-7. doi: 10.1200/CCI.18.00157. PMID: 30990737; PMCID: PMC6552675. ([PubMed](https://pubmed.ncbi.nlm.nih.gov/30990737/))\n\n[2] [HistoQC Wiki](https://github.com/choosehappy/HistoQC/wiki)", "input": [{"name": "Cpu per job"}, {"name": "Memory per job"}, {"name": "Whole slide images"}, {"name": "Output directory name"}, {"name": "Custom pipeline"}, {"name": "Pipeline"}, {"name": "Batch"}, {"name": "Number of processes"}, {"name": "Generate UI"}], "output": [{"name": "QC output directory"}, {"name": "User interface", "encodingFormat": "application/zip"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/choosehappy/HistoQC", "https://github.com/choosehappy/HistoQC/wiki", "https://github.com/choosehappy/HistoQC/releases/tag/v2.1", "https://github.com/choosehappy/HistoQC/archive/refs/tags/v2.1.tar.gz"], "applicationSubCategory": ["Imaging", "Machine Learning"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035757, "dateCreated": 1628256647, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/hover-net-61ffeca/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/hover-net-61ffeca/2", "applicationCategory": "CommandLineTool", "name": "HoVer-Net Inference", "description": "**HoVer-Net Inference** provides segmentation and classification of nuclei in multi-tissue histology images.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nHoVer-Net is a multiple branch network that performs nuclear instance segmentation and classification within a single network. The network leverages the horizontal and vertical distances of nuclear pixels to their centers of mass to separate clustered cells. A dedicated up-sampling branch is used to classify the nuclear type for each segmented instance [1].\n\n**HoVer-Net Inference** contains the official PyTorch implementation of HoVer-Net. The repository can be used for training HoVer-Net and to process images, and **HoVer-Net Inference** performs processing of image tiles or whole slide images (WSI) step.\nBoth tile and WSI processing modes output a JSON file that contains:\n- bounding box coordinates for each nucleus,\n- centroid coordinates for each nucleus,\n- contour coordinates for each nucleus,\n- nucleus type predictions and\n- per-class probabilities for each nucleus (optional).\n\nTile mode also outputs a MAT and overlay file. The MAT file contains:\n   - raw output of network,\n   - instance map containing values from 0 to N, where N is the number of nuclei and\n   - list of length N containing predictions for each nucleus.\n\nWSI mode also produces a low resolution thumbnail and a tissue mask by default, which can be disabled with **Save thumb** and **Save mask** parameters.\n\n### Changes Introduced by Seven Bridges\n\n- Input images are now provided as a list of tiles/WSIs in order to enable easier batching.\n- Model weights are provided through either **Checkpoints** file input or app parameter. Provided checkpoints are integrated into the app, for the following datasets:\n    - [CoNSeP](https://www.sciencedirect.com/science/article/abs/pii/S1361841519301045) - [checkpoint link](https://drive.google.com/file/d/1FtoTDDnuZShZmQujjaFSLVJLD5sAh2_P/view)\n    - [MoNuSAC](https://ieeexplore.ieee.org/abstract/document/8880654) - [checkpoint link](https://drive.google.com/file/d/13qkxDqv7CUqxN-l5CpeFVmc24mDw6CeV/view)\n    - [Kumar](https://ieeexplore.ieee.org/abstract/document/7872382) - [checkpoint link](https://drive.google.com/file/d/1NUnO4oQRGL-b0fyzlT8LKZzo6KJD0_6X/view)\n    - [CPM17](https://www.frontiersin.org/articles/10.3389/fbioe.2019.00053/full) - [checkpoint link](https://drive.google.com/file/d/1lR7yJbEwnF6qP8zu4lrmRPukylw9g-Ms/view)\n- The [PanNuke](https://arxiv.org/abs/2003.10778) dataset is licensed under Attribution-NonCommercial-ShareAlike 4.0 International, and therefore is not integrated into the app. Model weights for the PanNuke dataset can be found at this link - [PanNuke checkpoint](https://drive.google.com/file/d/1SbSArI3KOOWHxRlxnjchO7_MbWzB4lNR/view).\n- Some default parameters differ from the original tool, they are optimized for the PanNuke pre-trained model. The default values for each parameter can be found in their Description field.\n### Common Issues and Important Notes\n\n- In order to avoid the insufficient shared memory issue, default value for the **Batch size** parameter is set to `4`. In case this issue occurs due to the large size or number of input images (e.g. 1000 tiles), try lowering the **Batch size** parameter, or increasing the number of GPUs used.   \n- Default index value for **GPU list** input is `0`, which means that only one GPU will be used, and thus the default AWS instance used is p2.xlarge with 1 GPU available. When adding additional GPUs to the **GPU list** input be sure to also select an instance with corresponding number of GPUs available.  \n- Users can set model weights on the **Checkpoints** file input, obtained from training HoVer-Net to process input images/WSIs using PyTorch implementation of HoVer-Net. Alternatively, any of the pre-trained model weights given on the **Checkpoints** enum input can be used to process the data. Provided checkpoints are either trained for segmentation alone or for simultaneous segmentation and classification. Pre-trained models for simultaneous segmentation and classification are PanNuke, CoNSep and MoNuSAC, whereas the models trained for segmentation alone are Kumar and CPM17. Due to the licensing limitation, the PanNuke model is not integrated into the wrapper.\n- It is important to select the correct **Model mode** when running inference. Selecting `original` model mode refers to the method described in the original medical image analysis paper [2] with a 270x270 patch input and 80x80 patch output. Selecting `fast` model mode uses a 256x256 patch input and 164x164 patch output. Model checkpoints trained on Kumar, CPM17 and CoNSeP are from the original publication and therefore the `original` mode must be used. For PanNuke and MoNuSAC, the `fast` mode must be selected. The model mode for each checkpoint that is provided within the **Checkpoints** enum parameter is given in the filename. \n- **Type info** JSON is used to specify what RGB colors are used in the overlay. This file is used to control overlay boundary colors for a different dataset. If no file is provided the file adjusted to the PanNuke dataset will be used:\n\n\t```\n\t{\n\t\t\"0\" : [\"nolabe\", [0 , 0, 0]],\n\t\t\"1\" : [\"neopla\", [255, 0, 0]],\n\t\t\"2\" : [\"inflam\", [0 , 255, 0]],\n\t\t\"3\" : [\"connec\", [0 , 0, 255]],\n\t\t\"4\" : [\"necros\", [255, 255, 0]],\n\t\t\"5\" : [\"no-neo\", [255, 165, 0]]\n\t}\n\t```\n\t\n\tAdditionally, **Number of nuclei types to predict** needs to match this value in **Type info** JSON file. Default value of the **Number of nuclei types to predict** and corresponding value in the default JSON is `6`. When using a model trained only for segmentation, **Number of nuclei types to predict** must be set to `0`.\n- **HoVer-Net Inference** supports processing of both tile and whole slide images, and processing mode is selected with the **Command mode** parameter. The following tool parameters are mode-specific and will only be applied when run in corresponding mode:\n\t- WSI options\n\t    - **Directory containing tissue mask**\n\t    - **Magnification level**\n\t    - **Ambiguous size**\n\t    - **Chunk shape**\n\t    - **Save thumb**\n\t    - **Save mask**\n\t- Tile options\n\t    - **Memory usage**\n\t    - **Draw nuclei centroid on overlay**\n\t    - **Output QuPath v0.2.3 compatible format**\n\t    - **Save raw prediction**\n\t    \n\tAdditionally, **HoVer-Net Inference** can work with one or more input files in WSI mode, but at least two files must be provided for the tile mode.\n- Output directory name can be set with the **Output directory name** parameter. If no output name is set, **SampleID** will be used as directory name in case all images share the **SampleID**. If **SampleID** is not shared, or not set, the output directory will be named `inference_tile/wsi` with a time-stamp suffix.\n- **HoVer-Net Inference** relies on AWS GPU instances, available for projects located on AWS. To use this tool, please run it from an AWS project. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs on on-demand AWS instances for a set of samples with different file sizes:\n\n| Processing mode | Number of tiles/WSI | Average size | Batch size  |No. of GPUs| Duration | Cost | Instance (AWS) |\n|---|---|---|---|---|---|---|---|\n| tile | 400 |   2.5 MB | 4 | 1|1h 48min | $1.89 | p2.xlarge |\n| tile| 600 |  2.5 MB| 4 | 1|2h 53min | $3.01 | p2.xlarge |\n| tile | 1000 |   2.5 MB | 2 | 8|1h 8min | $8.41 | p8.xlarge |\n| WSI | 1 |   150 MB | 4 | 1|1h 3min | $1.09 | p2.xlarge |\n| WSI | 1 |   300 MB | 4 | 1|1h 47min | $1.86 | p2.xlarge |\n| WSI | 1 |   500 MB | 4 | 1|3h 13min | $3.36 | p2.xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n  \n### References\n\n[1] [HoVer-Net ](https://github.com/vqdang/hover_net)\n\n[2] SimonGraham et al. (2019) [Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images](https://www.sciencedirect.com/science/article/abs/pii/S1361841519301045?via%3Dihub) \nMedical Image Analysis Volume 58, December 2019", "input": [{"name": "GPU list"}, {"name": "Number of nuclei types to predict"}, {"name": "JSON type info"}, {"name": "Checkpoints"}, {"name": "Model mode"}, {"name": "Number of workers during inference"}, {"name": "Number of workers during post-processing"}, {"name": "Batch size"}, {"name": "Command mode"}, {"name": "Input tiles/WSIs"}, {"name": "Output directory name"}, {"name": "Memory usage"}, {"name": "Draw nuclei centroid on overlay"}, {"name": "Output QuPath v0.2.3 compatible format"}, {"name": "Save raw prediction"}, {"name": "Directory containing tissue mask"}, {"name": "Magnification level"}, {"name": "Ambiguous size"}, {"name": "Chunk shape"}, {"name": "Save thumb"}, {"name": "Save mask"}, {"name": "Checkpoints", "encodingFormat": "application/x-tar"}], "output": [{"name": "Output directory"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/vqdang/hover_net#readme", "https://github.com/vqdang/hover_net", "https://github.com/vqdang/hover_net/commit/61ffeca6d22bba8aae0727523192150175be8b18"], "applicationSubCategory": ["Imaging", "Machine Learning"], "project": "SBG Public Data", "creator": "Quoc Dang Vu, Simon Graham", "softwareVersion": ["v1.2"], "dateModified": 1648035173, "dateCreated": 1635776089, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/htseq-count/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/htseq-count/6", "applicationCategory": "CommandLineTool", "name": "HTSeq-count", "description": "HTSeq-count is a tool for quantifying gene expression. \n\nIt takes aligned reads and a list of genomic features as inputs and outputs a table with counts for each feature.\n\nIt is designed with a specific use case in mind - namely to quantify gene expression for subsequent testing for differential expression, which is why, for example, the script does not count reads that map to multiple genes.", "input": [{"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Features", "encodingFormat": "application/x-gtf"}, {"name": "Order"}, {"name": "Stranded"}, {"name": "Quality threshold"}, {"name": "Feature type"}, {"name": "ID attribute"}, {"name": "Mode"}], "output": [{"name": "Raw count data", "encodingFormat": "text/plain"}, {"name": "Samout", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Quantification", "RNA-Seq"], "project": "SBG Public Data", "creator": "Simon Anders (EMBL Heidelberg)", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033584, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/humann2/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/humann2/13", "applicationCategory": "CommandLineTool", "name": "HUMAnN2", "description": "**HUMAnN2** (the HMP Unified Metabolic Analysis Network) provides a method for efficiently and accurately determining the presence/absence and abundance of metabolic pathways in a microbial community from metagenomic sequencing data. It is appropriate for any type of microbial community shotgun sequence profiling (i.e. not just the human microbiome) [1]. \n\n**HUMAnN2** introduces a novel tiered search algorithm that provides highly accurate profiles for characterized members of microbial communities, with fallback to translated search for uncharacterized members. Tiered search provides taxonomic stratification of microbial functions at the species level, thus quantifying the community abundance of functions while assigning them to specific contributors.\n\n\nThe **HUMAnN2** tool requires the following input files:\n- **MetaPhlAn database** file in TAR.GZ format, a database containing all clade-specific marker gene sequences used by **MetaPhlan2**,\n- **ChocoPhlAn database** file in TAR.GZ format, a pangenome database created by clustering the NCBI coding sequences,\n- **UniRef database** file in TAR.GZ format, with gene family definitions which may be clustered at different levels, and\n- **Input file** containing metagenomic reads. It can be in FASTQ, FASTQ.GZ, FASTA or FASTA.GZ format.\n\n\n### Common Use Cases\n**HUMAnN2** takes s file from the **Input file** input node with raw reads (presumably cleaned from host genomic sequences as recommended by [Human Microbiome Project](https://hmpdacc.org/)) and three databases, **MetaPhlAn database**, **ChocoPhlAn database** and **UniRef database** (in TAR.GZ format) needed for this functional metagenomic analysis.\n**HUMAnN2** first identifies known microbial species in a sample by screening input reads with **MetaPhlAn2**. A sample-specific database is then preconstructed, containing functionally annotated pangenomes of the identified species. After that, **HUMAnN2** performs nucleotide-level mapping of all sample reads against the sample\u2019s pangenome database. Relative to comprehensive translated search, nucleotide-level mapping against relevant pangenomes quickly explains a large fraction of reads with fewer opportunities for spurious alignment. Reads that do not align to identified species\u2019 pangenomes are subjected to accelerated translated search against a comprehensive protein database, **UniRef90** or **UniRef50** databases. **UniRef90** is built by clustering **UniRef100** sequences with 11 or more residues, with each cluster being composed of sequences that have at least 90% sequence identity and 80% overlap with the longest sequence (a.k.a. seed sequence) of the cluster. Similarly, **UniRef50** is built by clustering **UniRef90** seed sequences that have at least 50% sequence identity and 80% overlap with the longest sequence in the cluster.\nThis tiered search generates mappings of input reads to gene sequences with known or ambiguous taxonomy. These mappings are weighted by quality and sequence length to estimate per-organism and community-total gene family abundance. Finally, gene families annotated to metabolic enzymes are further analyzed to reconstruct and quantify complete metabolic pathways (by default, **MetaCyc** is used) in the community and per organism.\n\nThe results of the functional profiling are presented in three main output files:\n\n- **Gene families**, a tab delimited file that details the abundance of each gene family in the community. Gene families are groups of evolutionarily-related protein-coding sequences that often perform similar functions. Gene family abundance at the community level is stratified to show the contributions from known and unknown species. Individual species' abundance contributions sum to the community total abundance.\n**HUMAnN2** uses the **MetaPhlAn2** software along with the **ChocoPhlAn** database and translated search database for this computation.\nGene family abundance is reported in RPK (reads per kilobase) units to normalize for gene length; RPK units reflect relative gene (or transcript) copy number in the community. The resulting line in this file  looks like the following one:    \n    *UniRef50_O83668: Fructose-bisphosphate aldolase|g__Bacteroides.s__Bacteroides_vulgatus  31.0*\n\n- **Pathway abundance**, a tab delimited file which details the abundance of each pathway in the community as a function of the abundances of the pathway's component reactions, with each reaction's abundance being computed as the sum over abundances of genes catalyzing the reaction. By default, **HUMAnN2** uses *MetaCyc* pathway definitions and **MinPath** to identify a parsimonious set of pathways which explain observed reactions in the community.The resulting line in this file looks like the following one:   \n    *PWY-5484: glycolysis II (from fructose-6P)|g__Bacteroides.s__Bacteroides_caccae 16.7*\n\n- **Pathway coverage**, a tab delimited file, which provides an alternative description of the presence (1) and absence (0) of pathways in a community, independent of their quantitative abundance. **HUMAnN2** assigns a confidence score to each reaction detected in the community. Reactions with abundance greater than the median reaction abundance are considered to be more confidently detected than those below the median abundance. **HUMAnN2** then computes pathway coverage using the same algorithms described above in the context of pathway abundance, but substituting reaction confidence for reaction abundance. The resulting line in this file looks like the following one:\n    *PWY0-1301: melibiose degradation|g__Bacteroides.s__Bacteroides_caccae   1.0*\n\n\nBeside these main output files, there are two more outputs from the **HUMAnN2** tool:\n- **Output temp files**, which contains a file with the bugs list output from **MetaPhlAn2**, a file with the reduced alignment results from **Bowtie2**, a file with results from **DIAMOND**, and all other reports and output files from tools within the **HUMAnN2** tool.\n\n \n### Changes Introduced by Seven Bridges\n\nNo changes were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n* It is recommended that input files be cleaned from host sequences.\n* End-pairing relationships are currently not taken into account during **HUMAnN2**'s alignment steps. This is due to the fact that **HUMAnN2** strictly aligns reads to isolated coding sequences: either at the nucleotide level or through translated search. The best way to use paired-end sequencing data with **HUMAnN2** is simply to concatenate all reads into a single FASTA or FASTQ file.\n\n### Performance Benchmarking\n\n**HUMAnN2** requires at least 16GB of memory (according to the tool specifications) in order to work properly. Based on heavy load testing, setting the c4.4xlarge AWS instance will be sufficient, in which case it is recommended to use all available CPUs (by setting the number of threads to 16). To use more available CPUs for faster execution, we recommend using the c4.8xlarge AWS instance, and setting the number of threads for **HUMAnN2** to 32.\n\nIn the following table you can find estimates of **HUMAnN2** running time and cost. The benchmarking was performed on one sample with full **MetaPhlAn2** and **ChocoPhlAn** databases, and both **Uniref50** and **Uniref90** databases.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input files sizes | Duration | Cost | Instance (AWS) and number of threads for HUMAnN2\n| --- | --- | --- | --- |\n|Uniref50 database 2.5 GB, FASTQ file 450 MB| 30m| $0.40 | c4.4xlarge / 16 threads|\n|Uniref50 database 2.5 GB, FASTQ file 3.5 GB| 1h 6m| $0.88 | c4.4xlarge / 16 threads|\n|Uniref50 database 2.5 GB, FASTQ file 7.0 GB | 1h 57m| $1.58  | c4.4xlarge / 16 threads|\n|Uniref90 database 5.9 GB, FASTQ file 7.0 GB | 1h 42m| $2.80  | c4.8xlarge / 16 threads|\n|Uniref90 database 5.9 GB, FASTQ file 7.0 GB | 1h 36m| $2.56 | c4.8xlarge / 32 threads|\n\n\n\n[1] Franzosa EA, McIver LJ*, Rahnavard G, Thompson LR, Schirmer M, Weingart G, Schwarzberg Lipson K, Knight R, Caporaso JG, Segata N, Huttenhower C. *Species-level functional profiling of metagenomes and metatranscriptomes*. Nat Methods 15: 962-968 (2018).", "input": [{"name": "MetaPhlAn database", "encodingFormat": "application/x-tar"}, {"name": "ChocoPhlAn database", "encodingFormat": "application/x-tar"}, {"name": "UniRef database", "encodingFormat": "application/x-tar"}, {"name": "Input file", "encodingFormat": "application/x-fasta"}, {"name": "Output folder name"}, {"name": "Number of threads"}, {"name": "Bypass prescreen step"}, {"name": "Bypass the nucleotide index step"}, {"name": "Bypass the translated search step"}, {"name": "Bypass the nucleotide search steps."}, {"name": "Annotation gene index"}, {"name": "Evalue treshold"}, {"name": "Metaphlan options"}, {"name": "Log level"}, {"name": "Remove temp output"}, {"name": "Prescreen threshold"}, {"name": "Identity threshold"}, {"name": "Translated subject coverage threshold"}, {"name": "Translated query coverage threshold"}, {"name": "Taxonomic profile"}, {"name": "ID mapping"}, {"name": "Translated alignment"}, {"name": "Xipe"}, {"name": "Minpath"}, {"name": "Pick frames"}, {"name": "Gap fill"}, {"name": "Output format"}, {"name": "Output max decimals"}, {"name": "Output basename"}, {"name": "Remove stratified output"}, {"name": "Remove column description output"}, {"name": "Input format"}, {"name": "Pathways"}, {"name": "Memory use"}, {"name": "Pathway databases"}], "output": [{"name": "Gene families"}, {"name": "Output temp files"}, {"name": "Temp folder"}, {"name": "Pathway abundance"}, {"name": "Pathway coverage"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Metagenomics", "Functional Characterization"], "project": "SBG Public Data", "creator": "The Huttenhower Lab", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648560771, "dateCreated": 1551887518, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/idr-2-0-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/idr-2-0-2/3", "applicationCategory": "CommandLineTool", "name": "IDR 2.0.2", "description": "The **IDR** (Irreproducible Discovery Rate) framework is a uni\ufb01ed approach to measure the reproducibility of \ufb01ndings identi\ufb01ed from replicate experiments and provide highly stable thresholds based on reproducibility. Unlike the usual scalar measures of reproducibility, the IDR approach creates a curve, which quantitatively assesses when the \ufb01ndings are no longer consistent across replicates. In layman's terms, the IDR method compares a pair of ranked lists of identifications (such as ChIP-seq peaks). These ranked lists should not be pre-thresholded i.e. they should provide identifications across the entire spectrum of high confidence/enrichment (signal) and low confidence/enrichment (noise). The IDR method then fits the bivariate rank distributions over the replicates in order to separate signal from noise based on a defined confidence of rank consistency and reproducibility of identifications i.e the IDR threshold.\n\n\n### Peak matching\n\nThe method in which peaks are matched can significantly affect the output. We have chosen defaults that we believe are reaosnable in the vast majoroity of cases, but it may be worth exploring the various options for your data set.\n\n* --peak-list is *not* provided\n\nPeaks are grouped by overlap and then merged. The merged peak aggregate value is determined by --peak-merge-method. \n\nPeaks that don't overlap another peak in every other replicate are not included unless --use-nonoverlapping-peaks is set. \n\n* --peak-list *is* provided \n\nPeaks are grouped by overlap, and then for each oracle peak a single peak from each replicate is chosen that overlaps the oracle peak. If there are multiple peaks that overlap the oracle, then ties are broken by applying the following criteria in order: 1) choose the replicate peak with a summit closest to the oracle peak's summit 2) choose the replicate peak that has the largest overlap with the oracle peak 3) choose the replicate peak with the highest score", "input": [{"name": "peak_rep1", "encodingFormat": "text/x-bed"}, {"name": "peak_rep2", "encodingFormat": "text/x-bed"}, {"name": "pooled_peaks", "encodingFormat": "text/x-bed"}, {"name": "input_file_type"}, {"name": "rank"}, {"name": "output_file"}, {"name": "log_output_file"}, {"name": "idr_threshold"}, {"name": "soft_idr_threshold"}, {"name": "use_old_output_format"}, {"name": "plot"}, {"name": "use_nonoverlapping_peaks"}, {"name": "peak_merge_method"}, {"name": "initial_mu"}, {"name": "initial_sigma"}, {"name": "initial_rho"}, {"name": "initial_mix_param"}, {"name": "fix_mu"}, {"name": "fix_sigma"}, {"name": "random_seed"}, {"name": "max_iter"}, {"name": "convergence_eps"}, {"name": "only_merge_peaks"}, {"name": "verbose"}, {"name": "quiet"}], "output": [{"name": "output_files"}, {"name": "output plot"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/nboley/idr"], "project": "SBG Public Data", "creator": "Nathan Boleu - Kundaje Lab, Dept. of Genetics, Stanford University Anshul Kundaje - Assistant Professor, Dept. of Genetics, Stanford University Peter J. Bickel - Professor, Dept. of Statistics, University of California at Berkeley", "softwareVersion": ["sbg:draft-2"], "dateModified": 1500041721, "dateCreated": 1500041663, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/indexcov-0-2-4-cwl1-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/indexcov-0-2-4-cwl1-1/4", "applicationCategory": "CommandLineTool", "name": "Indexcov 0.2.4 CWL1.1", "description": "**Indexcov CWL1.1** tool can be used to quickly quality check cohort WGS data from BAM or CRAM index files [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Indexcov CWL1.1** can be used to quickly (in minutes) assess the coverage distribution (16KB resolution) of a WGS cohort from BAI/CRAI files. The tool can also be used to infer sex (by plotting the inferred copy number of sex chromosomes). \n\nTo run the tool, please provide either one or more BAM files with the associated BAI index files (**Input BAM files**) or index (BAI or CRAI) files (**Input BAI or CRAI files**). If only BAI/CRAI files are provided, the **FASTA index FAI file** input should be provided as well and should match the reference associated with provided index files.\n\n\n### Changes Introduced by Seven Bridges\n\n* To preserve linking between individual plots, Interactive HTML plots are provided as a TGZ archive (**TGZ archive with plot files** output). Upon downloading, the integrated results can be viewed by opening the index.html file in a browser.\n* The `--directory` parameter corresponds to the **Output file name prefix** input parameter and defaults to 'out_indexcov' string.\n\n### Common Issues and Important Notes\n\n* By default, the tool uses `X,Y` as values for sex chromosome names (`--sex`). Please ensure that **Names of sex chromosomes** input parameter is set to values matching sex chromosome names in your data, if this part of analysis is of interest. If sex chromosomes are absent from your data, please provide an empty string ('') to this input parameter.\n* Please consider performing the analysis using BAI or CRAI inputs, to optimize storage requirements.\n* Input files provided through the **Input BAI or CRAI files** input parameter are staged (copied to the task working directory) to avoid issues with command line length if a prohibitively large number of input files should be analyzed. \n* Input parameter **Memory per job [MB]** should be used to allocate more memory if a task fails because of insufficient RAM. \"Killed\" is a commonly seen error message under these circumstances.\n* For CRAI inputs, the use of the input parameter **Extra normalize** is recommended.\n\n### Performance Benchmarking\n\nDuration and cost of **Indexcov CWL1.1** tasks depend on the type (BAMs vs. BAI and CRAI only) of provided inputs and the number of samples in the cohort. BAI and CRAI inputs are preferable. \n\nProcessing 10 single WGS sample CRAIs took under a minute on a c4.2xlarge on-demand AWS instance with negligible cost ($0.01).\n\n**Indexcov CWL1.1** requires considerable amounts or RAM when processing a large number of samples, as all index files are stored in memory. Approximately 48 GB RAM was necessary to process 5000 WGS samples (task duration: 100 minutes, AWS c4.8xlarge on-demand instance, cost $2.65 + $0.25). \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**Indexcov** was tested with cwltool version 3.1.20211107152837. The `in_fai` and `in_bais_or_crais` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [Indexcov publication](https://www.ncbi.nlm.nih.gov/pubmed/29048539)\n\n[2] [Indexcov documentation](https://github.com/brentp/goleft/tree/master/indexcov)", "input": [{"name": "Chromosome to extract depth from"}, {"name": "Exclude pattern"}, {"name": "Plot GL chromosomes"}, {"name": "Input BAI or CRAI files"}, {"name": "Input BAM files", "encodingFormat": "application/x-bam"}, {"name": "FASTA index FAI file"}, {"name": "Memory per job [MB]"}, {"name": "Names of sex chromosomes"}, {"name": "Number of CPUs per job"}, {"name": "Output file name prefix"}, {"name": "Extra normalize"}], "output": [{"name": "BED indexcov output", "encodingFormat": "text/x-bed"}, {"name": "PED/FAM file with inferred sex"}, {"name": "ROC output"}, {"name": "TGZ archive with plot files"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/goleft", "https://github.com/brentp/goleft/tree/master/indexcov", "https://github.com/brentp/goleft/releases", "https://github.com/brentp/goleft/tree/master/indexcov#indexcov"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612351848, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/isoformswitchanalyzer-1-12-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/isoformswitchanalyzer-1-12-0/3", "applicationCategory": "CommandLineTool", "name": "IsoformSwitchAnalyzeR", "description": "The **IsoformSwitchAnalyzeR** performs statistical identification of the isoform switching comparing two sample groups.\n\nThis app is based on the **IsoformSwitchAnalyzeR 1.12.0** R/Bioconductor package [1] and enables statistical identification of differential usage of isoforms in different conditions, often referred to as isoform switching [2]. Isoform switches are found in many diseases (especially cancer) and the majority of them can not be detected using only the differential gene expression approach.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***  \n\n\n### Common Use Cases\n\nTo be able to run an analysis, the tool needs to be provided with:\n- **Counts.** Transcript-level estimated abundances. Depending on the software used for quantification (salmon, kallisto, rsem), the **Input type** parameter should be specified adequately. \n- **Gene annotation file.** GTF file in uncompressed (.gtf) or compressed (.gtf.gz) form.\n- **Phenotype information.**\n\nThere are two options for providing phenotype information:\n\n**a)** By indicating API keys for metadata fields that need to be included in the design. For example, if the effect of **sample_type** is being analysed (for example: treated, untreated) on DTU controlling for **sex** (male, female) **sample_id**, **sample_type** and **sex** metadata fields need to be populated for all count files, and **Covariate of interest** and **Control variables** need to be set to **sample_type** and **sex** respectively. \n\n**b)** By including a CSV file (Phenotype data input) - see the example below. Phenotype data must contain the **sample_id** column with values that match values from the metadata **sample_id** field of the count files. To run an analysis, parameters **Covariate of interest** and **Control variables** (optional) need to be set to the adequate column names. For example, entering **sample_type** for the value of the **Covariate of interest** parameter and **library** in **Control variables** will test for differential transcript usage between treated and untreated samples, while controlling for effect of library preparation.\n\nExample CSV content below:\n\n```\nsample_id,sample_type,library,sex\ntreated1,treated,paired-end,male\ntreated2,treated,single-end,male\ntreated3,treated,paired-end,female\nuntreated1,untreated,single-end,male\nuntreated2,untreated,paired-end,female\nuntreated3,untreated,paired-end,female\nuntreated4,untreated,paired-end,male\n\n```\n\n- Filtering data, i.e removing transcripts that are irrelevant for the analysis (like single isoform genes or isoforms that are not expressed) has positive effects on the performance and also speeds up the analysis. The **Gene Expression Cutoff** parameter sets the level of expression (most likely in FPKM/TPM/RPKM) that gene needs to exceed in at least one condition to be included in the analysis (default = **1**). The **Isoform Expression Cutoff** parameter sets the level of expression that isoform needs to exceed in at least one condition (default = **0**, which removes completely unused isoforms). The **IF cutoff** parameter sets the cutoff on isoform usage (measured as Isoform Fraction, IF = isoform expression / gene expression) that isoforms must be used more than in at least one condition (default = **0**, which removes non-contributing isoforms). NULL disables any of the three mentioned filtering parameters. Furthermore, the genes containing only one isoform will be automatically removed from the analysis.\n\n- The **Denominator** parameter indicates the sample group that should be used as a reference. For example, if **Covariate of interest** consists of values `treated` and `untreated`, and we want to use `untreated` as the control group, the **Denominator** parameter should be set to `untreated`. If the parameter is not specified, the function will take the annotation of the first sample.\n\n- The tool uses DEXSeq method for identification of the isoform switching. Two parameters are important in defining the significant result: the **Alpha** parameter which sets the cutoff that the FDR corrected p-values must be smaller than for calling significant switches (default = **0.05**), and the **dIF cutoff** parameter that sets the cutoff for (absolute) change in an isoform usage between conditions that must be exceeded before an isoform is considered significant (default = **0.1**, i.e 10%). This cutoff is analogous to having a cutoff on log2 fold change in a normal differential expression analysis to ensure that genes have a certain effect size. \n\nAs a result of an analysis, the tool will output:\n\n- CSV result table containing for each isoform: isoform_id, gene_id, gene_name, gene_biotype, isoform_biotype, isoform fractions (IF1 and IF2), dIF, isoform_switch_q_value, gene_switch_q_value, and PTC (presents of premature termination codons). \n- HTML report - a summary of the results including the Genome-wide analysis of alternative splicing.\n- Optionally, by setting the **Output result R object** parameter to **TRUE**, the result R object will be outputted, so IsoformSwitchAnalyzeR functions for visualisation and further exploration of the data and results can be used locally.\n\n### Common Issues and Important Notes\n- All imported count files must have a unique sample id set in the **sample_id** metadata field.\n- Any metadata key entered as **Covariate of interest** or **Control variables** needs to exist and to be populated in all of the samples (**Count Files**) if phenotype data is read from the metadata. Otherwise, if a CSV is supplied - these keys need to match the column names in its header. Keep in mind that metadata keys are usually different from what is seen on the front-end. To match metadata keys to their corresponding values on the front-end, please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). Learn how to [add a custom metadata field](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#modifying-metadata-via-the-visual-interface) to expression data files.\n- Choosing the reference. For human and mouse using the Gencode FASTA/GTF combination is recommended. For all other species, it is recommended to use Ensembl reference files. NOTE: Ensembl transcriptome reference sequence (FASTA) is divided into two files (the \u201ccDNA\u201d and the \u201cncRNA\u201d), meaning that the user has to decide what kind of sequences they want to quantify (using Salmon, Kallisto or RSEM) in advance. If using one of these two FASTA files, the GTF file that contains haplotypes (named: .chr_patch_hapl_scaff.gtf) has to be chosen.\n\n\n### Performance Benchmarking\n\nRuntime and task cost for different numbers of input files (by default, the tool will use a c4.2xlarge AWS instance). The duration of the genome-wide analysis of alternative splicing (thus the whole analysis) also depends on the number of significant isoforms.  \n\n| Input size |  # of input files | # of significant isoforms | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:---------:|:----------:|:----------:|:------------:|\n|      27.6 MB     |     8   |     642     |   11 min   |   $0.10    |    c4.2xlarge     |\n|      27.6 MB     |     8   |     2800     |   17 min   |   $0.16    |    c4.2xlarge     |\n|      27.6 MB     |    16   |     213     |   16 min   |   $0.15    |    c4.2xlarge     |\n|      27.6 MB     |    30   |     251     |    46 min   |   $0.42    |    c4.2xlarge     |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n[1] [IsoformSwitchAnalyzeR package](https://bioconductor.org/packages/3.12/bioc/html/IsoformSwitchAnalyzeR.html)\n[2] [IsoformSwitchAnalyzeR vignette](https://bioconductor.org/packages/3.12/bioc/vignettes/IsoformSwitchAnalyzeR/inst/doc/IsoformSwitchAnalyzeR.html#analyzing-open-reading-frames)", "input": [{"name": "Counts"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Phenotype data"}, {"name": "Output name prefix"}, {"name": "Out result R object"}, {"name": "Denominator"}, {"name": "Covariate of interest"}, {"name": "Control variables"}, {"name": "Gene Expression Cutoff"}, {"name": "Isoform Expression Cutoff"}, {"name": "IF cutoff"}, {"name": "dIF cutoff"}, {"name": "Alpha"}, {"name": "Reduce To Switching Genes"}], "output": [{"name": "IsoformSwitchAnalyzeR result R object"}, {"name": "IsoformSwitch results"}, {"name": "Summary report", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/kvittingseerup/IsoformSwitchAnalyzeR"], "applicationSubCategory": ["RNA-Seq", "Differential Splicing"], "project": "SBG Public Data", "softwareVersion": ["v1.1"], "dateModified": 1648035484, "dateCreated": 1619093324, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-bus-0-46-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-bus-0-46-0/3", "applicationCategory": "CommandLineTool", "name": "Kallisto BUS", "description": "The **Kallisto BUS** tool is used to produce BUS (Barcode-UMI-Set) files from single-cell RNA-seq data produced by any single-cell RNA-Seq technology. \nBUS format is a file format for single-cell RNA-Seq data designed to facilitate the development of modular workflows for data processing. It consists of a binary representation of barcode and UMI sequences from scRNA-seq reads, along with sets of equivalence classes obtained by pseudoalignment of reads to a reference transcriptome (hence the acronym Barcode, UMI, Set) [1, 2].\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n###Common Use Cases\n\n- **Kallisto BUS** works with raw FASTQ files for single-cell RNA-Seq datasets. For each read the cell barcode and UMI information and the equivalence class resulting from pseudoalignment are stored in an output BUS file, along with matrix.ec and transcripts.txt which store information about the equivalence classes and transcript names for downstream processing.\n\n### Changes Introduced by Seven Bridges\n\n- Instead of having the default names (output.bus, matrix.ec and transcripts.txt), output files will be named after the Sample_ID metadata value if present; if not, the output files will be named by the common name prefix of the FASTQ files used on input.\n\n### Common Issues and Important Notes\n\n- Paired-end metadata field needs to be properly set for the input FASTQ files.\n- In case all the input FASTQ files are from the same sample, Sample ID metadata field can be set, which will be used for naming the output files.\n\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the size of the cDNA FASTQ files. Following table showcases the metrics for the task running on the c5.2xlarge on demand AWS instance. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\nProtocol | Dataset | Fastq file 1 size(.gz) | Fastq file 2 size(.gz) | Duration | Cost | Instance type (AWS) |\n|--------|---------|---------------|-----------------|-----------|--------|-----|\n| 10x Chromium v3| 1k_v3 | 1.2 GB | 2.9 GB | 4 min. | $0.03 | c5.2xlarge |\n| 10x Chromium v2 | 4k_v2 | 7.6 GB | 22.4 GB | 19 min. | $0.09 | c5.2xlarge |\n| 10x Chromium v2| 8k_v2 | 12.9 GB | 49.8 GB | 40 min. |$0.23 | c5.2xlarge |\n| 10x Chromium v3| 10k_v3 | 11.4 GB | 27.3 GB | 32 min. | $0.17 | c5.2xlarge |\n\n### References\n\n[1] [Modular and efficient pre-processing of single-cell RNA-seq](https://www.biorxiv.org/content/10.1101/673285v1)\n\n[2] [The barcode, UMI, set format and BUStools](https://www.biorxiv.org/content/10.1101/472571v2)", "input": [{"name": "Input FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Kallisto index"}, {"name": "Sequencing technology"}, {"name": "Threads"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Output prefix"}], "output": [{"name": "BUS output file"}, {"name": "Matrix output file"}, {"name": "Transcripts output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["Transcriptomics", "Single Cell", "CWL1.0"], "project": "SBG Public Data", "creator": "Pachter Lab", "softwareVersion": ["v1.0"], "dateModified": 1575458926, "dateCreated": 1575458750, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-h5dump-0-43-1/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-h5dump-0-43-1/7", "applicationCategory": "CommandLineTool", "name": "Kallisto H5DUMP", "description": "The **Kallisto H5DUMP** tool converts the **Kallisto Quant** HDF5-formatted results to a plaintext.\n\nDue to the large amount of data that may be produced when the number of bootstrap samples is high, **Kallisto** outputs bootstrap results in **HDF5 format**. The most convenient way to analyse bootstrap results is with the **Sleuth** tool. However, you can use this **Kallisto H5DUMP** tool instead to convert the HDF5-formatted results to a plaintext [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The **HDF5 formatted abundance file**, produced by the **Kallisto Quant** tool is needed for running this tool. \n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected. \n\n### Performance Benchmarking\n\nThe **Kallisto H5DUMP** tool executes in a short amount of time, usually under 5 minutes, costing around $0.05 on the default c4.2xlarge instance (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Kallisto homepage](http://pachterlab.github.io/kallisto/)", "input": [{"name": "HDF5 formatted abundance file"}], "output": [{"name": "Abundance file in plaintext format", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pachterlab/kallisto", "https://github.com/pachterlab/kallisto/releases/download/v0.43.1/kallisto_linux-v0.43.1.tar.gz"], "applicationSubCategory": ["File Format Conversion"], "project": "SBG Public Data", "creator": "Nicolas Bray, Harold Pimentel, P\u00e1ll Melsted, Lior Pachter", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649156264, "dateCreated": 1514564121, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-index-0-46-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-index-0-46-0/6", "applicationCategory": "CommandLineTool", "name": "Kallisto Index", "description": "The **Kallisto Index** tool builds an index from a transcriptome FASTA formatted file of target sequences, necessary for the **Kallisto Quant** tool.  \n\n**Pseudoalignment** is a process of assigning reads to transcripts, without doing the exact base-to-base alignment. Seeing that for estimating transcript abundances, the main information needed is which transcript a read originates from and not the actual mapping coordinates, the idea with the **Kallisto** tool was to implement a procedure that does exactly that, using a *transcriptome De Brujin graph* as the main data structure for the underlying algorithm [1].\n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilise the full likelihood model, while keeping near-optimal probabilistic RNA-seq quantification results [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- A **Transcriptome FASTA file** needs to be provided as an input to the tool. \n\n### Changes Introduced by Seven Bridges\n\n- An already generated **Kallisto index file** can be provided to the **Kallisto Index** tool (**Transcriptome FASTA or Kallisto Index** input), in order to skip indexing and save a little bit of time if this tool is part of a bigger workflow and there already is an index file that can be provided.\n\n- Included parameter **Create transcript to gene annotation** which enables translating Ensembl transcript IDs to gene IDs, required for processing single cell BUS file format. To use this parameter it is required to provide gene annotation file in GTF format.\n\n### Common Issues and Important Notes\n\n- The input FASTA file (if provided instead of the already generated kallisto index) should be a transcriptome FASTA, not a genomic FASTA.\n\n### Performance Benchmarking\n\nThe **Kallisto Index** tool builds the index structure for **Kallisto** in a very short time, therefore it is expected that all tasks using this tool should finish under 5 minutes, costing around $0.05 on the default c4.2xlarge instance (AWS). \n\n*Cost can be significantly reduced by  using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [Kallisto paper](https://www.nature.com/articles/nbt.3519)", "input": [{"name": "K-mer length"}, {"name": "Make unique"}, {"name": "Transcriptome FASTA or Kallisto Index", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Create transcript to gene annotation"}, {"name": "GTF annotation", "encodingFormat": "application/x-gtf"}], "output": [{"name": "Transcripts index"}, {"name": "Transcripts to genes"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pachterlab/kallisto", "https://github.com/pachterlab/kallisto/releases/download/v0.46.0/kallisto_linux-v0.46.0.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Indexing", "CWL1.0"], "project": "SBG Public Data", "creator": "Nicolas Bray, Harold Pimentel, P\u00e1ll Melsted, Lior Pachter", "softwareVersion": ["v1.0"], "dateModified": 1614258903, "dateCreated": 1575458750, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-pseudo-0-46-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-pseudo-0-46-0/3", "applicationCategory": "CommandLineTool", "name": "Kallisto Pseudo", "description": "The **Kallisto Pseudo** tool runs the pseudoalignment step of the **Kallisto** software and is meant for usage in a single cell RNA-seq analysis. \n\n**Pseudoalignment** is a process of assigning reads to transcripts, without doing the exact base-to-base alignment. Seeing that for estimating transcript abundances, the main information needed is which transcript a read originates from and not the actual mapping coordinates, the idea with the **Kallisto** tool was to implement a procedure that does exactly that, using a *transcriptome De Brujin graph* as the main data structure for the underlying algorithm [1]. \n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilise the full likelihood model, while keeping near-optimal probabilistic RNA-seq quantification results [1] . \n\nThe form of the **Kallisto Pseudo** command and the meaning of the parameters are identical to the **Kallisto Quant** command. However, **Kallisto Pseudo** does not run the EM-algorithm to quantify abundances. In addition, the **Kallisto Pseudo** command has an option to specify many cells in a batch file which will read information about each cell in the *batch.txt* file and process all cells simultaneously. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The main input to the tool are **Reads (FASTQ files)** that can be single end or paired end, either gzipped or not. \n- A **Kallisto transcript index file** file needs to be provided (which can be generated by the **Kallisto Index** tool). \n- A **Batch file** (`--batch`) can be provided in addition to the FASTQ read files. In this case, **Kallisto Pseudo** will run in *batch* mode, meaning that information about all cells, present in that file, will be processed simultaneously. The **Batch file** file should be text file in tab delimited format, where first column is the cell name and second and third columns are names of the paired-end FASTQ files for that cell sample. If working with single-end reads, the third column is not needed, but the **Single end reads for batch mode** (`--single`) parameter needs to be specified. \n- If **UMI** (`--umi`) option is specified, the **Batch file** should be in different format. More about that can be found in the **Kallisto** manual page [2].\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n - It is important to properly set the **Paired End** metadata field for all paired-end read files that are found on the input **Reads (FASTQ files)** node.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform unit ID/File segment number**).\n\n\n### Performance Benchmarking\n\n**Kallisto** is not compute-intensive, and as so, it is expected that a single sample at around 2x20GB would be processed in under 15min with the **Kallisto Pseudo** tool, costing around $0.10 on the c4.2xlarge instance (AWS). \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [Kallisto paper](https://www.nature.com/articles/nbt.3519)   \n[2] [Kallisto manual](https://pachterlab.github.io/kallisto/manual)", "input": [{"name": "Batch file", "encodingFormat": "text/plain"}, {"name": "Fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Kallisto transcript index file"}, {"name": "Number of threads"}, {"name": "Reads (FASTQ files)", "encodingFormat": "text/fastq"}, {"name": "Single end reads for batch mode"}, {"name": "UMI"}, {"name": "Use EM algorithm"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "Output folder archive", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pachterlab/kallisto", "https://github.com/pachterlab/kallisto/releases/download/v0.46.0/kallisto_linux-v0.46.0.tar.gz"], "applicationSubCategory": ["Transcriptomics", "CWL1.0"], "project": "SBG Public Data", "creator": "Nicolas Bray, Harold Pimentel, P\u00e1ll Melsted, Lior Pachter", "softwareVersion": ["v1.0"], "dateModified": 1575458926, "dateCreated": 1575458750, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kallisto-quant-0-46-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kallisto-quant-0-46-0/4", "applicationCategory": "CommandLineTool", "name": "Kallisto Quant", "description": "The **Kallisto Quant** tool infers transcript abundance estimates from the **RNA-seq data**, using a process called **pseudoalignment**. \n\n**Pseudoalignment** is a process of assigning reads to transcripts, without doing the exact base-to-base alignment. Seeing that for estimating transcript abundances, the main information needed is which transcript a read originates from and not the actual mapping coordinates, the idea with the **Kallisto** tool was to implement a procedure that does exactly that, using a *transcriptome De Brujin graph* as the main data structure for the underlying algorithm [1].\n\nThe result is a software running at speeds orders of magnitude faster than other tools which utilise the full likelihood model, while keeping near-optimal probabilistic RNA-seq quantification results [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- The main input to the tool are **Reads (FASTQ files)** that can be single end or paired end, either gzipped or not. \n- A **Kallisto transcript index file** generated by the **Kallisto Index** tool needs to be provided. \n- The **Kallisto Quant** tool will generate transcript abundance estimates in plaintext and H5 formats (useful for processing by downstream tools for differential expression, like [Sleuth](https://pachterlab.github.io/sleuth/about)). \n\n### Changes Introduced by Seven Bridges\n\n- The **Abundance file in plaintext format** will always be outputted by default (there is no need to specify the `--plaintext` parameter). \n- The options for specifying strandedness (`--fr-stranded` and `rf-stranded`) have been merged into a single options of enum type - **Strandedness**. \n\n### Common Issues and Important Notes\n\n- It is important to properly set the **Paired End** metadata field for all paired-end read files that are found on the input **Reads (FASTQ files)** node.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform unit ID/File segment number**).\n\n### Performance Benchmarking\n\nThe main advantage of the Kallisto software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Therefore, it is optimized to be run in scatter mode, so a c4.8xlarge  (AWS) instance is used by default. \n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes.\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance  (AWS)|\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   5min   | $0.05| c4.2xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M     |     101     |   12min  | $0.10 | c4.2xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Kallisto paper](https://www.nature.com/articles/nbt.3519)", "input": [{"name": "Bias"}, {"name": "Bootstrap samples"}, {"name": "Fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Fusion"}, {"name": "Kallisto transcript index file"}, {"name": "Number of threads"}, {"name": "Output prefix"}, {"name": "Pseudobam"}, {"name": "Reads (FASTQ files)", "encodingFormat": "text/fastq"}, {"name": "Seed"}, {"name": "Strandedness"}, {"name": "Single overhang"}, {"name": "Genomebam"}, {"name": "GTF annotation", "encodingFormat": "application/x-gtf"}, {"name": "Chromosome names and lengths", "encodingFormat": "text/plain"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "HDF5 formatted abundance file"}, {"name": "Abundance file in plaintext format"}, {"name": "Fusion candidates", "encodingFormat": "text/plain"}, {"name": "Pseudoalignments", "encodingFormat": "application/x-bam"}, {"name": "Run info JSON"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pachterlab/kallisto", "https://github.com/pachterlab/kallisto/releases/download/v0.46.0/kallisto_linux-v0.46.0.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Quantification", "CWL1.0"], "project": "SBG Public Data", "creator": "Nicolas Bray, Harold Pimentel, P\u00e1ll Melsted, Lior Pachter", "softwareVersion": ["v1.0"], "dateModified": 1581095501, "dateCreated": 1575458750, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kraken2-2-0-9/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kraken2-2-0-9/7", "applicationCategory": "CommandLineTool", "name": "Kraken2", "description": "**Kraken2** is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer [4, 5].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n**Kraken2** classifies sequences provided on the **Input reads** port. The tool offers several options for running classification:\n- `Multithreading`  - set the number of threads on the **Number of threads** input.\n- `Quick operation` - rather than searching all l-mers in a sequence, stop classification after the first database hit; set **Quick operation** to `True` to enable this mode [1].\n- `Hit group threshold` - the **Minimum number of hit groups** option will allow you to require multiple hit groups (a group of overlapping k-mers that share a common minimizer that is found in the hash table) be found before declaring a sequence classified, which can be especially useful with custom databases when testing to see if sequences either do or do not belong to a particular genome [1].\n- `Sequence filtering` - Classified or unclassified sequences can be sent to a file for later processing, using the **--classified-out** and **--unclassified-out** switch [1].\n- `Compressed input` - Kraken 2 can handle gzip and bzip2 compressed files as inputs by specifying the proper **--gzip-compressed** or **--bzip2-compressed** [1].\\\n*The following inputs are required:*\n* **Kraken2 database** build using the **Kraken2 Build** tool.\n* **Input reads** containing the sequences to be classified. All **Input reads** files have to have the sample ID field set in metadata.\n* **Database name** - name of the database to be used for classifying, which has to be present in the provided **Kraken2 database** archive.\\\n*Some input parameters are only relevant to specific task options and incompatible with others:*\n* `--use-mpa-style` and `--report-zero-counts` can be used only when `--report` is specified.\\\n\n### Changes Introduced by Seven Bridges\n* The main difference to a local execution is that the Kraken2 database is expected and provided as a TAR.GZ archive containing the database.\n* Input parameters `--help` and `--version` have been omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* The default  **Memory per job [MB]** value which the Kraken2 will use is 500 MB. The amount of requested memory can be adjusted through the **Memory per job [MB]** input parameter. **Kraken 2** will load the database into process-local RAM by default; the `--memory-mapping` switch will avoid doing so. Users should choose the appropriate value for RAM depending on the size of the database they are using.\n* **Use smart resource allocation** input when set to `True` will automatically set **Memory per job [MB]** equal to the size of the Kraken2 database. **IMPORTANT**: use this option carefully as it might allocate a bigger instance, which in turn could lead to increased costs.\n* **Number of threads** input parameter describes the number of threads used when running Kraken2. The default number of threads used is 1.\n\n### Performance Benchmarking\n\n* Two benchmark tasks were run using the same file from Human Microbiome Project as Input reads, `SRS014689.denovo_duplicates_marked.trimmed.singleton.fastq` (size: 249.4 MB). One task was run with one small Archaea database (size: 6.6 GB) and the other one with a complete standard Kraken2 database (size: 61.3 GB).\n     \n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Archaea database | 7 min |$0.03 + $0.02 | r5.xlarge | \n| Standard Kraken2 datatbase | 41 min | $0.66 + $0.09 | r5.4xlarge |  \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n\n###References\n[1] [Homepage](https://ccb.jhu.edu/software/kraken2/)\\\n[2] [Source Code](https://github.com/DerrickWood/kraken2)\\\n[3] [Download](http://github.com/DerrickWood/kraken2/archive/v2.0.9-beta.tar.gz)\\\n[4] [Publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0)\\\n[5] [Documentation](https://github.com/DerrickWood/kraken2/wiki/Manual)", "input": [{"name": "Kraken2 database", "encodingFormat": "application/x-tar"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Input reads", "encodingFormat": "application/x-fasta"}, {"name": "Quick operation"}, {"name": "File name for unclassified sequences"}, {"name": "File name for classified sequences"}, {"name": "Output filename"}, {"name": "Confidence score threshold"}, {"name": "Minimum base quality"}, {"name": "File name for the report with aggregate counts/clade"}, {"name": "Format report like Kraken 1 MPA report"}, {"name": "Report zero counts"}, {"name": "Memory mapping"}, {"name": "Print scientific names"}, {"name": "gzip compressed input files"}, {"name": "bzip2 compressed input files"}, {"name": "Minimum number of hit groups"}, {"name": "Database name"}, {"name": "Extension for classified output file"}, {"name": "Extension for unclassified output file"}, {"name": "Use smart resource allocation"}], "output": [{"name": "Unclassified sequences", "encodingFormat": "application/x-fasta"}, {"name": "Classified sequences", "encodingFormat": "application/x-fasta"}, {"name": "Kraken2 output file"}, {"name": "Report with aggregate counts/clade"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/DerrickWood/kraken2", "https://github.com/DerrickWood/kraken2/releases/tag/v2.0.9-beta", "https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown"], "applicationSubCategory": ["Metagenomics"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648039728, "dateCreated": 1612269282, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kraken2-build-2-0-9/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kraken2-build-2-0-9/6", "applicationCategory": "CommandLineTool", "name": "Kraken2 Build", "description": "**Kraken2 Build** tool is used to prepare the reference database for **Kraken2** [1,2]. It is a part of the **Kraken2** toolkit which is used in Metagenomics for assigning taxonomic labels to DNA sequences. **Kraken2 Build** should be used in order to build a database before running the **Metagenomics Profiling - Kraken2 workflow** analysis.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Kraken2 Build** has several distinct modes of operation, selected using the **Task option** input parameter:\n- `standard` - Download library and taxonomy files from NCBI and build a standard (default) Kraken2 database with most commonly used reference genomes.\n- `special` - Download and build preselected 16S databases. If this mode is selected, please also specify the desired database through the **Special database type** input parameter. Please check the licencing of the linked databases and ensure you are permitted to access them before downloading:\n\n    [Greengenes](http://greengenes.lbl.gov/Download/) (`greengenes`) uses all available 16S data\n    [RDP](http://rdp.cme.msu.edu/) (`rdp`) uses bacterial and archaeal 16S data\n    [SILVA](https://www.arb-silva.de/) (`silva`) uses the Small subunit NR99 sequence set\n\n- `download-library` - Download reference files for prespecified organisms from NCBI. If this mode is selected, the desired organism group should be specified through the **Download library type** input parameter.\n- `download-taxonomy` - Download taxonomy data (accession number to taxon maps, tree and taxonomic name) from NCBI. If accession numbers are not needed (they are required for the 'plasmid' group and custom sequences added to libraries), downloading them can be skipped with the **Avoid downloading accession number to taxid maps** input parameter.\n- `build` - Builds a Kraken2 database using one or more library and taxonomy archives provided as inputs.\n- `add-to-library` - Adds a specified sequence file (**File to add to library** input) to the provided library archive (**Library TAR.GZ archive** input)\n- `clean` - Removes intermediary files from a provided Kraken2 database archive (**Database to clean** input) to reduce its size. Please note that some of the intermediary files may be required by downstream tools (like **Bracken**).\n\nThe database creation process can be done in one task (`standard` and `special` ) or in stages, by invoking the **Kraken2 Build** tool multiple times: one or more `download-library` and `download-taxonomy` executions, optionally followed by `add-to-library` if custom sequences are needed, and finalized with `build` and optionally `clean` steps.\n\n\n\n### Changes Introduced by Seven Bridges\n\n* The main difference to a local execution is that input and output database, library and taxonomy files are expected and provided as TAR.GZ archives, containing the database top-level folder.\n* Input parameters `--help` and `--version` have been omitted from the wrapper.\n* Input parameter **Output archive name prefix** has been added to allow optional renaming of generated TAR.GZ archives. The database folders within the archives will still be named based on the **DB name** input parameter.\n* Input parameter **Use FTP 'na' issue workaround** has been added to allow a workaround for a known tool issue [3]. If selected, this parameter will replace the Kraken2 rsync_from_ncbi.pl script with the modified version and use it when downloading files.\n\n### Common Issues and Important Notes\n\n* The following inputs are required:\n1. **Task Option** selects the mode of operation for the tool.\n2. **DB name** specifies the name of the Kraken2 database being generated or processed.\n\n* User should be aware that many input parameters are only relevant to specific task options and incompatible with others, for example:\n1. **Special database type** must be specified if task option `special` is used.\n2. **Download library type** must be specified if task option `download-library` is used.\n\n* **Build a protein database for translated search** input parameter (`--protein`):\n1. should be used for building a protein database and for all **Kraken2 Build** steps.\n2. should be used for downloading the `nr` database. This parameter is incompatible with downloads of `UniVec` and `UniVec_Core`.\n3. when used together with downloading library files from NCBI, it is observed that tasks may fail due to a known issue with the tool not handling 'na' given as the path on the server. A workaround involves modifying the tool rsync_from_ncbi.pl script source code and can be provided on request, but was not included in the public wrapper.\n\n* Performance inputs:\n1. **Number of threads** input parameter describes the number of threads used when building a Kraken2 database. It is recommended to allocate multiple threads to speed up execution when the tool is to build a database (either alongside the downloads or separately with `build`).\n2. The tool is set to use 30000 MB RAM by default. The amount of requested memory can be adjusted through the **Memory per job [MB]** input parameter.\n\n\n* Custom sequences should be in FASTA format (for details please see the Kraken2 manual [1]).\n* During testing, connection issues were occasionally observed when trying to access NCBI resources through rsync (which is the tool default). If you encounter rsync-related task errors and failures, please enable the **Use FTP instead of RSYNC** (`--use-ftp`) input parameter.\n\n\n### Performance Benchmarking\n\n* **Kraken2 Build** performance greatly depends on the size and complexity of the database being built, the size of requested files for downloading and the download mode. FTP, while slower, was shown to be more stable than rsync during our testing. If Kraken2 standard database is to be built at least 60 GB of RAM and 30 threads are recommended. Please note that the build step of the tool invocation is the only one actively benefiting from multiple threads.\n\n     \n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| 'standard' mode | 9 h 40 min |$14.79 + $0.39 | c5.9xlarge - 300 GB EBS | \n| 'special' mode 'silva' | 6 min | $0.04 + $0.01 | c4.2xlarge - 1000 GB EBS | \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [Kraken2 manual](https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown)\n\n[2] [Kraken2 publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0)\n\n[3] [FTP 'na' issue workaround](https://github.com/DerrickWood/kraken/issues/114#issuecomment-610912961)", "input": [{"name": "Task option"}, {"name": "Download library type"}, {"name": "Special database type"}, {"name": "File to add to library", "encodingFormat": "application/x-fasta"}, {"name": "Taxonomy files", "encodingFormat": "application/x-tar"}, {"name": "Library TAR.GZ archive", "encodingFormat": "application/x-tar"}, {"name": "Database to clean", "encodingFormat": "application/x-tar"}, {"name": "DB name"}, {"name": "Number of threads"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Kmer length"}, {"name": "Minimizer length in bp/aa"}, {"name": "Minimizer spaces"}, {"name": "Build a protein database for translated search"}, {"name": "No masking"}, {"name": "Max DB size"}, {"name": "Use FTP instead of RSYNC"}, {"name": "Avoid downloading accession number to taxid maps"}, {"name": "Proportion of the hash table to be populated"}, {"name": "Output archive name prefix"}, {"name": "Use FTP 'na' workaround"}], "output": [{"name": "TAR.GZ archive with Kraken 2 database files", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/DerrickWood/kraken2", "https://github.com/DerrickWood/kraken2/releases/tag/v2.0.9-beta", "https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown"], "applicationSubCategory": ["Metagenomics", "Indexing"], "project": "SBG Public Data", "creator": "Derrick Wood", "softwareVersion": ["v1.0"], "dateModified": 1648560771, "dateCreated": 1612269282, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/kraken2-inspect-2-0-9/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/kraken2-inspect-2-0-9/6", "applicationCategory": "CommandLineTool", "name": "Kraken2 Inspect", "description": "**Kraken2 Inspect** is a utility tool for inspecting the contents of Kraken2 databases [1]. It is part of the Kraken2 toolkit which is used in Metagenomics for assigning taxonomic labels to DNA sequences. **Kraken2 Inspect** works on the database generated by **Kraken2 Build**.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Kraken2 Inspect** can be used to obtain the number of minimizers in the provided database which match to each organism group. The outputs match the format of the Kraken2 report (`--report`) option [1].\n\n### Changes Introduced by Seven Bridges\n\n* Kraken2 database should be provided as a TAR.GZ archive containing the database top-level folder (**Kraken2 database to inspect** input).\n* The name of the Kraken2 database top-level folder should be provided with the **Kraken2 DB name** input parameter, which is required.\n* Command line options `--help` and `--version` were omitted from this wrapper.\n\n### Common Issues and Important Notes\n\n* Inputs **Kraken2 database to inspect** and **Kraken2 DB name** are required.\n\n### Performance Benchmarking\n\n* **Kraken2 Inspect** performance depends on the allocated resources and the size/complexity of the input database file. \n* To inspect the standard Kraken2 database, 60 GB of RAM is recommended.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Kraken2 standard database, 30 threads | 34 min |$0.87 + $0.03 | c5.9xlarge - 300 GB EBS | \n| Archaea database | 7 min | $0.02 + $0.01 | c4.2xlarge - 1000 GB EBS |  \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [Kraken2 manual](https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown)", "input": [{"name": "Kraken2 DB name"}, {"name": "Kraken2 database to inspect", "encodingFormat": "application/x-tar"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Only print database summary statistics"}, {"name": "Format output like Kraken 1 kraken-mpa-report"}, {"name": "Report zero counts"}, {"name": "Output file name prefix"}], "output": [{"name": "Output database contents report"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/DerrickWood/kraken2", "https://github.com/DerrickWood/kraken2/releases/tag/v2.0.9-beta", "https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown"], "applicationSubCategory": ["Metagenomics", "Indexing"], "project": "SBG Public Data", "creator": "Derrick Wood", "softwareVersion": ["v1.0"], "dateModified": 1648560771, "dateCreated": 1612269282, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/lima-2-5-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/lima-2-5-0/3", "applicationCategory": "CommandLineTool", "name": "lima", "description": "**lima** is a tool used with PacBio single-molecule sequencing data for barcode and primer sequences identification [1].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* **lima** use-cases and execution setups are described [here](https://lima.how/get-started.html#example-executions) [1].\n\n### Changes Introduced by Seven Bridges\n\n* Input parameters `--version`, `--log-level `, and `--log-file` are not part of the SevenBridges wrapper.\n\n\n### Common Issues and Important Notes\n\n* Inputs **Subread or SubreadSet** and **Barcode FASTA** are required.\n* Barcodes should be in a file format described [here](https://lima.how/get-started#barcodes) [1].\n* Input/output file types valid for CLR data are only XML and BAM. For CCS/HiFi data, [this](https://lima.how/get-started#in--and-output-compatibility-matrix) should be used. It is advised that output file type is the same as the input file type [1].\n* The tool uses 1 CPU and 1024MB of RAM by default. An instance with more resources can be requested by setting input parameters **CPUs per job** and **Memory per job [MB]**.\n* The tool was tested on the Seven Bridges platform by using [CCS data with Barcoded overhang adapter kit 8B](https://downloads.pacbcloud.com/public/dataset/RepeatExpansionDisorders_NoAmp/). The raw data was first run through [CCS tool](https://ccs.how/), and then **lima** was run. Users should be aware that other use-cases were not tested - it is possible that other use-cases might require more time/resources and might cost more.\n* The Seven Bridges wrapper uses Bioconda installation. Users should be aware of the warning from the tool helper: This program comes with ABSOLUTELY NO WARRANTY; it is intended for Research Use Only and not for use in diagnostic procedures.\n\n\n### Performance Benchmarking\n\nWhen it was run in the real use-case described in the previous section (CCS data prepared with Barcoded overhang adapter kit 8B), the tool completed in a few minutes on a c4.2xlarge AWS on-demand instance with the cost of under $1.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n###Portability\n\nThis app was tested with cwltool version 3.1.20211107152837. The **Minimum number of ZMWs observed to whitelist barcodes**, **Try to guess the used barcodes, using the provided mean score threshold**, **CCS mode**, **Only keep same barcodes**, **Barcode FASTA**, **Subread or SubreadSet** inputs were set in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Lima documentation page](https://lima.how/)", "input": [{"name": "Subread or SubreadSet", "encodingFormat": "text/fastq"}, {"name": "Barcode FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Only keep same barcodes"}, {"name": "Only keep different barcodes"}, {"name": "Only output barcode pairs that are neighbors"}, {"name": "Hifi preset"}, {"name": "Omit barcode infix in file name"}, {"name": "Do not tag per ZMW, but per read"}, {"name": "Only use subreads flanked by adapters for barcode identification"}, {"name": "Only use up to N barcode pair regions to find the barcode"}, {"name": "Analyze at maximum the provided number of barcodes per ZMW"}, {"name": "Analyze at maximum the provided number of adapters per ZMW"}, {"name": "Minimal number of full passes"}, {"name": "Minimum sequence length after clipping"}, {"name": "Maximum input sequence length"}, {"name": "Maximum ratio of bad adapter"}, {"name": "Barcodes may be substrings of others"}, {"name": "The candidate region size multiplier"}, {"name": "The candidate region size in bp"}, {"name": "Minimum reference span relative to the barcode length"}, {"name": "Minimum number of barcode regions"}, {"name": "Reads below the minimum barcode score are removed"}, {"name": "Minimum end barcode score threshold is applied"}, {"name": "The minimal score difference"}, {"name": "The minimal score lead required to call  a barcode pair significant"}, {"name": "Keep identified order of barcode pair indices in BC tag"}, {"name": "Keep identified order of barcode pair  indices in split output names"}, {"name": "CCS mode"}, {"name": "Score for a sequence match"}, {"name": "Penalty for a mismatch"}, {"name": "Deletions penalty"}, {"name": "Insertion penalty"}, {"name": "Branch penalty"}, {"name": "Split output by barcode pair"}, {"name": "Split output by resolved barcode pair name"}, {"name": "Group maximum numbers of split barcode output files per directory"}, {"name": "Output each barcode into its own sub-directory"}, {"name": "Use UUIDs from BioSamples"}, {"name": "Use UUID from input dataset XML"}, {"name": "Call barcodes, but do not clip them from read sequences"}, {"name": "Maximum number of open output files"}, {"name": "Dump clipped regions in a separate output file"}, {"name": "Store unbarcoded reads to <prefix>.removed.SUFFIX"}, {"name": "Do not generate demultiplexed output"}, {"name": "Do not generate reports"}, {"name": "Output all barcode pairs from biosamples, irrespective of presence"}, {"name": "Assign single side barcodes by score clustering"}, {"name": "Minimum ratio of scored vs sequenced adapters"}, {"name": "Ignore flanks of consensus reads labeled as missing adapter"}, {"name": "Activate specialized IsoSeq mode"}, {"name": "Demux the first N ZMWs and return the mean score"}, {"name": "Try to guess the used barcodes, using the provided mean score threshold"}, {"name": "Minimum number of ZMWs observed to whitelist barcodes"}, {"name": "Try to infer the used barcodes subset"}, {"name": "Ignore <BioSamples> from XML input"}, {"name": "CSV file, barcode pairs with associated biosample names"}, {"name": "Number of threads to use"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Demultiplexed subread(set)", "encodingFormat": "text/fastq"}, {"name": "Output clips"}, {"name": "Out SUFFIX file"}, {"name": "Output counts file"}, {"name": "Output report file"}, {"name": "Output summary file"}, {"name": "Output xml file"}, {"name": "Output guess file"}, {"name": "Output json file"}, {"name": "Output consensus xml file"}, {"name": "Output removed bam file", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/PacificBiosciences/pbbioconda", "https://github.com/pacificbiosciences/barcoding/"], "applicationSubCategory": ["Long Reads", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Pacific Biosciences", "softwareVersion": ["v1.2"], "dateModified": 1649410294, "dateCreated": 1649410294, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/lumpy-express/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/lumpy-express/4", "applicationCategory": "CommandLineTool", "name": "Lumpy-express", "description": "**LUMPY** is an SV discovery framework that naturally integrates multiple SV signals jointly across multiple samples.\n\n\n **LUMPY Express** is a simplified wrapper for standard analyses. \n\n### Common Use Cases\n\nThis tool is used for structural variation calling, either for germline calling or somatic calling. It is used for whole genome analysis only.\n\nIt supports the following analyses and their inputs:\n\n* Single Diploid Sample Analysis: normal sample\n* Joint Diploid Sample Analysis: n * normal sample\n* Tumor Normal Analysis: n * tumor sample +n * normal sample\n\nA list of all inputs and parameters with corresponding descriptions can be found at the end of the page.\n\n### Common Issues and Important Notes : \n\n* LUMPY Express expects **BWA-MEM aligned BAM files as input**. It automatically parses sample, library, and read group information using the @RG tags in the BAM header. Each BAM file is expected to contain exactly one sample. [ 1 ]\n\n* The minimum input is a **coordinate-sorted BAM file** (`-B`), from which LUMPY Express extracts splitters and discordants using SAMBLASTER before running LUMPY. Optionally, users may supply **coordinate-sorted splitter (`-S`) and discordant (`-D`) BAM files** which will bypass SAMBLASTER extraction for faster analysis.\n\nFor known limitations and more about this tool, please see the [LUMPY](https://github.com/arq5x/lumpy-sv).\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference index** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*         \n\n\n| Total sample size (GB) | Duration | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 61                     | 4h 48 min           | 16    | 3.8     | c4.4xlarge |\n| 134                    | 45h 36 min            | 8     | 18    | c4.2xlarge |\n\n\n### References\n\n[1] [LUMPY- github](https://github.com/arq5x/lumpy-sv)", "input": [{"name": "coordinate-sorted BAM file(s)", "encodingFormat": "application/x-bam"}, {"name": "Split reads BAM file(s)", "encodingFormat": "application/x-bam"}, {"name": "Discordant reads BAM files", "encodingFormat": "application/x-bam"}, {"name": "BED file to exclude", "encodingFormat": "text/x-bed"}, {"name": "Output probability curves for each variant"}, {"name": "Minimum sample weight for a call"}, {"name": "Trim threshold"}, {"name": "Temp directory"}, {"name": "Keep temporary files"}, {"name": "Path to lumpyexpress.config file"}, {"name": "verbose"}, {"name": "Output file name"}, {"name": "Reference genome fasta file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Output file in VCF format", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/arq5x/lumpy-sv"], "applicationSubCategory": ["Variant Calling", "Structural Variant Calling"], "project": "SBG Public Data", "creator": "Ryan M Layer, Colby Chiang, Aaron R Quinlan and Ira M Hall", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151253, "dateCreated": 1539005554, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/macs2/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/macs2/8", "applicationCategory": "CommandLineTool", "name": "MACS2 - callpeak", "description": "Model-based Analysis of ChIP-Seq (**MACS**) is an algorithm for identifying peaks from ChIP-seq data.\n\n**MACS2** performs several steps, ranging from duplicate filtering and peak model building to the actual peak detection and multiple testing corrections. It also has an option to link nearby peaks together to call broad peaks [1].\n\nMACS2 captures the influence of genome complexity to evaluate the significance of enriched ChIP regions and improves the spatial resolution of binding sites through combining the information of both sequencing tag position and orientation [2]. \n    \nMACS2 peak caller is widely used for analysing:\n* transcription factor binding patterns\n* narrow histone marks (to be run with MACS2 narrow option): H3K4me3, H3K4me1, H3K27ac.\n* broad histone marks (to be run MACS2 broad option): H3K9me3, H3K27me3, H3K79me2, H3K36me3, H4k20me1.\n* to call enriched regions from DNAase-seq data (to be run with MACS2 broad peak option). \n\nInput files:\n* treatment BAM file - Alignment file for treatment sample. If you have more than one alignment file, MACS2 will pool up all files together (Required input). \n* control BAM file - Alignment file for the control sample. If you have more than one alignment file, MACS2 will pool up all files together (Optional input)\n\n###**Common Use Cases**\nMACS2 could be used with one of the following options:\n* narrow peak - identification of protein-DNA binding sites or histone modifications at regulatory elements\n* broad peak - identification of regions associated with histone modifications spanning large genomic domains\n\nInclude **BDG** `-b` parameter to enable visual inspection of discovered binding sites. This way extended fragment pileup and local lambda tracks will be saved in bedGraph files. Also, include **spmr** `--SPMR` parameter to normalize extended read coverage to a library size of one million mapped reads.\n\n###**Common Issues and Important Notes**\n\n* selecting narrow peak as the **analysis_type** parameter, **BROAD** `--broad` parameter will be set to  FALSE and **QVALUE** `--qvalue` parameter will be set to 0.05\n* selecting broad peak as the **analysis_type** parameter, **BROAD** `--broad` parameter will be set to  TRUE and **QVALUE** `--qvalue` parameter will be set to 0.01\n\nReferences \n1. Zhang Y, Liu T, Meyer CA, Eeckhoute J, Johnson DS, Bernstein BE, Nusbaum C, Myers RM, Brown M, Li W, Liu XS. (2008) Model-based Analysis of ChIP-Seq (MACS), Genome Biology, 2008;9(9):R137.\n2. https://github.com/taoliu/MACS", "input": [{"name": "ChIP-seq treatment file.", "encodingFormat": "text/x-bed"}, {"name": "Format of Input files"}, {"name": "Name of organism to calculate effective genome size. 'hs' for human (2.7e9), 'mm' for mouse   (1.87e9), 'ce' for C. elegans (9e7) and 'dm' for fruitfly (1.2e8)"}, {"name": "Buffer size"}, {"name": "Keep duplicates"}, {"name": "Experiment name which will be used to name the output files"}, {"name": "Save extended fragment pileup, and local lambda tracks into a bedgraph file."}, {"name": "Set verbose level of runtime message"}, {"name": "If True, MACS will save signal per million  reads for fragment pileup profiles"}, {"name": "Band width for picking regions to compute fragment size"}, {"name": "Select the regions within MFOLD range of high-confidence enrichment ratio against background to build model"}, {"name": "Whether turn on the auto pair model process"}, {"name": "Whether or not to build the shifting model"}, {"name": "(NOT the legacy --shiftsize option!) The arbitrary shift in bp"}, {"name": "The arbitrary extension size in bp"}, {"name": "Minimum FDR (q-value) cutoff for peak detection"}, {"name": "Pvalue cutoff for peak detection"}, {"name": "When set, scale the small sample up to the bigger sample"}, {"name": "When set, use a custom scaling ratio of ChIP/control (e.g. calculated using NCIS) for linear scaling"}, {"name": "When set, random sampling method will scale down the bigger sample"}, {"name": "Set the random seed while down sampling data"}, {"name": "Optional directory to store temp files"}, {"name": "If True, MACS will use fixed background lambda as local lambda for every peak region"}, {"name": "The small nearby region in basepairs to calculate dynamic lambda"}, {"name": "The large nearby region in basepairs to calculate dynamic lambda"}, {"name": "If set, MACS will try to call broad peaks by linking nearby highly enriched regions"}, {"name": "Cutoff for broad region"}, {"name": "While set, MACS2 will analyze number or total length of peaks that can be called by different p-value cutoff then output a summary table to help user decide a better cutoff"}, {"name": "If set, MACS will use a more sophisticated signal processing approach to find subpeak summits in each enriched peak region"}, {"name": "When set, the value will be used to filter out peaks with low fold-enrichment"}, {"name": "Tag Size"}, {"name": "Analysis type"}, {"name": "Control file", "encodingFormat": "text/x-bed"}], "output": [{"name": "Excel file"}, {"name": "Summit bed file"}, {"name": "Narrow Peak file"}, {"name": "Rmodel"}, {"name": "Broad Peak file"}, {"name": "Sample pileup"}, {"name": "Control lambda values"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/taoliu/MACS", "https://github.com/taoliu/MACS/", "https://github.com/taoliu/MACS/wiki/Advanced:-Call-peaks-using-MACS2-subcommands"], "applicationSubCategory": ["ChIP-Seq"], "project": "SBG Public Data", "creator": "Yong Zhang, Tao Liu, Xiaole Shirley Liu", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155967, "dateCreated": 1500041063, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/manta-1-4-0/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/manta-1-4-0/17", "applicationCategory": "CommandLineTool", "name": "Manta 1.4.0", "description": "Manta is a structural variants (SVs) and indels caller which performs calling from mapped paired-end sequencing reads.\n\nIt is optimized for analysis of germline variation in small sets of individuals and somatic variation in tumor/normal sample pairs. Manta discovers, assembles and scores large-scale SVs, medium-sized indels and large insertions within a single efficient workflow. Manta combines paired and split-read evidence during SV discovery and scoring to improve accuracy, but does not require split-reads or successful breakpoint assemblies to report a variant in cases where there is strong evidence otherwise. It provides scoring models for germline variants in small sets of diploid samples and somatic variants in matched tumor/normal sample pairs. There is experimental support for analysis of unmatched tumor samples as well.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThis tool is used for structural variation calling, either for germline calling, somatic calling or tumor-only calling. It is used both for whole exome and for whole genome variant calling. It supports analysis of RNA-Seq data.\n\nIt supports the following analyses and their inputs:\n\n* Single Diploid Sample Analysis: reference + normal sample\n* Joint Diploid Sample Analysis: reference + n * normal sample,  n<=10 \n* Tumor Normal Analysis: reference + tumor sample + normal sample \n* Tumor-Only Analysis: reference + tumor sample\n* Single RNA-Seq Sample Analysis: reference + normal sample\n\nFiles used as inputs, **Tumor sample** (`--tumorBam`) and/or **Normal sample** (`--bam`), should be sorted and indexed. This can be done by using **Sambamba Sort**. The **Reference file** (`--referenceFasta`) should be indexed too, this can be done by using **SBG FASTA Indices**. \n\n###Changes Introduced by Seven Bridges\n\n* Sorting and indexing of a BED file is added as a preprocessing step as an indexed BED file is required by Manta.\n\n* Selecting RNA-Seq analysis mode increases detection sensitivity by lowering \"minEdgeObservations\" and \"minCandidateSpanningCount\" to 2 (default is 3).\n\n### Common Issues and Important Notes : \n\n* At least one BAM or CRAM file must be provided for the **Normal sample** (`--bam`) or **Tumor sample** (`--tumorBam`). All input alignment (BAM or CRAM) files and the reference sequence must contain the same chromosome names in the same order and must be sorted and indexed. This can be done by using **Sambamba Sort**.\n\n* For larger files, additional storage might me required. This issue is manifested by \"no disk storage available\" in error log. If so, attach additional EBS storage in the tool's hints.\n\n* For known limitations and more information about this tool, please see the [Manta User Guide](https://github.com/Illumina/manta/blob/master/docs/userGuide/README.md).\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n        \"reference_fasta\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"Manta 1.4.0 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running time and cost. All samples are aligned against **GRCh37 human reference index** with default options. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n| Total sample size (GB) | Duration (min) | Cores | Cost ($) | Instance (AWS)  |\n|--------------------------|----------------|-------|----------|------------|\n| 26                     | 11             | 16    | 0.15     | c4.4xlarge |\n| 61                     | 27             | 16    | 0.34     | c4.4xlarge |\n| 297                    | 58            | 16     | 0.77     | c4.4xlarge |\n| 657                    | 118            | 16    | 1.57    | c4.4xlarge + 1024GB EBS |\n\n###References\n\n[1][Manta GitHub page](https://github.com/Illumina/manta)", "input": [{"name": "Normal sample", "encodingFormat": "application/x-bam"}, {"name": "Tumor sample", "encodingFormat": "application/x-bam"}, {"name": "Reference fasta", "encodingFormat": "application/x-fasta"}, {"name": "Analysis type"}, {"name": "Regions"}, {"name": "Call Regions BED", "encodingFormat": "text/x-bed"}, {"name": "RNA"}, {"name": "Unstranded RNA"}], "output": [{"name": "Diploid SV file", "encodingFormat": "application/x-vcf"}, {"name": "Somatic SV file", "encodingFormat": "application/x-vcf"}, {"name": "Candidate SV file", "encodingFormat": "application/x-vcf"}, {"name": "Candidate Small Indels file", "encodingFormat": "application/x-vcf"}, {"name": "Tumor SV file (tumor-only analysis)", "encodingFormat": "application/x-vcf"}, {"name": "RNA SV file (RNA-Seq analysis)", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/Illumina/manta", "https://github.com/Illumina/manta/blob/master/LICENSE.txt"], "applicationSubCategory": ["Structural Variant Calling", "Variant Calling"], "project": "SBG Public Data", "creator": "Illumina", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649159218, "dateCreated": 1533907621, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/maxquant-1-6-5-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/maxquant-1-6-5-0/8", "applicationCategory": "CommandLineTool", "name": "MaxQuant", "description": "**MaxQuant** is a quantitative proteomics tool designed for analysing large mass-spectrometric data [1].\n\n**MaxQuant** integrates algorithms specifically developed for high-resolution, quantitative MS data, and it is applicable only for shotgun proteomics. It has its own peptide database search engine, called **Andromeda**. **MaxQuant** can provide identification of peptides, proteins and PTM sites. It uses target-decoy search strategy in order to estimate and control extent of false-positives. Within the target-decoy strategy, **MaxQuant** applies the concept of posterior error probability (PEP) to integrate multiple peptide properties (e.g. length, charge, number of modifications) together with Andromeda score into a single quantity, reflecting the quality of a peptide spectrum match (PSM) [2].\n\n\nThe original version of **MaxQuant** could be run only on Microsoft Windows, and thus its use was restricted in high-performance computing environments. Recently, **MaxQuant** has gone Linux, after getting the code base to work with the Mono/C# runtime. The code base is identical for Windows and for Linux [3,4]. However, if using on Linux, one python script, *gen_mqpar.py* is required. It takes an *mqpar.xml* template, folder(s) of raw files, and some other parameters as inputs and generates both a filled-out *mqpar.xml* file and a bash script that can be run by [slurm](https://slurm.schedmd.com/documentation.html) [3].\n\nThe tool has one optional and three required inputs:\n\n* **Input xml** - an XML format file containing parameters for the analysis.\n\n* **Raw files** - an array of RAW format files or a compressed folder with RAW files; It can be in RAW or TAR.GZ format.\n\n* **Input fasta** - a reference proteome FASTA file.\n\n* **Experimental design** (`--exp_design`) - an optional input file in TSV format with the information on *ParameterGroup*, *Experiment*, *Fractions* and *PTM* values.\n\nThe tool produces the following output:\n\n* **Output tables** - a directory named using the **Prefix** parameter or by **Sample ID** of RAW inputs; It contains a list of files with the information about quantification of proteins and PTMs; Output tables are described in detail in the *tables.pdf* file, located within the folder.    \n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**MaxQuant** is a tool designed for shotgun proteomics analyses. It takes files with high-resolution, quantitative MS data and produces output tables which contain information about quantification of proteins and PTMs. \n\n**MaxQuant** can be used for analysing data derived from any major relative quantification techniques (Label-free quantification (LFQ), MS1-level labelling  and isobaric MS2-level labelling). Furthermore, it provides quantification algorithms for all common forms of tandem mass (TMT) and isobaric tags for relative and absolute quantitation (iTRAQ) labelling (including higher-plex TMT and multinotch MS3 quantification) [2].\n\n**Seven Bridges** provides a template **Input xml** file with standard parameters, available on the platform. Some of the parameters can be modified in the App Settings field during task execution and, if needed, all parameter values can be set by loading a previously created XML-format file. \n\nAll parameters for the **MaxQuant** analysis and their descriptions are listed in the [MaxQuant Publication](https://evvail.com/wp-content/uploads/2020/09/tyanova2016.pdf). For the purpose of **MaxQuant** wrapping, **Seven Bridges** included the most important parameters and adapted them to the platform environment. \n\nIt is sufficient to perform only the following settings [2]:\n\n* Setting *Type* using the **Type** (`--type`) parameter:\n\n    * If **Type** (`--type`) is set to **Standard** (default) or **BoxCar**, the number of MS1 labels that are quantified against each other can be defined by the **Multiplicity** (`--multiplicity`) parameter:\n\n        * **Multiplicity** (`--multiplicity`) should be set to **1** if no isotopic labeling was used;  \n\n        * If **Multiplicity** (`--multiplicity`) is set to **2**, **Heavy label** (`--labelheavy`) should be specified; \n\n        * If **Multiplicity** (`--multiplicity`) is set to **3**, **Medium label** (`--labelmedium`) for the *Medium labels* and **Heavy label** (`--labelheavy`) for the *Heavy labels* should be specified; \n\n        * For **Multiplicity** (`--multiplicity`) values **2** and **3**, parameter **Max. labeled amino acids** (`--maxlabeled`) can be changed.\n\n    * If **Type** (`--type`) is set to **Reporter ion MS2** or **Reporter ion MS3**, *Isobaric labels* should be defined, using the **Isobaric label** (`--isobariclabel`) parameter; It is possible to add *Correction factors* for each label using:\n\n        * **Correction factor -2 [%]** (`--corrM2`), \n\n        * **Correction factor -1 [%]** (`--corrM1`), \n\n        * **Correction factor +1 [%]** (`--corrP1`) and \n\n        * **Correction factor +2 [%]** (`--corrP2`) parameters. \n\n        * Additional parameters are available for  **Reporter ion MS2** type: \n\n            * **Mass tolerance** (`--masstolerance`), \n\n            * **Filter Pif** (`--filterpif`) (if **True**, **Min. reporter Pif** (`--reporterpif`) can be set), \n\n            * **Min. base peak ratio** (`--reporterbasepeakratio`), \n\n            * **Min. reporter fraction** (`--reporter fraction`), \n\n            * **Reporter mode** (`--reportermode`) and \n\n            * **Normalization** (`--normalization`).\n\n        * For  **Reporter ion MS3** type:\n\n            * only **Mass tolerance** (`--masstolerance`) and \n\n            * **Normalization** (`--normalization`) parameters should be set. \n\n    * If **Type** (`--type`) is set to **NeuCode**, **NeuCode labels** (`--neucodelabels`) should be defined:\n\n        * If *2Plex* label is wanted, choose **K2020** and **K0400** values, \n\n        * for *2Plex**, choose **K6020** and **K0800**, and \n\n        * for *3Plex* choose **K6020**, **K3410** and **K0800**, or set the other values which correspond to the experiment. \n\n       * Additionally, for **NeuCode** type, the following parameters can be set: \n\n            * **Max. labeled amino acids** (`--maxlabeled`), \n\n            * **Max. mass deviation [ppm]** (`--neucodemaxppm`), \n\n            * **Smoothing** (`--neucoderesolution`), \n\n            * **Smoothing in mDa** (`--neucoderesolutioninmda`), \n\n            * **Calculate low res data** (`--neucodeinsilicosowres`) and \n\n            * **Intensity mode** (`--neucodeintensitymode`).\n\n   * If **Type** (`--type`) is set to **TIMS-DDA**:\n\n        * **TIMS-DDA half width** (`--timshalfwidth`), \n\n        * **TIMS-DDA step** (`--timsstep`), \n\n        * **TIMS-DDA resolution** (`--timsresolution`), \n\n        * **TIMS-DDA min. ms/ms intensity** (`--timsminmsmsintensity`), \n\n        * **TIMS-DDA remove precursor** (`--timsremoveprecursor`), \n\n        * **TIMS-DDA collapse ms/ms** (`--timscollapsemsms`) and \n\n        * **TIMS-DDA reporter ions** (`--timsisobariclabels`) can be set; \n\n        * If the parameter **TIMS-DDA reporter ions** (`--timsisobariclabels`) is set to **True**:\n\n            * *isobaric labels* should be defined using the **Isobaric label** (`--isobariclabel`) parameter, as well as\n\n            * **Mass tolerance** (`--masstolerance`) and \n\n            * **Normalization** (`--normalization`) parameters.\n\n* Setting *Digestion mode* and *Digestion enzyme* using the **Digestion mode** (`--mode`) and **Digestion enzymes** (`--enzyme`) parameters:\n\n    * If **Digestion mode** (`--mode`) is set to **Specific** (default), the number of missed cleavages of the protein sequences that are maximally tolerated in the in-silico digestion can be changed using the **Max. missed cleavages** (`--cleavages`) parameter; \n\n    * If using **Unspecific** or **No digestion** value for **Digestion mode** (`--mode`), the **Digestion enzymes** (`--enzyme`) parameter should not be changed. \n\n\n\n* Setting *Variable modifications* and *Fixed modifications* by changing the **Variable modifications** (`--variable`) and **Fixed modifications** (`--fixed`) parameters; \n\n    * Maximum number of modifications per peptide can be set as well, using the **Max. number of modifications** (`--maxnmods`) parameter.\n\n\n* *Enabling LFQ (Label free quantification)* can be done using the **Label-free quantification** (`--lfq`) parameter; It applies the algorithm for label-free protein quantification and adds an additional Parameter Group; \n\n    * If **Label-free quantification** (`--lfq`) is enabled, additional parameters can be set: \n\n        * **LFQ min. ratio count** (`--lfqminratio`), \n\n        * **LFQ normalisation type** (`--lfqnormtype`) and \n\n        * **Fast LFQ** (`--fastlfq`); \n\n        * If **Fast LFQ** (`--fastlfq`) is set to **True**:\n\n            * **LFQ min. number of neighbors** (`--lfqminedgespernode`) and \n\n            * **LFQ average number of neighbors** (`--lfqavedgespernode`) values can be changed; These parameters refer to both Parameter Groups.\n\n        * The output of the label-free algorithm can be found in the *proteinGroups* table in the columns starting with *LFQ intensity*.\n\n\n* The use of an **Experimental design** (`--exp_design`) to specify which LC-MS runs or groups of LC-MS runs correspond to the different samples is obligatory here; **MaxQuant** supports analysis with two Parameter Groups (**0** and **1**) at most;\n\n    * Several parameters can be set for the second Parameter Group specifically: \n\n        * **Digestion enzymes for the second Protein Group** (`--enzyme1`), \n\n        * **Digestion mode for the second Protein Group** (`--mode1`), \n\n        * **Max. missed cleavages for the second Protein Group** (`--cleavages1`), \n\n        * **Fixed modifications for the second Protein Group** (`--fixed1`), \n\n        * **Variable modifications for the second Protein Group** (`--variable1`), \n\n        * **Max. number of modifications for the second Protein Group** (`--maxnmods1`) and \n\n        * **LFQ normalisation type for the second Protein Group** (`--lfqnormtype1`).\n\n    \n* Setting *Number of threads* using the **Number of threads** (`-t`) parameter; This number should not exceed the available number of CPUs for the chosen instance type. \n\n* Setting *Experimental design* by adding an optional input, **Experimental design** (`--exp_design`); It should contain four columns named as: **ParameterGroup**, **Experiment**, **Fractions** and **PTM**, and the information about their values for each RAW input file in a single row; Order of the RAW files and parameter values should match.\n\n* Type of instrument the data was generated on can be set using the **Instrument type**\u00a0(`\u2013instrument`) parameter. Some internal parameters, e.g. in peak detection, are set to optimal values based on the machine type. Currently Thermo Fisher Orbitrap and FT like instruments are supported, as well as ToF instruments like Bruker Impact HD and AB Sciex Triple TOF 5600.\n\nOutput tables produced by **MaxQuant** are described in detail in the *tables.pdf* file, which is located within the output folder. \n\nThe results can be further analysed or visualised for the more meaningful insights.\n\n\n### Changes Introduced by Seven Bridges\n\n* All the **Raw files** are moved to a newly created folder, as the tool produces some temporary files and requires that all the processes occur in the same directory.\n\n* **MaxQuant** allows a TAR.GZ compressed file as the input; TAR.GZ file should contain a directory with the same nameroot as the TAR.GZ file, e.g. input *Test_dir.tar.gz* should contain a directory named *Test_dir*, and RAW files should be in this directory.\n\n* The *gen_mqpar.py* python script has been significantly changed, so it could prepare the **Input xml** file and apply different parameter values for the analysis; Paths of the inputs have been slightly changed as well.\n\n* As a solution for a bug with long names of **Raw files** causing errors and task failing, the files are renamed during *gen_mqpar.py* python script execution.\n\n* Before renaming, RAW inputs are sorted by name, so they could match by position with **Experimental design** (`--exp_design`) values, if this optional input is added.\n\n* **MaxQuant version** (`-mq/--mq-version`) parameter (available for *gen_mqpar.py* running) is excluded from the tool, as **MaxQuant** uses the 1.6.5.0 version adapted for the Linux system on all occasions. \n\n* Only files within the *txt* output folder are available at the output port, and they are located in a folder named by **Prefix** value or **Sample ID** of the **Raw files** input.\n\n\n\n### Common Issues and Important Notes\n\n* If using a TAR.GZ file for the **Raw files** input, it should contain a directory with the same nameroot as the TAR.GZ file (e.g. input *Test_dir.tar.gz* should contain a directory named *Test_dir*), and RAW files should be in it.\n\n* The input files (decompressed files) have to be in RAW format; In case of having files in another format, please convert them to RAW format.\n\n* If adding an optional **Experimental design** (`--exp_design`) input file, note that columns have to be named as: **ParameterGroup**, **Experiment**, **Fractions** and **PTM**. \n\n* *ParameterGroup* should be set to **0** for all RAW files if **Label-free quantification** (`--lfq`) is not used. \n\n* If  **Label-free quantification** (`--lfq`) is enabled, *ParameterGroup* should be set to **0** for the first group, and to **1** for the second. \n\n* *Experiment* should contain string values, *Fractions* should contain integers and *PTM* should be valued as *True* or *False*.\n\n* **Experimental design** input file should have the information for each RAW input, listed in the same order as the sorted RAW inputs.\n\n* **Prefix** value or **Sample ID** for **Raw files** should be specified for successful **MaxQuant** execution.\n\n* If providing **Input xml**, be aware that it should be adjusted for the Linux system.\n\n* **Number of threads** should not exceed the available number of CPUs for the chosen instance type.\n\n* Available parameters and their usage are described in the **Common Use Cases** field; Please use them as written above.\n\n* When setting parameters, please be aware that setting excluded parameters will produce an error; Parameters should be changed only if the changed value makes sense and if it is applicable together with the other defined parameters.\n\n* Although *gen_mqpar.py* python script modifies parameter values for **TIMS-DDA** **Type** (`--type`) correctly, testing this parameter didn't work properly with Seven Bridges test files.\n\n* When setting **Iodo6plex TMT** **Isobaric label** (`--isobariclabel`) parameter, an error occurred, even though *gen_mqpar.py* python script modifies parameter values correctly.\n\n\n### Performance Benchmarking\n\n**MaxQuant** uses 15GB of RAM Memory and 8 CPUs by default. For larger RAW inputs, it is suggested to use an instance type with more RAM Memory and CPUs. Besides the inputs size, Running time and Cost depend on the parameters set for task execution. Be aware that different App Setting can change Cost and Running time significantly. Additionally, note that the **Number of threads** should not exceed the available number of CPUs for the chosen instance type, and if it does, the task can fail.  \n \nIn the following table you can find estimates of **MaxQuant** running time and cost. All tasks were run with default parameters, using human reference FASTA (13.2 MB) and **Seven Bridges** suggested XML template.\n\n                   \n| Raw files | Running time| AWS instance | Number of threads | Cost |\n|--------------------------------|--------------------------|---------------------|--------------------------|----------------------|-----------------------|---------------------------|\n| 6x ~215 MB|   49m | c4.2xlarge (on-demand) | 4 | $0.44|\n| 6x ~215 MB|  31min | c5.9xlarge (on-demand) |18 | $0.86 |\n| 25x ~1.2 GB|  9h 18min| c4.2xlarge (on-demand) | 4 | $4.98 |\n| 25x ~1.2 GB|   2h 40min | c5.9xlarge (on-demand) | 18 | $4.45 |\n| 25x ~1.2 GB|   2h 13min | c5.9xlarge (on-demand) | 36 | $3.69 |\n| ~25 GB(TAR.GZ)|  9h 38min| c4.2xlarge (on-demand) | 4 | $5.16 |\n| ~25 GB(TAR.GZ)|   2h 51min | c5.9xlarge (on-demand) | 18 | $4.75 |\n| ~25 GB(TAR.GZ)|   2h 16min | c5.9xlarge (on-demand) | 36 | $3.78 |\n\n\n\nAs an example of load testing, the 10plex TMT data set (identifier: [MSV000079033](https://massive.ucsd.edu/ProteoSAFe/dataset.jsp?task=9c3f5d8472c1486a8fceda556598ac94)), given in the [MaxQuant Publication](https://evvail.com/wp-content/uploads/2020/09/tyanova2016.pdf) was tested. All the parameters were set as in the XML template for this data set, available on the [page](https://datashare.biochem.mpg.de/s/eb8kCWOrk6yTAQf?path=%2FTMT%20dataset) with MaxQuant testing results. The task with all the RAW input files (150x 1.5-2.5GB) had been running for 2 days on the c5.9xlarge instance before it was aborted, while using only 2 RAW input files from the same data set, the task ran for 4h 19min and completed successfully. Please be cautious when running a large number of files and, if possible, test scaling with closely monitoring resources usage before running the analysis as a whole. \n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \u00a0 \n\n\n### References\n\n[1] [MaxQuant documentation](http://coxdocs.org/doku.php?id=maxquant:start)\n\n[2] [MaxQuant Publication](https://evvail.com/wp-content/uploads/2020/09/tyanova2016.pdf)\n\n[3] [MaxQuant Linux adaptation](https://atchen.me/research/2019/03/21/mq-linux.html)\n\n[4] [MaxQuant Linux Publication](https://www.nature.com/articles/s41592-018-0018-y)", "input": [{"name": "Input xlm"}, {"name": "Raw files", "encodingFormat": "application/x-tar"}, {"name": "Number of threads"}, {"name": "Input fasta", "encodingFormat": "application/x-fasta"}, {"name": "Prefix"}, {"name": "Digestion enzymes"}, {"name": "Digestion mode"}, {"name": "Max. missed cleavages"}, {"name": "Fixed modifications"}, {"name": "Variable modifications"}, {"name": "Max. number of modifications"}, {"name": "Label-free quantification"}, {"name": "LFQ min. ratio count"}, {"name": "LFQ normalisation type"}, {"name": "Fast LFQ"}, {"name": "LFQ min. number of neighbors"}, {"name": "LFQ average number of neighbors"}, {"name": "Experimental design"}, {"name": "Type"}, {"name": "Multiplicity"}, {"name": "Max. labeled amino acids"}, {"name": "Light label"}, {"name": "Medium label"}, {"name": "Heavy label"}, {"name": "Isobaric label"}, {"name": "Mass tolerance"}, {"name": "Normalization"}, {"name": "Filter Pif"}, {"name": "Min. reporter Pif"}, {"name": "Min. base peak ratio"}, {"name": "Min. reporter fraction"}, {"name": "Reporter mode"}, {"name": "NeuCode labels"}, {"name": "Max. mass deviation [ppm]"}, {"name": "Smoothing"}, {"name": "Smoothing in mDa"}, {"name": "Calculate low res data"}, {"name": "Intensity mode"}, {"name": "TIMS-DDA half width"}, {"name": "TIMS-DDA step"}, {"name": "TIMS-DDA resolution"}, {"name": "TIMS-DDA min. ms/ms intensity"}, {"name": "TIMS-DDA remove precusrsor"}, {"name": "TIMS-DDA collapse ms/ms"}, {"name": "TIMS-DDA reporter ions"}, {"name": "CPU per job"}, {"name": "Memory per job"}, {"name": "Correction factor -2 [%]"}, {"name": "Correction factor -1 [%]"}, {"name": "Correction factor +1 [%]"}, {"name": "Correction factor +2 [%]"}, {"name": "Instrument type"}, {"name": "Separate LFQ"}, {"name": "Match between runs"}, {"name": "Matching time window [min]"}, {"name": "Alignment time window [min]"}, {"name": "Digestion enzymes for the second Protein Group"}, {"name": "Digestion mode for the second Protein Group"}, {"name": "Max. missed cleavages for the second Protein Group"}, {"name": "Fixed modifications for the second Protein Group"}, {"name": "Variable modifications for the second Protein Group"}, {"name": "Max. number of modifications for the second Protein Group"}, {"name": "LFQ normalisation type for the second Protein Group"}], "output": [{"name": "Output tables"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/JurgenCox/compbio-base"], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "Stefka Tyanova, Tikira Temu, and Juergen Cox", "softwareVersion": ["v1.1"], "dateModified": 1648048380, "dateCreated": 1617978523, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/mbased/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/mbased/4", "applicationCategory": "CommandLineTool", "name": "MBASED", "description": "The app implements the **MBASED** algorithm for detecting allele-specific gene expression from RNA count data, where allele counts at individual loci (SNVs) are integrated into a gene-specific measure of ASE, and utilizes simulations to appropriately assess the statistical significance of observed ASE.\n\nThe app outputs ASE candidates as a list of genes with their corresponding p-values.\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* The app takes a normal or VEP annotated VCF file. In case of the normal VCF, only SNPs that belong to the genes from a GTF file are taken into account for the analysis. Using VEP annotated VCF files, the user can more specifically define which genes belong to which SNPs.  \n\n* VCF files have to contain the sample-specific information filed with integer allelic depths for the ref and alt alleles.    \n\n* In case of phased VCF files, the **genotype (GT)** is in the form 0|1, where 0 indicates the reference allele and 1 indicates the alternative allele, i.e it is heterozygous. The vertical pipe | indicates that the genotype is phased, and is used to indicate which chromosome the alleles are on. If this is a slash / rather than a vertical pipe, it means we don\u2019t know which chromosome they are on ([EMBL-EBI standard](https://www.ebi.ac.uk/training/online/course/human-genetic-variation-introduction/exercise-title/want-know-how-we-did-it)). \n\n### Changes Introduced by Seven Bridges\n\n* SNPs that belong to various genes (overlapping or different strand) are accounted in all of them with the same REF/ALT ratio as written in the VCF.   \n\n* If the output prefix is not provided, the output filenames prefix is the input VCF filename without extension. \n\n\n### Common Issues and Important Notes\n\n* By default, **gene names** are used in the analysis. If **gene names** are missing, corresponding **gene ids** will be used. \n\n### Performance Benchmarking \n\nMBASED is not a computationally heavy algorithm (as alignment is). Therefore, all benchmarking was performed on the default c4.2xlarge (8 cpus, 15Gb RAM) instance (AWS), while using the maximum number of threads the instance allows, showing that the tool benefits from multithreading.  \n\n| Input size [Gb] | Paired-end | Duration [min] |  Cost [$] |   Instance  |\n|:---------------:|:----------:|:--------------:|:------------:|:-----------:|\n|       37      |     Yes    |      7       |     6     | c4.2xlarge  |\n|      33     |     Yes    |       8       |     0.5     | c4.2xlarge  |\n|       44       |     Yes    |       10       |     0.15     |  c4.2xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [MBASED paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0405-3)", "input": [{"name": "Annotated VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Number of Simulations"}, {"name": "Output name (without extension)"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Output html report"}, {"name": "VCF VEP annotated"}, {"name": "Number of Threads"}, {"name": "Gene Name or Gene ID"}, {"name": "Number of cpus per job"}], "output": [{"name": "ASE per gene"}, {"name": "html report", "encodingFormat": "text/html"}, {"name": "SNPs"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["Transcriptomics", "Allele Specific Expression", "CWL1.0"], "project": "SBG Public Data", "creator": "Oleg Mayba, Houston Gilbert", "softwareVersion": ["v1.1"], "dateModified": 1612262429, "dateCreated": 1612262363, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/mendelian-violation-detector-v1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/mendelian-violation-detector-v1-0/7", "applicationCategory": "CommandLineTool", "name": "Mendelian Violation Detector", "description": "**Mendelian Violation Detector** tool is a fast and accurate trio concordance analysis tool, that identifies Mendelian violations using advanced variant comparison to resolve variant representation differences in family trios.\n\nVariants can be represented with different ways due to the current VCF standard and repetitive structure of human genome. This representation issues often cause wrong Mendelian decision assessment in position by position variant comparison tools.  If different variant representations are produced for different members of the pedigree, comparing the three sets of calls position by position will result in detection of Mendelian violations, even though the underlying haplotypes are Mendelian compliant. Mendelian Violation Detector tool uses the 3-way variant matching problem to deal with ambiguities arising from different variant representations. \n\nMendelian Violation Detector extends the variant comparison algorithm of vcfeval [1] for trio concordance analysis to detect Mendelian violations. The trio matching problem is defined as finding the optimal sets of variants of mother, father and child i.e. the variant sets that maximize the number of child variants that follow Mendelian Inheritance rules.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* Mendelian Violation Detector requires three VCF files as inputs, the mother VCF file on its **Mother VCF** input (`-mother`) , the father VCF file on its  **Father VCF** input (`-father`) and the child VCF file on its **Child VCF** input (`-child`). It also requires reference sequence on its **Reference** input (`-reference`). In addition to the VCF files and reference sequence, BED file can also be provided on **BED file** input (`-bed`). BED file dictates the genomic regions inside which the comparison will be performed. The tool can also process multi-sample VCF files. In the case of multi-sample VCF files, inputs **Sample Father Name** (`--sample-father`), **Sample Mother Name** (`--sample-mother`) and **Sample Child Name** (`--sample-child`) should be set or pedigree (PED) file should be provided on **Pedigree** input (`-pedigree`). PED file contains a structured description of the familial relationships between samples.\n\n* On its output, the tool generates statistics for variant sub categories and a VCF file **Merged Annotated VCF** as a trio by merging mother, father and child variants at the same position. This merged VCF file contains Mendelian decision annotations for each child variant.\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output Prefix** parameter. In case **Output Prefix** is not provided, output prefix will be generated by the concatenation of the **Sample Father Name**, **Sample Mother Name** and **Sample Child Name** parameters, if they exist. If these parameters are not set, output prefix will be inferred from the **Pedigree** file, if this file is added. Otherwise, output prefix will be inferred from the **Father VCF**, **Mother VCF** and **Child VCF** filenames. This way, having identical names of the output files between runs is avoided.\n\n###Common Issues and Important Notes\n\n* When processing multi-sample VCF files, it is very important to specify different sample names for mother, father and child using the following input parameters, respectively: **Sample Mother Name** (`-sample-mother`), **Sample Father Name** (`-sample-father`) and **Sample Child Name** (`-sample-child`) or to add PED file on **Pedigree** input (`-pedigree').\n\n###Performance Benchmarking\n\n* The tool relies on the algorithm that uses dynamic programming techique to solve the maximization problem. Thus, it is not computationally challenging with reasonable running time, which is about 10 minutes on c4.2xlarge AWS instance for the whole human genome. The task costs around 0.07 $.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [vcfeval paper](https://www.biorxiv.org/content/biorxiv/early/2015/08/02/023754.full.pdf)", "input": [{"name": "Father VCF", "encodingFormat": "application/x-vcf"}, {"name": "Mother VCF", "encodingFormat": "application/x-vcf"}, {"name": "Child VCF", "encodingFormat": "application/x-vcf"}, {"name": "Sample Father Name"}, {"name": "Sample Mother Name"}, {"name": "Sample Child Name"}, {"name": "No Call Mode"}, {"name": "Pedigree"}, {"name": "Filter Name"}, {"name": "Disable Reference Overlapping"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "BED File", "encodingFormat": "text/x-bed"}, {"name": "Output Info Tags"}, {"name": "Thread Count"}, {"name": "Process Autosome Only"}, {"name": "Output Prefix"}], "output": [{"name": "Output Logs", "encodingFormat": "text/plain"}, {"name": "Tab Delimited [TSV] Logs"}, {"name": "Merged Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/sbg/VBT-TrioAnalysis", "https://github.com/sbg/VBT-TrioAnalysis", "https://github.com/sbg/VBT-TrioAnalysis", "https://github.com/sbg/VBT-TrioAnalysis"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Berke Cagkan Toptas (berke.toptas@sbgdinc.com), Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649156263, "dateCreated": 1520434434, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/metal-2020-05-05/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/metal-2020-05-05/3", "applicationCategory": "CommandLineTool", "name": "METAL", "description": "**METAL** can be used for meta analysis of GWAS data.\n\n**METAL** is a tool for meta-analysis genome-wide association scans. **METAL** can combine either (a) test statistics and standard errors or (b) p-values across studies (taking sample size and direction of effect into account). **METAL** analysis is a convenient alternative to a direct analysis of merged data from multiple studies. It is especially appropriate when data from the individual studies cannot be analyzed together because of differences in ethnicity, phenotype distribution, gender or constraints in sharing of individual level data imposed. Meta-analysis results in little or no loss of efficiency compared to analysis of a combined dataset including data from all individual studies. [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**METAL** is used for meta analysis of GWAS data. **METAL** is configured using a **METAL script**, provided as an input file. Instructions on creating a **METAL script** file can be found in the official documentation [1]. Seven Bridges can provide an example file upon request. For an example file, please contact our [support](mailto:support@sbgenomics.com).\n\nWhen writing a **METAL script**, have in mind that all files will be available in the App's working directory, eliminating the need to include file paths in the script.\n\n\n### Common Issues and Important Notes\n\nMost common issues with **METAL** come from writing the **METAL script** file. Please make sure to meet the following requirements:\n* For each input file declare the appropriate SEPARATOR if the separator is not whitespace.\n* For each input file, make sure that appropriate MARKERLABEL, ALLELELABELS, PVALUELABEL and EFFECTLABEL are set.\n* If performing a weighted analysis, make sure that WEIGHTLABEL is set for each input file.\n* Make sure to process every input file after specifying it.\n* Make sure to start the analysis at the end of the **METAL script**.\n* **METAL** expects that output file extension will be *.tbl*. As the output file name is defined within the **METAL script**, it is necessary to define it in such a way that the file name ends with *.tbl*. If another extension is used, the **Output Extension** app setting must be provided in order to properly catch the output file. Otherwise, **METAL** will not be able to detect the output file and no files will be returned.\n* **Note** Make sure that the same variants or regions have the same names (markers) across all input files. This app will work either way, but will output improper results in case that variants have inconsistent naming.\n\n\n### Performance Benchmarking\n\n**METAL** runs finish in minutes regardless of the number of variants. Even though the App has not been tested with a high number of input files, it is expected that even with large number of files runtime should not exceed an hour. The [publication](http://www.sph.umich.edu/csg/abecasis/publications/pdf/Bioinformatics.vol.26-pp.2190.pdf) describes the use case of 74 studies with more than 2.5M variants, each taking 30 minutes of runtime.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding Platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id, app_id = \"your_username/project\", \"your_username/project/app\"\n# Get file names from files in your project. Example: Names are taken from Data/Public Reference Files.\ninputs = {\n    \"in_gwas_files\": api.files.query(project=project_id, names=[\"name_of_first_file\", \"name_of_second_file\"]),\n    \"in_metal_sccript\": api.files.query(project=project_id, names=[\"name_of_metal_script_file\"])[0]\n}\ntask = api.tasks.create(name='METAL - API Run', project=project_id, app=app_id, inputs=inputs, run=False)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [METAL documentation](https://genome.sph.umich.edu/wiki/METAL_Documentation)", "input": [{"name": "GWAS files", "encodingFormat": "text/plain"}, {"name": "METAL Script", "encodingFormat": "text/plain"}, {"name": "Memory per job [MB]"}, {"name": "Output Extension"}], "output": [{"name": "Meta analysis files"}, {"name": "Meta analysis helper file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/METAL", "https://github.com/statgen/METAL/releases/tag/2020-05-05", "https://github.com/statgen/METAL/blob/master/LICENSE.twister"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.2"], "dateModified": 1649762846, "dateCreated": 1638296809, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/methyldackel-extract-0-5-1/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/methyldackel-extract-0-5-1/7", "applicationCategory": "CommandLineTool", "name": "MethylDackel Extract", "description": "**MethylDackel Extract** is a universal methylation extractor for BS-seq alignments. As a part of the MethylDackel toolkit, it processes coordinate-sorted, indexed alignment files and extracts per-base methylation metrics from them [1].\n\nThe tool has two required inputs:\n\n* **Reference file** - an indexed file in FASTA or FA format;\n\n* **Alignment file** - a coordinate-sorted, indexed alignment file in BAM or CRAM format. \n\nIn addition to required inputs, there are three optional inputs, used with defined parameters:\n\n* **BED file** (`-l`) - a BED format file, with the regions for inclusion;\n\n* **BigWig mappability data file** (`-M`) - a BW format file containing mappability data for filtering reads;\n\n* **BBM mappability data file** (`-B`) - a BBM format file for the reads filtering.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**MethylDackel Extract** is designed to efficiently extract per-base methylation metrics from a coordinate-sorted, indexed alignment in a WGBS analysis. It requires an alignment BAM or CRAM file and an indexed FASTA or FA file containing the reference genome. It is highly recommended for the processing of the alignments generated by the **BitMapperBS Align** tool. \n\n**MethylDackel Extract** groups all Cytosines into one of three sequence contexts: CpG, CHG, and CHH (H is the IUPAC ambiguity code for any nucleotide other than G). By default, it will only calculate metrics for Cytosines in a CpG context, but metrics for those in CHG and CHH contexts are supported as well. If an N is encountered in the reference sequence, the context will be assigned to CHG or CHH. If a Cytosine is close enough to the end of a chromosome/contig such that its context can not be inferred, it is categorised as CHH [1].\n\n**MethylDackel Extract** produces a variant of a BEDGRAPH file, in which each line consists of 6 tab separated columns:\n\n1) The chromosome/contig/scaffold name\n2) The start coordinate\n3) The end coordinate\n4) The methylation percentage rounded to an integer\n5) The number of alignments/pairs reporting methylated bases\n6) The number of alignments/pairs reporting unmethylated bases\n\nAll coordinates are 0-based half open, which conforms to the BEDGRAPH definition. When paired-end reads are aligned, it can often occur that their alignments overlap. In such cases, **MethylDackel** will not count both reads of the pair in its output, as doing so would lead to incorrect downstream statistical results [1].\n\nAn example of the output is below:\n\n\n    track type=\"bedGraph\"  description=\"SRR1182519.sorted  CpG methylation levels\"\n    1 25115  25116 \t100   3   0 \n    1 29336  29337 \t50    1   1     \n\n\n### Changes Introduced by Seven Bridges\n\n* No changes were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n* **Alignment file** must be coordinate-sorted and indexed;\n\n* The parameters **Extract fractional methylation** (`--fraction`), **Extract base counts** (`--counts`) and **Extract logit** (`--logit`) are mutually exclusive.\n\n\n### Performance Benchmarking\n\n**MethylDackel Extract** is a fast and memory-efficient tool. \n\nThe following table contains estimates of **MethylDackel Extract** running time and cost. The alignment files used for the benchmarking are produced by the **BitMapperBS Align** tool. They are preprocessed by the **SAMtools Fixmate CWL1.0**, **Samtools Sort CWL1.0**, **Samtools Markdup CWL1.0** and **Samtools Index CWL1.0** tools. The files are in BAM format. \n\n\n| Reference file size | Alignment file size |  Running time| AWS instance | Threads | Cost |\n|--------------------------------|--------------------------|---------------------|--------------------------|----------------------|-----------------------|---------------------------|\n| 3 GB | ~ 767 MB |~ 2min | c4.2xlarge (on-demand)  | 8 | ~ $0.02 |\n| 3 GB | ~ 19 GB | ~ 9min | c4.2xlarge (on-demand) | 8 | ~ $0.08 |\n| 3 GB | ~ 37 GB |~ 15min | c4.2xlarge (on-demand) | 8 | ~ $0.14 |\n| 3 GB | ~ 37 GB |~ 5min | c5.9xlarge (on-demand) | 36 | ~ $0.14 |\n| 3 GB | ~ 65 GB |~ 22min | c4.2xlarge (on-demand) | 8 | ~ $0.20 |\n\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [MethylDackel GitHub](https://github.com/dpryan79/MethylDackel)", "input": [{"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Alignment file", "encodingFormat": "application/x-bam"}, {"name": "Minimum MAPQ threshold"}, {"name": "Minimum Phred threshold"}, {"name": "Maximum per-base depth"}, {"name": "Minimum per-base depth"}, {"name": "Region string"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Keep strand"}, {"name": "Threads"}, {"name": "Size of the genome processed by a thread"}, {"name": "Merge per-Cytosine metrics"}, {"name": "Output filename prefix"}, {"name": "Keep duplicates"}, {"name": "Keep singleton"}, {"name": "Keep discordant"}, {"name": "Ignore flags"}, {"name": "Require flags"}, {"name": "Exclude CpG metrics"}, {"name": "Output CHG metrics"}, {"name": "Output CHH metrics"}, {"name": "Extract fractional methylation"}, {"name": "Extract base counts"}, {"name": "Extract logit"}, {"name": "Minimum opposite depth"}, {"name": "Maximum variant fraction"}, {"name": "Output in the format required by methylKit"}, {"name": "Output cytosine report"}, {"name": "Methylation calls from top strand reads"}, {"name": "Methylation calls from bottom strand reads"}, {"name": "Methylation calls from complementary top original top reads"}, {"name": "Methylation calls from complementary top original bottom reads"}, {"name": "Exclude original top strand reads"}, {"name": "Exclude original bottom strand reads"}, {"name": "Exclude complementary top original top reads"}, {"name": "Exclude complementary top original bottom reads"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "BigWig mappability data file"}, {"name": "Mappability threshold"}, {"name": "Minimum mappable bases"}, {"name": "Output binary bismap file"}, {"name": "Output BBFile name"}, {"name": "BBM mappability data file"}], "output": [{"name": "CpG BedGraph file"}, {"name": "Cytosine report", "encodingFormat": "text/plain"}, {"name": "MethylKit"}, {"name": "Output binary bismap file"}, {"name": "CHG BedGraph file"}, {"name": "CHH BedGraph file"}, {"name": "Methylation fraction file"}, {"name": "Logit BedGraph file"}, {"name": "Base counts file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dpryan79/MethylDackel", "https://github.com/dpryan79/MethylDackel/releases", "https://github.com/dpryan79/MethylDackel/archive/0.5.1.tar.gz", "https://github.com/dpryan79/MethylDackel/blob/master/README.md"], "applicationSubCategory": ["Methylation", "Methylation Extraction"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1612361355, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/methyldackel-mbias-0-5-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/methyldackel-mbias-0-5-1/6", "applicationCategory": "CommandLineTool", "name": "MethylDackel Mbias", "description": "**MethylDackel Mbias** is a tool used to make a methylation bias plot for BS-seq alignments. As a part of the MethylDackel toolkit, it processes coordinate-sorted, indexed alignment files and extracts mbias graphs in SVG format that can be viewed in most modern web browsers [1].\n\nThe tool has two required inputs and an optional one that can be used with the defined parameter:\n\n* **Reference file** - an indexed reference file in FASTA or FA format;\n\n* **Alignment file** - a coordinate-sorted, indexed alignment file in BAM or CRAM format;\n\n* **BED file** (`-l`) - a BED format file with the regions for inclusion.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**MethylDackel Mbias** creates a methylation bias (mbias) plot for each of the strands for which there are valid alignments. In practice, there are often increases/decreases in observed methylation rate at the ends of reads. Including such regions in the extracted methylation metrics will result in noisier and less accurate data. For this reason, users are strongly encouraged to make a methylation bias plot [1]. \n\nFor paired-end data, both reads in the pair will be shown separately. The program will suggest regions for inclusion (\"--OT 2,0,0,98\" below) and mark them on the plot, if applicable. These suggestions should not be accepted blindly; users are strongly encouraged to tweak the actual bounds as appropriate. The lines indicate the average methylation percentage at a given position and the shaded regions indicate the 99.9% confidence interval around it. The spike in methylation at the end of read #2 and the dip at the beginning of read #1  can be ignored with the suggested trimming bounds. The recommended numbers refer to the first and last base that should be included during methylation extraction [1].\n\nThe resulting mbias graphs are in SVG format and can be viewed in most modern web browsers:\n\n![Mbias plot](https://camo.githubusercontent.com/0beb90dc00a4fc9afd6393a36164375fdc106063/68747470733a2f2f7261776769742e636f6d2f64707279616e37392f4d657468796c4461636b656c2f6d61737465722f6578616d706c655f4f542e737667 \"Mbias plot\")\n\n### Changes Introduced by Seven Bridges\n\n* No changes were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n* **Alignment file** must be coordinate-sorted and indexed.\n\n### Performance Benchmarking\n\n**MethylDackel Mbias** is a fast and memory-efficient tool. \n\nThe following table contains estimates of **MethylDackel Mbias** running time and cost. The alignment files used for the benchmarking are produced by the **BitMapperBS Align** tool. They are preprocessed by the **SAMtools Fixmate CWL1.0**, **Samtools Sort CWL1.0**, **Samtools Markdup CWL1.0** and **Samtools Index CWL1.0** tools. The files are in BAM format.\n\n\n| Reference file size | Alignment file size |  Running time| AWS instance | Threads | Cost |\n|--------------------------------|--------------------------|---------------------|--------------------------|----------------------|-----------------------|---------------------------|\n| 3 GB | ~ 767 MB | ~ 2min | c4.2xlarge (on-demand) | 8 | ~ $0.02 |\n| 3 GB | ~ 37 GB | ~ 13min | c4.2xlarge (on-demand) | 8  | ~ $0.12 |\n| 3 GB | ~ 37 GB | ~ 4min | c5.9xlarge (on-demand) | 36  | ~ $0.11 |\n| 3 GB | ~ 65 GB |~ 22min | c4.2xlarge (on-demand) | 8  | ~ $0.20 |\n| 3 GB | ~ 65 GB |~ 25min | c5.9xlarge (on-demand) | 36  | ~ $0.70 |\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [MethylDackel GitHub](https://github.com/dpryan79/MethylDackel)", "input": [{"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Alignment file", "encodingFormat": "application/x-bam"}, {"name": "Minimum MAPQ threshold"}, {"name": "Minimum Phred threshold"}, {"name": "Maximum per-base depth"}, {"name": "Region string"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Keep strand"}, {"name": "Threads"}, {"name": "Size of the genome processed by a thread"}, {"name": "Keep duplicates"}, {"name": "Keep singleton"}, {"name": "Keep discordant"}, {"name": "Ignore flags"}, {"name": "Require flags"}, {"name": "Output tab separated metrics"}, {"name": "Exclude SVG"}, {"name": "Exclude CpG metrics"}, {"name": "Output CHG metrics"}, {"name": "Output CHH metrics"}, {"name": "Exclude original top strand reads"}, {"name": "Exclude original bottom strand reads"}, {"name": "Exclude complementary top original top reads"}, {"name": "Exclude complementary top original bottom reads"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Output filename prefix"}], "output": [{"name": "Mbias plot"}, {"name": "Tab separated metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dpryan79/MethylDackel", "https://github.com/dpryan79/MethylDackel/releases", "https://github.com/dpryan79/MethylDackel/archive/0.5.1.tar.gz", "https://github.com/dpryan79/MethylDackel/blob/master/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1612361355, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/methyldackel-mergecontext-0-5-1/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/methyldackel-mergecontext-0-5-1/7", "applicationCategory": "CommandLineTool", "name": "MethylDackel MergeContext", "description": "**MethylDackel MergeContext** is a tool for producing per-CpG/CHG metrics rather than per-Cytosine metrics. As a part of the MethylDackel toolkit, it takes BEDGRAPH files containing per-Cytosine metrics, converting them to per-CpG/CHG metrics and making a new BEDGRAPH file as the output [1].\n\nThe tool has two required inputs:\n\n* **Reference file** - an indexed file in FASTA or FA format;\n\n* **Input bedGraph file** - a BEDGRAPH file produced by the **MethylDackel Extract** tool. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**MethylDackel MergeContext** is designed to merge single Cytosine methylation metrics into per-CpG/CHG metrics. It requires a BEDGRAPH file, produced by the **MethylDackel Extract** tool,  and an indexed FASTA or FA file containing the reference genome and produces a new BEDGRAPH file.\n\nIf **MethylDackel MergeContext** is used, then the input:\n\n    track type=\"bedGraph\"  description=\"SRR1182519.sorted  CpG methylation levels\"\n    1 25114  25115 \t66   2   1 \n    1 25115  25116 100   3   0     \n\nis changed to the output:\n\n    track type=\"bedGraph\"  description=\"SRR1182519.sorted  merged CpG methylation levels\"\n    1 25114  25116 \t83   5   1 \n\nIt works for CpG and CHG-level metrics.\n\n### Changes Introduced by Seven Bridges\n\n* **Merged bedGraph output** has the BEDGRAPH extension. \n\n### Common Issues and Important Notes\n\n* Even though **Input bedGraph file** is not set as required for the purpose of **WGBS Analysis - BitMapperBS with MethylDackel** development, both **Input bedGraph file** and **Reference file** inputs are necessary for successful task execution.\n \n\n### Performance Benchmarking\n\n**MethylDackel MergeContext** is a fast and memory-efficient tool.\n\nThe following table contains estimates of **MethylDackel MergeContext** running time and cost. The input files used for the benchmarking are BEDGRAPH files produced by the **MethylDackel Extract** tool. \n\n                   \n| Reference file size | bedGraph file size |  Running time| AWS instance | Cost |\n|--------------------------------|--------------------------|---------------------|--------------------------|----------------------|-----------------------|---------------------------|\n| 3 GB |  ~ 72 MB| ~ 2min | c4.2xlarge (on-demand)  | ~ $0.02 |\n| 3 GB |~ 250 MB |~ 3min | c4.2xlarge (on-demand)  | ~ $0.03 |\n| 3 GB | ~ 1.5 GB|  ~ 7min | c4.2xlarge (on-demand)  | ~ $0.07 |\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n\n### References\n\n[1] [MethylDackel GitHub](https://github.com/dpryan79/MethylDackel)", "input": [{"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Input bedGraph file"}, {"name": "Output filename prefix"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Merged bedGraph file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dpryan79/MethylDackel", "https://github.com/dpryan79/MethylDackel/releases", "https://github.com/dpryan79/MethylDackel/archive/0.5.1.tar.gz", "https://github.com/dpryan79/MethylDackel/blob/master/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1612361354, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/methyldackel-perread-0-5-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/methyldackel-perread-0-5-1/6", "applicationCategory": "CommandLineTool", "name": " MethylDackel perRead", "description": "**MethylDackel perRead** is a tool used to produce per-read CpG methylation output. As a part of the MethylDackel toolkit, it processes coordinate-sorted, indexed alignment files and extracts per-read methylation metrics from them [1]. \n\nThe tool has two required inputs and an optional one that can be used with the defined parameter:\n\n* **Reference file** - an indexed file in FASTA or FA format;\n\n* **Alignment file** - a coordinate-sorted, indexed alignment file in BAM or CRAM format; \n\n* **BED file** (`-l`) - a BED format file, with the regions for inclusion.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**MethylDackel perRead** generates a per-read methylation summary. It computes the average CpG methylation level of a given read and produces a TXT format output.\n \nThe output is a tab-separated file with the following columns:\n - read name\n - chromosome\n - position\n - CpG methylation (%)\n - number of informative bases\n\n\n### Changes Introduced by Seven Bridges\n\n* No changes were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n* **Alignment file** must be coordinate-sorted and indexed;\n\n* Fragments longer than 10kb are currently not handled correctly [1].\n\n### Performance Benchmarking\n\n**MethylDackel perRead** is a fast and memory-efficient tool. \n\nThe following table contains estimates of **MethylDackel perRead** running time and cost. The alignment files used for the benchmarking are produced by the **BitMapperBS Align** tool. They are preprocessed by the **SAMtools Fixmate CWL1.0**, **Samtools Sort CWL1.0**, **Samtools Markdup CWL1.0** and **Samtools Index CWL1.0** tools. The files are in BAM format.\n\n\n| Reference file size | Alignment file size |  Running time| AWS instance | Threads | Cost |\n|--------------------------------|--------------------------|---------------------|--------------------------|----------------------|-----------------------|---------------------------|\n| 3 GB |  ~ 767 MB| ~ 2min | c4.2xlarge (on-demand) | 8 | ~ $0.02 |\n| 3 GB | ~ 19 GB |~ 11min  | c4.2xlarge (on-demand) | 8 | ~ $0.10 |\n| 3 GB | ~ 19 GB |~ 4min  | c5.9xlarge (on-demand) | 36 | ~ $0.11 |\n| 3 GB | ~ 65 GB |~ 34min  | c4.2xlarge (on-demand) | 8 | ~ $0.31 |\n| 3 GB | ~ 65 GB |~ 32min  | c5.9xlarge (on-demand) | 36 | ~ $0.89 |\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [MethylDackel GitHub](https://github.com/dpryan79/MethylDackel)", "input": [{"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Alignment file", "encodingFormat": "application/x-bam"}, {"name": "Minimum MAPQ threshold"}, {"name": "Minimum Phred threshold"}, {"name": "Region string"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Keep strand"}, {"name": "Ignore flags"}, {"name": "Require flags"}, {"name": "Threads"}, {"name": "Size of the genome processed by a thread"}, {"name": "Output filename prefix"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}], "output": [{"name": "Per-read report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dpryan79/MethylDackel", "https://github.com/dpryan79/MethylDackel/releases", "https://github.com/dpryan79/MethylDackel/archive/0.5.1.tar.gz", "https://github.com/dpryan79/MethylDackel/blob/master/README.md"], "applicationSubCategory": ["Methylation"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1612361355, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/microrazers/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/microrazers/6", "applicationCategory": "CommandLineTool", "name": "MicroRazerS", "description": "**MicroRazerS** is a tool for mapping millions of short reads obtained from small \nRNA sequencing onto a reference genome. \n\n**MicroRazerS** searches for the longest perfect prefix match of each read where the minimum prefix match length (the seed length) can currently be varied between 14 and 22. Optionally, one \nmismatch can be tolerated in the seed. **MicroRazerS** guarantees to find all matches and reports a configurable maximum number of equally-best matches. Perfect matches are given preference over matches containing mismatches, even if this means mapping a shorter prefix [1].\n\n**MicroRazerS** uses a k-mer index of all reads and counts common k-mers of reads and the reference genome in parallelograms. In **MicroRazerS**, this index is built over the first seedlength many bases of each read only. Each parallelogram with a k-mer count above a certain threshold triggers a verification. On success, the genomic subsequence and the read number are stored and later written to the output file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n\n### Common Use Cases\n\n**MicroRazerS** expects a reference genome and one FASTQ file that contains reads that should be \nmapped against the reference. Without any additional parameters **MicroRazerS**\nwould map all reads against both strands of the reference genome requiring a perfect \nprefix seed match of length >= 16. Up to 100 equally best (longest) matches \nwould then be dumped in the default output file.\n\nThe output default format is SAM. MicroRazerS native format \".razers\" is also available through the **Output file format** (`-o`) input option. \n\n\n### Changes Introduced by Seven Bridges\n\n* FASTA ids of the reads are used in the output file. Reads could be also enumerated\n  beginning with 1 but the option to use read sequence itself is not enabled. \n\n### Common Issues and Important Notes\n\n* Read files should not be larger than 16777216 lines (roughly 500 Mb). Only single-ended reads are allowed. \n \n### Performance Benchmarking\n\nMiRNA mapping as not as computationally heavy as standard RNA/DNA mapping problems, therefore the following benchmarking analysis was performed on an m2.xlarge AWS instance. The database of human miRNAs from miRBase was used as a reference genome.  \n\n| FASTQ Input size [Mb] | Duration [min] |  Cost [$] |   Instance  |\n|:---------------:|:--------------:|:------------:|:-----------:|\n|       95       |      15       |     0.06     | m2.xlarge  |\n|       150     |      13       |     0.05     |  m2.xlarge |\n|       280     |      14       |     0.06     |  m2.xlarge |\n|       360     |      19       |     0.08     |  m2.xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [MicroRazerS paper](https://www.ncbi.nlm.nih.gov/pubmed/19880369)", "input": [{"name": "Reference file", "encodingFormat": "application/x-tar"}, {"name": "MiRNA reads", "encodingFormat": "text/fastq"}, {"name": "Output file format"}, {"name": "Seed length"}, {"name": "Seed Error"}, {"name": "NUM of the best matches"}, {"name": "Map against a given strand"}, {"name": "N considered as errors or any value"}, {"name": "Omit reads"}, {"name": "Dump the alignment"}, {"name": "Output genome names"}, {"name": "Output read names"}, {"name": "Output match order"}, {"name": "Output positions format"}], "output": [{"name": "Aligned SAM/MicroRazerS", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement", "sbg:Metadata"], "codeRepository": ["https://github.com/seqan/seqan/tree/master/apps/micro_razers", "https://github.com/seqan/seqan/tree/master/apps/micro_razers", "https://github.com/seqan/seqan/tree/master/apps/micro_razers", "https://github.com/seqan/seqan/tree/master/apps/micro_razers"], "applicationSubCategory": ["MiRNA", "RNA"], "project": "SBG Public Data", "creator": "Anne-Katrin Emde", "softwareVersion": ["sbg:draft-2"], "dateModified": 1545144756, "dateCreated": 1520434395, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/minimac4-1-0-2-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/minimac4-1-0-2-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "Minimac4", "description": "**Minimac4** is a genetic imputation algorithm  [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Minimac4** can be used to impute genotypes in a genomic region starting from a reference panel in M3VCF format (**Reference haplotypes**) and pre-phased target GWAS haplotypes (**Pre-phased target genotype data**). By default, **Minimac4** automates the chunking process.\n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--processReference` was omitted from the wrapper as the option is currently deactivated in the tool.\n* Parameter `--help` was omitted from the wrapper.\n* Parameter `--noPhoneHome` is hardcoded in the wrapper, consequently the `--phoneHomeThinning` parameter was omitted from the wrapper.\n* Parameters `--meta`, `--vcfBuffer`, `--memUsage`, `--ChunkLengthMb`, `--ChunkOverlapMb`, `--minimac3`, `--probThreshold`, `--diffThreshold` and `--topThreshold` were absent from the tool usage print out, but were added to the wrapper based on the tool documentation page.\n* If not provided by the user, the `--prefix` parameter was set to default to the root of the **Pre-phased target genotype data** input file, in order to facilitate batch executions.\n* If the tool is run in **Estimate memory usage** mode, the log file will be automatically created (`--log`) to store the results of the task.\n* **Create a log file for debugging** input was added to help with debugging failed tasks. If set, a debug.log file will be created and kept among task execution logs.\n\n### Common Issues and Important Notes\n\n* Inputs **Reference haplotypes** and **Pre-phased target genotype data** are required.\n* **Reference haplotypes** file must be in M3VCF format. Reference panels in VCF format should be converted to M3VCF (along with parameter estimation) with Minimac3 before use.\n* If **Chromosome for imputation** input is used, **Start position for imputation** and **End position for imputation** must be specified as well.\n* When run in **Estimate memory usage** mode, the tool will create dummy outputs containing only headers. These files can be ignored - the memory estimation results are written in the log file.\n\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| 1000g chr20 | 102 mins | $0.67 + $0.23 | c4.2xlarge - 1024 GB EBS | \n| 1000g chr20 - 15 cpus | 48 mins | $0.54 + $0.11 | c5.4xlarge - 1024 GB EBS | \n| 1000g chr1 - 30 cpus | 136 mins | $3.47 + $0.31 | c5.9xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**Minimac4** was tested with cwltool version 3.1.20211107152837. The `in_refhaps` and `in_haps` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [Minimac2 publication](https://pubmed.ncbi.nlm.nih.gov/25338720/)\n\n[2] [Minimac4 documentation](https://genome.sph.umich.edu/wiki/Minimac4)", "input": [{"name": "Reference haplotypes"}, {"name": "Import only PASS variants"}, {"name": "Import only RS IDs"}, {"name": "Pre-phased target genotype data", "encodingFormat": "application/x-vcf"}, {"name": "Output file names prefix"}, {"name": "Do not GZIP outputs"}, {"name": "FORMAT fields"}, {"name": "Include all genotyped sites"}, {"name": "Chromosome for imputation"}, {"name": "Start position for imputation"}, {"name": "End position for imputation"}, {"name": "Buffer region size"}, {"name": "Write log file"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Number of CPUs for parallel computing"}, {"name": "Create a file for meta-imputation"}, {"name": "VCF buffer"}, {"name": "Estimate memory usage only"}, {"name": "Chunk length [Mb]"}, {"name": "Chunk overlap [Mb]"}, {"name": "Use Minimac3 algorithm for imputation"}, {"name": "Approximation levels - probThreshold"}, {"name": "Approximation levels - diffThreshold"}, {"name": "Approximation levels - topThreshold"}, {"name": "Create a log file for debugging"}], "output": [{"name": "Optional Minimac4 log file"}, {"name": "Minimap4 imputation results", "encodingFormat": "application/x-vcf"}, {"name": "Minimac4 info file"}, {"name": "Optional empirical dosage file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/statgen/Minimac4", "https://github.com/statgen/Minimac4/releases/tag/v1.0.2"], "applicationSubCategory": ["Imputation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "The Center for Statistical Genetics at the University of Michigan School of Public Health", "softwareVersion": ["v1.2"], "dateModified": 1648045277, "dateCreated": 1627654004, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/minimap2-2-22/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/minimap2-2-22/4", "applicationCategory": "CommandLineTool", "name": "minimap2", "description": "**Minimap2** is a sequence alignment program that aligns DNA or mRNA sequences against reference database [1].\n\n**Minimap2** is a general-purpose alignment program to map DNA or long mRNA sequences against a large reference database. It works with accurate short reads of \u2265100\u2009bp in length, \u22651\u2009kb genomic reads at error rate \u223c15%, full-length noisy Direct RNA or cDNA reads and assembly contigs or closely related full chromosomes of hundreds of megabases in length. **Minimap2** does split-read alignment, employs concave gap cost for long insertions and deletions and introduces new heuristics to reduce spurious alignments. It is 3\u20134 times as fast as mainstream short-read mappers at comparable accuracy, and is \u226530 times faster than long-read genomic or cDNA mappers at higher accuracy, surpassing most aligners specialized in one type of alignment [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n###Common Use Cases\n\nTypical uses of **minimap2** include [1]: \n- mapping PacBio or Oxford Nanopore genomic reads to the human genome\n- finding overlaps between long reads with error rate up to ~ 15%\n- splice-aware alignment of PacBio Iso-Seq or Nanopore cDNA or Direct RNA reads against a reference genome\n- aligning Illumina single- or paired-end reads\n- assembly-to-assembly alignment\n- full-genome alignment between two closely related species with divergence below ~ 15%\n\nFor additional guidelines to all **minimap2** functionalities, please refer to [minimap2 cookbook](https://github.com/lh3/minimap2/blob/master/cookbook.md).\n\n\n###Changes Introduced by Seven Bridges\n\n- Output format (PAF or SAM) is selected with the **Output file type** parameter (**output_type**) which automatically adds `-a` to the command line for SAM output format. \n- Please note that the following parameters, although representing numbers, are wrapped as strings because string suffixes are accepted (e.g. 4G, 200k etc.): `-I` (**Load bases into RAM**), `--max-qlen` (**Maximum query length**),  `-g` (**Stop chain elongation**),  `-G` (**Max gap on the reference**),  `-r` (**Chaining and DP-based alignment bandwidth**),  `-K` (**Number of bases loaded into memory to process in a mini-batch**),  `--cap-sw-mem` (**Skip alignment if the DP matrix size is above NUM**),  `--cap-kalloc` (**Thread-local kalloc memory reservoir**),  `--mask-len` (**Mask length**).\n- Parameter `-f` (**Repetitive minimizers filter out fraction**) is wrapped as an array of float numbers because it can accept either one float number, or one or two integers. Please refer to the parameter description to see what each option represents.\n- The following parameters have not been wrapped: `--no-kalloc`, `--print-qname`, `--print-seeds`\n\n\n###Common Issues and Important Notes\n\n- For the human reference genome, **minimap2** takes a few minutes to generate a minimizer index for the reference before mapping. Index can be prebuilt using the **minimap2 Build Index** tool, if the reference is used for multiple samples with the same input parameters. Please note that once the index is built, indexing parameters such as `-k` (**Minimizer k-mer length**), `-w` (**Minimizer window size**), `-H` (**Use HPC minimizers**) and `-I` (**Load bases into RAM**) can't be changed during mapping. If you are running **minimap2** for different data types, generating multiple indexes with different parameters may be required. This makes **minimap2** different from **BWA** which always uses the same index regardless of query data types.\n- **minimap2** outputs PAF files by default. When the **Output type** parameter is set to 'SAM', the tool will output alignments in SAM format instead. Additional parameters influencing output formatting are available, please find more information in [minimap2 manual](https://lh3.github.io/minimap2/minimap2.html). Note that choosing SAM as output type results in longer runtime. \n- Without `-a` (**Output file type** is set to SAM), `-c` (**Generate cigar**) or `--cs` (**Outpur cs tag**), minimap2 only finds approximate mapping locations without detailed base alignment. In particular, the start and end positions of the alignment are imprecise. With one of those options, minimap2 will perform base alignment, which is generally more accurate but is much slower [3]. For other FAQs please refer to [FAQs](https://github.com/lh3/minimap2/blob/master/FAQ.md).\n- **minimap2** works with gzip'd FASTA and FASTQ formats as inputs. Conversion between FASTA and FASTQ or decompressing gzip'd files is not necessary. \n- Authors of **minimap2** provide a couple of preset options for different use cases, such as mapping PacBio CLR reads (`-x map-pb`) or PacBio HiFi reads (`-x map-hifi`), ONT reads (`-x map-ont`), long assembly to reference mapping (`-x asm5`, `-x asm10`, `-x asm20`), PacBio CLR (`-x ava-pb`) or ONT (`ava-ont`) all-vs-all overlap mapping, long read spliced alignment (`-x splice`),  long read splice alignment for PacBio CCS reads (`-x splice:hq`) and short read alignment (`-x sr`). Accessing these options is available via the **Preset options** (`-x`) input. To see more details on which preset option enables which arguments, please refer to [minimap2 manual](https://lh3.github.io/minimap2/minimap2.html).\n- Number of CPUs used can be set using the **CPU per job** parameter, and memory per job can be set using the **Memory per job** input. Setting these parameters influences the instance on which the task is run. There is also parameter **Number of threads** (`-t`) which adds the desired number of threads to **minimap2** via the command line. If  **CPU per job** is not specified, and **Number of threads** is set, scheduler will choose an instance with CPUs equal to **Number of threads**.  Default value for the **Memory per job** input is set to 15000 MB of RAM memory and default value for **Number of threads** is set to 7 CPUs.\n\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depends on the size of the FASTQ/FASTA file containing the reads. The following table showcases the metrics for the task running on a c4.2xlarge or c5.9xlarge on-demand AWS instance. \n| Platform        | Strategy     | Input file size | Output file type | Duration      | Price  | Instance                          |\n|-----------------|--------------|-----------------|------------------|---------------|--------|-----------------------------------|\n| Oxford Nanopore | targeted     | 121MB           | PAF              | 3 minutes     |  $0.03 | c4.2xlarge                        |\n| Oxford Nanopore | whole genome | 83.1GB            | PAF              | 37 minutes    |  $1.05 | c5.9xlarge                        |\n| PacBio          | whole genome | 57.6GB            | PAF              | 27 minutes    |  $0.77 | c5.9xlarge                        |\n| PacBio          | whole genome | 57.6GB            | SAM              | 52 minutes    |  $1.45 | c5.9xlarge                        |\n| PacBio          | whole genome | 1003.1GB          | PAF              | 3h 4 minutes |  $5.59 | c5.9xlarge + 2TB attached storage |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###Portability\n\n**Minimap2** is tested with cwltool version: \"3.0.20201203173111\". The `number_of_threads`, `create_cigar`, `preset_options`, `in_reads` and `in_reference` were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [minimap2 github page](https://github.com/lh3/minimap2)\n\n[2] [minimap2 publication](https://academic.oup.com/bioinformatics/article/34/18/3094/4994778)\n\n[3] [FAQs](https://github.com/lh3/minimap2/blob/master/FAQ.md)", "input": [{"name": "Reference or Reference Index", "encodingFormat": "application/x-fasta"}, {"name": "Output file type"}, {"name": "Output name prefix"}, {"name": "Minimizer k-mer length"}, {"name": "Minimizer window size"}, {"name": "Use HPC minimizers"}, {"name": "Load bases into RAM"}, {"name": "Do not store target sequences"}, {"name": "ALT drop"}, {"name": "ALT contigs", "encodingFormat": "text/plain"}, {"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Preset options"}, {"name": "Minimal secondary-to-primary score ratio"}, {"name": "Maximum output secondary alignments"}, {"name": "Maximum fragment length"}, {"name": "Enable splice mode"}, {"name": "Short read mode"}, {"name": "Fragment mode"}, {"name": "Forward/reverse strand mapping"}, {"name": "No read pairing"}, {"name": "Matching score"}, {"name": "Mismatching penalty"}, {"name": "Gap open penalty"}, {"name": "Gap extension penalty"}, {"name": "Minimal peak DP alignment score"}, {"name": "Junctions BED", "encodingFormat": "text/x-bed"}, {"name": "Ignore base qualities"}, {"name": "Write CIGAR to CG tag"}, {"name": "Read group line"}, {"name": "Copy FASTQ comments"}, {"name": "Generate cigar"}, {"name": "Output cs tag"}, {"name": "Output MD tag"}, {"name": "Output CIGAR operators"}, {"name": "Soft clipping in SAM"}, {"name": "Seed"}, {"name": "Number of threads"}, {"name": "Use two I/O threads"}, {"name": "Output secondary alignments"}, {"name": "Maximum query length"}, {"name": "Output unmapped queries in PAF"}, {"name": "Output unmapped queries in SAM"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Repetitive minimizers filter out fraction"}, {"name": "Stop chain elongation"}, {"name": "Max gap on the reference"}, {"name": "Chaining and DP-based alignment bandwidth"}, {"name": "Minimal number of minimizers"}, {"name": "Minimal chaining score"}, {"name": "Skip self and dual mappings"}, {"name": "Z-drop score"}, {"name": "How to find GT-AG"}, {"name": "Cost  for  a  non-canonical  GT-AG  splicing"}, {"name": "End bonus"}, {"name": "Score of a mismatch involving ambiguous bases"}, {"name": "Splice flank"}, {"name": "Junctions bonus"}, {"name": "End seed pen"}, {"name": "No end filter"}, {"name": "Skip alignment if the DP matrix size is above NUM"}, {"name": "Thread-local kalloc memory reservoir"}, {"name": "Number of bases loaded into memory to process in a mini-batch"}, {"name": "Lower and upper bounds of k-mer occurrences"}, {"name": "Fraction of occurence of a query minimizer"}, {"name": "Sample a high-frequency minimizer every INT basepairs"}, {"name": "Ignore anchors from perfect self match"}, {"name": "Retain all chains and don't attempt to set primary chains"}, {"name": "Include lexicographically greater query name pairs"}, {"name": "Chain overlap fraction"}, {"name": "Use the minigraph chaining algorithm"}, {"name": "Hard mask level"}, {"name": "Mask length"}, {"name": "Max chain skip"}, {"name": "Max chain iterations"}, {"name": "Chain gap scale"}, {"name": "Disable the long gap patching heuristic"}, {"name": "Prefix to create temporary files"}, {"name": "Heap sort"}], "output": [{"name": "Out alignments", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/lh3/minimap2", "https://github.com/lh3/minimap2/releases", "https://github.com/lh3/minimap2/releases/tag/v2.22"], "applicationSubCategory": ["Alignment", "Long Reads", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Heng Li", "softwareVersion": ["v1.2"], "dateModified": 1648047601, "dateCreated": 1638375730, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/minimap2-build-index-2-22/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/minimap2-build-index-2-22/5", "applicationCategory": "CommandLineTool", "name": "minimap2 build index", "description": "**Minimap2 build index** is a reference indexer for **minimap2** aligner.\n\n**Minimap2** is a sequence alignment program that aligns DNA or mRNA sequences against reference database [1]. It follows a typical seed-chain-align procedure as is used by most full-genome aligners. **Minimap2** collects minimizers of the reference sequences and indexes them in a hash table. Afterwards, for each query sequence, **minimap2** takes query minimizers as seeds, finds exact matches (i.e. anchors) to the reference, and identifies sets of collinear anchors as chains [2]. **Minimap2 build index** is a tool that indexes the provided reference file. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n###Common Use Cases\n\n**Minimap2 build index** takes a couple of minutes for an entire human genome. Please note that once the index is built, indexing parameters such as `-k`, `-w`, `-H` and `-I` can't be changed during mapping with **minimap2**. If **minimap2** is run for different data types, generating multiple indexes with different parameters might be needed. Please note that `-k`, `-w`, `-H` and `-I` options set in the provided index file will override those set in a **minimap2** run. \n\n\n###Changes Introduced by Seven Bridges\n\n**Minimap2 build index** takes multiple file formats as inputs, including gzip'd files. If MMI index file is provided, the tool is run in pass-through mode, which is especially suitable for in-workflow running. \n\n###Common Issues and Important Notes\n\nNumber of CPUs used can be set using the **CPU per job** parameter, and memory per job can be set using the **Memory per job** input (this input should be defined in MB). Default values are 7 CPUs and 15000 MB of RAM memory. Setting these parameters influences the instance on which the task is run.\n\n### Performance Benchmarking\n\nIndexing with **minimap2** is fast, and should not last for longer than a couple of minutes for the entire human genome.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###Portability\n\n**Minimap2 build index** is tested with cwltool version: \"3.0.20201203173111\". The `in_reference` was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [minimap2 github page](https://github.com/lh3/minimap2)\n\n[2] [minimap2 publication](https://academic.oup.com/bioinformatics/article/34/18/3094/4994778)", "input": [{"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Output name prefix"}, {"name": "Minimizer k-mer length"}, {"name": "Minimizer window size"}, {"name": "Use HPC minimizers"}, {"name": "Load bases into RAM"}, {"name": "Do not store target sequences"}, {"name": "ALT drop"}, {"name": "ALT contigs", "encodingFormat": "text/plain"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Indexed reference"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/lh3/minimap2", "https://github.com/lh3/minimap2/releases", "https://github.com/lh3/minimap2/releases/tag/v2.22"], "applicationSubCategory": ["FASTA Processing", "Indexing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Heng Li", "softwareVersion": ["v1.2"], "dateModified": 1648047601, "dateCreated": 1638375771, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/msisensor-msi-0-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/msisensor-msi-0-2/6", "applicationCategory": "CommandLineTool", "name": "MSIsensor msi", "description": "**MSIsensor** is a C++ program for automatically detecting somatic and germline variants at microsatellite regions. It computes length distributions of microsatellites per site in paired tumor and normal sequence data, subsequently using these to statistically compare observed distributions in both samples. It uses sorted and indexed BAM files for tumor and normal samples as input. Comprehensive testing indicates **MSIsensor** is an efficient and effective tool for deriving MSI status from standard tumor-normal paired sequence data.\n\n### Common Use Cases\n\nMicrosatelllites or short tandem repeats (STRs) are stretches of repetitive DNA, usually considered to have core repeat units of 1 - 6bp [1]. They can be found anywhere in genome and they generally occupy 3% of it. Approximately 99% of them are shorter than 40bp.\n\nMicrosatellite instability (MSI) refers to widespread somatic changes in the number of repeats at microsatellite loci in the genome. MSI typically reflects defects in the DNA mismatch repair machinery, which are a hallmark of certain types of cancers. This tool enables profiling MSI using WES or WGS data.\n\n**MSIsensor msi** computes length distributions of homopolymers and microsatellites across specified loci and evaluates the statistical significance of the differences in distribution between normal and tumor samples. The output of the analysis are accrued distributions, lists of germline and somatic loci, as well as the percentage of somatic sites, as fractions of all sites with sufficient coverage. This percentage can be used to assess microsatellite instability status, as proposed in [2]. To run **MSIsensor msi**, sorted and indexed BAM files have to be provided, containing reads aligned against a chosen reference genome. This same reference sequence should be used to generate a list of homopolymers and microsatellites that are to be used in the analysis; this file will be produced by **MSIsensor scan**.\n\n### Changes Introduced by Seven Bridges\n\n* Pass through mode is added - in case a normal sample isn't provided, **MSIsensor** will just go to the completed stage. This can be useful when using this tool as a part of a workflow.\n\n### Common Issues and Important Notes\n\n* **MSIsensor msi fails with exit code 139**: This issue occurs when contigs from the reference genome, represented in the microsatellites list, are absent from the BAM/BAI file supplied as input. Most often, this is caused by different chromosome naming conventions (e.g. 'chr1' vs '1') and indicates that different reference genomes were mistakenly used for aligning reads and generating the microsatellites list. However, this is also known to happen when UCSC HG19 is used as reference (consistently). Namely, 'chrM' and various 'chrUn' are present in the reference sequence, but have no reads aligned to them in the BAM/BAI files. This issue is a limitation of the tool itself and can be circumvented by manually editing the microsatellites list to include only the 22 autosomes and the sex chromosomes.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of running times and costs. All samples are aligned and sorted BAMs that were previously aligned to the HG19 human reference genome. If not specified otherwise tool will be executed on the default instance (c4.2xlarge - AWS). If you want to change the instance type you can visit [knowledge center](https://docs.sevenbridges.com/docs/set-execution-hints-at-workflow-level) for details.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n#####Example runs:\n\n| Experiment type | Input size (T+N)| Paired-end | Read length | Duration | Cost | Instance (AWS)| Instance type |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|------------|\n| WES         | 4.8 GB + 5.5 GB     | Yes        | 100         | 21m   | $0.14            | c4.2xlarge      | On-demand        |\n| WES         | 32 GB + 42 GB     | Yes        | 100         | 48m   | $1.34            | c4.8xlarge      | On-demand        |\n\n### References\n\n[1] [A Brief Review of Short Tandem Repeat Mutation, Fan et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5054066/)\n\n[2] [MSIsensor: microsatellite instability detection using paired tumor-normal sequence data, Niu et al.](http://bioinformatics.oxfordjournals.org/content/30/7/1015)", "input": [{"name": "Microsatellites list", "encodingFormat": "text/plain"}, {"name": "Normal BAM file", "encodingFormat": "application/x-bam"}, {"name": "Tumor BAM file", "encodingFormat": "application/x-bam"}, {"name": "Prefix for output files"}, {"name": "BED file for selecting analysis regions", "encodingFormat": "text/x-bed"}, {"name": "Region selection"}, {"name": "FDR threshold"}, {"name": "Coverage threshold"}, {"name": "Minimal homopolymer size"}, {"name": "Minimal homopolymer size for distribution analysis"}, {"name": "Maximal homopolymer size for distribution analysis"}, {"name": "Minimal number of repeats"}, {"name": "Minimal microsatellite size for distribution analysis"}, {"name": "Maximal microsatellite size for distribution analysis"}, {"name": "Span size"}, {"name": "Output homopolymers only"}, {"name": "Output microsatellites only"}, {"name": "Number of threads"}], "output": [{"name": "Distribution of microsatellite and homopolymer lengths", "encodingFormat": "text/plain"}, {"name": "Germline loci", "encodingFormat": "text/plain"}, {"name": "Somatic loci", "encodingFormat": "text/plain"}, {"name": "MSI score", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/ding-lab/msisensor", "https://github.com/ding-lab/msisensor/archive/master.zip"], "applicationSubCategory": ["Microsatellites"], "project": "SBG Public Data", "creator": "Beifang Niu and Kai Ye, Washington University in St. Louis", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155967, "dateCreated": 1511878996, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/msisensor-scan-0-2/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/msisensor-scan-0-2/8", "applicationCategory": "CommandLineTool", "name": "MSIsensor scan", "description": "**MSIsensor scan** enables defining a list of homopolymers/microsatellites from a reference genome sequence. The output is a tab-delimited plain text file containing the genomic coordinates of the relevant sites and a description of their flanking regions. This file is typically used as an input in subsequent microsatellite instability profiling using **MSIsensor msi** tool. It uses a FASTA or FA file as **Reference genome** input.\n\n### Common Use Cases\n\nMicrosatelllites or short tandem repeats (STRs) are stretches of repetitive DNA, usually considered to have core repeat units of 1 - 6bp [1]. They can be found anywhere in genome and they generally occupy 3% of it. Approximately 99% of them are shorter than 40bp.\n\nMicrosatellite instability (MSI) refers to widespread somatic changes in the number of repeats at microsatellite loci in the genome. MSI typically reflects defects in the DNA mismatch repair machinery, which are a hallmark of certain types of cancers. **MSIsensor msi** enables profiling MSI using WES or WGS data, while **MSIsensor scan** is used to build a reference file.\n\n**MSIsensor** [2] is a C++ program for automatically detecting somatic and germline variants at microsatellite regions. It computes length distributions of microsatellites per site in paired tumor and normal sequence data, subsequently using these to statistically compare observed distributions in both samples. Comprehensive testing indicates **MSIsensor** is an efficient and effective tool for deriving MSI status from standard tumor-normal paired sequence data.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected. \n\n### Performance Benchmarking\n\nThe execution time for human gene annotation takes several minutes on the default instance; the price is negligible (~ 0.05$).\n\n#####Example run:\n\n| Input size | Duration | Cost | Instance (AWS) | Instance type |\n|-----------------|-------------|-------------|---------------|------------|\n| 3GB     | 4m   | $0.05            | c4.4xlarge      | On-demand        |\n\n### References\n\n[1] [A Brief Review of Short Tandem Repeat Mutation, Fan et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5054066/)\n\n[2] [MSIsensor: microsatellite instability detection using paired tumor-normal sequence data, Niu et al.](http://bioinformatics.oxfordjournals.org/content/30/7/1015)", "input": [{"name": "Reference genome", "encodingFormat": "application/x-fasta"}, {"name": "Output file"}, {"name": "Minimal homopolymer size"}, {"name": "Context length"}, {"name": "Maximal homopolymer size"}, {"name": "Maximal microsatellite repeat length"}, {"name": "Minimal number of repeats"}, {"name": "Output homopolymers only"}], "output": [{"name": "List of homopolymers and microsatellites", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/ding-lab/msisensor", "https://github.com/ding-lab/msisensor/archive/master.zip"], "applicationSubCategory": ["Microsatellites"], "project": "SBG Public Data", "creator": "Beifang Niu and Kai Ye, Washington University in St. Louis", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155968, "dateCreated": 1511878910, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/multiqc-1-9/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/multiqc-1-9/4", "applicationCategory": "CommandLineTool", "name": "MultiQC", "description": "**MultiQC** aggregates results from bioinformatics analyses across many samples into a single report [1]. \n\n__MultiQC__ searches a given directory for analysis logs and compiles an HTML report. It's a general purpose tool, perfect for aggregating and summarizing the output from numerous bioinformatics tools [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n- To successfully run __MultiQC__, just supply it with outputs from one of the many currently supported modules. The list of all supported tools and how to properly forward their outputs into __MultiQC__ can be found via the following [link](http://multiqc.info/docs/#multiqc-modules). As an output, a collective report with all of the analyses you provided in the input will be generated in an interactive HTML format as shown in [1]. \n\n### Changes Introduced by Seven Bridges\n\n* The option to compress the data directory (`-z` or `--zip-data-dir` ) is turned on by default. If it is necessary to have all files from the multiqc_data directory in an unzipped format, the **Output unzipped MultiQC data directory** option can be used for that. \n* All plots in output reports are shown as static images.\n\n### Common Issues and Important Notes\n\n- All output report files will be prefixed by the string specified on **Report filename** input. If this input is not specified, prefix will be defined from inputs **Sample ID** metadata field (if it exists). If there are multiple samples, the output name will be defined as **reports.multiqc**.\n\n### Performance Benchmarking\n\n__MultiQC__ is a tool that reports metrics, and as such does not require a lot of resources. Almost all **MultiQC** tasks will finish within 2 minutes when running on the default instance, costing around $0.01 using on-demand instances. \n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [MultiQC homepage](http://multiqc.info/)", "input": [{"name": "Input reports to parse", "encodingFormat": "application/zip"}, {"name": "Leave full file names"}, {"name": "Report page header"}, {"name": "Custom comment in report"}, {"name": "Report filename prefix"}, {"name": "Report template to use"}, {"name": "Use only modules tagged with this keyword"}, {"name": "View available tags and corresponding modules"}, {"name": "Ignore analysis files (glob expression)"}, {"name": "Ignore sample names (glob expression)"}, {"name": "File with alternative sample names"}, {"name": "Modules to exclude"}, {"name": "Use only these modules"}, {"name": "Output format for parsed data"}, {"name": "Export static plots"}, {"name": "Use strict linting"}, {"name": "Create PDF report"}, {"name": "Config file", "encodingFormat": "text/plain"}, {"name": "Config YAML on command line"}, {"name": "Increase output verbosity"}, {"name": "Memory per job [MB]"}, {"name": "Output unzipped MultiQC data directory"}], "output": [{"name": "MultiQC HTML report", "encodingFormat": "text/html"}, {"name": "Output parsed data"}, {"name": "MultiQC static plots"}, {"name": "MultiQC PDF report"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ewels/MultiQC"], "applicationSubCategory": ["Quality Control", "Utilities"], "project": "SBG Public Data", "creator": "Phil Ewels", "softwareVersion": ["v1.0"], "dateModified": 1648048648, "dateCreated": 1600686185, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/nanoplot-1-33-0-cwl1-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/nanoplot-1-33-0-cwl1-1/6", "applicationCategory": "CommandLineTool", "name": "NanoPlot CWL1.1", "description": "**NanoPlot CWL1.1** is a plotting QC tool for long reads sequencing data [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**NanoPlot** can be used for quality control of long read sequencing data, including summary sequencing files, raw reads (FASTQ, FASTA) and alignments (BAM, uBAM, CRAM) [2]. The tool also supports re-using data from previous runs, if it was stored as a PICKLE file. The main outputs of the tool are an HTML report, a textual summary statistics file and either a number of plots or a TAR archive with plots (controlled via the **Return a TAR archive with plots** input parameter). The contents of the HTML report and the created plots will vary with the provided input files.\n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--outdir` is hardcoded to the current working directory in the wrapper.\n* Parameters `--listcolors` and `--listcolormaps` were omitted from the wrapper as no plotting is done.\n* Parameter `--feather` was omitted from the wrapper as tasks with this input type failed with tool test data (LZ4 compression error).\n* Parameter `--no-N50` was omitted from the wrapper, as whether or not the N50 mark is shown on the read length is controlled with its opposite parameter `--N50` (**Show the N50 mark in the read length histogram**).\n* Input parameter **Return a TAR archive with plots** was added. Turning this parameter on will output a TAR archive with plots, instead of individual images. This setting is useful when running many tasks in batch mode, however, the images will not be directly accessible for viewing. The HTML report is not included in the archive.\n* If **Output file name prefix** input is not provided, the outputs will be prefixed with **Sample ID** metadata value of the provided input files, if one exists. If not, the name root of the provided input file will be used.\n\n### Common Issues and Important Notes\n\n* Input parameter **Return a TAR archive with plots** is required. Setting it to True will omit outputting individual plot images and return a TAR archive with the plots.\n* One or more files of the same type should be provided when running the tool, using one of the following inputs: **Input FASTQ files**, **Inputs FASTA files**, **Input FASTQ files with channel and time info**, **Input FASTQ files for minimal extraction**, **Input summary files**, **Input sorted BAM files**, **Input unmapped BAM files**, **Input sorted CRAM files** or **Input PICKLE file**.\n* This version of **NanoPlot** (1.33.0) requires network access for successful execution. The tool uses the network connection to download a plotting library, it does not transmit any analysis data.\n\n### Performance Benchmarking\n\n**NanoPlot** performance depends on the type and size of the provided inputs. Extracting information is fastest from sequencing summary files (**Input summary files**) [2]. Please note that additional cores are not used throughout the analysis, which makes using a lower number of cores more cost-effective overall. \n\nDatasets:\n\n * [Cliveome2](https://github.com/nanoporetech/ONT-HG1/blob/master/ver2/CONTENTS.md) ONT data (merged BAM file, 130 GB)\n * [NA12878 ONT rel6 FASTQ.GZ](https://github.com/nanopore-wgs-consortium/NA12878/blob/58326b8365cb59cfbdd60a550cacaa91ecac9ca3/Genome.md) - 136 GB\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Cliveome2 BAM - 4 cores | 2 h 6 min |$0.84 + $0.34 | c4.2xlarge 1024 GB EBS | \n| Cliveome2 BAM - 15 cores | 1 h 8 min |$0.77 + $0.17 | c5.4xlarge 1024 GB EBS | \n| NA12878 FASTQ - 1 core | 3 h 17 min |$0.51 + $0.50 | c4.2xlarge 1024 GB EBS |\n| NA12878 FASTQ - 4 cores | 3 h 17 min |$0.51 + $0.50 | c4.2xlarge 1024 GB EBS |\n| NA12878 FASTQ - 15 cores | 2 h 54 min |$1.97 + $0.42 | c5.4xlarge 1024 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### References\n\n[1] [NanoPlot publication](https://academic.oup.com/bioinformatics/article/34/15/2666/4934939)\n\n[2] [NanoPlot documentation](https://github.com/wdecoster/NanoPlot)", "input": [{"name": "Number of threads to use"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Verbose mode"}, {"name": "Pickle extracted data for future plotting"}, {"name": "Store extracted data in a TSV file"}, {"name": "Input data is one very large file"}, {"name": "Output file names prefix"}, {"name": "Outputs stats in TSV format"}, {"name": "Maximum read length to display"}, {"name": "Minimum read length to display"}, {"name": "Drop outlier reads with extreme length"}, {"name": "Number of reads to downsample to"}, {"name": "Show logarithmic scaling of lengths"}, {"name": "Use qualities as theoretical percent identities"}, {"name": "Use aligned read lengths [bam mode]"}, {"name": "Minimum average quality of reads to keep"}, {"name": "Only take the first N hours of a run"}, {"name": "Read type to extract info from summary"}, {"name": "Split summary file by barcode"}, {"name": "Remove supplementary alignments"}, {"name": "Matplotlib color for plots"}, {"name": "Matplotlib colormap for plots"}, {"name": "Output format for the plots"}, {"name": "Bivariate plots to create"}, {"name": "Show the N50 mark in the read length histogram"}, {"name": "Add a title to all plots"}, {"name": "Scale the font of the plots by a factor"}, {"name": "DPI for saving images"}, {"name": "Omit Pearson R stats in some bivariate plots"}, {"name": "Input FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Inputs FASTA files", "encodingFormat": "application/x-fasta"}, {"name": "Input FASTQ files with channel and time info", "encodingFormat": "text/fastq"}, {"name": "Input FASTQ files for minimal extraction", "encodingFormat": "text/fastq"}, {"name": "Input summary files", "encodingFormat": "application/zip"}, {"name": "Input sorted BAM files", "encodingFormat": "application/x-bam"}, {"name": "Input unmapped BAM files", "encodingFormat": "application/x-bam"}, {"name": "Input sorted CRAM files"}, {"name": "Input PICKLE file"}, {"name": "Return a TAR archive with plots"}], "output": [{"name": "HTML report", "encodingFormat": "text/html"}, {"name": "NanoPlot plots"}, {"name": "NanoPlot stats file", "encodingFormat": "text/plain"}, {"name": "PICKLE file"}, {"name": "Optional file with extracted data"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/wdecoster/NanoPlot", "https://github.com/wdecoster/NanoPlot/tree/master/nanoplot", "https://github.com/wdecoster/NanoPlot/releases/tag/1.33.0", "https://github.com/wdecoster/NanoPlot/blob/master/README.md"], "applicationSubCategory": ["Quality Control", "Long Reads", "Plotting"], "project": "SBG Public Data", "creator": "Wouter De Coster", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612306346, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/nanostringqcpro-1-10-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/nanostringqcpro-1-10-0/5", "applicationCategory": "CommandLineTool", "name": "NanoStringQCPro", "description": "**NanoStringQCPro** app performs basic QC steps and data normalization of NanoString mRNA gene expression data.\n\nThe company [NanoString](https://www.nanostring.com/)\u00ae has introduced nCounter technology for direct counting of molecules in samples, which enables direct detection of specific RNA, DNA and protein molecules. It provides highly robust data across clinically relevant samples while reducing hands-on time and simplifying analysis. There is no need for complex steps such as converting RNA to cDNA, amplification of DNA and others, which can introduce variability and lead to biased data and inability to replicate results. This technology is also recommended for samples with already degraded genetic material, such as formalin-fixed paraffin-embedded (FFPE) samples.\nUsing molecular barcode technology, RNA is directly tagged with a capture probe and a reporter probe that are specific to the target of interest creating a unique target-probe complex. After hybridization, excess probes are removed leaving only purified target-probe complexes. These complexes are immobilized and aligned on the imaging surface. The sample is then scanned by an automated fluorescence microscope where labelled barcodes are directly counted and the data analysed through an analysis software. \n\nAlthough nCounter molecular barcoding technology can be used with RNA, miRNA, protein, and DNA analytes, **NanoStringQCPro** is developed with the focus on mRNA gene expression data. It performs basic QC steps and data normalization, as suggested in Gene Expression Data Analysis Guidelines [1]. The app is a wrapper around an R script based on the functions from NanoStringQCPro Bioconductor/R package [2]. \n\nAs an input, **NanoStringQCPro** takes the **NanoString Raw Data Files** directory with raw count data given in either a set of .RCC files or in a .CSV file produced using the RCC Collector Tool Format Export feature of NanoString\u00ae nSolver Analysis Software. If data are given in .RCC files, argument **Data in RCC files** must be checked. Additionally, a Reporter Library File (.RLF) with details for each probe in the codeset used in the experiment must be provided. Optionally, NanoStringQCPro can take a .CSV file with additional details for each probe (usually provided by NanoString\u00ae in the \u201cDesign Data\u201d tab of the Codeset Design Report - CDR). Please consult *Common Use Cases* section for instructions how to organize files in the directory.\n\nAs outputs,  **NanoStringQCPro** produces an **HTML report** with results from all QC steps, and several .CSV files with processed data (**Processed Data**). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n\n### Common Use Cases\n\nRaw data from nCounter system can be provided either in .RCC files or in a .CSV file, with additional metadata and annotations files. All input files must be stored within **NanoString Raw Data Files** having the following subdirectory structure:\n\n* *RCC* subdirectory - containing a set of .RCC files with raw counts;\n* *nSolver* subdirectory - containing a .CSV file produced using the RCC Collector Tool Format Export feature of NanoString\u00ae nSolver Analysis Software;\n* *RLF* subdirectory - containing a Reporter Library File (.RLF) with details for each probe in the codeset used in the experiment;\n* *CDR* subdirectory - containing a .CSV file with additional details for each probe;\n* *extraPdata* subdirectory - containing sample annotation data in .CSV format with a column labelled FileName whose values are the exact .RCC file names and a SampleType column used primarily by\nthe preprocessing and QC report functions to identify blank samples (i.e. water runs). Also note that the **Blank Label** argument should be the same as the value used in the annotation file.\n\nIf data are stored in .RCC files, the **Data in RCC files** argument must be checked.\n\n\n### Changes Introduced by Seven Bridges\n\nAlthough *NanoStringQCPro* R package contains several functions which can be combined in different ways and with different parameters values, **NanoStringQCPro** CWL wrapper runs an R script taken from official NanoStringQCPro documentation [3] and adapted for running on the SevenBridges Platform. It performs the following actions:\n\n* loads raw count data with annotations (if any) and sets experiment data to be used later for reporting;\n* pre-processes data using positive control normalization, subtraction of background estimates and RNA content normalization via housekeeping or global median scaling (Default: global median scaling);\n* extracts processed data;\n* creates a QC report in .HTML format.\n\n\n### Common Issues and Important Notes\n\n* Please make sure the input files are stored in the directory structure as explained in the ***Common Use Case*** section.\n* Check the **Data in RCC files** argument if data are stored in .RCC files.\n\n\n### Performance Benchmarking\n\n**NanoStringQCPro** app has been tested with example data from NanoStringQCPro package (a set of 25 RCC files). It ran only a few minutes on c4.2xlarge and it is not demanding in terms of memory and CPU.\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [MAN-C0011-04 Gene Expression Data Analysis Guidelines](https://www.nanostring.com/download_file/view/251/8241)\n\n[2] Nickles D, Sandmann T, Ziman R, Bourgon R (2017). *NanoStringQCPro: Quality metrics and data processing methods for NanoString mRNA gene expression data*. R package version 1.10.0.\n\n[3] [NanoStringQCPro R/Bioconductor package vignette](https://bioconductor.org/packages/release/bioc/manuals/NanoStringQCPro/man/NanoStringQCPro.pdf)", "input": [{"name": "NanoString Raw Data Files"}, {"name": "Blank Label"}, {"name": "Add EntrezGene Annotations"}, {"name": "Drop phenoData columns"}, {"name": "Drop featureData Columns"}, {"name": "Experiment Name"}, {"name": "Experiment Lab"}, {"name": "Experiment Contact"}, {"name": "Experiment Title"}, {"name": "Experiment Abstract"}, {"name": "Experiment URL"}, {"name": "Experiment Other"}, {"name": "Output Basename"}, {"name": "Data in RCC files"}, {"name": "Normalization Method"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "ZIP file with HTML report", "encodingFormat": "application/zip"}, {"name": "Processed data"}, {"name": "HTML report", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Quality Control"], "project": "SBG Public Data", "creator": "Robert Ziman", "softwareVersion": ["v1.0"], "dateModified": 1648050523, "dateCreated": 1572014310, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/nome-filtering/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/nome-filtering/3", "applicationCategory": "CommandLineTool", "name": "NOMe filtering", "description": "**NOMe filtering**  is used for filtering reads in yacht file (output of **Bismark Methylation Extractor**), which contains additional information about the reads that methylation calls belonged to. It is a part of Bismark toolkit, a set of tools for time-efficient analysis of Bisulfite-Seq (BS-Seq) data in which bisulfite-treated reads are aligned to a reference genome and cytosine methylations are called at the same time [1].\n\n**NOMe filtering** has two required inputs:\n\n* **Yacht file** is a file with additional information about the read a methylation call belongs to. This file needs to have been generated with **Bismark Methylation Extractor** with the option **Yacht** specified.\n* **Reference genome file(s)** is(are) a reference genome file(s) in .FASTA or .FA format.\n\nThis tool has one output:\n\n* **Report file** reports cytosines in CpG context only if they are in A-CG or T-CG context, and cytosines in GC context only when the C is not in CpG context. The output file is in tab-delimited format.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**NOMe filtering**  is used for filtering reads in yacht file (output of Bismark Methylation Extractor), which contains additional information about the reads that methylation calls belonged to. It processes entire (**Single-end**) reads and then filters calls for NOMe-Seq positions (nucleosome occupancy and methylome sequencing) where accessible DNA gets methylated in a GpC context:\n \n* Filters CpGs to only output cytosines in A-CG and T-CG context;\n\n* Filters GC context to only report cytosines in GC-A, GC-C and GC-T context.\n\nBoth of these measures aim to reduce unwanted biases, i.e. the influence of G-CG (intended) and C-CG (off-target) on endogenous CpG methylation, and the influence of CpG methylation on (the NOMe-Seq specific) GC context methylation.\n\n### Changes Introduced by Seven Bridges\n\n* **NOMe\\_filtering** tool creates **genome\\_folder** which accepts provided genome reference files, instead of providing a path to genome folder (as required by the original NOMe_filtering script `--genome_path`).\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges platform have been detected.\n\n### Performance Benchmarking\n\n| Input size (Yacht file) | Duration | Cost | Instance (AWS) |\n| --- | --- | --- | --- |\n|4.4 GB|30 min|$0.27|c4.2xlarge (1024GB storage)|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*    \n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pubmed/21493656). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14.", "input": [{"name": "Reference genome file(s)", "encodingFormat": "application/x-fasta"}, {"name": "Yacht file", "encodingFormat": "text/plain"}], "output": [{"name": "Report file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/FelixKrueger/Bismark", "https://github.com/FelixKrueger/Bismark/releases", "https://github.com/FelixKrueger/Bismark/blob/master/Docs/README.md"], "applicationSubCategory": ["Methylation", "Filtering"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648561065, "dateCreated": 1519389842, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/novobreak/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/novobreak/9", "applicationCategory": "CommandLineTool", "name": "novoBreak", "description": "**novoBreak** is a tool used in cancer genomic studies to discover SV (both somatic and germline) breakpoints. It can report accurate breakpoints of Deletions (DEL), Duplications (DUP), Inversions (INV) and Translocations (TRA) (you should consider some of them are mobile elements insertions or templated insertions). For novel insertions, we may only report the breakpoints but not the inserted sequence. Please forget about novel insertions at the moment. We will work on that later. It was designed for Illumina paired-end data. The program takes two BAM files as an input (tumor and normal) and outputs a VCF file with detected breakpoints.\n\n**Preprocessing:** For virus integration analysis, please add the virus genome(s) to reference and realign the reads to the new reference. The integration should be a TRA event.\n\nTo increase **sensitivity**, novoBreak tries to infer as many SVs as possible based on the local assembly results. But many of the inferred SVs may be false positives due to misassembly or lack of enough evidence. So we provided a default filter to get a relatively stringent filtered callset based on real data experience. We empirically defined the minimum SV size as 100 bp and no upper limit. Users can change the filter and cutoffs based on the utility and the knowledge as needed.\n\n**Common Issues:** novoBreak requires around 40GB of physical memory for execution. Out of Memory error is explicitly shown in the error log in case of memory shortage.", "input": [{"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Normal BAM", "encodingFormat": "application/x-bam"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Number of CPUs"}, {"name": "Memory in MB"}], "output": [{"name": "Output novoBreak", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Zechen Chong / MD Anderson Cancer Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649156264, "dateCreated": 1490969212, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/olinkanalyze-de-2-0-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/olinkanalyze-de-2-0-0/4", "applicationCategory": "CommandLineTool", "name": "OlinkAnalyze DE", "description": "**OlinkAnalyze DE** performs differential expression analysis on Olink Normalised Protein eXpression (NPX) data. It uses linear and linear mixed effects models and it represents a wrapper around the script based mostly on the functions provided in the OlinkAnalyze R package. This package is developed by the Olink data science team to facilitate NPX data handling.\n\nThe script is available under the AGPL-3.0.\n\nOlink Proteomics has developed a series of protein biomarker panels, which use a unique technology (Proximity Extension Assay) enabling high-throughput, multiplex immunoassay of proteins that simplify the process of measuring relative protein expression. NPX is Olink\u2019s relative quantification unit of protein expression level in a log2 scale [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\nAfter initial NPX data filtering by excluding samples/assays with QC warning, outlier samples if provided, assays with more than **LOD limit** % of NPX values below limit of detection ([more information](https://www.olink.com/faq/how-is-the-limit-of-detection-lod-estimated-and-handled/)) and replacing NPX values \u200b\u200bbelow LOD with LOD values, the differential expression analysis is performed in two steps. First, the model is fit independently to each assay with NPX as the dependant variable, including variables of interest, covariates to control for any potential confounding and in case of linear mixed effect model a random variable to account for the correlation between samples coming from the same level of the random variable. Significance of model terms are determined with an F-test and p-values from this test are adjusted for multiple testing using the Benjamini-Hochberg method. Assays that have an adjusted F-test p-value less than **FDR cutoff** are then moved on to a post-hoc analysis. This second step of the analysis is done to determine which specific groups differ from each other. These p-values are adjusted using the Tukey method and means are estimated using the emmeans package in R. In order for a post-hoc result to be significant, the assay must pass the global F-test adjusted p-value cut-off and have a Tukey adjusted p-value less than **FDR cutoff**. [3,4]\n\nImportant inputs and parameters for running **OlinkAnalyze DE**:\n\n* **NPX data** file with phenotype data in Olink long format with column names that correspond to the defined model variables is a required input. **NPX data** file can be the one exported from NPX Manager/MyData Cloud software or delivered by Olink Analysis Service. ([more information](https://www.olink.com/our-platform/our-pea-technology/data-generation-and-qc/)). But in that case, no alterations to the output NPX Manager format are allowed and **Phenotype data** is no longer an optional input but a required one, with column names that correspond to the defined model variables. In addition, **Phenotype data** must have the SampleID column populated with IDs identical to the Sample IDs listed in the **NPX data** file before any data preprocessing.\n* **Variable(s) of interest** for the analysis is a required parameter. It represents variable(s) according to which the samples will be grouped. It needs to match either column name(s) in the provided **NPX data** or **Phenotype data**. If more than one name is provided, the included variable names will be used in crossed (interaction) analyses.\n* **Covariates**  is an optional parameter, an array of strings representing the names of potential confounders. Must correspond to column names in the supplied **NPX data** or **Phenotype data** file. For covariates, crossed analyses need to be specified explicitly. Takes ':' or '*' notation.\n* If **Random variable** is provided, linear mixed effects model will be fitted to each assay. If not, ANOVA model will be fitted. Must correspond to column name in the supplied **NPX data** or **Phenotype data** file.\n* **Posthoc effect** is the model term on which to perform the post-hoc analysis. Must be subset of or identical to the **Variable(s) of interest**. In crossed analysis, interaction effect will be default. Otherwise, variable of interest will be default.\n\nAs a result of an analysis, the tool will output:\n\n* **OlinkAnalyze DE HTML report** with results of a differential expression analysis on **NPX data**. It contains input data summary after filtering, summary of significance model term(s) with a histogram of p-values, a post-hoc analysis result table with volcano plots, and additional plots for identified differentially expressed assays: boxplots (or point-range plots) and heatmaps. The post-hoc table in the analysis report will only show comparisons from assays that pass the global F-test adjusted p-value cutoff. If no assays pass this threshold, post-hoc results for the top 10 assays with the smallest p-values will be displayed. \n* Tables of the model and the post-hoc test results for all assays. Explanation of column names of these tables can be found in the [OlinkAnalyze vignette](https://github.com/Olink-Proteomics/OlinkRPackage).\n\n\n### Changes Introduced by Seven Bridges\n\n* The script covers use cases given in reference [4]. If **Random variable** is specified, linear mixed effect model will be fitted to each assay, otherwise, linear model will be fitted. \n* Functions from OlinkAnalyze related to modelling and post-hoc analysis have been modified so that **FDR cutoff** can also be defined by the user. In addition, a parameter for coloring by variable has been added to the OlinkAnalyze boxplot function.\n\n\n### Common Issues and Important Notes\n\n* If **NPX data** file is the one exported from NPX Manager, no alterations to the file should be made. \n* **Phenotype data** file, if provided, must contain SampleID column name with IDs identical to the ones present in the **NPX data** file before preprocessing.\n* Model variables must correspond to column names in the provided **NPX data** or **Phenotype data**. Note that column names are case sensitive.\n* In order for a post-hoc result to be significant, the assay must pass the global F-test adjusted p-value cut-off and have a Tukey adjusted p-value less than **FDR cutoff**.\n* Post-hoc results with p-values less than **FDR cutoff** for an assay that did not pass the global F-test p-value threshold, should not be considered statistically significant. This is necessary for properly controlling the Type I error rate [3]. \n* In crossed analysis, interaction effect will be the default effect for post-hoc testing because results may be misleading if any other main effect is specified due to involvement in interactions [5]. \n* Numerical variables are not converted to factors. If a numerical variable is to be used as a factor, this conversion needs to be done before running **OlinkAnalyze DE** [2].\n\n\n### Limitations\n\nThe tool was tested on a dummy dataset contained in the OlinkAnalyze package that resembles Olink data generated with the Target 96 platform accompanied by clinical variables. As on a public COVID-19 data, provided by the MGH Emergency Department COVID-19 Cohort (Filbin, Goldberg, Hacohen) with Olink Proteomics, generated with the Olink Explore 1536 platform ([4]).\n\n\n### Performance Benchmarking\n\nThe execution time for running **OlinkAnalyze DE** takes under 10 minutes on an on-demand (c4.2xlarge) AWS instance costing less than $0.1. \n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n\n### Portability\n\n**OlinkAnalyze DE** was tested with cwltool version 3.1.20211107152837. The 'in_npx_file', 'variable', 'covariates', 'random_var', 'sample_warning', 'out_prefix' inputs were provided in the job.yaml file and used for testing.\n\n\n###References\n\n[1] [Olink proteomics](https://www.olink.com/)\n\n[2] [OlinkAnalyze R package](https://github.com/Olink-Proteomics/OlinkRPackage)\n\n[3] [Olink Insights Stat Analysis](https://www.olink.com/products-services/data-analysis-products/insights-stat-analysis-app/)\n\n[4] [Longitudinal proteomic analysis of severe COVID-19 reveals survival-associated signatures, tissue-specific cell death, and cell-cell interactions](https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(21)00115-4)\n\n[5] [Interaction analysis in emmeans](https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html)", "input": [{"name": "NPX data", "encodingFormat": "text/plain"}, {"name": "Phenotype data", "encodingFormat": "text/plain"}, {"name": "Output filenames prefix"}, {"name": "Outlier samples"}, {"name": "Exclude samples with warning"}, {"name": "Exclude assays with warning"}, {"name": "LOD limit"}, {"name": "Replace data below LOD"}, {"name": "FDR cutoff"}, {"name": "Variable(s) of interest"}, {"name": "Covariates"}, {"name": "Random variable"}, {"name": "Posthoc effect"}, {"name": "Number of proteins to plot"}, {"name": "Number of proteins per plot"}], "output": [{"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Model results"}, {"name": "Post-hoc test results"}, {"name": "RData file"}, {"name": "Filtered NPX data in long format"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Olink-Proteomics/OlinkRPackage"], "applicationSubCategory": ["Proteomics", "Differential Expression", "CWLtool Tested"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648049240, "dateCreated": 1642604587, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/olinkanalyze-qc-2-0-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/olinkanalyze-qc-2-0-0/4", "applicationCategory": "CommandLineTool", "name": "OlinkAnalyze QC", "description": "**OlinkAnalyze QC** generates a quality control and exploratory analysis report on Olink NPX data. It is a wrapper around the script based mostly on the functions provided in the OlinkAnalyze R package. This package is developed by the Olink data science team to facilitate NPX data handling. \n\nThe script is available under the AGPL-3.0.\n\nOlink Proteomics has developed a series of protein biomarker panels, which use a unique technology (Proximity Extension Assay) enabling high-throughput, multiplex immunoassay of proteins that simplify the process of measuring relative protein expression. NPX, Normalised Protein eXpression, is Olink\u2019s relative quantification unit of protein expression level in a log2 scale [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**OlinkAnalyze QC** is used for an initial QC plotting and inspection of **NPX data**. Its report contains a short data summary and the following visualisations:\n* Sample-wise expression distribution plots per panel: Boxplots with sample ID\u2019s along the x-axis and NPX values along the y-axis colored by **Color variable**.\n* Median vs IQR (QC) plots: Scatterplots per panel showing the median and the IQR (inter-quartile range defined as the difference between the 75th and 25th percentiles) for all samples. Horizontal and vertical dashed lines indicate +/- specified number standard deviations of all sample medians (x-axis) and IQR\u2019s (y-axis).  Data points are colored using **Color variable** [2].\n* PCA plots: Principal component analysis (PCA) is performed on the NPX data file and all samples are plotted as a scatterplot along the selected principal components. All assay distributions are centered at 0 and scaled to have a standard deviation of 1 and proteins with missing NPX values are removed from the corresponding assay(s) before performing the PCA analysis. Data points are colored using **Color variable**. [2].\n* Heatmaps:  Hierarchical clustering based on centered and scaled NPX values is performed on both samples and assays to determine row and column ordering. The centered and scaled NPX values are then used to color the heatmaps. Column side colors are added to the figure based on **Color variable**.\n\nIf **NPX data** is generated using the Olink Explore platform or the number of samples is greater than 200, expression distribution plots and heatmaps are omitted from the report. However, interactive versions of these plots are still available as tool outputs.\n\nTo run **OlinkAnalyze QC**,  **NPX data** file is a required input. This file is either exported from Olink NPX Manger/MyData Cloud software or delivered by Olink Analysis Service. Generally, Olink delivers data in two formats: wide and long, and the OlinkAnalyze package provides a function to import a wide-format NPX file and convert the data to the long-format required for further analysis. To be able to use the OlinkAnalyze read NPX data function, no alterations to the output NPX Manager format are allowed.\n\nImportant optional input, parameters, output:\n* **Phenotype data** is an optional input file with phenotype data for all samples. Note that the SampleID column must be present in the **Phenotype data** file and that IDs in the SampleID column must be identical to the Sample IDs listed in the **NPX data** file. All additional columns can have any names and can be used as **Color variable**. If provided, this file is merged with **NPX data** in long format for further statistical analysis.\n* **Color variable** is an optional parameter used for coloring data in each visualization. The default value is the **NPX data** file QC_Warning column indicating whether the sample passed Olink QC or not. If **Phenotype data** is provided, phenotype column names can be used as **Color variable**.\n*  **IQR** and **Median outlier thresholds** are the numbers of standard deviations from the mean IQR and sample median, respectively, that define an outlier on the QC plot. \n* PCs outlier thresholds are the numbers of standard deviations along the principal components plotted on the x-axis and on the y-axis, respectively, that define an outlier. Both parameters must be specified to be able to label outliers on the PCA plot.\n* **Outlier samples** file is one of the tool outputs if there are samples marked as outliers on the QC plots (outside IQR or median outlier thresholds) in at least **Panel number threshold** panel or on the PCA plot(s) (outside PCs outlier thresholds) in at least **Panel number threshold** panel (if **Perform the PCA per panel** is set to TRUE).\n\n\n### Changes Introduced by Seven Bridges\n\n* None\n\n\n### Common Issues and Important Notes\n\n* No modifications to the **NPX data** file exported from NPX Manager should be made.\n* **Phenotype data** file must contain SampleID column name with IDs identical to the ones present in the **NPX data** file. Note that this column name is case sensitive.\n* **PC x-axis outlier threshold** and **PC y-axis outlier threshold** options must be used together to mark outliers on the PCA plot. If exactly one of these options is used, **OlinkAnalyze QC** will fail.\n\n### Limitations\n\nThe tool was tested on a dummy dataset contained in the OlinkAnalyze package that resembles Olink data generated with the Target 96 platform accompanied by clinical variables. As on a public COVID-19 data, provided by the MGH Emergency Department COVID-19 Cohort (Filbin, Goldberg, Hacohen) with Olink Proteomics, generated with the Olink Explore 1536 platform ([4]).\n\n\n### Performance Benchmarking\n\nThe execution time for running **OlinkAnalyze QC** takes under 10 minutes (even with a big number of samples and assays e.g. 784 samples and 1429 assays) on an on-demand (c4.2xlarge) AWS instance, costing less than $0.1. \n\n*Cost can be significantly reduced by using\u00a0**spot instances**. Visit the\u00a0[knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances)\u00a0for more details.*\n\n\n### Portability\n\n**OlinkAnalyze QC** was tested with cwltool version 3.1.20211107152837. The 'in_npx_file', 'color_variable', 'pca_bypanel', 'outlier_pcx', 'outlier_pcy' , 'out_prefix' inputs were provided in the job.yaml file and used for testing.\n\n\n###References\n\n[1] [Olink proteomics](https://www.olink.com/)\n\n[2] [OlinkAnalyze R package](https://github.com/Olink-Proteomics/OlinkRPackage)", "input": [{"name": "NPX data", "encodingFormat": "text/plain"}, {"name": "Phenotype data", "encodingFormat": "text/plain"}, {"name": "Output filenames prefix"}, {"name": "Color variable"}, {"name": "Label outliers"}, {"name": "IQR outlier threshold"}, {"name": "Median outlier threshold"}, {"name": "Draw outlier lines on the QC plot"}, {"name": "Panel number threshold"}, {"name": "Principal component to plot along the x-axis"}, {"name": "Principal component to plot along the y-axis"}, {"name": "Label samples"}, {"name": "Drop assays"}, {"name": "Drop samples"}, {"name": "Number of loadings to plot"}, {"name": "List of OlinkIDs"}, {"name": "Perform the PCA per panel"}, {"name": "PC x-axis outlier threshold"}, {"name": "PC y-axis outlier threshold"}, {"name": "Draw outlier lines on the PCA plot"}], "output": [{"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Outlier samples"}, {"name": "NPX file in long format", "encodingFormat": "text/plain"}, {"name": "RData file"}, {"name": "Interactive distribution plots", "encodingFormat": "text/html"}, {"name": "Interactive heatmaps", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Olink-Proteomics/OlinkRPackage"], "applicationSubCategory": ["Proteomics", "Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648049240, "dateCreated": 1642604835, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/oncofuse-1-1-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/oncofuse-1-1-0/10", "applicationCategory": "CommandLineTool", "name": "Oncofuse", "description": "This tool predicts the oncogenic potential of fusion genes found by NGS in cancer cells. It is a post-processing step that tries to validate in-silico the predictions made by fusion detection software. Oncofuse is NOT a fusion detection software: its goal is NOT to identify fusion sequences but to assign a functional prediction score (oncogenic potential, for instance, the probability of being 'driver' event) to fusion sequences identified by certain fusion finder. Oncofuse is a naive bayesian classifier built using information from Shugay et al. 2012 and is described in Shugay et al. 2013.\n\nOncofuse can directly validate fusions obtained by following fusion finders (input\\_type): Tophat-fusion (tophat), FusionCatcher software (fcathcer), RNASTAR (rnastar), STAR-Fusion (starfusion). Beside a .txt file that contains fusions (but does not contain tissue of origin), input_type is required input as well as tissue\\_type. There are four pre-built libraries, corresponding to the four supported tissue types: EPI (epithelial origin), HEM (hematological origin), MES (mesenchymal origin) and AVG (average expression, if tissue source is unknown). \n\nOncofuse can be also used for processing outputs of fusion finders not listed above. For this purpose provided input file has to be tab-delimited file with lines containing 5' and 3' breakpoint positions (first nucleotide lost upon fusion) and tissue of origin. You can find more about appropriate file format at http://www.unav.es/genetica/oncofuse.html. It is necessary to set  \"coord\" as input_type format and to set \"-\" as tissue type  (as tissue of origin is already included in a file). File parser (SBG Bedpe4Oncofuse) for ChimneraScan tool fusion finder can be found on our Platform and used prior to Oncofuse.\n\n###Common Issues###\nIf one of the listed fusion finders is set as input_type, origin tissue type (tissue_type) has to be set to EPI, HEM, MES or AVG, otherwise task will fail. \nIf input_type is set to \"coord\", but tissue\\_type is set to EPI, HEM, MES or AVG instead of \"-\" task will fail.\n\nPaper:\nMikhail Shugay, Inigo Ortiz de Mend\u00edbil, Jose L. Vizmanos and Francisco J. Novo. Oncofuse: a computational framework for the prediction of the oncogenic potential of gene fusions. Bioinformatics. 16 Aug 2013. doi:10.1093/bioinformatics/btt445.", "input": [{"name": "Tissue type"}, {"name": "Input type"}, {"name": "Chimeric file", "encodingFormat": "text/plain"}, {"name": "Genome assembly version"}, {"name": "Number of threads"}], "output": [{"name": "Oncofuse output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/mikessh/oncofuse", "https://github.com/mikessh/oncofuse/releases/tag/1.1.1", "https://github.com/mikessh/oncofuse"], "applicationSubCategory": ["Prioritization"], "project": "SBG Public Data", "creator": "Mikhail Shugay (mikhail.shugay@gmail.com)", "softwareVersion": ["sbg:draft-2"], "dateModified": 1478183224, "dateCreated": 1453799767, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/oncogemini-bottleneck-1-0-0-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/oncogemini-bottleneck-1-0-0-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "OncoGEMINI Bottleneck", "description": "**OncoGEMINI Bottleneck** identifies somatic variants with increasing allele frequency in longitudinal data [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\nAll **OncoGEMINI** tools require an **OncoGEMINI database** file, which should be prepared using **Vcf2db**, starting from an annotated VCF file and a samples manifest file (in place of the pedigree file usually used by **Vcf2db**). The annotated VCF file should contain somatic variants (please consider excluding germline variants; if germline variants are present, please consult the tool documentation [2] for workarounds) for one or more tumor samples with the associated normal sample, if present. The file can be annotated with **VEP**, **SnpEff** or **Vcfanno**. Cancer Related Annotations Bin repository [3] contains an example **Vcfanno** configuration file and useful annotation data sources.\n\n**OncoGEMINI Bottleneck** takes an **OncoGEMINI database** file, usually with longitudinal data, and reports variants with increasing AFs over time.\n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--help` was omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* **OncoGEMINI database** input is required and should be prepared as described in the Common Use Cases section.\n* **Only include variants marked as somatic** input parameter should only be used on database files prepared with the `oncogemini set_somatic` utility.\n* **Restrict results to specified cancer types** input requires that the **OncoGEMINI database** file includes `civic_gene_abbrevations` and/or `cgi_gene_abbreviations`. Cancer abbreviations which this input accepts can be found [at the CRAB GitHub repository](https://github.com/fakedrtom/crab/blob/master/cancer_names_abbreviations.txt).\n* If multiple patients are present in the **OncoGEMINI database** file, the patient to analyze must be specified using the **Patient to filter** input.\n\n### Performance Benchmarking\n\nTypical tasks last <5 minutes (< $0.04) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**OncoGEMINI Bottleneck** was tested with cwltool version 3.1.20211107152837. The `in_oncogemini_db` and `patient` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [OncoGEMINI publication](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-021-00854-6)\n\n[2] [OncoGEMINI documentation](https://github.com/fakedrtom/oncogemini#readme)\n\n[3] [CRAB GitHub repository](https://github.com/fakedrtom/crab)", "input": [{"name": "OncoGEMINI database"}, {"name": "Minimum depth required in all samples"}, {"name": "Minimum genotype quality required in all samples"}, {"name": "Maximum normal sample AF"}, {"name": "Minimum slope for AFs across samples"}, {"name": "Minimum r correlation coefficient for AFs"}, {"name": "Samples to include"}, {"name": "Minimum AF of the final timepoint sample"}, {"name": "Minimum required AF time series difference"}, {"name": "Patient to filter"}, {"name": "Columns to return"}, {"name": "Restrictions to apply to variants"}, {"name": "Use purity estimates"}, {"name": "Only include variants marked as somatic"}, {"name": "Restrict results to specified cancer types"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}], "output": [{"name": "Bottleneck variants"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/fakedrtom/oncogemini", "https://github.com/fakedrtom/oncogemini/tree/master/oncogemini", "https://github.com/fakedrtom/oncogemini/releases/tag/v1.0.0", "https://github.com/fakedrtom/oncogemini#readme"], "applicationSubCategory": ["Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Thomas J Nicholas", "softwareVersion": ["v1.1"], "dateModified": 1648040158, "dateCreated": 1631622402, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/oncogemini-loh-1-0-0-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/oncogemini-loh-1-0-0-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "OncoGEMINI Loh", "description": "**OncoGEMINI Loh** performs loss of heterozygosity analysis  [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\nAll **OncoGEMINI** tools require an **OncoGEMINI database** file, which should be prepared using **Vcf2db**, starting from an annotated VCF file and a samples manifest file (in place of the pedigree file usually used by **Vcf2db**). The annotated VCF file should contain somatic variants (please consider excluding germline variants; if germline variants are present, please consult the tool documentation [2] for workarounds) for one or more tumor samples with the associated normal sample, if present. The file can be annotated with **VEP**, **SnpEff** or **Vcfanno**. Cancer Related Annotations Bin repository [3] contains an example **Vcfanno** configuration file and useful annotation data sources.\n\n**OncoGEMINI Loh** takes an **OncoGEMINI database** file and identifies variants which appear as homozygous in tumor samples but appear heterozygous in the normal sample. A normal sample must be included in the **OncoGEMINI database**.\n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--help` was omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* **OncoGEMINI database** input is required and should be prepared as described in the Common Use Cases section.\n* **Restrict results to specified cancer types** input requires that the **OncoGEMINI database** file includes `civic_gene_abbrevations` and/or `cgi_gene_abbreviations`. Cancer abbreviations which this input accepts can be found [at the CRAB GitHub repository](https://github.com/fakedrtom/crab/blob/master/cancer_names_abbreviations.txt).\n* A normal sample must be included in the **OncoGEMINI database** input file.\n* If multiple patients are present in the **OncoGEMINI database** file, the patient to analyze must be specified using the **Patient to filter** input.\n\n### Performance Benchmarking\n\nTypical tasks last <5 minutes (< $0.04) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**OncoGEMINI Loh** was tested with cwltool version 3.1.20211107152837. The `in_oncogemini_db` and `patient` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [OncoGEMINI publication](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-021-00854-6)\n\n[2] [OncoGEMINI documentation](https://github.com/fakedrtom/oncogemini#readme)\n\n[3] [CRAB GitHub repository](https://github.com/fakedrtom/crab)", "input": [{"name": "OncoGEMINI database"}, {"name": "Minimum depth required in all samples"}, {"name": "Minimum genotype quality required in all samples"}, {"name": "Maximum normal sample AF"}, {"name": "Samples to include"}, {"name": "Patient to filter"}, {"name": "Columns to return"}, {"name": "Restrictions to apply to variants"}, {"name": "Use purity estimates"}, {"name": "Restrict results to specified cancer types"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Minimum normal sample AF"}, {"name": "Minimum AF for tumor samples"}, {"name": "Search for LOH variants in a single sample"}], "output": [{"name": "Loh results"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/fakedrtom/oncogemini", "https://github.com/fakedrtom/oncogemini/tree/master/oncogemini", "https://github.com/fakedrtom/oncogemini/releases/tag/v1.0.0", "https://github.com/fakedrtom/oncogemini#readme"], "applicationSubCategory": ["Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Thomas J Nicholas", "softwareVersion": ["v1.1"], "dateModified": 1648040158, "dateCreated": 1631622354, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/oncogemini-truncal-1-0-0-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/oncogemini-truncal-1-0-0-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "OncoGEMINI Truncal", "description": "**OncoGEMINI Truncal** recovers variants appearing in all tumor samples, but absent in the normal sample [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\nAll **OncoGEMINI** tools require an **OncoGEMINI database** file, which should be prepared using **Vcf2db**, starting from an annotated VCF file and a samples manifest file (in place of the pedigree file usually used by **Vcf2db**). The annotated VCF file should contain somatic variants (please consider excluding germline variants; if germline variants are present, please consult the tool documentation [2] for workarounds) for one or more tumor samples with the associated normal sample, if present. The file can be annotated with **VEP**, **SnpEff** or **Vcfanno**. Cancer Related Annotations Bin repository [3] contains an example **Vcfanno** configuration file and useful annotation data sources.\n\n**OncoGEMINI Truncal** takes an **OncoGEMINI database** file containing one or more tumor samples and a normal sample and returns variants detected in all tumor samples, but seemingly absent from the normal sample (determined based on the associated AFs) [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--help` was omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* **OncoGEMINI database** input is required and should be prepared as described in the Common Use Cases section. This file must contain a normal sample for the tool to work.\n* **Only include variants marked as somatic** input parameter should only be used on database files prepared with the `oncogemini set_somatic` utility.\n* **Restrict results to specified cancer types** input requires that the **OncoGEMINI database** file includes `civic_gene_abbrevations` and/or `cgi_gene_abbreviations`. Cancer abbreviations which this input accepts can be found [at the CRAB GitHub repository](https://github.com/fakedrtom/crab/blob/master/cancer_names_abbreviations.txt).\n* If multiple patients are present in the **OncoGEMINI database** file, the patient to analyze must be specified using the **Patient to filter** input.\n\n### Performance Benchmarking\n\nTypical tasks last <5 minutes (< $0.04) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**OncoGEMINI Truncal** was tested with cwltool version 3.1.20211107152837. The `in_oncogemini_db` and `patient` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [OncoGEMINI publication](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-021-00854-6)\n\n[2] [OncoGEMINI documentation](https://github.com/fakedrtom/oncogemini#readme)\n\n[3] [CRAB GitHub repository](https://github.com/fakedrtom/crab)", "input": [{"name": "OncoGEMINI database"}, {"name": "Minimum depth required in all samples"}, {"name": "Minimum genotype quality required in all samples"}, {"name": "Maximum normal sample AF"}, {"name": "Samples to include"}, {"name": "Patient to filter"}, {"name": "Columns to return"}, {"name": "Restrictions to apply to variants"}, {"name": "Use purity estimates"}, {"name": "Only include variants marked as somatic"}, {"name": "Restrict results to specified cancer types"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Amount to increase truncal AF filter"}], "output": [{"name": "Truncal variants"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/fakedrtom/oncogemini", "https://github.com/fakedrtom/oncogemini/tree/master/oncogemini", "https://github.com/fakedrtom/oncogemini/releases/tag/v1.0.0", "https://github.com/fakedrtom/oncogemini#readme"], "applicationSubCategory": ["Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Thomas J Nicholas", "softwareVersion": ["v1.1"], "dateModified": 1648040158, "dateCreated": 1631622324, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/oncogemini-unique-1-0-0-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/oncogemini-unique-1-0-0-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "OncoGEMINI Unique", "description": "**OncoGEMINI Unique** identifies somatic variants unique to a subset of samples [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\nAll **OncoGEMINI** tools require an **OncoGEMINI database** file, which should be prepared using **Vcf2db**, starting from an annotated VCF file and a samples manifest file (in place of the pedigree file usually used by **Vcf2db**). The annotated VCF file should contain somatic variants (please consider excluding germline variants; if germline variants are present, please consult the tool documentation [2] for workarounds) for one or more tumor samples with the associated normal sample, if present. The file can be annotated with **VEP**, **SnpEff** or **Vcfanno**. Cancer Related Annotations Bin repository [3] contains an example **Vcfanno** configuration file and useful annotation data sources.\n\n**OncoGEMINI Unique** takes an **OncoGEMINI database** file and by default returns variants with AF > 0 in selected samples (**Samples to identify unique variants from**) and AF = 0 in the remaining samples in the input database file.\n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--help` was omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* **OncoGEMINI database** and  **Samples to identify unique variants from** inputs are required.\n* **Only include variants marked as somatic** input parameter should only be used on database files prepared with the `oncogemini set_somatic` utility.\n* **Restrict results to specified cancer types** input requires that the **OncoGEMINI database** file includes `civic_gene_abbrevations` and/or `cgi_gene_abbreviations`. Cancer abbreviations which this input accepts can be found [at the CRAB GitHub repository](https://github.com/fakedrtom/crab/blob/master/cancer_names_abbreviations.txt).\n* If multiple patients are present in the **OncoGEMINI database** file, the patient to analyze must be specified using the **Patient to filter** input.\n\n### Performance Benchmarking\n\nTypical tasks last <5 minutes (< $0.04) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**OncoGEMINI Unique** was tested with cwltool version 3.1.20211107152837. The `in_oncogemini_db`, `patient` and `specific` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [OncoGEMINI publication](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-021-00854-6)\n\n[2] [OncoGEMINI documentation](https://github.com/fakedrtom/oncogemini#readme)\n\n[3] [CRAB GitHub repository](https://github.com/fakedrtom/crab)", "input": [{"name": "OncoGEMINI database"}, {"name": "Minimum depth required in all samples"}, {"name": "Minimum genotype quality required in all samples"}, {"name": "Maximum sample AF in other samples"}, {"name": "Samples to include"}, {"name": "Patient to filter"}, {"name": "Columns to return"}, {"name": "Restrictions to apply to variants"}, {"name": "Use purity estimates"}, {"name": "Only include variants marked as somatic"}, {"name": "Restrict results to specified cancer types"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Samples to identify unique variants from"}, {"name": "Amount to increase AF filter"}], "output": [{"name": "Unique variants"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/fakedrtom/oncogemini", "https://github.com/fakedrtom/oncogemini/tree/master/oncogemini", "https://github.com/fakedrtom/oncogemini/releases/tag/v1.0.0", "https://github.com/fakedrtom/oncogemini#readme"], "applicationSubCategory": ["Variant Filtration", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Thomas J Nicholas", "softwareVersion": ["v1.1"], "dateModified": 1648040158, "dateCreated": 1631622237, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/optitype-adjusted-1-2/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/optitype-adjusted-1-2/15", "applicationCategory": "CommandLineTool", "name": "OptiType adjusted", "description": "**OptiType** is a novel HLA genotyping algorithm based on integer linear programming, capable of producing accurate 4-digit HLA genotyping predictions from NGS data by simultaneously selecting all major and minor HLA Class I alleles [1]. \n\nThere is large interest in calling HLA types from individual samples and OptiType can be used either as an individual tool or as a part of some larger workflow (e.g. workflows dealing with neoantigens). \n\n\n### Common Use Cases\nOptiType can work with genome, exome and RNA data (both single-end and paired-end data).\n\nOptiType receives FASTQ files as inputs and outputs sample HLA types. \n\nOnly **required** filed is to set the tool parameter whether experimental strategy was DNA or RNA. \n\nNot required, but would be recommended to set **sample ID** metadata filed, because tool relies on it for naming. \n\n### Changes Introduced by Seven Bridges\n\n* Adjustment to the tool has been made, so both **4-digit and 8-digit HLA type** resolutions are outputted.\n* Also, there has been added option for unique HLA types, so the tool can output only list of unique HLA types. This is applicable only to **HLA types** output.\n* Tool names output files based on timestamp. This was changed in tool wrapper, so the files are renamed according to **sample ID** field in metadata.\n*  OptiType depends on RazerS 3 for read mapping. RazerS 3 is designed in such way that it loads all reads into the memory. This was the most often reason for the tool failure when working with larger files. Because of this, automatization had been added to the tool's CWL wrapper. Now, depending on the input file size, tool will require necessary amount of memory in order to perform well. This is usually size of the larger FASTQ file on input or for gunzipped files 5 times the size of the larger file (because of the compression).\n\n### Common Issues and Important Notes\n\n*  **Exome kit hadn't captured any reads from HLA region** - \ndepending on the exome kit/experiment, it might cause the tool failure. Some exome kit's wont capture reads from HLA region, and after the  filtering which comes after the alignment step, it might result missing or empty BAM files. This will cause the tool to break, since there are no reads that will go to the second step.\n\n### Performance Benchmarking\n\n| Experiment type | Input size | Duration | Cost | Instance |\n|-----------------------|-----------------|------------|-------------|---------------|\n| Exome         | 2 x 2.7   GB    | 22 min   | $0.4           | c4.2xlarge     |\n| Exome         | 2 x 10.5 GB    | 30 min   | $0.4           | c4.2xlarge     |\n| Exome         | 2 x 17.7 GB    | 12 min   | $0.8           | c4.4xlarge     |\n| Exome         | 2 x 25.5 GB    | 13 min   | $0.8           | c4.4xlarge     |\n| Exome         | 2 x 33.3 GB    | 16 min   | $1.3           | r3.4xlarge     |\n| Exome         | 2 x 45    GB    | 22 min   | $1.3           | r3.4xlarge     |\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*       \n\n### References\n\n[1] [OptiType home page](https://github.com/FRED-2/OptiType)", "input": [{"name": "Sequencing data"}, {"name": "Verbose"}, {"name": "Input files", "encodingFormat": "text/fastq"}, {"name": "Beta"}, {"name": "Enumerate"}, {"name": "Number of threads for mapping"}, {"name": "Delete intermediate bam"}, {"name": "Use reads with only one pair mapped"}, {"name": "Use discordant read pairs"}, {"name": "Number of threads for ILP solver"}, {"name": "Unique HLA types"}], "output": [{"name": "HLA results"}, {"name": "BAM files", "encodingFormat": "application/x-bam"}, {"name": "Log file", "encodingFormat": "text/plain"}, {"name": "Coverage plot"}, {"name": "HLA Types"}, {"name": "Full HLA types"}, {"name": "Config output"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/FRED-2/OptiType", "https://github.com/FRED-2/OptiType", "https://github.com/FRED-2/OptiType/releases/tag/v1.2.1"], "applicationSubCategory": ["HLA Typing"], "project": "SBG Public Data", "creator": "Andr\u00e1s Szolek, Benjamin Schubert, Christopher Mohr", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038486, "dateCreated": 1500294202, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/optitype-1-2-cwl1-0/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/optitype-1-2-cwl1-0/12", "applicationCategory": "CommandLineTool", "name": "OptiType CWL1.0", "description": "OptiType is a novel HLA genotyping algorithm that accepts single-end or paired-ends reads in FASTQ file/s or aligned reads in a BAM file. \nIt is based on integer linear programming, capable of producing accurate 4-digit HLA genotyping predictions from NGS data by simultaneously selecting all major and minor HLA Class I alleles. \n\n**Parameters:**\n\n- **Beta (\u03b2)** - The regularization term is weighted by a constant  representing the proportion of reads that have to be additionally explained by an allele combination to choose a heterozygous solution over a homozygous one. Default value: 0.009\n\n- **Use discordant read pairs** - We call a read pair discordant if its two ends best-map to two disjoint sets of alleles. Such reads can be either omitted or either of their ends treated as unpaired hits.\n\n- **Use reads with only one pair mapped** - This setting allows the user to keep them with an optionally reduced weight. Takes real values between 0 and 1.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases ###\n\n\n- When provided a pair of FASTQ files, the tool will determine the required resources for the task execution. Assuming the reads have decent quality and coverage the tool will perform well.\n\n### Changes introduced by SBG ###\n\n- Original OptiType script evaluates full resolution HLA types and returns the trimmed allele identifications to 4 digit resolution. Adjustment has been made to the tool, so it outputs the HLA alleles both before and after the trimming as separate outputs. Therefore, our tool results in both 4-digit and 8-digit HLA types.\n\n### Common Issues ###\n\n- **Specific BAM files**\n\nOptitype might fail to execute with some BAM files. So far we know it always accepts BAM files created by Yara aligner.\n\n- **Tool automatization**\n\nOptiType depends on RazerS 3 for read mapping. RazerS 3 is designed in such a way that it loads all reads into the memory. This was the most frequent reason for failure of the tool when working with larger files. Because of this, automatization has been added to the tool's CWL wrapper. Now, depending on the input file size, the tool will require the necessary amount of memory in order to perform well. This is usually the size of the larger FASTQ file on input or, for gunzipped files, 5 times the size of the larger file (because of the compression).\n\n\n- **Exome kit hadn't captured any reads from the HLA region**\n\nIf the exome kit/experiment is inadequate, it might cause the tool to fail. Some exome kits won\u2019t capture reads from the HLA region. After the filtering, which comes after the alignment step, this might result in missing/empty BAM files. This will cause the tool to break, since there are no reads that will go to the second step.\n\n\n### Benchmarking ###\n\nSeveral experiment task were performed on the Platform using pairs of FASTQ files in .fastq.gz (gzip) format. The table below shows the results:\n\n| Experiment type | Input size | Paired-end | Read length | Duration | Execution cost | Instance type |\n| ---------------- | ---------- | ---------- | ------------- | -------- | ----- | --------- | ----------------------: |\n| WXS| 2 x 2.8 GB | Yes | 30M | 6min. | $0.09 | c5.4xlarge |\n| WXS| 2 x 4.65 GB | Yes | 112M | 9min. | $0.15 | m5.4xlarge  |\n| WXS| 2 x 14.9 GB | Yes | 194M | 26min. | $0.94 | m4.10xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Input files", "encodingFormat": "text/fastq"}, {"name": "Delete intermediate bam"}, {"name": "Use reads with only one pair mapped [0, 1]"}, {"name": "Use discordant read pairs"}, {"name": "Sequencing data"}, {"name": "Beta"}, {"name": "Enumerate"}, {"name": "Verbose"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Prefix string added to output file names"}, {"name": "Unique HLA types"}, {"name": "Output full HLA type strings"}], "output": [{"name": "HLA results"}, {"name": "Output BAM files", "encodingFormat": "application/x-bam"}, {"name": "Log file", "encodingFormat": "text/plain"}, {"name": "Coverage plot"}, {"name": "HLA Types"}, {"name": "Full HLA results"}, {"name": "Config output"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FRED-2/OptiType", "https://github.com/FRED-2/OptiType/releases/tag/v1.2.1"], "applicationSubCategory": ["HLA Typing"], "project": "SBG Public Data", "creator": "Andr\u00e1s Szolek, Benjamin Schubert, Christopher Mohr", "softwareVersion": ["v1.0"], "dateModified": 1648048380, "dateCreated": 1560936774, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/personal-cancer-genome-reporter-0-9-1-cwl1-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/personal-cancer-genome-reporter-0-9-1-cwl1-1/5", "applicationCategory": "CommandLineTool", "name": "Personal Cancer Genome Reporter", "description": "**Personal Cancer Genome Reporter** is a tool for functional annotation and classification of somatic variants [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Personal Cancer Genome Reporter** can be used to analyze a VCF with somatic variants (**Input variants**) obtained using WES, WGS or targeted sequencing (**DNA sequencing assay**) [1,2]. It is recommended that the **Input variants** input is BGZIP compressed and Tabix indexed. If multiallelic variants are present they will be decomposed. Only variants with FILTER status PASS will be used for reporting [2]. Optionally, CNA segments can be provided (**Somatic copy number alterations**). After optional validation of the input VCF with **vcf-validator**, the variants are annotated with **Variant Effect Predictor** and **Vcfanno** and classified into tiers. Additional analyses (**Estimate TMB**, **Estimate microsatellite instability status** and **Estimate mutational signatures**) can be requested. The tool can process somatic variants obtained from matched tumor-normal pairs or from tumor-only sequencing (with or without a PoN).\n\n**PCGR data sources** input is required for the analysis and should contain the reference data bundle of the tool, matching the **Genome assembly** value provided.\n\n**Personal Cancer Genome Reporter** uses a configuration file in TOML format to control many aspects of the analysis. This file can either be prepared by the user and provided as the **PCGR configuration file in TOML format** input or can be built interactively at task run time, as the tool input parameters beginning with **Conf -** relate to the corresponding fields in the configuration file. If a value for a field is not provided, the field will be populated with the value from the [default tool configuration file](https://github.com/sigven/pcgr/blob/f3dd15cb7ace0a8811226b98867bc1a0876a6c77/conf/pcgr.toml).\n\nPlease note that the outputs of the tool depend on the provided input files and parameters. For example, the interactive HTML reports will not be generated if the **Only annotate [basic mode]** input parameter is used.\n\nThe users are advised to customize the tool executions to match their specific analysis requirements, by specifying the tumor site, any caller-specific VCF tags, tumor purity and ploidy, and any desired filtering criteria. Please consult the tool documentation [2] for details.\n\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--force_overwrite`, `--debug` and `--docker-uid` were omitted from the wrapper as they do not apply to task executions on Seven Bridges platforms.\n* Parameters `--version` and `--help` were omitted from the wrapper.\n* Parameter `--no-docker` was hardcoded in the wrapper and is always applied.\n* Parameters `--pcgr-dir` and `--output-dir` were hardcoded in the wrapper to point to the current working directory of the task.\n* Parameter `--sample-id` is no longer required. If a user provides a value (via the **Sample ID** input) it will be used, otherwise the tool will attempt to read the **Sample ID** metadata field of the provided input VCF file. In the absence of the **Sample ID** metadata field, the input VCF file name will be used as the prefix for the output file names.\n* The configuration file (`--conf`) input is not required with this wrapper, but is still recommended. If the user does not provide a file to use (**PCGR configuration file in TOML format**), the wrapper will attempt to build one from the GUI input parameters. For all unspecified parameters, the values from the tool default example configuration file will be used.\n* To assist in debugging tasks, the tool standard output and error streams are captured in the debug.log file, which is accessible with other task logs. Please consult this log if any issues with the task are encountered (missing outputs or errors).\n\n### Common Issues and Important Notes\n\n* **Input variants**, **PCGR data sources** and **Genome assembly** inputs are required. \n* It is recommended that the **Input variants** input is BGZIP compressed and **Tabix** indexed. If a VCF.GZ file is provided as **Input variants**, the corresponding TBI index file must be present in the project. If multiallelic variants are present they will be decomposed. Important note: Only variants with FILTER status PASS will be used for reporting [2].\n* **Cell line sequencing** input requires **Tumor-only sequencing** input to be set.\n* **Only annotate [basic mode]** input parameter disables the creation of interactive HTML reports.\n* The generated outputs and contents of the interactive reports will depend on the provided inputs, specified analyses and input parameters.\n* It is fairly common for somatic VCFs to deviate slightly from the VCF file format specification, but many such files can still be successfully analyzed. If you are unable to obtain outputs for your VCF input, please check the debug.log file to see if the input is failing the validation step. If this is the case and you believe that the reported validation issues can be safely ignored, you can disable the input VCF validation using the **Skip validation of input VCF** input parameter.\n\n### Performance Benchmarking\n\nThe performance of the **Personal Cancer Genome Reporter** depends on the number of variants in the input VCF file, the requested analyses and additional inputs. Typical tasks are expected to complete in <30 minutes. The number of cores assigned to VEP and Vcfanno, either through the user-provided **PCGR configuration file in TOML format** or, in the absence of this file, through the **Conf - Number of Vcfanno processors** and **Conf - Number of VEP forks** input parameters may improve tool performance. The default value for both parameters (4) should be sufficient for most applications. Disk storage requirements of the tool are <100 GB.\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| example COAD with CNA | 13 min | $0.09 + $0.03 | c4.2xlarge 1024 GB EBS |\n| WES Mutect2 VCF.GZ file | 12 min | $0.08 + $0.03 | c4.2xlarge 1024 GB EBS |\n| WGS Mutect2  VCF.GZ file | 22 min | $0.15 + $0.05 | c4.2xlarge 1024 GB EBS |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### References\n\n[1] [PCGR publication](https://academic.oup.com/bioinformatics/article/34/10/1778/4764004)\n\n[2] [PCGR documentation](https://github.com/sigven/pcgr)", "input": [{"name": "PCGR data sources", "encodingFormat": "application/x-tar"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Somatic copy number alterations"}, {"name": "Log ratio-threshold for CNV gain regions"}, {"name": "Log ratio-threshold for homozygous deletion regions"}, {"name": "Mean percent overlap for CNA and transcript"}, {"name": "PON variants", "encodingFormat": "application/x-vcf"}, {"name": "Tumor site"}, {"name": "Estimated tumor purity"}, {"name": "Estimated tumor ploidy"}, {"name": "Minimum tumor sequencing depth"}, {"name": "Minimum tumor variant allelic fraction"}, {"name": "Minimum control sequencing depth"}, {"name": "Maximum control variant allelic frequency"}, {"name": "Target Mb size - mutational burden analysis"}, {"name": "Tumor-only sequencing"}, {"name": "Cell line sequencing"}, {"name": "DNA sequencing assay"}, {"name": "Include clinical trials"}, {"name": "Estimate TMB"}, {"name": "Estimate microsatellite instability status"}, {"name": "Estimate mutational signatures"}, {"name": "TMB algorithm"}, {"name": "Minimum SNPs for mutational signatures"}, {"name": "Use all reference mutational signatures"}, {"name": "Only annotate [basic mode]"}, {"name": "Skip validation of input VCF"}, {"name": "Genome assembly"}, {"name": "PCGR configuration file in TOML format"}, {"name": "Sample ID"}, {"name": "Conf - tumor only - 1000g MAF cutoffs"}, {"name": "Conf - tumor-only - gnomAD MAF cutoffs"}, {"name": "Conf - Exclude PON variants"}, {"name": "Conf - Exclude likely homozygous germline"}, {"name": "Conf - Exclude likely heterozygous germline"}, {"name": "Conf - Exclude dbSNP non-somatic"}, {"name": "Conf - Exclude all non-exonic variants"}, {"name": "Conf - tumor DP tag"}, {"name": "Conf - Tumor allelic fraction tag"}, {"name": "Conf - control DP tag"}, {"name": "Conf - Control allelic fraction tag"}, {"name": "Conf - Call confidence tag"}, {"name": "Conf - Visual theme of the report"}, {"name": "Conf - Custom tags"}, {"name": "Conf - list noncoding"}, {"name": "Conf - Number of Vcfanno processors"}, {"name": "Conf - Number of VEP forks"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Conf - Customize VEP pick order"}, {"name": "Conf - Omit intergenic variants in VEP"}, {"name": "Conf - Generate a MAF for input VCF"}], "output": [{"name": "PCGR HTML report", "encodingFormat": "text/html"}, {"name": "MAF output file"}, {"name": "PASS variants", "encodingFormat": "application/x-vcf"}, {"name": "TSV results"}, {"name": "PCGR Rmd HTML report", "encodingFormat": "text/html"}, {"name": "JSON results"}, {"name": "Mutational signatures"}, {"name": "TSV PASS variants"}, {"name": "CNA segments"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/sigven/pcgr", "https://github.com/sigven/pcgr/tree/master/src", "https://github.com/sigven/pcgr/releases/tag/v0.9.1", "https://github.com/sigven/pcgr/blob/master/README.md"], "applicationSubCategory": ["Annotation", "Variant Filtration"], "project": "SBG Public Data", "creator": "Sigve Nakken", "softwareVersion": ["v1.1"], "dateModified": 1648219355, "dateCreated": 1648219355, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-addorreplacereadgroups-2-25-7-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-addorreplacereadgroups-2-25-7-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "Picard AddOrReplaceReadGroups", "description": "**Picard AddOrReplaceReadGroups** assigns all reads to the specified read group [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard AddOrReplaceReadGroups** can be used to assign a single read group to all reads in an alignments file (in SAM or BAM format). Pre-existing RG values in the file will be lost [1].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--QUIET`, `--TMP_DIR`, `--version` were omitted from the wrapper.\n* GA4GH URLs are not supported as tool inputs.\n* Parameter `--OUTPUT` is optional in this wrapper. If the value for the **Output file name prefix** input parameter is not provided, the outputs will be named based on the **Sample ID** metadata field value of the **Input alignments** file, or if this is absent, following the base name of the **Input alignments** file.\n\n### Common Issues and Important Notes\n\n* **Input alignments**, **Read group library**, **Read group platform**, **Read group platform unit** and **Read group sample name** inputs are required.\n* Tag values must follow the SAM specification with the regex `'^[ -~]+$'`, with only `<Space>` allowed from non-printing characters [1].\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 BAM file (48.4 GB) | 161 mins | $1.07 + $0.37 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard AddOrReplaceReadGroups** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `read_group_library`, `read_group_platform`, `read_group_platform_unit` and `read_group_sample_name` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard AddOrReplaceReadGroups documentation](https://broadinstitute.github.io/picard/command-line-overview.html#AddOrReplaceReadGroups)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Output file name prefix"}, {"name": "Read group library"}, {"name": "Read group platform"}, {"name": "Read group platform unit"}, {"name": "Read group sample name"}, {"name": "Compression level"}, {"name": "Create index"}, {"name": "Create MD5 digest file"}, {"name": "Maximum records in RAM"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Read group sequencing center"}, {"name": "Read group description"}, {"name": "Read group run date"}, {"name": "Read group flow order"}, {"name": "Read group ID"}, {"name": "Read group key sequence"}, {"name": "Read group program group"}, {"name": "Read group predicted insert size"}, {"name": "Read group platform model"}, {"name": "Sort order"}, {"name": "Use JDK deflater"}, {"name": "Use JDK inflater"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}], "output": [{"name": "New RG alignments file", "encodingFormat": "application/x-sam"}, {"name": "MD5 digest file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044989, "dateCreated": 1636465275, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-bedtointervallist-2-25-7-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-bedtointervallist-2-25-7-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "Picard BedToIntervalList", "description": "**Picard BedToIntervalList** converts a BED file to a Picard INTERVAL_LIST format [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard BedToIntervalList** can be used to obtain an INTERVAL_LIST file required by some **Picard** tools, from a starting BED file. \n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--COMPRESSION_LEVEL`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER`, `--VALIDATION_STRINGENCY` and `--version` were omitted from the wrapper.\n* Parameter `--OUTPUT` corresponding to the **Output file name prefix** input is no longer required. If a value is not provided by the user, the output will be named based on the **Input BED file**.\n\n### Common Issues and Important Notes\n\n* **Input BED file** and **Sequence dictionary** inputs are required.\n\n\n### Performance Benchmarking\n\nTypical tasks take 2-3 minutes (<$0.1 cost) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard BedToIntervalList** was tested with cwltool version 3.1.20210628163208. The `in_intervals` and `in_sequence_dictionary` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard BedToIntervalList documentation](https://broadinstitute.github.io/picard/command-line-overview.html#BedToIntervalList)", "input": [{"name": "Input BED file", "encodingFormat": "text/x-bed"}, {"name": "Sort the output interval list"}, {"name": "Unique output intervals"}, {"name": "Sequence dictionary", "encodingFormat": "application/x-sam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Max records in RAM"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Output file name prefix"}, {"name": "Drop missing contigs"}, {"name": "Verbosity of logging"}], "output": [{"name": "Picard interval list"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["File Format Conversion", "Utilities", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044990, "dateCreated": 1636465941, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-bedtointervallist-2-21-6-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-bedtointervallist-2-21-6-cwl1-0/7", "applicationCategory": "CommandLineTool", "name": "Picard BedToIntervalList CWL1.0", "description": "**Picard BedToIntervalList** converts a BED file to a Picard Interval List [1]. Note that the coordinate system of BED files is such that the first base or position in a sequence is numbered \"0\", while in interval_list files it is numbered \"1\" [1]. BED files contain sequence data displayed in a flexible format that includes nine optional fields, in addition to three required fields within the annotation tracks [1]. \nThe interval list file format is relatively simple and reflects the SAM alignment format to a degree [1]. A SAM style header must be present in the file that lists the sequence records against which the intervals are described [1]. \n\n### Common Use Cases\n\n**Picard BedToIntervalList** can be used for easy conversion from BED file to the Picard Interval List format which is required by many Picard processing tools [1]. \n\n### Changes Introduced by Seven Bridges\n\n- Inputs **Use jdk deflater** (USE_JDK_DEFLATER=),  **Use jdk inflater** (USE_JDK_INFLATER=) and **Create md5 file** (CREATE_MD5_FILE=) are not used in the tool wrapper.\n\n- Input file **Sequence dictionary** (SEQUENCE_DICTIONARY=) set to optional.\n\n- Added  **Input FASTA file**  for getting sequence dictionary from FA or FASTA file due to platform wrapper constraints.\n\n### Common Issues and Important Notes\n\nThis tool requires a sequence dictionary, provided with the **sequence_dictionary** input file or with **in_fasta** input file. \n\nThe value given to **Sequence dictionary** (SEQUENCE_DICTIONARY=) can be any of the following:\n\n- A file with .dict extension generated using Picard's CreateSequenceDictionaryTool\n- Another IntervalList with @SQ lines in the header from which to generate a dictionary\n- A VCF that contains #contig lines from which to generate a sequence dictionary\n- A SAM or BAM file with @SQ lines in the header from which to generate a dictionary\n\n\nThe value given to **Input FASTA file**  can be an FA or FASTA file with a DICT file in the same directory.\n\nNote that only one file should be provided, either **Sequence dictionary** (SEQUENCE_DICTIONARY=)  or **Input FASTA file**. If both input files are provided, the tool will use the **Sequence dictionary** (SEQUENCE_DICTIONARY=) input file.\n\n### Performance Benchmarking\n\nTwo experiment tasks were performed on the platform using the On-Demand AWS instance (c4.2xlarge) and BED files with different sizes. One task was processing a 749 KB BED file and took 3 minutes ($0.01) and second task was processing a 2.8 KB BED file and took 3 minutes ($0.01).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n### References\n\n[1] [Picard BedToIntervalList documentation](https://broadinstitute.github.io/picard/command-line-overview.html#BedToIntervalList)", "input": [{"name": "Input file", "encodingFormat": "text/x-bed"}, {"name": "Sort"}, {"name": "Unique"}, {"name": "Sequence dictionary", "encodingFormat": "application/x-vcf"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Compression level"}, {"name": "Create index"}, {"name": "Max record in ram"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Memory per job"}, {"name": "Memory overhead per job"}, {"name": "CPU per job"}, {"name": "Input FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Output file prefix name"}], "output": [{"name": "Interval list"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["File Format Conversion", "Utilities"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648045900, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-buildbamindex-1-140/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-buildbamindex-1-140/7", "applicationCategory": "CommandLineTool", "name": "Picard BuildBamIndex", "description": "Picard BuildBamIndex generates a BAM index (.bai) file.", "input": [{"name": "Input BAM", "encodingFormat": "application/x-bam"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Verbosity"}, {"name": "Memory per job"}, {"name": "Input BAI"}, {"name": "Output only index file"}], "output": [{"name": "Indexed BAM", "encodingFormat": "application/x-bam"}, {"name": "Indexed BAM (.BAI)"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Indexing", "Utilities"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046178, "dateCreated": 1453799772, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-calculatehsmetrics-1-140/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-calculatehsmetrics-1-140/3", "applicationCategory": "CommandLineTool", "name": "Picard CalculateHsMetrics", "description": "Calculates a set of Hybrid Selection specific metrics from an aligned SAMor BAM file. If a reference sequence is provided, AT/GC dropout metrics will be calculated, and the PER_TARGET_COVERAGE option can be used to output GC and mean coverage information for every target.", "input": [{"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Reference"}, {"name": "Verbosity"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Memory per job"}, {"name": "Bait intervals", "encodingFormat": "text/plain"}, {"name": "Bait set name"}, {"name": "Target intervals", "encodingFormat": "text/plain"}, {"name": "Metric accumulation level"}, {"name": "Per target coverage"}], "output": [{"name": "HS metrics", "encodingFormat": "text/plain"}, {"name": "Coverage", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649240568, "dateCreated": 1490711750, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-checkfingerprint-2-25-7-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-checkfingerprint-2-25-7-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Picard CheckFingerprint", "description": "**Picard CheckFingerprint** checks sample identity of provided data against known genotypes [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CheckFingerprint** can be used to check sample identity of BAM/CRAM/SAM or VCF data (**Input sample data**) against known genotypes (**Genotypes**) by computing and comparing genotype information (fingerprints) at specific genome locations and reporting a LOD score [1]. The tool also requires a **Haplotype map** file [2]. \n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--COMPRESSION_LEVEL`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n* Parameters **Detailed metrics output file name** and **Summary metrics output file name** are not required by the wrapper, but if given, they will take precedence over the **Output file name prefix**. In the absence of user input, the outputs will be named using `--OUTPUT` and based on the **Sample ID** metadata field and base name of the **Input sample data** file provided.\n\n### Common Issues and Important Notes\n\n* **Input sample data**, **Genotypes** and **Haplotype map** inputs are required.\n* If the **Input sample data** is a VCF file with multiple samples, **Observed sample alias** must be provided to indicate the sample to analyze.\n* PL, GL and GT fields of a VCF file are used in the identity check in the order specified [1].\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 G.VCF.GZ file (8.9 GB) | 4 mins | $0.03 + $0.01 | c4.2xlarge - 1024 GB EBS | \n| WGS NA12878 BAM file (48 GB) | 8 mins | $0.06 + $0.02 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CheckFingerprint** was tested with cwltool version 3.1.20210628163208. The `in_sample_data`, `in_genotypes` and `in_haplotype_map` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CheckFingerprint documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CheckFingerprint)\n\n[2] [Haplotype map format](https://gatk.broadinstitute.org/hc/en-us/articles/360035531672?id=9526)", "input": [{"name": "Input sample data file", "encodingFormat": "application/x-vcf"}, {"name": "Output file name prefix"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Detailed metrics output file name"}, {"name": "Summary metrics output file name"}, {"name": "Genotypes", "encodingFormat": "application/x-vcf"}, {"name": "Haplotype map", "encodingFormat": "text/plain"}, {"name": "Expected sample alias"}, {"name": "Genotype LOD threshold"}, {"name": "Ignore read groups"}, {"name": "Observed sample alias"}], "output": [{"name": "Fingerprinting summary metrics"}, {"name": "Fingerprinting detailed metrics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1636465941, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-cleansam-1-140/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-cleansam-1-140/4", "applicationCategory": "CommandLineTool", "name": "Picard CleanSam", "description": "Picard CleanSam cleans the provided SAM or BAM file, soft-clips beyond-end-of-reference alignments, and sets MAPQ to 0 for unmapped reads.", "input": [{"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Output format"}, {"name": "Create index"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Verbosity"}, {"name": "Memory per job"}], "output": [{"name": "Cleaned BAM", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046178, "dateCreated": 1453799664, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectalignmentsummarymetrics-2-25-7-cwl1-2/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectalignmentsummarymetrics-2-25-7-cwl1-2/7", "applicationCategory": "CommandLineTool", "name": "Picard CollectAlignmentSummaryMetrics", "description": "**CollectAlignmentSummaryMetrics** produces a summary of alignment metrics from a SAM or BAM file [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CollectAlignmentSummaryMetrics** analyzes **Input alignments** and creates a file with alignment metrics.  \n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--COMPRESSION_LEVEL`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER`, and `--version` were omitted from the wrapper.\n* **Return histogram output file** input parameter was added to the wrapper to control the creation of the **Read length histogram file** output. This parameter **must** be set to True for the histogram file to be created. A custom name for the created output histogram file can still be specified with the **Histogram file name** input parameter.\n \n### Common Issues and Important Notes\n\n- **Input alignments** input is required.\n- Metrics labeled as percentages are actually expressed as fractions [1].\n- **Reference sequence** input is not required. However, without this input a small subset of MISMATCH-related metrics will not be calculated. If a reference sequence is provided, it must be accompanied by a sequence dictionary [1]. \n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 BAM file (48.4 GB) | 76 mins | $0.50 + $0.17 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CollectAlignmentSummaryMetrics** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CollectAlignmentSummaryMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectAlignmentSummaryMetrics)", "input": [{"name": "Max insert size"}, {"name": "Expected pair orientations"}, {"name": "Adapter sequence"}, {"name": "Metric accumulation level"}, {"name": "Is bisulfite sequenced"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Assume sorted"}, {"name": "Stop after"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Output file name prefix"}, {"name": "Collect alignment information"}, {"name": "Histogram file name"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Return histogram output file"}], "output": [{"name": "Summary metrics", "encodingFormat": "text/plain"}, {"name": "Read length histogram file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1645100810, "dateCreated": 1636465940, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectalignmentsummarymetrics-2-21-6-cwl1-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectalignmentsummarymetrics-2-21-6-cwl1-0/4", "applicationCategory": "CommandLineTool", "name": "Picard CollectAlignmentSummaryMetrics CWL1.0", "description": "**CollectAlignmentSummaryMetrics** produces a summary of alignment metrics from a SAM or BAM file [1]. \n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n### Common Use Cases\n\n**Picard CollectAlignmentSummaryMetrics** can be used to assess the quality of alignment by analyzing a SAM or BAM file. The tool takes a SAM/BAM file input and produces metrics detailing the quality of the read alignments as well as the proportion of the reads that passed machine signal-to-noise threshold quality filters [1].  \n\n### Changes Introduced by Seven Bridges\n\n- Inputs **Use jdk deflater** (USE_JDK_DEFLATER=) and **Use jdk inflater** (USE_JDK_INFLATER =) are not used in the tool wrapper.\n \n### Common Issues and Important Notes\n\n- Metrics labeled as percentages are actually expressed as fractions [1].\n- Reference sequence file - Note that while this argument isn't required, without it only a small subset of the metrics will be calculated. Note also that if a reference sequence is provided, it must be accompanied by a sequence dictionary [1]. \n\n### Performance Benchmarking\n\nThree experiment tasks were performed on the platform using the On-Demand AWS instance (c4.2xlarge) and three BAM files with different sizes. One task was processing 25 GB BAM file and took 1 hour and 32 minutes ($0.42), second task was processing 3 GB BAM file and took 4 minutes ($0.02) and third task was processing 170 MB BAM file and took 2 minutes ($0.01).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard CollectAlignmentSummaryMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectAlignmentSummaryMetrics)", "input": [{"name": "Max insert size"}, {"name": "Expected pair orientations"}, {"name": "Adapter sequence"}, {"name": "Metric Accumulation Level"}, {"name": "Is bisulfite sequenced"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Input SAM or BAM file", "encodingFormat": "application/x-bam"}, {"name": "Assume sorted"}, {"name": "Stop after"}, {"name": "Memory per job"}, {"name": "Memory overhead per job"}, {"name": "CPU per job"}, {"name": "Quiet"}, {"name": "Basename for output files"}], "output": [{"name": "Summary metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648045899, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectgcbiasmetrics-1-140/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectgcbiasmetrics-1-140/8", "applicationCategory": "CommandLineTool", "name": "Picard CollectGcBiasMetrics", "description": "Picard CollectGcBiasMetrics reads a SAM or BAM file, writes a file containing metrics about the statistical distribution of insert size (excluding duplicates), and generates a histogram plot.\n\n### Common issues\n1) BAM file - Sort order should be coordinate based.\n2) BAM file - Collect GC bias 1.140 version won't work with big input files. Task will last for a infinite time.", "input": [{"name": "Assume sorted"}, {"name": "Compression level"}, {"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Max records in RAM"}, {"name": "Metric accumulation level"}, {"name": "Quiet"}, {"name": "Stop after"}, {"name": "Validation stringency"}, {"name": "Verbosity"}, {"name": "Memory per job"}, {"name": "Scan window size"}, {"name": "Minimum genome fraction"}, {"name": "Is bisulfite sequenced"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Summary", "encodingFormat": "text/plain"}, {"name": "Chart"}, {"name": "Metrics table", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799200, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collecthsmetrics-2-25-7-cwl1-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collecthsmetrics-2-25-7-cwl1-2/5", "applicationCategory": "CommandLineTool", "name": "Picard CollectHsMetrics", "description": "**Picard CollectHsMetrics** collects hybrid-selection metrics for alignments in SAM or BAM format [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CollectHsMetrics**  requires an aligned SAM or BAM file (**Input alignments**) as well as a target (**Target intervals**) and bait (**Bait intervals**) interval files in Picard INTERVAL_LIST format. If interval files are in BED format, **Picard BedToIntervalList** tool can be used to convert them before use.\n\nA reference sequence (**Reference sequence** input) is required to calculate AT_DROPOUT and GC_DROPOUT metrics[1]. \n\nA file with the GC content and mean sequence depth information for every target interval can be obtained using the **Per-target coverage file name** input parameter (`--PER_TARGET_COVERAGE`).\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--COMPRESSION_LEVEL`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n* The naming of optional per-target coverage, per-base coverage and theoretical sensitivity outputs is automatic. Creating per-target, per-base and theoretical sensitivity outputs is controlled with **Output per target coverage**, **Output per base coverage** and **Output theoretical sensitivity results** boolean input parameters, respectively.\n\n### Common Issues and Important Notes\n\n* **Input alignments**, **Target intervals** and **Bait intervals** inputs are required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WES NA12878 BAM file (16.2 GB) | 34 mins | $0.23 + $0.08 | c4.2xlarge - 1024 GB EBS | \n| BAM file (48 GB) | 68 mins | $0.45 + $0.15 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CollectHsMetrics** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `in_intervals_bait`, `in_intervals_target` and `theoretical_sensitivity_output` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CollectHsMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectHsMetrics)", "input": [{"name": "Bait intervals"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Bait set name"}, {"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Clip overlapping reads"}, {"name": "Target intervals"}, {"name": "Metric accumulation level"}, {"name": "Output per target coverage"}, {"name": "Output per base coverage"}, {"name": "Near distance"}, {"name": "Coverage cap"}, {"name": "Sample size"}, {"name": "CPUs per job"}, {"name": "Memory overhead per job [MB]"}, {"name": "Memory per job [MB]"}, {"name": "Validation stringency"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Output file name prefix"}, {"name": "Allele fraction"}, {"name": "Include indels"}, {"name": "Output theoretical sensitivity results"}, {"name": "Verbosity of logging"}], "output": [{"name": "Output metrics file", "encodingFormat": "text/plain"}, {"name": "Per target coverage output file"}, {"name": "Per base coverage output file"}, {"name": "Theoretical sensitivity output", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044990, "dateCreated": 1636465940, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collecthsmetrics-2-21-6-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collecthsmetrics-2-21-6-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Picard CollectHsMetrics CWL1.0", "description": "**Picard CollectHsMetrics** collects hybrid-selection (HS) metrics for a SAM or BAM file[1]. This tool takes a SAM/BAM file input and collects metrics that are specific for sequence datasets generated through hybrid-selection. Hybrid-selection (HS) is the most commonly used technique to capture exon-specific sequences for targeted sequencing experiments such as exome sequencing[1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Picard CollectHsMetrics**  requires an aligned SAM or BAM file as well as bait and target interval files in Picard interval_list format[1]. You should use the bait and interval files that correspond to the capture kit that was used to generate the capture libraries for sequencing, which can generally be obtained from the kit manufacturer[1]. If the baits and target intervals are provided in BED format, you can convert them to the Picard interval_list format using Picard's BedToInterval tool[1].\n\nIf a reference sequence is provided, this program will calculate both AT_DROPOUT and GC_DROPOUT metrics[1]. \n\nIf you are interested in getting G/C content and mean sequence depth information for every target interval, use the **PER_TARGET_COVERAGE** option[1].\n\n### Changes Introduced by Seven Bridges\n\n- Inputs **use_jdk_deflater**,  **use_jdk_inflater** and **create_md5_file**  are not used in the tool wrapper.\n\n### Common Issues and Important Notes\n\n* Inputs **in_alignments**, **in_intervals_bait** and **in_intervals_target** are required.\n* If **bait_set_name** is not provided it is inferred from the filename of the bait intervals[1].\n\n### Performance Benchmarking\n\nTwo experiment tasks were performed on the platform using the on-Demand AWS instance (r4.2xlarge is used because of the large input files) and BAM files with different sizes. One task was processing a 170 MB RNA-SEQ BAM file and took 19 minutes ($0.10) and the second task was processing a 2.8 GB RNA-SEQ BAM file and took 29 minutes ($0.15).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n### References\n\n[1] [Picard CollectHsMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectHsMetrics)", "input": [{"name": "Interval list file of baits"}, {"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Bait set name"}, {"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Clip overlapping reads"}, {"name": "Interval list file of targets"}, {"name": "Metric Accumulation Level"}, {"name": "Per target coverage"}, {"name": "Per base coverage"}, {"name": "Near distance"}, {"name": "Coverage cap"}, {"name": "Sample size"}, {"name": "CPU per job"}, {"name": "Memory overhead per job"}, {"name": "Memory per job"}, {"name": "Validation stringency"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Max record in ram"}, {"name": "Quiet"}, {"name": "Create index"}, {"name": "Compression level"}, {"name": "Output file prefix name"}], "output": [{"name": "Output metrics file", "encodingFormat": "text/plain"}, {"name": "Per target coverage output file"}, {"name": "Per base coverage output file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648045900, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectinsertsizemetrics-1-140/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectinsertsizemetrics-1-140/4", "applicationCategory": "CommandLineTool", "name": "Picard CollectInsertSizeMetrics", "description": "Picard CollectInsertSizeMetrics reads a SAM or BAM file and writes a file containing metrics about the statistical distribution of insert size (excluding duplicates) and generates a histogram plot.", "input": [{"name": "Assume sorted"}, {"name": "Compression level"}, {"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Max records in RAM"}, {"name": "Metric accumulation level"}, {"name": "Quiet"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Stop after"}, {"name": "Validation stringency"}, {"name": "Verbosity"}, {"name": "Deviations"}, {"name": "Histogram width"}, {"name": "Minimum PCT"}, {"name": "Memory per job"}], "output": [{"name": "Size metrics", "encodingFormat": "text/plain"}, {"name": "Histogram"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046178, "dateCreated": 1453799318, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectmultiplemetrics-2-25-7-cwl1-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectmultiplemetrics-2-25-7-cwl1-2/6", "applicationCategory": "CommandLineTool", "name": "Picard CollectMultipleMetrics", "description": "**Picard CollectMultipleMetrics** collects BAM statistics by running multiple Picard modules at once [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CollectMultipleMetrics** can be used for gathering QC metrics from alignments data. Available modules include CollectAlignmentSummaryMetrics, CollectInsertSizeMetrics, QualityScoreDistribution,  MeanQualityByCycle, CollectBaseDistributionByCycle, CollectGcBiasMetrics, RnaSeqMetrics, CollectSequencingArtifactMetrics and CollectQualityYieldMetrics [1]. CollectAlignmentSummaryMetrics module only outputs a TXT file, whereas other modules produce PDF and TXT files.\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--COMPRESSION_LEVEL`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER`, `--version` were omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* All modules are executed with default parameters and fixed output names. \n* Input **Extra arguments for submodules** can be used to override values for most, but not all submodule parameters.\n* Input **Reference sequence** is required to run some modules.\n* Possible values of the **Program** input parameter are: `CollectAlignmentSummaryMetrics`, `CollectInsertSizeMetrics`, `QualityScoreDistribution`, `MeanQualityByCycle`, `CollectBaseDistributionByCycle`, `CollectGcBiasMetrics`, `RnaSeqMetrics`, `CollectSequencingArtifactMetrics`, and `CollectQualityYieldMetrics`.\n* Values for the **Extra arguments for submodules** input parameter should be provided enclosed in double quotes (e.g., `\"CollectInsertSizeMetrics::--HISTOGRAM_WIDTH 200\"`).\n\n### Performance Benchmarking\n\nPerformance depends on the selected Picard sub-modules and input size. \n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 BAM file (48.4 GB) - default modules | 116 mins | $0.77 + $0.27 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CollectMultipleMetrics** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CollectMultipleMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectMultipleMetrics)", "input": [{"name": "Assume sorted"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Stop after"}, {"name": "Verbosity"}, {"name": "Memory per job [MB]"}, {"name": "Program"}, {"name": "Memory overhead per job [MB]"}, {"name": "Metric accumulation level"}, {"name": "File extension"}, {"name": "Intervals to restrict the analysis to"}, {"name": "dbSNP file", "encodingFormat": "application/x-vcf"}, {"name": "Include unpaired"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Output metrics file names prefix"}, {"name": "Gene annotations in refFlat format", "encodingFormat": "text/plain"}, {"name": "Sequence to ignore mappings"}, {"name": "Extra arguments for submodules"}, {"name": "Validation stringency"}, {"name": "CPUs per job"}], "output": [{"name": "Alignment summary metrics file"}, {"name": "Read length histogram"}, {"name": "Insert size metrics file"}, {"name": "Insert size histogram"}, {"name": "Quality distribution metrics"}, {"name": "Quality distribution"}, {"name": "Quality by cycle metrics"}, {"name": "Quality by cycle"}, {"name": "Base distribution by cycle metrics file"}, {"name": "Base distribution by cycle"}, {"name": "GC bias detailed metrics"}, {"name": "GC bias summary metrics"}, {"name": "GC bias"}, {"name": "RNA metrics"}, {"name": "RNA coverage"}, {"name": "Bait bias detailed metrics"}, {"name": "Bait bias summary metrics"}, {"name": "Pre-adapter detailed metrics"}, {"name": "Pre-adapter summary metrics"}, {"name": "Error summary metrics"}, {"name": "Quality yield metrics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044990, "dateCreated": 1636465940, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectmultiplemetrics-2-21-6-cwl1-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectmultiplemetrics-2-21-6-cwl1-0/8", "applicationCategory": "CommandLineTool", "name": "Picard CollectMultipleMetrics CWL1.0", "description": "**Picard CollectMultipleMetrics** collects BAM statistics by running multiple Picard modules at once [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Picard CollectMultipleMetrics** can be used for gathering QC metrics from alignments data. Available modules include CollectAlignmentSummaryMetrics, CollectInsertSizeMetrics, QualityScoreDistribution,  MeanQualityByCycle, CollectBaseDistributionByCycle, CollectGcBiasMetrics, RnaSeqMetrics, CollectSequencingArtifactMetrics and CollectQualityYieldMetrics [1]. CollectAlignmentSummaryMetrics module only outputs a TXT file, whereas other modules produce PDF and TXT files.\n\n### Changes Introduced by Seven Bridges\n\n* When running the **RnaSeqMetrics** program, a `REF_FLAT` input is required. However, a GTF can also be supplied, in which case the ref_flat file will be generated automatically from the input GTF.\n* Similarly, the `RIBOSOMAL_INTERVALS` file can be supplied when running the **RnaSeqMetrics** module. If this file is not supplied, and a GTF is supplied on the **ref_flat** input, the ribosomal intervals file will be automatically generated, by parsing the GTF and looking for *rRNA* entries in it. \n \n### Common Issues and Important Notes\n\n* All modules are executed with default parameters and fixed output names. \n* Input **Extra arguments for submodules** can be used to override values for most, but not all submodule parameters.\n* Input **Reference sequence** is required to run some modules.\n\n### Performance Benchmarking\n\nPerformance depends on the selected Picard sub-modules and input size. With the default set of analysis modules, using the default AWS instance and 30x and 50x WGS BAM input files, tasks took 92 minutes ($0.6) and 151 minutes ($1), respectively.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard CollectMultipleMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectMultipleMetrics)", "input": [{"name": "Assume sorted"}, {"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Stop after"}, {"name": "Verbosity"}, {"name": "Memory per job [MB]"}, {"name": "Program"}, {"name": "Memory overhead per job [MB]"}, {"name": "Metric accumulation level"}, {"name": "File extension"}, {"name": "Intervals to restrict the analysis to", "encodingFormat": "text/x-bed"}, {"name": "dbSNP file", "encodingFormat": "application/x-vcf"}, {"name": "Include unpaired"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Output metrics file names prefix"}, {"name": "Gene annotations in GTF or refFlat format"}, {"name": "Sequence to ignore mappings"}, {"name": "Extra arguments for submodules"}, {"name": "Ribosomal intervals"}, {"name": "Generate ribosomal intervals"}, {"name": "Strand specificity"}], "output": [{"name": "Output TXT metrics", "encodingFormat": "text/plain"}, {"name": "Output PDF metrics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/2.20.3", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045899, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectrnaseqmetrics-1-140/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectrnaseqmetrics-1-140/6", "applicationCategory": "CommandLineTool", "name": "Picard CollectRnaSeqMetrics", "description": "Picard CollectRnaSeqMetrics collects metrics about the alignment of RNA to various functional classes of loci in the genome: coding, intronic, UTR, intergenic, and ribosomal. This tool also determines strand-specificity for strand-specific libraries.", "input": [{"name": "Assume sorted"}, {"name": "Compression level"}, {"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Max records in RAM"}, {"name": "Metric accumulation level"}, {"name": "Quiet"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Stop after"}, {"name": "Validation stringency"}, {"name": "Verbosity"}, {"name": "Ref flat", "encodingFormat": "text/plain"}, {"name": "Ribosomal intervals"}, {"name": "Strand specificity"}, {"name": "Minimum length"}, {"name": "Ignore sequence"}, {"name": "Rrna fragment percentage"}, {"name": "Memory per job"}], "output": [{"name": "Rna seq metrics", "encodingFormat": "text/plain"}, {"name": "Chart"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control", "RNA-Seq"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046177, "dateCreated": 1453799247, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectsequencingartifactmetrics-2-25-7-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectsequencingartifactmetrics-2-25-7-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "Picard CollectSequencingArtifactMetrics", "description": "**CollectSequencingArtifactMetrics** collects metrics to quantify single-base sequencing artifacts [1]. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CollectSequencingArtifactMetrics** examines two sources of errors associated with hybrid selection sequencing protocols [1].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--COMPRESSION_LEVEL`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n \n### Common Issues and Important Notes\n\n* **Input alignments** and **Reference sequence** inputs are required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| BAM (48.4 GB) | 113 mins | $0.75 + $0.26 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**Picard CollectSequencingArtifactMetrics** was tested with cwltool version 3.1.20210628163208. The `in_alignments` and `in_reference` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CollectSequencingArtifactMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectSequencingArtifactMetrics)", "input": [{"name": "Intervals"}, {"name": "dbSNP file", "encodingFormat": "application/x-vcf"}, {"name": "Minimum quality score"}, {"name": "Minimum mapping quality"}, {"name": "Minimum insert size"}, {"name": "Maximum insert size"}, {"name": "Include unpaired"}, {"name": "Include duplicates"}, {"name": "Include non-PF reads"}, {"name": "Tandem reads"}, {"name": "Use original quality scores"}, {"name": "Context size"}, {"name": "Contexts to print"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Assume sorted"}, {"name": "Stop after"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Validation stringency"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Output file name prefix"}, {"name": "File extension for outputs"}, {"name": "Verbosity of logging"}], "output": [{"name": "Pre-adapter detailed metrics", "encodingFormat": "text/plain"}, {"name": "Pre-adapter summary metrics", "encodingFormat": "text/plain"}, {"name": "Bait bias summary metrics", "encodingFormat": "text/plain"}, {"name": "Bait bias detailed metrics", "encodingFormat": "text/plain"}, {"name": "Error summary metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044990, "dateCreated": 1636465939, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectsequencingartifactmetrics-2-21-6-cwl1-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectsequencingartifactmetrics-2-21-6-cwl1-0/4", "applicationCategory": "CommandLineTool", "name": "Picard CollectSequencingArtifactMetrics  2.21.6 CWL1.0", "description": "**CollectSequencingArtifactMetrics** collects metrics to quantify single-base sequencing artifacts [1]. \n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n### Common Use Cases\n\n**Picard CollectSequencingArtifactMetrics** can be used to examine two sources of sequencing errors associated with hybrid selection protocols [1]. These errors are divided into two broad categories, pre-adapter and bait-bias [1]. \nThe tool produces four files; summary and detail metrics files for both pre-adapter and bait-bias artifacts [1]. The detailed metrics show the error rates for each type of base substitution within every possible triplet base configuration [1]. The summary metrics provide likelihood information on the 'worst-case' errors [1].\n\n### Changes Introduced by Seven Bridges\n\n- Inputs **Use jdk deflater** (USE_JDK_DEFLATER=),  **Use jdk inflater** (USE_JDK_INFLATER =) and **File extension** (FILE_EXTENSION=) are not used in the tool wrapper.\n \n### Common Issues and Important Notes\n\n- Option **Reference sequence** (REFERENCE_SEQUENCE=) is required.\n\n### Performance Benchmarking\n\nThree experiment tasks were performed on the platform using the on-Demand AWS instance (c4.2xlarge) and three BAM files with different sizes. One task was processing 25 GB BAM file and took 6 minutes ($0.03), second task was processing 3 GB BAM file and took 6 minutes ($0.03) and third task was processing 170 MB BAM file and took 2 minutes ($0.01).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard CollectSequencingArtifactMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectSequencingArtifactMetrics)", "input": [{"name": "Intervals"}, {"name": "dbSNP file", "encodingFormat": "application/x-vcf"}, {"name": "Minimum quality score"}, {"name": "Minimum mapping quality"}, {"name": "Minimum insert size"}, {"name": "Maximum insert size"}, {"name": "Include unpaired"}, {"name": "Include duplicates"}, {"name": "Tandem reads"}, {"name": "Use original quality scores"}, {"name": "Context size"}, {"name": "Contexts to print"}, {"name": "Input SAM or BAM file", "encodingFormat": "application/x-sam"}, {"name": "Assume sorted"}, {"name": "Stop after"}, {"name": "Memory per job"}, {"name": "Memory overhead per job"}, {"name": "CPU per job"}, {"name": "Create index"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Basename for output files"}], "output": [{"name": "PreAdapter Detail Metrics", "encodingFormat": "text/plain"}, {"name": "PreAdapter Summary Metrics", "encodingFormat": "text/plain"}, {"name": "Bait Bias Summary Metrics", "encodingFormat": "text/plain"}, {"name": "Bait Bias Detail Metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045900, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectvariantcallingmetrics-2-25-7-cwl1-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectvariantcallingmetrics-2-25-7-cwl1-2/6", "applicationCategory": "CommandLineTool", "name": "Picard CollectVariantCallingMetrics", "description": "**Picard CollectVariantCallingMetrics** can be used to collect variant call statistics after variant calling [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Picard CollectVariantCallingMetrics** can assess various variant-level QC metrics. \n\n### Changes Introduced by Seven Bridges\n\n* Undocumented option `--THREAD_COUNT` and the following common Picard toolkit options have been omitted from this wrapper:\n\n`--arguments_file`, `--help`, `--version`, `--showHidden`, `--TMP_DIR`, `--VERBOSITY`, `--QUIET`, `--VALIDATION_STRINGENCY`, `--COMPRESSION_LEVEL`, `--MAX_RECORDS_IN_RAM`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`,  `--GA4GH_CLIENT_SECRETS`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER`\n\n### Common Issues and Important Notes\n\n* **dbSNP** and **Input variants** inputs are required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS HGDP01078 VCF.GZ  file (243 MB) | 6 mins | $0.04 + $0.01 | c4.2xlarge - 1024 GB EBS | \n| WGS NA12878 GVCF.GZ  file (8.9 GB) | 40 mins | $0.26 + $0.09 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CollectVariantCallingMetrics** was tested with cwltool version 3.1.20210628163208. The `in_variants` and `in_dbsnp` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CollectVariantCallingMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectVariantCallingMetrics)", "input": [{"name": "Gvcf input"}, {"name": "dbSNP", "encodingFormat": "application/x-vcf"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Memory per job [MB]"}, {"name": "Sequence dictionary file"}, {"name": "Target intervals file"}, {"name": "Output metrics file name prefix"}, {"name": "Memory overhead per job [MB]"}, {"name": "Reference sequence file", "encodingFormat": "application/x-fasta"}, {"name": "CPUs per job"}], "output": [{"name": "Detailed metrics", "encodingFormat": "text/plain"}, {"name": "Metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044990, "dateCreated": 1636465872, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectvariantcallingmetrics-2-21-6-cwl1-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectvariantcallingmetrics-2-21-6-cwl1-0/3", "applicationCategory": "CommandLineTool", "name": "Picard CollectVariantCallingMetrics CWL1.0", "description": "**Picard CollectVariantCallingMetrics** can be used to collect variant call statistics after variant calling [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Picard CollectVariantCallingMetrics** can assess various variant-level QC metrics. \n\n### Changes Introduced by Seven Bridges\n\n* Undocumented option `THREAD_COUNT=` and the following common Picard toolkit options have been omitted from this wrapper:\n\n`TMP_DIR=`, `VERBOSITY=`, `QUIET=`, `VALIDATION_STRINGENCY=`, `COMPRESSION_LEVEL=`, `MAX_RECORDS_IN_RAM=`, `CREATE_INDEX=`, `CREATE_MD5_FILE=`,  `GA4GH_CLIENT_SECRETS=`, `USE_JDK_DEFLATER=`, `USE_JDK_INFLATER=`, `OPTIONS_FILE=`\n\n* dbSNP (**Input dbSNP file**) should be in VCF.GZ format with the accompanying TBI index. Natively, the tool also accepts a VCF file with an IDX index.\n* **Input VCF file** should be in VCF.GZ format with the accompanying TBI index. Natively the tool also accepts a VCF file with an IDX index.\n\n### Common Issues and Important Notes\n\n* Inputs **Input dbSNP file** and **Input VCF file** are required.\n\n### Performance Benchmarking\n\nProcessing a 100 MB single-sample WGS VCF.GZ file (NA12878) took 6 minutes (cost: $0.04), using the default AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard CollectVariantCallingMetrics documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CollectVariantCallingMetrics)", "input": [{"name": "Gvcf input"}, {"name": "Input dbSNP file", "encodingFormat": "application/x-vcf"}, {"name": "Input VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Memory per job"}, {"name": "Sequence dictionary file"}, {"name": "Target intervals file"}, {"name": "Output metrics file name prefix"}, {"name": "Memory overhead per job [MB]"}, {"name": "Reference sequence file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Detailed metrics", "encodingFormat": "text/plain"}, {"name": "Metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045482, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectwgsmetrics-1-140/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectwgsmetrics-1-140/9", "applicationCategory": "CommandLineTool", "name": "Picard CollectWgsMetrics", "description": "Picard CollectWgsMetrics computes a number of metrics that are necessary for evaluating the coverage and performance of whole genome sequencing experiments.\n\n### Common issues\n1) BAM file - Sort order should be coordinate based.", "input": [{"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Coverage cap"}, {"name": "Minimum base quality"}, {"name": "Minimum mapping quality"}, {"name": "Stop after"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Verbosity"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Include base quality histogram"}, {"name": "Count unpaired"}, {"name": "Memory per job"}], "output": [{"name": "WGS metrics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control", "WGS"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799881, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectwgsmetricswithnonzerocoverage-2-25-7-cwl1-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectwgsmetricswithnonzerocoverage-2-25-7-cwl1-2/5", "applicationCategory": "CommandLineTool", "name": "Picard CollectWgsMetricsWithNonZeroCoverage", "description": "**Picard CollectWgsMetricsWithNonZeroCoverage** evaluates the coverage and performance of WGS experiments [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CollectWgsMetricsWithNonZeroCoverage** can be used for quality control of WGS data. This tool extends CollectWgsMetrics by including metrics for sites with coverage >0 [1]. Please note that this tool is marked as experimental by the tool authors [1].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--COMPRESSION_LEVEL`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER`, and `--version` were omitted from the wrapper.\n* Parameter `--CHART_OUTPUT`, corresponding to the **Output chart file name** input is not required. If a value is not provided, the **Input alignments** file name base will be used.\n\n### Common Issues and Important Notes\n\n* **Input alignments** should be a coordinate-sorted SAM or BAM file. \n* **Reference sequence** and **Input alignments** inputs are required.\n* Note from tool documentation [1]: Metrics labeled as percentages are actually expressed as fractions.\n* The tool is marked as experimental by the tool authors [1].\n* Please consult the tool documentation [1] for a detailed explanation of the reported metrics.\n\n### Performance Benchmarking\n\nPerformance of the tool depends on the size/coverage of the **Input alignments** file.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 BAM (48.4 GB) | 156 mins | $1.03 + $0.36 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CollectWgsMetricsWithNonZeroCoverage** was tested with cwltool version 3.1.20210628163208. The `in_alignments` and `in_reference` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard documentation](http://broadinstitute.github.io/picard/command-line-overview.html#CollectWgsMetricsWithNonZeroCoverage)", "input": [{"name": "Output chart file name"}, {"name": "Count unpaired"}, {"name": "Coverage cap"}, {"name": "Include base quality histogram"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Intervals file"}, {"name": "Memory per job [MB]"}, {"name": "Minimum base quality"}, {"name": "Minimum mapping quality"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Stop after"}, {"name": "Validation stringency"}, {"name": "Verbosity"}, {"name": "Output metrics file prefix"}, {"name": "Locus accumulation cap"}, {"name": "Sample size for theoretical het sensitivity sampling"}, {"name": "Output name prefix for theoretical sensitivity"}, {"name": "Allele fraction for theoretical sensitivity"}, {"name": "Use fast algorithm"}, {"name": "Average read length"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}], "output": [{"name": "Output chart"}, {"name": "WGS metrics", "encodingFormat": "text/plain"}, {"name": "Theoretical sensitivity output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "WGS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044990, "dateCreated": 1636465872, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-collectwgsmetricswithnonzerocoverage-2-21-6-cwl1-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-collectwgsmetricswithnonzerocoverage-2-21-6-cwl1-0/4", "applicationCategory": "CommandLineTool", "name": "Picard CollectWgsMetricsWithNonZeroCoverage CWL1.0", "description": "**Picard CollectWgsMetricsWithNonZeroCoverage** evaluates the coverage and performance of whole genome sequencing experiments [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Picard CollectWgsMetricsWithNonZeroCoverage** can be used for quality control of WGS data.\n\n### Changes Introduced by Seven Bridges\n\nNo significant changes were introduced. \n\n### Common Issues and Important Notes\n\n* Input  **Input SAM or BAM file** is required and the file should be coordinate sorted. \n* Input **Reference** is required.\n\n### Performance Benchmarking\n\nPerformance of the tool depends on the size of the input alignments file. Analysing 30x (40 GB) and 50x (72.3 GB) coverage WGS BAM files on the default on-demand AWS instances took 2 h 9 min ($0.86) and 3 h 32 min ($1.41), respectively.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard documentation](http://broadinstitute.github.io/picard/command-line-overview.html#CollectWgsMetricsWithNonZeroCoverage)", "input": [{"name": "Output chart file name"}, {"name": "Compression level"}, {"name": "Count unpaired"}, {"name": "Coverage cap"}, {"name": "Include base quality histogram"}, {"name": "Input SAM or BAM file", "encodingFormat": "application/x-bam"}, {"name": "Intervals file"}, {"name": "Max records in RAM"}, {"name": "Memory per job [MB]"}, {"name": "Minimum base quality"}, {"name": "Minimum mapping quality"}, {"name": "Quiet"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Stop after"}, {"name": "Validation stringency"}, {"name": "Verbosity"}, {"name": "Output metrics file prefix"}, {"name": "Locus accumulation cap"}, {"name": "Sample size for theoretical het sensitivity sampling"}, {"name": "Output file name prefix for theoretical sensitivity"}, {"name": "Allele fraction for theoretical sensitivity"}, {"name": "Use fast algorithm"}, {"name": "Average read length"}, {"name": "Memory overhead per job [MB]"}], "output": [{"name": "Output chart"}, {"name": "WGS metrics", "encodingFormat": "text/plain"}, {"name": "Theoretical sensitivity output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control", "WGS"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045482, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-createsequencedictionary-1-140/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-createsequencedictionary-1-140/7", "applicationCategory": "CommandLineTool", "name": "Picard CreateSequenceDictionary", "description": "Picard CreateSequenceDictionary reads fasta or fasta.gz containing reference sequences and writes this information as a SAM or BAM file with only a sequence dictionary.", "input": [{"name": "Genome assembly"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Uri"}, {"name": "Species"}, {"name": "Truncate names at whitespaces"}, {"name": "Num sequences"}, {"name": "Verbosity"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}], "output": [{"name": "Dictionary"}, {"name": "Output file", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Indexing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799172, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-crosscheckfingerprints-2-25-7-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-crosscheckfingerprints-2-25-7-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "Picard CrosscheckFingerprints", "description": "**Picard CrosscheckFingerprints** checks a set of data files for sample identity [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard CrosscheckFingerprints** can be used to check whether a set of data files (**Input sample data**) all come from the same individual [1]. The tool also requires a **Haplotype map** file [2]. If **Second input files** are provided, data files for a specific sample can be linked together. Please see the tool documentation [1] for further details on execution modes and output contents.\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--COMPRESSION_LEVEL`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n\n\n### Common Issues and Important Notes\n\n* **Input sample data** and **Haplotype map** inputs are required.\n* **Calculate tumor-aware results** (`--CALCULATE_TUMOR_AWARE_RESULTS`) input parameter is by default set to `true`. This parameter roughly doubles the execution time of the tool and disabling it if unneeded can significantly speed-up comparisons of many groups [1].\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 BAM and GVCF.GZ files (48 GB, 8.9 GB) | 10 mins | $0.07 + $0.02 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard CrosscheckFingerprints** was tested with cwltool version 3.1.20210628163208. The `in_sample_data`, `in_haplotype_map` and `exit_code_when_mismatch` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard CrosscheckFingerprints documentation](https://broadinstitute.github.io/picard/command-line-overview.html#CrosscheckFingerprints)\n\n[2] [Haplotype map format](https://gatk.broadinstitute.org/hc/en-us/articles/360035531672?id=9526)", "input": [{"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "Output file name prefix"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Haplotype map", "encodingFormat": "application/x-vcf"}, {"name": "Allow duplicate reads"}, {"name": "Calculate tumor-aware results"}, {"name": "Crosscheck by"}, {"name": "Crosscheck mode"}, {"name": "Exit code - mismatch"}, {"name": "Exit code - no valid checks"}, {"name": "Expect all groups to match"}, {"name": "Genotyping error rate"}, {"name": "Input sample file map"}, {"name": "Input sample map"}, {"name": "LOD threshold"}, {"name": "Loss of heterozygosity rate"}, {"name": "Matrix output file name"}, {"name": "Maximum effect of each haplotype block"}, {"name": "Number of threads"}, {"name": "Output errors only"}, {"name": "Sample individual map"}, {"name": "Second input files", "encodingFormat": "application/x-vcf"}, {"name": "Second input sample map"}, {"name": "Test input readability"}], "output": [{"name": "Matrix output file"}, {"name": "CrosscheckFingerprints metrics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1636465872, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-estimatelibrarycomplexity-1-140/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-estimatelibrarycomplexity-1-140/4", "applicationCategory": "CommandLineTool", "name": "Picard EstimateLibraryComplexity", "description": "Picard EstimateLibraryComplexity attempts to estimate library complexity from the sequence of read pairs alone. It does so by sorting all reads by the first N bases (5 by default) of each read and then comparing reads with the first N bases identical to each other for duplicates.  Reads are considered to be duplicates if they match each other with no gaps and an overall mismatch rate less than or equal to MAX_DIFF_RATE (0.03 by default).\n\nReads of poor quality are filtered out to provide a more accurate estimate. The filtering removes reads with any no-calls in the first N bases or with a mean base quality lower than MIN_MEAN_QUALITY across either the first or second read.\n\nUnpaired reads are ignored in this computation.\n\nThe algorithm attempts to detect optical duplicates separately from PCR duplicates and excludes these in the calculation of library size. Since there is no alignment to screen out technical reads, one further filter is applied to the data.  After examining all reads, a histogram is built of [#reads in duplicate set -> #of duplicate sets]. All bins that contain exactly one duplicate set are then removed from the histogram as outliers before the library size is estimated.", "input": [{"name": "Compression level"}, {"name": "Input file"}, {"name": "Max records in RAM"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Verbosity"}, {"name": "Min identical bases"}, {"name": "Max diff rate"}, {"name": "Min mean quality"}, {"name": "Max group ratio"}, {"name": "Read name regex"}, {"name": "Optical duplicate pixel instance"}, {"name": "Memory per job"}], "output": [{"name": "Library complexity", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151401, "dateCreated": 1453798888, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-fastqtosam-2-25-7-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-fastqtosam-2-25-7-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Picard FastqToSam", "description": "**Picard FastqToSam** converts FASTQ files to an unaligned SAM or BAM file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard FastqToSam**  takes single or paired-end FASTQ files (**Input reads**) and converts the data to unaligned SAM/BAM format [1]. By default the output is query name sorted, but this can be changed using the **Sort order** (`--SORT_ORDER`) input parameter.\n\nFor processing paired-end data, metadata field **Paired-end** should be set on **Input reads** - if this field is not populated, the tool will attempt to assign `--FASTQ` and `--FASTQ2` files based on the order in which it receives the data, which is not guaranteed to be correct. If more than two files are provided as **Input reads** and **Use sequential FASTQs** (`--USE_SEQUENTIAL_FASTQS`) is requested, the tool will treat the first provided file as the 001 file. In this scenario, for paired-end data, the **Sample ID** metadata field must be set for all input files.\n\nThe base quality score encoding to use can be explicitly set using the **Quality format** (`--QUALITY_FORMAT`) input parameter (by default, the tool attempts to guess the format to use) [1].\n\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--QUIET`, `--STRIP_UNPAIRED_MATE_NUMBER` (deprecated), `--TMP_DIR` and `--version` were omitted from the wrapper.\n* **Input reads** should be used to provide all FASTQ files which should be analyzed. This input corresponds to `--FASTQ` and `--FASTQ2` input parameters and can be paired with **Use sequential FASTQs** (`--USE_SEQUENTIAL_FASTQS`).\n* **Sample name** (`--SAMPLE_NAME`) is not a required input in this wrapper, in order to facilitate batch processing of input data. If a value is not provided for this input, the wrapper will use the **Sample ID** metadata field of input FASTQ files to set the SM field of the output SAM/BAM file. If neither the **Sample name** nor the **Sample ID** metadata field values are available, the task will fail.\n* Parameter `--OUTPUT` is optional in this wrapper. If the value for the **Output file name prefix** input parameter is not provided, the outputs will be named based on the **Sample ID** metadata field value of the **Input reads** files, or if this is absent, following the base name of the first **Input reads** file. The extension of the output file is controlled using the **Output file type** custom input parameter (BAM by default).\n* The BAI index created using the **Create index** (`--CREATE_INDEX`) input parameter will always have the extension BAM.BAI.\n\n\n### Common Issues and Important Notes\n\n* **Input reads** input is required. For paired-end data, it is highly recommended (and in some scenarios, necessary) to populate the **Paired end** metadata field on all **Input reads** files.\n* **Sample name** (`--SAMPLE_NAME`) input is not required, but if this value is not provided and the metadata field **Sample ID** of **Input reads** is not populated, the tool will fail.\n* If **Create index** (`--CREATE_INDEX`) is used, the **Sort order** (`--SORT_ORDER`) must be set to `coordinate` and the **Output file type** should be set to `BAM`, as only coordinate-sorted BAM output files are eligible for indexing.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WES FQ.GZ (2x 4.8 GB) | 54 mins | $0.36 + $0.13 | c4.2xlarge - 1024 GB EBS | \n| WGS FQ.GZ (2x 47 GB) | 310 mins | $2.06 + $0.74 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard FastqToSam** was tested with cwltool version 3.0.20201203173111. The `in_reads` and `prefix` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard FastqToSam documentation](https://broadinstitute.github.io/picard/command-line-overview.html#FastqToSam)", "input": [{"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Output file name prefix"}, {"name": "Sort order"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Create index"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Create MD5 file"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Use JDK deflater"}, {"name": "Use JDK inflater"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Output file type"}, {"name": "Sample name"}, {"name": "Allow and ignore empty lines"}, {"name": "Comments to include in the output header"}, {"name": "Description for the read group header"}, {"name": "Library name for the LB header attribute"}, {"name": "Maximum quality"}, {"name": "Minimum quality"}, {"name": "Platform type for the PL header attribute"}, {"name": "Platform model for the read group header"}, {"name": "Platform unit for the PU header attribute"}, {"name": "Predicted insert size for the read group header"}, {"name": "Program group for the read group header"}, {"name": "Quality format"}, {"name": "Read group name"}, {"name": "Run date in ISO 8601 date format"}, {"name": "Sequencing center for the CN header attribute"}, {"name": "Use sequential FASTQs"}], "output": [{"name": "Unaligned SAM or BAM file", "encodingFormat": "application/x-sam"}, {"name": "Optional MD5 digest file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["File Format Conversion", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648040158, "dateCreated": 1637675816, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-filtersamreads-1-140/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-filtersamreads-1-140/10", "applicationCategory": "CommandLineTool", "name": "Picard FilterSamReads", "description": "Picard FilterSamReads produces a new SAM or BAM file from the INPUT SAM or BAM file by including (or excluding) aligned reads or a list of reads names supplied in the READ_LIST_FILE.\n\n### Common issues\n1) BAM file - Sort order should be query name based.\n2) BAM file - If a read has multiple alignments reported in the bam file, only the primary alignment should be kept, all others should be removed.", "input": [{"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Verbosity"}, {"name": "Output format"}, {"name": "Create index"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Filter"}, {"name": "Read list", "encodingFormat": "text/plain"}, {"name": "Sort order"}, {"name": "Write reads file"}, {"name": "Memory per job"}], "output": [{"name": "Filtered BAM", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799185, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-fixmateinformation-1-140/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-fixmateinformation-1-140/8", "applicationCategory": "CommandLineTool", "name": "Picard FixMateInformation", "description": "Picard FixMateInformation ensures that all mate-pair information is synchronized between each read and its mate pair.  If no output file is supplied, the output is written to a temporary file and then copied over to the input file.  Reads marked with the secondary alignment flag are written to the output file unchanged.", "input": [{"name": "Input", "encodingFormat": "application/x-sam"}, {"name": "Verbosity"}, {"name": "Add mate cigar"}, {"name": "Assume sorted"}, {"name": "Sort order"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Create index"}, {"name": "Output format"}, {"name": "Memory per job"}], "output": [{"name": "Fixed file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453800021, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-gatherbamfiles-2-25-7-cwl1-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-gatherbamfiles-2-25-7-cwl1-2/6", "applicationCategory": "CommandLineTool", "name": "Picard GatherBamFiles", "description": "**Picard GatherBamFiles** merges BAM files after a scattered analysis [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Picard GatherBamFiles** concatenates BAM files (**Input alignments**) into a single file. The files should be provided in the order in which they should be merged. The header is copied over from the first file provided [1].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n \n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n* It is important to provide the input files in the order in which they should be merged.\n\n### Performance Benchmarking\n\n**Picard GatherBamFiles** performance depends on the size and number of provided input files. Creating an index of the combined output slightly prolongs the execution time.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| 19 WGS NA12878 BAM files (3-5 GB, 68 GB total) | 43 mins | $0.28 + $0.10 | c4.2xlarge - 1024 GB EBS | \n \n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard GatherBamFiles** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `create_md5_file` and `prefix` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard GatherBamFiles documentation](https://broadinstitute.github.io/picard/command-line-overview.html#GatherBamFiles)", "input": [{"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Create index"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Create MD5 file"}, {"name": "Output file name prefix"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Verbosity of logging"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}], "output": [{"name": "Gathered BAM file", "encodingFormat": "application/x-bam"}, {"name": "MD5 file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["SAM/BAM Processing", "Utilities", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044989, "dateCreated": 1636465871, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-gatherbamfiles-2-21-6-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-gatherbamfiles-2-21-6-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Picard GatherBamFiles CWL1.0", "description": "**Picard GatherBamFiles** performs a rapid \"gather\" operation on BAM files. The files are generated as a result of the same process being performed on different regions of a original BAM file, creating many smaller BAM files that now need to be concatenated (reassembled) back together [1].\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n### Common Use Cases\n\n**Picard GatherBamFiles** can be used to concatenate one or more BAM files as efficiently as possible [1]. It operates via copying of the gzip blocks directly for speed but also supports generation of an MD5 on the output and indexing of the output BAM file [1]. \n\n### Changes Introduced by Seven Bridges\n\n- Inputs **Use jdk deflater** (USE_JDK_DEFLATER=) and **Use jdk inflater** (USE_JDK_INFLATER =) are not used in the tool wrapper.\n \n### Common Issues and Important Notes\n\n- Header in output BAM file is copied from the first input BAM file provided.\n- The tool only supports BAM files, not SAM files [1].\n\n### Performance Benchmarking\n\nThree experiment tasks were performed on the platform using an On-Demand AWS instance (c4.2xlarge) and three sets of BAM files with different sizes. One task was processing four BAM files (4.9GB, 4.7GB, 5.0GB, 4.8GB) and took 15 minutes ($0.07), second task was processing four BAM files (0.4KB, 0.4KB, 0.4KB, 5.0KB) and took 2 minutes ($0.01) and third task was processing four BAM files (27.2GB, 34GB, 39.1GB, 39.5GB) and took 1 hour and 25 minutes ($0.37).\n \n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard GatherBamFiles documentation](https://broadinstitute.github.io/picard/command-line-overview.html#GatherBamFiles)", "input": [{"name": "Input Bam files", "encodingFormat": "application/x-bam"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max record in ram"}, {"name": "Create index"}, {"name": "Memory per job"}, {"name": "Memory overhead per job"}, {"name": "CPU per job"}, {"name": "Create MD5 file"}], "output": [{"name": "Gathered BAM", "encodingFormat": "application/x-bam"}, {"name": "MD5 file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648045900, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-genotypeconcordance-2-25-7-cwl1-2/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-genotypeconcordance-2-25-7-cwl1-2/5", "applicationCategory": "CommandLineTool", "name": "Picard GenotypeConcordance", "description": "**Picard GenotypeConcordance** calculates genotype concordance between two VCF files [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard GenotypeConcordance** takes a call (**Input variants**) and a truth (**Truth VCF file**) VCF file and reports concordance between genotype data in the files, for SNPs and indels separately [1].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--CREATE_MD5_FILE`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* Inputs **Truth VCF file** and **Input variants** are required.\n* If **Intervals** are specified, **Truth VCF file** and **Input variants** must be indexed.\n* If **Output file name prefix** is not provided, the base for output file names will be the same as the base name of **Input variants**.\n* **Truth sample name** and **Call sample name** inputs are not required if only one sample exists. \n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS VCF.GZ (243 MB) | 3 mins | $0.03 + $0.01 | c4.2xlarge - 1024 GB EBS | \n| WGS GVCF.GZ to GVCF.GZ (8 GB) | 60 mins | $0.40 + $0.14 | c4.2xlarge - 1024 GB EBS |\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**Picard GenotypeConcordance** was tested with cwltool version 3.1.20210628163208. The `in_variants` and `in_variants_truth` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard GenotypeConcordance documentation](https://broadinstitute.github.io/picard/command-line-overview.html#GenotypeConcordance)", "input": [{"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "Truth VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Output VCF with concordance information"}, {"name": "Truth sample name"}, {"name": "Call sample name"}, {"name": "Intervals"}, {"name": "Intersect intervals"}, {"name": "Minimum GQ"}, {"name": "Minimum DP"}, {"name": "Output all rows"}, {"name": "Use VCF index"}, {"name": "Missing sites HOM REF"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Output file name prefix"}, {"name": "Create index"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Use JDK deflater"}, {"name": "Use JDK inflater"}, {"name": "CPUs per job"}, {"name": "Ignore FILTER status"}, {"name": "Verbosity of logging"}], "output": [{"name": "Output VCF file with concordance information", "encodingFormat": "application/x-vcf"}, {"name": "Genotype concordance summary metrics"}, {"name": "Genotype concordance detailed metrics"}, {"name": "Genotype concordance contingency metrics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["VCF Processing", "Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044989, "dateCreated": 1636465871, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-genotypeconcordance-2-21-6-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-genotypeconcordance-2-21-6-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Picard GenotypeConcordance CWL1.0", "description": "**Picard GenotypeConcordance** evaluates the concordance between genotype calls. Two genotype calls are used for samples in different callsets where one is being considered as the truth (aka standard, or reference) and the other as the call that is being evaluated for accuracy.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n**Picard GenotypeConcordance** can be used for obtaining summary, detailed and contingency metrics from two samples in two different VCFs, one being considered the truth (or reference) the other being the call. For each set of metrics, the data is broken into separate sections for SNPs and INDELs. \n\nThe tool can output a VCF annotated with concordance information. The concordance state will be stored in the \"CONC_ST\" tag in the INFO field. The truth sample name will be \"truth\" and the call sample name will be \"call\".\n\n### Changes Introduced by Seven Bridges\n\nNo significant changes were introduced.\n\n### Common Issues and Important Notes\n\n* Inputs **Truth vcf** and **Call vcf** are required.\n* If **Intervals** are specified, **Truth vcf** and **Call vcf** must be indexed. In that case inputs **Index call file** and **Index truth file** are required.\n* If input **Basename for output files** is not defined, basename for output files will be the same as the basename of **Call vcf**.\n* Input **Truth sample name** is not required if only one sample exists. \n* Input **Call sample name** is not required if only one sample exists. \n\n### Performance Benchmarking\n\nTwo experiment tasks were performed on the platform using the default AWS instance (c4.2xlarge) and VCF files with different sizes. One task was processing a 102 MB single-sample WGS VCF.GZ file (NA12878) and took 5 minutes($0.05) and the second task was processing a 30KB three-sample VCF file (NA12878, NA12891, NA12892) and took 2 minutes($0.01).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard GenotypeConcordance documentation](https://broadinstitute.github.io/picard/command-line-overview.html#GenotypeConcordance)", "input": [{"name": "Memory per job"}, {"name": "Memory overhead per job"}, {"name": "Truth vcf", "encodingFormat": "application/x-vcf"}, {"name": "Output vcf"}, {"name": "Truth sample name"}, {"name": "Call sample name"}, {"name": "Intervals"}, {"name": "Intersect intervals"}, {"name": "Min Gq"}, {"name": "Min Dp"}, {"name": "Output all rows"}, {"name": "Use vcf index"}, {"name": "Missing sites home ref"}, {"name": "Call vcf", "encodingFormat": "application/x-vcf"}, {"name": "Basename for output files"}, {"name": "Create MD5 file"}, {"name": "Create index"}, {"name": "Max record in ram"}, {"name": "Compression level"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Use jdk deflater"}, {"name": "Use jdk inflater"}, {"name": "Input variants index file"}, {"name": "Input truth variants index file"}, {"name": "CPU per job"}], "output": [{"name": "Output VCF", "encodingFormat": "application/x-vcf"}, {"name": "Genotype concordance summary metrics"}, {"name": "Genotype concordance detail metrics"}, {"name": "Genotype concordance contingency metrics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1648045900, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-markduplicates-2-25-7-cwl1-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-markduplicates-2-25-7-cwl1-2/6", "applicationCategory": "CommandLineTool", "name": "Picard MarkDuplicates", "description": "**Picard MarkDuplicates** marks duplicate reads in alignment files [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard MarkDuplicates** can be used to identify duplicate reads in a SAM or BAM file [1]. \n\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP` (obsolete), `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER` and `--version` were omitted from the wrapper.\n* Outputs (`--METRICS_FILE` and `--OUTPUT`) are named based on the **Output file name prefix** input parameter. If a value for it is not provided, the **Sample ID** metadata field value of **Input alignments** is used if it exists. Otherwise, the base name of the provided **Input alignments** inputs is used to name the outputs.\n* The **Output format** input parameter was added to allow the users to select the format of the created output file.\n \n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n* If the **Input alignments** file is coordinate-sorted, supplementary/secondary alignments and unmapped mates of mapped reads are not marked as duplicates, whereas these reads are included in the duplication test if the input is query-grouped [1].\n* Optical duplicate detection can be skipped by setting the **Read name regex** (`--READ_NAME_REGEX`) input to `null`.\n* The default value of **Optical duplicate pixel distance** input parameter (100) is appropriate for unpatterned Illumina platforms; however, for patterned flowcell models 2500 is more appropriate [1].\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WGS NA12878 BAM file (48.4 GB) | 233 mins | $1.55 + $0.54 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard MarkDuplicates** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard MarkDuplicates documentation](https://broadinstitute.github.io/picard/command-line-overview.html#MarkDuplicates)", "input": [{"name": "Read name regex"}, {"name": "Sorting collections size ratio"}, {"name": "Max file handles for read ends map"}, {"name": "Assume sorted"}, {"name": "Remove duplicates"}, {"name": "Comments"}, {"name": "Program group name"}, {"name": "Program group command line"}, {"name": "Program group version"}, {"name": "Program record ID"}, {"name": "Verbosity of logging"}, {"name": "Output format"}, {"name": "Create index"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Maximum records in RAM"}, {"name": "Optical duplicate pixel distance"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Memory per job [MB]"}, {"name": "Duplicate scoring strategy"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Barcode SAM tag"}, {"name": "Read one barcode SAM tag"}, {"name": "Read two barcode SAM tag"}, {"name": "Tag duplicate set members"}, {"name": "Remove sequencing duplicates"}, {"name": "Tagging policy"}, {"name": "Assume sort order"}, {"name": "Basename for output files"}, {"name": "Add PG tag to reads"}, {"name": "Clear DT tag"}, {"name": "Create MD5 digest file"}, {"name": "Treat UMIs as duplex stranded"}, {"name": "Maximum optical duplicate set size"}, {"name": "Molecular identifier tag"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Metrics file"}, {"name": "Deduped alignments file", "encodingFormat": "application/x-sam"}, {"name": "MD5 digest file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["CWLtool Tested", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648044989, "dateCreated": 1636465814, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-markduplicates-2-21-6-cwl1-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-markduplicates-2-21-6-cwl1-0/3", "applicationCategory": "CommandLineTool", "name": "Picard MarkDuplicates CWL1.0", "description": "**Picard MarkDuplicates** locates and tags duplicate reads in a BAM or SAM file. Duplicate reads are defined as originating from a single fragment of DNA [1]. \nThe tool works by comparing sequences in the 5 prime positions of both reads and read-pairs in a SAM/BAM file [1]. After duplicate reads are collected, the tool differentiates the primary and duplicate reads using an algorithm that ranks reads by the sums of their base-quality scores (default method) [1].\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n### Common Use Cases\n\n**Picard MarkDuplicates** can be used to identify duplicate reads [1].  \n\nThe tool's main output is a new SAM or BAM file, in which duplicates have been identified in the SAM flags field for each read [1]. Although the bitwise flag annotation indicates whether a read was marked as a duplicate, it does not identify the type of duplicate [1]. By invoking the **Tagging policy** (TAGGING_POLICY=) option, you can instruct the program to mark all the duplicates (All), only the optical duplicates (OpticalOnly), or no duplicates (DontTag) [1]. The records within the output of a SAM/BAM file will have values for the 'DT' tag, as either library/PCR-generated duplicates (LB), or sequencing-platform artifact duplicates (SQ) [1].\n\nThis tool uses the **Read name regex** (READ_NAME_REGEX=) and the **Optical duplicate pixel distance** (OPTICAL_DUPLICATE_PIXEL_DISTANCE=) options as the primary methods to identify and differentiate duplicate types [1]. Set **Read name regex** (READ_NAME_REGEX=) to null to skip optical duplicate detection, e.g. for RNA-seq or other data where duplicate sets are extremely large and estimating library complexity is not an aim [1]. Note that without optical duplicate counts, library size estimation will be inaccurate [1].\n\nThe **Barcode SAM tag** (BARCODE_TAG=) option is available to facilitate duplicate marking using molecular barcodes [1]. \n\nIf desired, duplicates can be removed using the **Remove duplicates** (REMOVE_DUPLICATE=) and **Remove sequencing duplicates** (REMOVE_SEQUENCING_DUPLICATES=) options [1].\n\nMarkDuplicates also produces a metrics file indicating the numbers of duplicates for both single- and paired-end reads [1].\n\n\n### Changes Introduced by Seven Bridges\n\n- Inputs **Use jdk deflater** (USE_JDK_DEFLATER=) and **Use jdk inflater** (USE_JDK_INFLATER=) are not used in the tool wrapper.\n\n \n### Common Issues and Important Notes\n\n- The program can take either coordinate-sorted or query-sorted inputs, however the behavior is slightly different [1]. When the input is coordinate-sorted, unmapped mates of mapped records and supplementary/secondary alignments are not marked as duplicates [1]. However, when the input is query-sorted (actually query-grouped), then unmapped mates and secondary/supplementary reads are not excluded from the duplication test and can be marked as duplicate reads [1].\n\n### Performance Benchmarking\n\nThree experiment tasks were performed on the platform using an On-Demand AWS instance (c4.2xlarge) and three BAM files with different sizes. One task was processing an 18GB BAM file and took 1 hour and 4 minutes ($0.29), the second task was processing a 3.2GB BAM file and took 14 minutes ($0.07) while the third task was processing a 130MB BAM file and took 2 minutes ($0.01).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### References\n\n[1] [Picard MarkDuplicates documentation](https://broadinstitute.github.io/picard/command-line-overview.html#MarkDuplicates)", "input": [{"name": "Read name regex"}, {"name": "Sorting collections size ratio"}, {"name": "Max file handles for read ends map"}, {"name": "Max sequences for disk read ends map"}, {"name": "Assume sorted"}, {"name": "Remove duplicates"}, {"name": "Comment"}, {"name": "Program group name"}, {"name": "Program group command line"}, {"name": "Program group version"}, {"name": "Program record ID"}, {"name": "Verbosity"}, {"name": "Output format"}, {"name": "Create index"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Optical duplicate pixel distance"}, {"name": "Input bam", "encodingFormat": "application/x-bam"}, {"name": "Memory per job"}, {"name": "Duplicate scoring strategy"}, {"name": "Memory overhead per job"}, {"name": "CPU per job"}, {"name": "Barcode SAM tag"}, {"name": "One barcode SAM tag"}, {"name": "Two barcode SAM tag"}, {"name": "Tag duplicate set members"}, {"name": "Remove sequencing duplicates"}, {"name": "Tagging policy"}, {"name": "Assume sort order"}, {"name": "Basename for output files"}], "output": [{"name": "Metrics file"}, {"name": "Deduped BAM", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard", "https://github.com/broadinstitute/picard/releases/tag/2.21.6"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045900, "dateCreated": 1617276917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-meanqualitybycycle-1-140/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-meanqualitybycycle-1-140/6", "applicationCategory": "CommandLineTool", "name": "Picard MeanQualityByCycle", "description": "Picard MeanQualityByCycle generates a data table and .pdf chart of mean base quality by cycle from a SAM or BAM file. This function works best on a single lane or run of data, but it can be applied also to merged BAM files. It uses R to generate chart output.", "input": [{"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Verbosity"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Aligned reads only"}, {"name": "Pf reads only"}, {"name": "Assume sorted"}, {"name": "Memory per job"}, {"name": "Stop after"}], "output": [{"name": "Quality", "encodingFormat": "text/plain"}, {"name": "Chart file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799546, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-merge-bam-alignment/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-merge-bam-alignment/1", "applicationCategory": "CommandLineTool", "name": "Picard MergeBamAlignment", "description": "__Picard MergeBamAlignment__ merges alignment data from a SAM or BAM with data in an unmapped BAM file. This tool produces a new SAM or BAM file that includes all aligned and unaligned reads and also carries forward additional read attributes from the unmapped BAM (attributes that are otherwise lost in the process of alignment). The purpose of this tool is to use information from the unmapped BAM to fix up aligner output. The resulting file will be valid for use by other Picard tools. For simple BAM file merges, use MergeSamFiles. Note that MergeBamAlignment expects to find a sequence dictionary in the same directory as REFERENCE_SEQUENCE and expects it to have the same base name as the reference FASTA except with the extension \".dict\".", "input": [{"name": "Unmapped BAM/SAM", "encodingFormat": "application/x-sam"}, {"name": "Aligned BAM/SAM", "encodingFormat": "application/x-sam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Include secondary alignments"}, {"name": "Sequence dictionary"}, {"name": "Paired run"}, {"name": "Read1 alignment BAM/SAM files", "encodingFormat": "application/x-sam"}, {"name": "Read2 alignment BAM/SAM files", "encodingFormat": "application/x-sam"}, {"name": "Aligner program group ID"}, {"name": "Program group version"}, {"name": "Program group command line"}, {"name": "Program group name"}, {"name": "Clip adapters"}, {"name": "Is bisulfite sequence"}, {"name": "Aligned reads only"}, {"name": "Maximum insertions/deletions"}, {"name": "Attributes to retain"}, {"name": "Attributes to remove"}, {"name": "Trim read1"}, {"name": "Expected orientations"}, {"name": "Trim read2"}, {"name": "Aligner proper pair flags"}, {"name": "Sort order"}, {"name": "Primary alignment strategy"}, {"name": "Clip overlaping reads"}, {"name": "Add mate CIGAR"}, {"name": "Unmap contaminant reads"}, {"name": "Minimum clipped bases"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Create_index"}], "output": [{"name": "Output BAM/SAM", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/2.1.1"], "applicationSubCategory": ["SAM/BAM-Processing"], "project": "SBG Public Data", "creator": "Broad institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1499870339, "dateCreated": 1499870339, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-mergesamfiles-1-140/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-mergesamfiles-1-140/12", "applicationCategory": "CommandLineTool", "name": "Picard MergeSamFiles", "description": "Picard MergeSamFiles merges multiple SAM or BAM files into one file.", "input": [{"name": "Input (file)", "encodingFormat": "application/x-bam"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Create index"}, {"name": "Sort order"}, {"name": "Output format"}, {"name": "Assume sorted"}, {"name": "Merge sequence dictionaries"}, {"name": "Use threading"}, {"name": "Memory per job"}], "output": [{"name": "Merged BAM", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046178, "dateCreated": 1453799150, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-qualityscoredistribution-1-140/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-qualityscoredistribution-1-140/6", "applicationCategory": "CommandLineTool", "name": "Picard QualityScoreDistribution", "description": "Picard QualityScoreDistribution charts quality score distributions in a SAM or BAM file.", "input": [{"name": "Stop after"}, {"name": "Include no calls"}, {"name": "Verbosity"}, {"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Aligned reads only"}, {"name": "Pf reads only"}, {"name": "Assume sorted"}, {"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Memory per job"}], "output": [{"name": "Chart file"}, {"name": "Quality score distribution", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["Quality Control"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799136, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-reordersam-1-140/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-reordersam-1-140/4", "applicationCategory": "CommandLineTool", "name": "Picard ReorderSam", "description": "Picard ReorderSam reorders reads in a SAM or BAM file to match the contig ordering in a provided reference file, as determined by exact-name matching of contigs.  Reads mapped to contigs absent in the new reference are dropped. This runs substantially faster if the input is an indexed BAM file.\n\nNote: the Picard ReorderSam is not to be confused with Picard SortSam, which sorts a SAM or BAM file with a valid sequence dictionary.", "input": [{"name": "Input", "encodingFormat": "application/x-bam"}, {"name": "Output format"}, {"name": "Create index"}, {"name": "Quiet"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Allow incomplete dict concordance"}, {"name": "Allow contig length discordance"}, {"name": "Verbosity"}, {"name": "Memory per job"}], "output": [{"name": "Reordered bam", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046179, "dateCreated": 1453799762, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-samtofastq-1-140/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-samtofastq-1-140/8", "applicationCategory": "CommandLineTool", "name": "Picard SamToFastq", "description": "Picard SamToFastq extracts read sequences and qualities from the input SAM or BAM file and writes them into the output file in the Sanger FASTQ format. In the RC (reverse complementary) mode (default is True), if the read is aligned and the alignment is to the reverse strand on the genome, the read's sequence from the input SAM file will be reverse-complemented prior to being written to FASTQ in order to correctly restore the original read sequence as it was generated by the sequencer.\n\nBy default, the tool is set to process paired end BAM files, and it has 3 outputs - first_pair, second_pair and unpaired. If the provided file contains only single pair reads, all the reads will be contained in the unpaired output, the other two will be empty files. If the input is known to be single pair reads, the option is there to set this so that the only output will be first_pair - paired_endness, by default this option is set to process paired end BAM files.", "input": [{"name": "Max records in RAM"}, {"name": "Compression level"}, {"name": "Validation stringency"}, {"name": "Quiet"}, {"name": "Verbosity"}, {"name": "Input file", "encodingFormat": "application/x-bam"}, {"name": "Re-reverse"}, {"name": "Include non pf reads"}, {"name": "Clipping attribute"}, {"name": "Clipping action"}, {"name": "Read1 trim"}, {"name": "Read1 max bases to write"}, {"name": "Read2 trim"}, {"name": "Read2 max bases to write"}, {"name": "Include non primary alignment"}, {"name": "Interleave"}, {"name": "Memory per job"}, {"name": "Read group tag"}, {"name": "Output based on RG tags"}, {"name": "Single or paired end input"}], "output": [{"name": "First strand", "encodingFormat": "text/fastq"}, {"name": "Second strand", "encodingFormat": "text/fastq"}, {"name": "Unpaired", "encodingFormat": "text/fastq"}, {"name": "Output_per_rg output files", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/releases/tag/1.140", "https://github.com/broadinstitute/picard/zipball/master"], "applicationSubCategory": ["File Format Conversion"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648046178, "dateCreated": 1453799161, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-sortsam-2-25-7-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-sortsam-2-25-7-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Picard SortSam", "description": "**Picard SortSam** sorts alignment files (BAM or SAM) [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard SortSam** can be used to sort an aligned SAM or BAM file (**Input alignments**) in a user-specified **Sort order**. For sorting algorithm details, please see the tool documentation [1].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--QUIET`, `--TMP_DIR` and `--version` were omitted from the wrapper.\n* Parameter `--OUTPUT` is optional in this wrapper. If the value for the **Output file name prefix** input parameter is not provided, the outputs will be named based on the **Sample ID** metadata field value of the **Input alignments** file, or if this is absent, following the base name of the **Input alignments** file. The extension of the output file is controlled using the **Output file type** custom input parameter.\n* The BAI index created using the **Create index** input parameter will always have the extension BAM.BAI.\n\n### Common Issues and Important Notes\n\n* **Input alignments** and **Sort order** inputs are required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WES NA12878 BAM file (16.2 GB) coordinate sort | 66 mins | $0.44 + $0.15 | c4.2xlarge - 1024 GB EBS | \n| WES NA12878 BAM file (16.2 GB) queryname sort | 83 mins | $0.55 + $0.19 | c4.2xlarge - 1024 GB EBS | \n| WGS NA12878 BAM file (48 GB) coordinate sort | 188 mins | $1.25 + $0.44 | c4.2xlarge - 1024 GB EBS | \n| WGS NA12878 BAM file (48 GB) coordinate sort - 10000 MB RAM | 171 mins | $1.13 + $0.40 | c4.2xlarge - 1024 GB EBS |\n| WGS NA12878 BAM file (48 GB) queryname sort | 261 mins | $1.73 + $0.60 | c4.2xlarge - 1024 GB EBS | \n| WGS NA12878 BAM file (48 GB) queryname sort - 10000 MB RAM | 233 mins | $1.55 + $0.54 | c4.2xlarge - 1024 GB EBS |\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard SortSam** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `sort_order` and `prefix` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard SortSam documentation](https://broadinstitute.github.io/picard/command-line-overview.html#SortSam)", "input": [{"name": "Input alignments to sort", "encodingFormat": "application/x-sam"}, {"name": "Output file name prefix"}, {"name": "Sort order"}, {"name": "Compression level"}, {"name": "Max records in RAM"}, {"name": "Create index"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Create MD5 file"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Use JDK deflater"}, {"name": "Use JDK inflater"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Output file type"}], "output": [{"name": "Sorted alignments", "encodingFormat": "application/x-sam"}, {"name": "Optional MD5 digest file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1636465814, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/picard-validatesamfile-2-25-7-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/picard-validatesamfile-2-25-7-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Picard ValidateSamFile", "description": "**Picard ValidateSamFile** validates an alignments file against the SAM specification [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Picard ValidateSamFile** can be used to validate a SAM or BAM file (**Input alignments**) against the SAM format specification and identify potential issues which may cause downstream analysis of the file to fail. The default validation mode (set with the **Mode of output** input parameter) is `VERBOSE`, causing the tool to exit after encountering 100 warnings and/or errors. The alternative `SUMMARY` mode will validate the whole file, but only print out a summary of the encountered issues without any additional details. Specific types of issues can be ignored using the **Validation error types to ignore** input parameter. **Ignore warnings** input parameter allows the user to only include errors in the validation report.\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--showHidden`, `--arguments_file`, `--COMPRESSION_LEVEL`, `--CREATE_INDEX`, `--CREATE_MD5_FILE`, `--GA4GH_CLIENT_SECRETS`, `--help`, `--MAX_RECORDS_IN_RAM`, `--QUIET`, `--TMP_DIR`, `--USE_JDK_DEFLATER`, `--USE_JDK_INFLATER`, `--VALIDATE_INDEX` (deprecated) and `--version` were omitted from the wrapper.\n* The output is always returned in a file (the tool native default is printing the report to the stdout).\n* The expected non-zero exit codes of the tool (1, 2 and 3) caused by issues in the input data will result in successful task completion.\n\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n* The expected non-zero exit codes of the tool (1, 2 and 3) caused by issues in the input data will result in successful task completion.\n\n### Performance Benchmarking\n\nPerformance of **Picard ValidateSamFile** depends on the size of the analyzed input and the selected mode of execution. In `SUMMARY` mode, the entire file is parsed, whereas in the default `VERBOSE` mode, the tool exits after encountering the user-specified number of issues (100 by default), without traversing the whole file.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WES NA12878 BAM file (16.2 GB) SUMMARY | 23 mins | $0.15 + $0.05 | c4.2xlarge - 1024 GB EBS | \n| WGS NA12878 BAM file (48 GB) SUMMARY | 75 mins | $0.50 + $0.17 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Picard ValidateSamFile** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `mode` and `prefix` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Picard ValidateSamFile documentation](https://broadinstitute.github.io/picard/command-line-overview.html#ValidateSamFile)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Output file name prefix"}, {"name": "Memory per job [MB]"}, {"name": "Memory overhead per job [MB]"}, {"name": "CPUs per job"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Validation stringency"}, {"name": "Verbosity of logging"}, {"name": "Validation error types to ignore"}, {"name": "Ignore warnings"}, {"name": "Index validation stringency"}, {"name": "Bisulfite sequenced"}, {"name": "Maximum open temporary files"}, {"name": "Maximum number of output lines"}, {"name": "Mode of output"}, {"name": "Skip mate validation"}], "output": [{"name": "Validation report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/broadinstitute/picard/tree/master/src", "https://github.com/broadinstitute/picard/releases/tag/2.25.7"], "applicationSubCategory": ["SAM/BAM Processing", "Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1636465814, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/pizzly-0-37-3/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/pizzly-0-37-3/16", "applicationCategory": "CommandLineTool", "name": "Pizzly", "description": "The **Pizzly** tool detects fusions candidates from **cancer RNA-seq** by using the output of the **Kallisto Quant** tool. \n\nAs of version 0.43.1, **Kallisto** can output an additional file that contains information about reads that do not pseudoalign, which potentially means that they originate from fusion genes (if the `--fusions` parameter is specified in **Kallisto Quant**). This file can then further be processed by this tool called **Pizzly** (developed by some of the same authors that developed **Kallisto**), which reports candidate fusions by filtering false positives and assembling new transcripts from the fusion reads [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- In order to successfully run **Pizzly**, *fusions.txt* and *abundance.h5* outputs from **Kallisto Quant** need to be provided on the **Kallisto fusions** and **Kallisto abundance H5 file** input nodes respectively. In addition to those two files, the **Transcriptome FASTA file** used during **Kallisto** analysis is also required.\n- A **GTF file** is optional, but recommended, to get additional outputs like the **Genetable** parsed from the *run_info.json*. \n\n### Changes Introduced by Seven Bridges\n\n- The **Insert size** (`--insert_size`) parameter value will automatically be filled in if not provided, as the *get_fragment_length.py* script provided by the tool author will in that case compute that value. \n- The **K-mer length** (`--kmer_length`) parameter will automatically be filled in if not specified by the user. Value for that parameter will be propagated from the **Kallisto Index** tool, through the use of metadata fields. \n\n### Common Issues and Important Notes\n\n- The input FASTA file should be a transcriptome FASTA, not a genomic FASTA (the same one as used in the Kallisto step).\n- The GTF and FASTA files need to have compatible chromosome namings (i.e. >1, >2, ... or >chr1, >chr2, ...). \n\n### Performance Benchmarking\n\n**Pizzly** analyzes **Kallisto** outputs in a very short amount of time, so all runs are expected to last below 5 minutes, costing below $0.05 on the default c4.2xlarge instance (AWS). \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Pizzly paper](https://www.biorxiv.org/content/early/2017/07/20/166322)", "input": [{"name": "K-mer length"}, {"name": "Alignment score"}, {"name": "Insert size"}, {"name": "Output prefix"}, {"name": "Kallisto fusions", "encodingFormat": "text/plain"}, {"name": "GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Transcriptome FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Cache"}, {"name": "Kallisto abundance H5 file"}, {"name": "Ignore protein"}, {"name": "Fragment length distribution cutoff"}, {"name": "Output genetable"}], "output": [{"name": "Fusions FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Run info JSON"}, {"name": "GTF cache", "encodingFormat": "text/plain"}, {"name": "Genetable", "encodingFormat": "text/plain"}, {"name": "Unfiltered fusions FASTA", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/pmelsted/pizzly", "https://github.com/pmelsted/pizzly", "https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz", "https://github.com/pmelsted/pizzly"], "applicationSubCategory": ["RNA-Seq", "Gene Fusion"], "project": "SBG Public Data", "creator": "Pall Melsted, Shannon Hateley, Lior Pachter", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649685705, "dateCreated": 1514564373, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/pizzly-0-37-3-cwl-1-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/pizzly-0-37-3-cwl-1-0/5", "applicationCategory": "CommandLineTool", "name": "Pizzly CWL1.0", "description": "The **Pizzly** tool detects fusions candidates from **cancer RNA-seq** by using the output of the **Kallisto Quant** tool. \n\nAs of version 0.43.1, **Kallisto** can output an additional file that contains information about reads that do not pseudoalign, which potentially means that they originate from fusion genes (if the `--fusions` parameter is specified in **Kallisto Quant**). This file can then further be processed by this tool called **Pizzly** (developed by some of the same authors that developed **Kallisto**), which reports candidate fusions by filtering false positives and assembling new transcripts from the fusion reads [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n- In order to successfully run **Pizzly**, *fusions.txt* and *abundance.h5* outputs from **Kallisto Quant** need to be provided on the **Kallisto fusions** and **Kallisto abundance H5 file** input nodes respectively. In addition to those two files, the **Transcriptome FASTA file** used during **Kallisto** analysis is also required.\n- A **GTF file** is optional, but recommended, to get additional outputs like the **Genetable** parsed from the *run_info.json*. \n\n### Changes Introduced by Seven Bridges\n\n- The **Insert size** (`--insert_size`) parameter value will automatically be filled in if not provided, as the *get_fragment_length.py* script provided by the tool author will in that case compute that value. \n- The **K-mer length** (`--kmer_length`) parameter will automatically be filled in if not specified by the user. Value for that parameter will be propagated from the **Kallisto Index** tool, through the use of metadata fields. \n\n### Common Issues and Important Notes\n\n- The input FASTA file should be a transcriptome FASTA, not a genomic FASTA (the same one as used in the Kallisto step).\n- The GTF and FASTA files need to have compatible chromosome namings (i.e. >1, >2, ... or >chr1, >chr2, ...). \n\n### Performance Benchmarking\n\n**Pizzly** analyzes **Kallisto** outputs in a very short amount of time, so all runs are expected to last below 5 minutes, costing below $0.05 on the default c4.2xlarge instance (AWS). \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Pizzly paper](https://www.biorxiv.org/content/early/2017/07/20/166322)", "input": [{"name": "Alignment score"}, {"name": "Cache"}, {"name": "Fragment length distribution cutoff"}, {"name": "GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Ignore protein"}, {"name": "Insert size"}, {"name": "Kallisto abundance H5 file"}, {"name": "Kallisto fusions", "encodingFormat": "text/plain"}, {"name": "K-mer length"}, {"name": "Output genetable"}, {"name": "Output prefix"}, {"name": "Transcriptome FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "Fusions FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Genetable", "encodingFormat": "text/plain"}, {"name": "GTF cache", "encodingFormat": "text/plain"}, {"name": "Run info JSON"}, {"name": "Unfiltered fusions FASTA", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pmelsted/pizzly", "https://github.com/pmelsted/pizzly", "https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz", "https://github.com/pmelsted/pizzly"], "applicationSubCategory": ["Gene Fusion", "RNA-Seq"], "project": "SBG Public Data", "creator": "Pall Melsted, Shannon Hateley, Lior Pachter", "softwareVersion": ["v1.0"], "dateModified": 1649685706, "dateCreated": 1574857915, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/plink-1-90/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/plink-1-90/4", "applicationCategory": "CommandLineTool", "name": "PLINK", "description": "**PLINK** is a widely used open-source tool for genome-wide association studies and research in population genetics.\n\n**PLINK** is an open-source C/C++ whole genome association studies (WGAS) tool set. It can rapidly manipulate and analyze large datasets in their entirety. Data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation are only some of **PLINK**'s domains of function [1].\n\n\nThe tool has several inputs:\n\n* **Input files** - which is a required input. The tool can take two sets of input files: text **PLINK** data (a file which contains information on the individuals and their genotypes (in PED format) and a file that contains information on the genetic markers (in MAP format); or binary **PLINK** data (including a binary file that contains individual identifiers (IDs) and genotypes (in BED format), and two text files that contain information on the individuals (in FAM format) and on the genetic markers (in BIM format). [Detailed information on file formatting](https://www.cog-genomics.org/plink/1.9/formats).\n* **Phenotype file** in PHE or TXT format, providing the list of phenotype values,\n* **Extract file** in IN or TXT format, containing a list of variant IDs to be included in the current analysis,\n* **Remove individuals** in TXT format, containing the list of samples to be removed from the current analysis,\n* **Exclude SNPs** file in TXT format, containing the list of variants to be filtered out,\n* **Keep Individuals** file in TXT format, containing the list of samples not to be filtered out in the analysis,\n* **Specify cluster** file in TXT format, which defines disjoint clusters/strata of samples for permutation procedures and stratified analyses, \n* **Covariates file** in TXT format, designating the file to load covariates from, and \n* **Target variant ID file** in TXT format, containing a list of variant IDs.\n\n\nThe tool produces the following output:\n\n* **Output files** in LOG, ASSOC, ASSOC.PERM, TDT, MODEL, MODEL.BEST.MPERM, EPI.CC, EPI.CC.SUMMARY, BED, BIM, and FAM format, as well as many others, depending on the type of analysis performed with **PLINK**. If the **Input files** have metadata fields set, it will be propagated through the analysis, though it is not necessary to have the metadata set.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**PLINK** has many functionalities, each of which represents a step in a GWAS analysis. Those functionalities include Data management, Selection of SNPs and individuals, Inclusion threshold, Reporting summary statistics, Whole genome summary statistics, Clustering, Association analysis procedures, Permutation procedure options, Haplotype inference and linkage disequilibrium, Quality scores, Epistasis analysis, Proxy association and imputation methods, LD-based result clumping, Conditional haplotype association tests, Annotation meta-analysis of the results, LD pruning and pairwise LD, Definition of sets, and Data simulation options. One of the possible GWAS analyses would include investigation of missingness per individual and per SNP, checking for sex discrepancy, deletion of SNPs which are not in Hardy-Weinberg equilibrium, checking the datasets for cryptic relatedness, merging different datasets, removal of problematic SNPs from the datasets, performing a MDS (MultiDimentional Scaling) analysis, performing the association analyses, etc. Learn more about all [functionalities of the **PLINK** tool](http://zzz.bwh.harvard.edu/plink/tutorial.shtml). Depending on the type of analysis you perform, different output files will be generated.\n\n\n### Changes Introduced by Seven Bridges\n\n* The following functionalities of the **PLINK** tool are included in this wrapper Data management, Basic statistics, Input filtering, Linkage disequilibrium, Identity-by-descent, Association analysis, Family-based association, and Epistasis analysis. Not all options are included in some of these functionalities. \n\n* **Empty string CMD** is added to enable the use of other parameters that are not included in the wrapper. It enables the creation of a custom command line. Neither the 'plink' command nor the '--file/--bfile' and '--out' option should be included in this empty string command line. For example, if you wish to run the following command line: 'plink --out test2 --bfile inFiles --fast-epistasis --parallel 1 4', you will type '--fast-epistasis --parallel 1 4', providing the input files as usual, and defining the '--bfile' and '--out' parameters.\n\n\n### Common Issues and Important Notes\n\n* Depending on the **Input files** type (text or binary data), a proper input file prefix must be used, **Input files prefix** or **Input binary files prefix**.\n* **Distributed computation option** causes PLINK to complete only one part of a job; the job index is appended to the main output filename. This option is supported by the following flags: --r/--r2, --distance, --genome, --make-rel, --make-grm-gz/--make-grm-bin, --epistasis, and --fast-epistasis. For example, in order to do the fast epistasis on a certain dataset, dividing the analysis in 3 jobs, one would have to run 3 tasks, modifying the **Distributed computation option** each time, thus setting '1 3', '2 3' and '3 3', keeping the same **Output files prefix**. After that, the main reports must be concatenated. [More information](https://www.cog-genomics.org/plink/1.9/epistasis#reg).\n* Memory and CPU requirements are set to 15GB and 8, which corresponds to the default c4.2xlarge AWS instance, optimal for most of the tested **PLINK** functionalities. In case of more demanding tasks, please set **Memory in MB per job** and **Number of CPUs per job** accordingly (you can use the table below as a hint).\n\n\n### Performance Benchmarking\n\nDepending on the chosen AWS instance, **PLINK** will use from 1/3 up to 1/2 of the available RAM memory, while it will maximize the CPU use. In order to optimize the analysis, it is recommended to run demanding tasks on AWS c5.18xlarge instances. \n| Input BED/BIM/FAM file sizes | Options | Running time | Instance (AWS on-demand)| Running cost |\n| --- | --- | --- | --- | --- |\n| 3.8GB / 606MB / 15KB | Association | 5m| c5.18xlarge| $0.30|\n| 42MB / 28MB / 4KB | Epistasis | 4d 18h 26m| c4.2xlarge| $60.7|\n| 42MB / 28MB / 4KB | Epistasis | 14h 30m| c5.18xlarge| $53.50|\n| 3.8GB / 606MB / 15KB | Fast epistasis; Distributed computation (1 20) | 4d 13h 38m| c5.18xlarge| $392.05|\n| 42MB / 28MB / 4KB | Haplotype blocks estimation | 5m| c5.18xlarge| $0.43|\n| 42MB / 28MB / 4KB | LD-based variant pruning  | 18m| c4.2xlarge| $0.16|\n| 3.8GB / 606MB / 15KB | LD-based variant pruning (pairwise) | 6m| c4.2xlarge| $0.08|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] Purcell S. et al. (2007) [PLINK: A Tool Set for Whole-Genome Association\nand Population-Based Linkage Analyses](https://www.ncbi.nlm.nih.gov/pubmed/21493656). American Journal of Human Genetics. 81 (3): 559\u201375.", "input": [{"name": "Input files", "encodingFormat": "text/x-bed"}, {"name": "Phenotype file", "encodingFormat": "text/plain"}, {"name": "Recode"}, {"name": "Input files prefix"}, {"name": "Output files prefix"}, {"name": "Association"}, {"name": "Geno"}, {"name": "Extract File", "encodingFormat": "text/plain"}, {"name": "Missing data reports"}, {"name": "Generate binary fileset"}, {"name": "Input binary files prefix"}, {"name": "Remove individuals", "encodingFormat": "text/plain"}, {"name": "Maximum per-person missing"}, {"name": "Minor allele frequency report"}, {"name": "MAF threshold"}, {"name": "Hardy-Weinberg tests"}, {"name": "SNPs to exclude", "encodingFormat": "text/plain"}, {"name": "Output Genome-wide IBS/IBD"}, {"name": "Calculate inbreeding coefficients"}, {"name": "Perform IBS clustering"}, {"name": "File with individuals to keep", "encodingFormat": "text/plain"}, {"name": "Specify the cluster file", "encodingFormat": "text/plain"}, {"name": "Missing genotype"}, {"name": "Specify the missing phenotype"}, {"name": "Specify a single snp by name"}, {"name": "A collection of individual variant IDs"}, {"name": "A file to load covariates from", "encodingFormat": "text/plain"}, {"name": "SNP window"}, {"name": "Model for case/control association"}, {"name": "Define mperm option for model/association/linear/logistic/tdt"}, {"name": "Choose perm option for model/association/linear/logistic/tdt"}, {"name": "Transmission disequilibrium test"}, {"name": "Multi-covariate association analysis on a quantitative trait"}, {"name": "Multi-covariate association analysis on a case/control phenotype"}, {"name": "LD-based variant pruning"}, {"name": "LD-based variant pruning (pairwise)"}, {"name": "LD-based variant pruning (pairphase)"}, {"name": "Report raw inter-variant allele count correlations"}, {"name": "Report squared correlations"}, {"name": "LD statistic report options"}, {"name": "LD statistic report option value"}, {"name": "Inspect the relation between a single pair of variants"}, {"name": "Target variant ID file", "encodingFormat": "text/plain"}, {"name": "Show all tags"}, {"name": "Show tags options"}, {"name": "Distributed computation option"}, {"name": "Fast epistasis analysis"}, {"name": "Epistasis linear/logistic regression-based test"}, {"name": "Estimate haplotype blocks"}, {"name": "Additional block estimation option"}, {"name": "Maximum blocks kilobases"}, {"name": "Threshold for blocks procedure"}, {"name": "Low confidence interval value"}, {"name": "High confidence interval value"}, {"name": "Recombination threshold value"}, {"name": "Strong LD pairs threshold value within a haploblock"}, {"name": "Epistasis and Fast Epistasis options"}, {"name": "Fast-Epistasis gap option"}, {"name": "Epistasis and Fast-Epistasis epi1 p-value"}, {"name": "Epistasis and Fast-Epistasis epi2 p-value"}, {"name": "Two locus option"}, {"name": "Allow no sex option"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Empty string CMD"}], "output": [{"name": "Output files", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["GWAS", "Quality Control"], "project": "SBG Public Data", "creator": "Christopher Chang with NIH-NIDDK's Laboratory of Biological Modeling, the Purcell Lab et al.", "softwareVersion": ["v1.0"], "dateModified": 1648040157, "dateCreated": 1601574748, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/plink-2-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/plink-2-0/5", "applicationCategory": "CommandLineTool", "name": "PLINK2", "description": "**PLINK2** is a widely used open-source tool for genome-wide association studies and research in population genetics.\n\n**PLINK2** is an open-source C/C++ whole genome association studies (WGAS) tool set. It can rapidly manipulate and analyze large datasets in their entirety. Data management, summary statistics, population stratification and association analysis are only some of **PLINK2**'s domains of function [1]. The second-generation versions of PLINK offer improvements in performance and\ncompatibility. \n\n\nThe tool has several inputs:\n\n* **Input files** - which is a required input. The tool can take two sets of input files: standard binary **PLINK** data (including a binary file that contains individual identifiers (IDs) and genotypes (in BED format), and two text files that contain information on the individuals (in FAM format) and on the genetic markers (in BIM format), as well as set of files in **PLINK2** binary format, which include **PLINK2** binary genotype table (in PGEN format), **PLINK2** sample information file (in PSAM format) and **PLINK2** variant information file (in PVAR format). Learn more about [file formatting](https://www.cog-genomics.org/plink/2.0/formats).\n* **Phenotype file** in PHE or TXT format, providing the list of phenotype values,\n* **Extract file** in IN or TXT format, containing a list of variant IDs to be included in the current analysis,\n* **Remove individuals** in TXT format, containing the list of samples to be removed from the current analysis,\n* **Exclude all listed variants** file in TXT format, containing the list of variants to be filtered out,\n* **Keep Individuals** file in TXT format, containing the list of samples not to be filtered out in the analysis,\n* **Specify cluster** file in TXT format, which defines disjoint clusters/strata of samples for permutation procedures and stratified analyses, and\n* **Covariates file** in TXT format, designating the file to load covariates from.\n\nThe tool produces the following output:\n\n* **Output files** in LOG, BED, BIM, FAM, PVAR, PSAM, PGEN, HARDY, HARDY.X, GLM.LOGISTIC, GLM.FIRTH, RAW and VCFformat, as well as many others, depending on the type of analysis performed with **PLINK2**. If the **Input files** have metadata fields set, it will be propagated through the analysis, though it is not necessary to have the metadata set.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n### Common Use Cases\n\n**PLINK2** has many functionalities, each of which represents a step in a GWAS analysis. Those functionalities include data management, some basic statistics, linkage disequilibrium, sample comparison, sample-distance matrices calculation, population stratification, association analysis and linear scoring. One of the possible GWAS analyses would include investigation of missingness per individual and per SNP, checking for sex discrepancy, deletion of SNPs which are not in Hardy-Weinberg, performing the association analyses, etc. Learn more about [all functionalities of the **PLINK2** tool](http://zzz.bwh.harvard.edu/plink/tutorial.shtml). Depending on the type of analysis you perform, different output files will be generated.\n\n\n### Changes Introduced by Seven Bridges\n\n* The following functionalities of the **PLINK2** tool are included in this wrapper Data management, Basic statistics, Input filtering, Linkage disequilibrium and Association analysis. Not all options are included in some of these functionalities. \n* **Empty string CMD** is added to enable the use of other parameters that are not included in the wrapper. It enables the creation of a custom command line. Neither the 'plink2' command nor the '--pfile/--bfile', nor '--out' option should be included in this empty string command line. For example, if you wish to run the following command line: 'plink2 --out test2 --bfile inFiles --glm --parallel 1 4', you will type '--glm --parallel 1 4', providing the input files as usual.\n* Output files prefix can be set through the **Output files prefix** parameter. If it is not defined, the tool will take the **Output files suffix** parameter and add it to the input file's name, thus defining the prefix for the output files. In case the **Output files suffix** is not defined either, the tool will form the output files prefix by adding '.plink' to input file's name. \n\n\n### Common Issues and Important Notes\n\n* Depending on the **Input files** type (binary data or PLINK2 binary data, or any other data type listed below), a proper input file prefix will be automatically generated. **Custom prefix for input files** can be used to manually set the input files prefix. In that case all input files should have the same name (only different extension).E.g. if input files 'inFile,bed, inFile.bim, inFile.fam' are provided, the **Custom prefix for input files** should be set to '--bfile inFile'.\n* It is recommended to define the **Output files prefix** parameter, except when running batch task, when setting **Output files suffix** is advisable.  \n* Some options, such as --hardy (Hardy-Weinberg equilibrium), --hwe (Hardy-Weinberg equilibrium exact test filter), and all options that include providing the input file (--extract, --keep, --within, etc.) must have 'Generate binary fileset' (--make-bed) or 'Generate PLINK 2 binary fileset' (--make-pgen) option set to True. \n* **Distributed computation option** causes **PLINK2** to complete only one part of a job; the job index is appended to the main output filename. This option is supported by the following flags: --make-rel, --make-grm-list/--make-grm-bin, and --make-king[-table]. For example, in order to do the one of the listed analyses on a certain dataset, dividing the analysis in 3 jobs, one would have to run 3 tasks, modifying the **Distributed computation option** each time, thus setting '1 3', '2 3' and '3 3', keeping the same **Output files prefix**. After that, the main reports must be concatenated. [More information](https://www.cog-genomics.org/plink/2.0/parallel).\n* Memory and CPU requirements are set to 15GB and 8, which corresponds to the default c4.2xlarge AWS instance, optimal for most of the tested **PLINK** functionalities. In case of more demanding tasks, please set **Memory in MB per job** and **Number of CPUs per job** accordingly (you can use tables below as a hint).\n* This version of **PLINK2** is still an alpha release, having many usual GWAS functionalities missing. Authors recommend the older version of **PLINK** still be used, as it is stable and offers more complete analyses. \n\n\n### Performance Benchmarking\n\nDepending on the chosen AWS instance, **PLINK2** will use from 1/3 up to 1/2 of the available RAM memory, while it will maximize the CPU use for most calculations. In order to optimize the analysis, it is recommended to run demanding tasks on AWS c5.18xlarge instances. \n| Input BED/BIM/FAM file sizes | Options | Running time | Instance (AWS on-demand)| Running cost |\n| --- | --- | --- | --- | --- |\n| 3.8GB / 606MB / 15KB | Association | 4m| c4.2xlarge| $0.04|\n| 43.5GB / 2.5GB / 62KB | Association | 32m| c4.2xlarge| $0.30|\n| 43.5GB / 2.5GB / 62KB | Association | 7m| c5.18xlarge| $0.43|\n| 43.5GB / 2.5GB / 62KB | Export VCF | 1h 51m| c5.18xlarge| $5.96|\n| 43.5GB / 2.5GB / 62KB | Export VCF | 5h 18m| c4.2xlarge| $2.86|\n| 42MB / 28MB / 4KB | Export VCF  | 2m| c4.2xlarge| $0.02|\n| 3.8GB / 606MB / 15KB | LD-based variant pruning (pairwise) | 12m| c4.2xlarge| $0.12|\n| 3.8GB / 606MB / 15KB | LD-based variant pruning (pairwise) | 5m| c5.9xlarge| $0.14|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] Chang C. et al. (2015) [Second-generation PLINK: rising to the challenge of larger and richer datasets](https://gigascience.biomedcentral.com/articles/10.1186/s13742-015-0047-8). GigaScience 4 (7).", "input": [{"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "Phenotype file", "encodingFormat": "text/plain"}, {"name": "Export"}, {"name": "Output files prefix"}, {"name": "Primary association analysis command"}, {"name": "Geno"}, {"name": "Extract File", "encodingFormat": "text/plain"}, {"name": "Missing data reports"}, {"name": "Generate binary fileset"}, {"name": "Remove individuals", "encodingFormat": "text/plain"}, {"name": "Maximum per-sample missing"}, {"name": "Minor allele frequency report"}, {"name": "MAF threshold"}, {"name": "Hardy-Weinberg equilibrium"}, {"name": "Exclude all listed variants", "encodingFormat": "text/plain"}, {"name": "File with individuals to keep", "encodingFormat": "text/plain"}, {"name": "Specify the cluster file", "encodingFormat": "text/plain"}, {"name": "Specify a single snp by name"}, {"name": "A collection of SNPs"}, {"name": "A file to load covariates from", "encodingFormat": "text/plain"}, {"name": "SNP window"}, {"name": "Additional association options"}, {"name": "LD-based variant pruning (pairwise)"}, {"name": "Inspect the relation between a single pair of variants"}, {"name": "Distributed computation option"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Empty string CMD"}, {"name": "Additional export output options"}, {"name": "Generate PLINK 2 binary fileset"}, {"name": "Additional Hardy-Weinberg equilibrium options"}, {"name": "Hardy-Weinberg equilibrium exact test filter"}, {"name": "Oxford-format genotype modes"}, {"name": "Custom prefix for input files"}, {"name": "Output files suffix"}], "output": [{"name": "Output files", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["GWAS", "Quality Control"], "project": "SBG Public Data", "creator": "Christopher Chang with support from GRAIL, Inc. and Human Longevity, Inc., etc.", "softwareVersion": ["v1.0"], "dateModified": 1648040158, "dateCreated": 1601574748, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/prsice-2-2-3-3-cwl1-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/prsice-2-2-3-3-cwl1-1/5", "applicationCategory": "CommandLineTool", "name": "PRSice-2", "description": "**PRSice-2** is a tool for polygenic risk score (PRS) analyses [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**PRSice-2** can be used for calculating PRS based on GWAS data, estimating empirical P-values and visualizing the results of PRS analysis [1,2]. Starting from Plink (BED, BIM, FAM) or BGEN (1.2) target data and a GWAS results file with overlapping variants, **PRSice-2** can conduct PRS analysis of multiple traits with support for different inheritance models.\n\n### Changes Introduced by Seven Bridges\n\n* Input parameter `--dir` has been excluded from the wrapper as ggplot is already installed.\n* Input parameter `--prsice` has been hardcoded in the wrapper to the location of the binary PRSice executable.\n* Input parameter `--help` has been omitted from the wrapper.\n* If Plink files are used as **Target Files**, a common prefix is expected for BED, BIM and FAM files. If supplying data in per-chromosome files for autosomes (1-22), please use the **Custom --target value** parameter to set the desired value for the `--target` parameter and override the wrapper defaults.\n* The parameter `--background` is split into two input parameters, **Background file** and **Background file type**. Both inputs must be provided.\n* If a custom SNP set name is needed, it should be provided via the **Custom SNP set name** input parameter.\n* The `--x-range` parameter has been split into **File with SNP ranges to exclude** and **SNP ranges to exclude** inputs. Only one of these two inputs should be provided.\n* Input parameter `--bar-col-lower` was wrapped as `--bar-col-low` to match the parameter usage in the tool code.\n\n### Common Issues and Important Notes\n\n* Please note that not all combinations of input files and parameters were extensively tested, due to the large number of inputs and available parameters.\n* Inputs **Base file** and **Target files** are required.\n* If using BGEN files as target files, please set the **Target file type** input parameter to 'bgen'.\n* Inputs **Beta test statistic** and **Test statistic in OR format** are mutually exclusive.\n* Inputs **File with target samples to keep** and **File with target samples to remove** are mutually exclusive.\n* Inputs **File with LD reference samples to extract** and **File with LD reference samples to remove** are mutually exclusive.\n* If a background file (**Background file**) is supplied, a background type (**Background type**) must be supplied as well.\n* Input **GTF file containing gene boundaries** is required with **MSigDB file**.\n\n### Performance Benchmarking\n\nPerformance of **PRSice-2** depends on the size of the input dataset (number of individuals and variants) and the selected tool options. Please note that performing permutations will slow down the tool significantly. For illustration, analyzing a binary phenotype on a target dataset of 2548 participants (1283 controls, 1265 cases) and 73257632 variants (61658716 eligible for full analysis) took 99 minutes on an on-demand r5.4xlarge instance with 1000 GB storage (cost: $1.67 + $0.23), using 15 cores and 71 GB of RAM.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**PRSice-2** was tested with cwltool version 3.1.20211107152837. The `in_base_file` and `in_target_files` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [PRSice-2 publication](https://academic.oup.com/gigascience/article/8/7/giz082/5532407)\n\n[2] [PRSice-2 documentation](https://www.prsice.info/)", "input": [{"name": "Base INFO score filtering cutoff"}, {"name": "Base MAF filtering"}, {"name": "Beta test statistic"}, {"name": "SNP coordinates column name"}, {"name": "Chromosome column name"}, {"name": "Column indices provided"}, {"name": "Remove all default options"}, {"name": "Test statistic in OR format"}, {"name": "Column name for p-value"}, {"name": "Column name for SNP IDs"}, {"name": "Column with summary statistic"}, {"name": "Base file", "encodingFormat": "text/plain"}, {"name": "Target files", "encodingFormat": "text/x-bed"}, {"name": "Binary target phenotype"}, {"name": "Filter imputed SNPs based on INFO score"}, {"name": "File with target samples to keep", "encodingFormat": "text/plain"}, {"name": "Filter SNPs on MAF"}, {"name": "Keep nonfounders in the analysis"}, {"name": "Phenotype file", "encodingFormat": "text/plain"}, {"name": "Phenotype columns"}, {"name": "Prevalence for binary traits"}, {"name": "File with target samples to remove", "encodingFormat": "text/plain"}, {"name": "Custom --target value"}, {"name": "Target list file", "encodingFormat": "text/plain"}, {"name": "Target file type"}, {"name": "Allow intermediary file generation"}, {"name": "Missing call genotype probability cutoff"}, {"name": "Hard call cutoff"}, {"name": "Use hard coding instead of dosage"}, {"name": "Distance for clumping in kb"}, {"name": "R2 threshold for clumping"}, {"name": "P-value cutoff for clumping"}, {"name": "LD reference file", "encodingFormat": "text/x-bed"}, {"name": "LD dose threshold"}, {"name": "Genotype missingness cutoff to filter SNPs"}, {"name": "LD hard call threshold"}, {"name": "Filter imputed LD reference SNPs INFO score"}, {"name": "File with LD reference samples to extract"}, {"name": "File with LD reference prefixes"}, {"name": "MAF threshold to filter SNPs [LD]"}, {"name": "File with LD reference samples to remove"}, {"name": "LD reference file type"}, {"name": "Stop PRSice from performing clumping"}, {"name": "Proxy threshold for index SNP"}, {"name": "Covariate file", "encodingFormat": "text/plain"}, {"name": "Covariate columns"}, {"name": "Categorical covariate columns"}, {"name": "Bar chart levels"}, {"name": "Only calculate thresholds from bar chart levels"}, {"name": "Do not include the full model"}, {"name": "Step size of the interval threshold"}, {"name": "Starting p-value threshold"}, {"name": "Genetic model used for regression"}, {"name": "Method to handle missing genotypes"}, {"name": "Do not perform the regression analysis"}, {"name": "Polygenic score method"}, {"name": "Upper p-value threshold"}, {"name": "Background file", "encodingFormat": "text/plain"}, {"name": "Background file type"}, {"name": "Selected regions BED file", "encodingFormat": "text/x-bed"}, {"name": "Features to be included from the GTF file"}, {"name": "Use the whole genome as background"}, {"name": "GTF file containing gene boundaries", "encodingFormat": "application/x-gtf"}, {"name": "MSigDB file"}, {"name": "SNP set file"}, {"name": "Custom SNP set name"}, {"name": "Add N bases to the 3' region of each feature"}, {"name": "Add N bases to the 5' region of each feature"}, {"name": "Colour of the most predicting threshold"}, {"name": "Colour of the poorest predicting threshold"}, {"name": "Change the colour of bar to p-value threshold"}, {"name": "Bar chart palette"}, {"name": "Plotting device"}, {"name": "Multi plot"}, {"name": "Plotting only"}, {"name": "Gene set to plot"}, {"name": "Number of quantiles to plot"}, {"name": "Quantile groupings for plotting the strata plot"}, {"name": "Separate quantile sample IDs for plotting", "encodingFormat": "text/plain"}, {"name": "Reference quantile for quantile plot"}, {"name": "R2 as y-axis of the high-resolution plot"}, {"name": "Output PRS for ALL thresholds"}, {"name": "Construct RS IDs based on variant data"}, {"name": "File with SNPs to exclude from the analysis", "encodingFormat": "text/plain"}, {"name": "File with SNPs to include in the analysis", "encodingFormat": "text/plain"}, {"name": "Sample ID delimiter"}, {"name": "Ignore FID column in all inputs"}, {"name": "Keep ambiguous SNPs"}, {"name": "Use logistic regression for permutation"}, {"name": "Memory per job [MB]"}, {"name": "Number of CPUs per job"}, {"name": "Number of threads"}, {"name": "Calculate non-cumulative PRS"}, {"name": "Output file name prefix"}, {"name": "Number of permutations to perform"}, {"name": "Print all SNPs remaining in the analysis"}, {"name": "Seed to use for permutation"}, {"name": "Use reference samples for imputing missingness"}, {"name": "Ultra aggressive memory usage"}, {"name": "File with SNP ranges to exclude", "encodingFormat": "text/x-bed"}, {"name": "Ranges of SNPs to exclude from the analysis"}, {"name": "A1 column name"}, {"name": "A2 column name"}], "output": [{"name": "PRSice-2 log"}, {"name": "PRSice-2 summary file"}, {"name": "PRSice-2 model-fit results"}, {"name": "PRS individual scores at best-fit model"}, {"name": "PRSice-2 bar plot"}, {"name": "PRSice-2 high resolution plot"}, {"name": "PRSice-2 quantile plot"}, {"name": "PRSice-2 quantile data for plotting", "encodingFormat": "text/plain"}, {"name": "PRSice-2 multi set bar plot"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/choishingwan/PRSice", "https://github.com/choishingwan/PRSice/releases/download/2.3.3/PRSice_linux.zip"], "applicationSubCategory": ["PRS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Shing Wan Choi", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612364586, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/razers3/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/razers3/11", "applicationCategory": "CommandLineTool", "name": "RazerS 3", "description": "RazerS 3 is a tool for mapping millions of short genomic reads onto a\nreference genome. It was designed with focus on mapping next-generation\nsequencing reads onto whole DNA genomes. RazerS 3 searches for matches of\nreads with a percent identity above a given threshold (-i X), whereby it\ndetects alignments with mismatches as well as gaps.\n\nRazerS 3 consists of a filtration part, in which a k-mer filter scans the\ngenome for regions that possibly contain read matches, and a verification\npart, where results from the filtration are then subjected to a verification\nalgorithm. The user can choose between two filters: (1) a seed-based filter\nbased on the pigeonhole principle or (2) a k-mer counting filter based on the\nSWIFT algorithm (Rasmussen et al., 2006). The pigeonhole filter (default) is\nfaster for a broad range of read sets and error rates, whereas the swift\nfilter (-fl swift) is faster for short reads (<50bp) and high error rates\n(10-20%).\n\n### Common Use Cases\n\n* RazerS 3 expects a reference FASTA file and one or two FASTQ files. If two FASTQ files are given, both have to contain exactly the same number of reads, which are considered as read\npairs.\n\n* The output default format is BAM. MicroRazerS native format \".result\" and SAM format are also available through the output_name input option. \n\n\n### Changes Introduced by Seven Bridges\n\n* Input files must have paired-end metadata.\n \n### Performance Benchmarking\n\nBenchmarking was performed on m2.xlarge AWS instance. The database of human miRNAs from miRBase was used as reference genome. Seed-based pigeonhole filter was selected. \n\n| Input size [Mb] | Duration [min] |  Cost |   Instance  |\n|:---------------:|:--------------:|:------------:|:-----------:|\n|       95       |      6       |     0.02     | m2.xlarge  |\n|       150     |      8       |     0.03     |  m2.xlarge |\n|       280     |      7       |     0.03     |  m2.xlarge |\n|       360     |      7       |     0.03     |  m2.xlarge |\n|       505     |      7       |     0.03     |  m2.xlarge |", "input": [{"name": "Reference file", "encodingFormat": "application/x-tar"}, {"name": "MiRNA reads", "encodingFormat": "text/fastq"}, {"name": "Output name"}, {"name": "Set the percent identity threshold"}, {"name": "NUM of the best matches"}, {"name": "Map against a given strand"}, {"name": "N considered as errors or any value"}, {"name": "Omit reads"}, {"name": "Percent recognition rate"}, {"name": "Mismatches only errors"}, {"name": "Trim reads"}, {"name": "Mean library size"}, {"name": "Tolerated absolute deviation of the library size"}, {"name": "Best match of a read with E errors"}, {"name": "Filter"}, {"name": "Dump the alignment"}, {"name": "Output genome names"}, {"name": "Output read names"}, {"name": "Use full read id"}, {"name": "Output match order"}, {"name": "Output positions format"}], "output": [{"name": "Aligned SAM/razers3", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/seqan/seqan/tree/master/apps/razers3", "https://github.com/seqan/seqan/tree/master/apps/razers3", "https://github.com/seqan/seqan/tree/master/apps/razers3"], "applicationSubCategory": ["MiRNA", "RNA"], "project": "SBG Public Data", "creator": "D. Weese, M. Holtgrewe, K. Reinert", "softwareVersion": ["sbg:draft-2"], "dateModified": 1545144756, "dateCreated": 1520434395, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/regenie-2-0-1-cwl1-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/regenie-2-0-1-cwl1-1/5", "applicationCategory": "CommandLineTool", "name": "Regenie", "description": "**Regenie** is a tool for whole genome regression analysis [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**Regenie** can be used to perform association testing for binary and quantitative traits [1,2]. This wrapper supports both tool execution modes (step 1 and 2). The two steps should be executed together (step 1 followed by step 2).\n\nPlease consult the tool documentation [1] for details regarding input parameters and the recommended analysis flow.\n\nFor processing UK Biobank data, please consider following the recommendations [3] given by the tool authors.\n\nStep 1 of **Regenie** can be parallelized through multiple task executions [4]. To replicate this flow on the Seven Bridges platforms, run step 1 with the **Split level 0** parameter, specifying the prefix of the outputs and the desired number of jobs. The output files (**Split L0 files**) produced by this task should all be supplied as inputs (**Split L0 files to use**) for each subsequent task for individual level 0 jobs (**Run level 0 job** input parameter should be used to specify the current task index). Level 0 temporary outputs of these tasks (**Run L0 files**) should all be provided together as inputs (**Split L0 files to use**) to the final level 1 task, with the master split file given as the **Split level 0 master file to run level 1** input.\n\n### Changes Introduced by Seven Bridges\n\n* The step1 output file listing predictions (*.pred.list) (**File listing prediction files**) has been modified to strip the absolute paths from each row, leaving only file names. This was done to allow this file to be directly used as an input for step 2 analysis on the Seven Bridges platforms.\n* Input **Prediction files from step 1** was added to allow the staging of step 1 prediction outputs for use in step 2 tasks.\n* Input **Split L0 files to use** was added to allow parallelization of level 0 executions.\n* Parameters `--help` and `--helpFull` were omitted from the wrapper.\n* Parameter **Create debug log in task stats** was added to assist with troubleshooting tasks. If this input is used, the extra log can be found on the task stats page. In addition to error messages, this log will also contain redirected information from the regular tool log.\n\n### Common Issues and Important Notes\n\n* **Phenotype file**, **Step** and **Size of genotype blocks** inputs are required.\n* At least one of the **Plink files**, **Plink2 files** or **BGEN file** should be supplied.\n* **File with variant IDs to retain** and **File with variant IDs to remove** inputs are only applicable to step 1 executions.\n* Step 1 executions only support a single file containing genotypes.\n* In the **Phenotype file** all missing values should be coded as `NA`.\n* Binary traits should be coded as 0, 1, and NA for control, case, and missing respectively, unless the **Use control 1 case 2 encoding for binary traits** input is used.\n* To analyze chrX genotypes, males should be coded as 0/2 (diploid). For details on how to achieve this with **Plink**, please consult [1].\n* The same covariate file (**Covariate file**) should be used for step 1 and step 2 tasks in an analysis. No missing values are allowed in this file.\n* **File with step1 prediction files** input should be supplied for step 2 executions. The corresponding prediction files should be supplied through the **Prediction files from step 1** input.\n* By default, step 2 uses all available threads.\n* For troubleshooting common tool failures, please consult the tool FAQ page [5].\n\n### Performance Benchmarking\n\n**Regenie** performance greatly depends on the selected inputs and parameters. For details, please see the official tool documentation [1].\n\nTwo simulated datasets were used for testing: \n* sim2k - 2000 individuals, ~780k variants, 1 phenotype\n* sim485k - ~485k individuals ~500k variants (1.26 million variants for step 2), 3 phenotypes\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| sim2k - step1  | 40 min | $0.27 + $0.10 |  c4.2xlarge 1024 GB EBS |\n| sim2k - step2  | 2 min | $0.01 + $0.01 |  c4.2xlarge 1024 GB EBS |\n| sim485k - step1  | 13 h 33 min | $20.74 + $1.86 |  c5.9xlarge 1024 GB EBS |\n| sim485k - step2  | 16 h 29 min | $25.22 + $1.11 |  c5.9xlarge 500 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**Regenie** was tested with cwltool version 3.0.20201203173111. The `pheno_file`, `in_bed`, `step`, `bsize` and `bt` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [Regenie documentation](https://rgcgithub.github.io/regenie/)\n\n[2] [Regenie publication](https://www.biorxiv.org/content/10.1101/2020.06.19.162354v2)\n\n[3] [Regenie UK Biobank processing recommendations](https://rgcgithub.github.io/regenie/recommendations/)\n\n[4] [Regenie step 1 parallelization flow](https://github.com/rgcgithub/regenie/wiki/Further-parallelization-for-level-0-models-in-Step-1)\n\n[5] [Regenie FAQ page](https://rgcgithub.github.io/regenie/faq/)", "input": [{"name": "Step"}, {"name": "Plink files", "encodingFormat": "text/x-bed"}, {"name": "Plink2 files"}, {"name": "BGEN file"}, {"name": "BGEN sample file", "encodingFormat": "text/plain"}, {"name": "File with samples to keep", "encodingFormat": "text/plain"}, {"name": "File with samples to remove", "encodingFormat": "text/plain"}, {"name": "File with variant IDs to retain", "encodingFormat": "text/plain"}, {"name": "File with variant IDs to remove", "encodingFormat": "text/plain"}, {"name": "Phenotype file", "encodingFormat": "text/plain"}, {"name": "Phenotype column"}, {"name": "Phenotype columns to keep"}, {"name": "Covariate file", "encodingFormat": "text/plain"}, {"name": "Covariate columns"}, {"name": "Covariates to keep"}, {"name": "Analyze phenotypes as binary"}, {"name": "Use control 1 case 2 encoding for binary traits"}, {"name": "Size of genotype blocks"}, {"name": "Number of cross validation folds"}, {"name": "Use leave-one out cross validation"}, {"name": "Number of ridge parameters [l0]"}, {"name": "Number of ridge parameters to use [l1]"}, {"name": "Reduce memory usage with temporary files"}, {"name": "Prefix for step 1 temporary files"}, {"name": "Remove all samples with missingness"}, {"name": "Prefix for output files"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Prediction files from step 1"}, {"name": "Skip reading step 1 predictions"}, {"name": "Write sample IDs for each trait [step 2]"}, {"name": "Minimum minor allele count"}, {"name": "Split asssociation results into separate files"}, {"name": "Use Firth correction for p-values below cutoff"}, {"name": "Use approximation to Firth correction"}, {"name": "Use SPA for p-values below the cutoff"}, {"name": "P-value threshold for Firth/SPA correction"}, {"name": "Chromosomes to test in step 2"}, {"name": "List of chromosomes to test in step 2"}, {"name": "Test"}, {"name": "Compress output files"}, {"name": "File with step1 prediction files"}, {"name": "Reference allele is the first allele"}, {"name": "Split level 0"}, {"name": "Run level 0 job"}, {"name": "Split level 0 master file to run level 1"}, {"name": "Keep level 0 predictions"}, {"name": "Output whole genome PRS"}, {"name": "Use PRS"}, {"name": "Minimum imputation info score"}, {"name": "Range of positions to test in step 2"}, {"name": "Sets definition"}, {"name": "Extract sets file"}, {"name": "Exclude sets file"}, {"name": "Extract set list"}, {"name": "Exclude set list"}, {"name": "Variant annotations"}, {"name": "Annotation labels"}, {"name": "Mask definitions"}, {"name": "AAF for building masks"}, {"name": "AAF bins"}, {"name": "Rule to construct masks"}, {"name": "Singleton carrier"}, {"name": "Write masks in PLINK format"}, {"name": "Mask LOVO"}, {"name": "Skip test"}, {"name": "Check inputs consistency"}, {"name": "Exit early with inconsistent inputs"}, {"name": "Split L0 files to use"}, {"name": "Verbose screen output"}, {"name": "Set l0 ridge parameters"}, {"name": "Set l1 ridge parameters"}, {"name": "Number of autosomes"}, {"name": "Number of blocks to use"}, {"name": "Maximum number of logistic regression iterations"}, {"name": "Maximum step size in null Firth logistic regression"}, {"name": "Maximum number of iterations in null Firth logistic regression"}, {"name": "Categorical covariates list"}, {"name": "Maximum categorical covariates levels"}, {"name": "Force step 1"}, {"name": "Force impute"}, {"name": "Firth SE"}, {"name": "Print phenotype name [step 2]"}, {"name": "Create debug log in task stats"}], "output": [{"name": "Regenie results", "encodingFormat": "text/plain"}, {"name": "Prediction files"}, {"name": "File listing prediction files"}, {"name": "Step2 regenie results"}, {"name": "Regenie IDs files"}, {"name": "Split L0 files"}, {"name": "Run L0 files"}, {"name": "Mask files in Plink format", "encodingFormat": "text/x-bed"}, {"name": "PRS files"}, {"name": "Check burden files report", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/rgcgithub/regenie", "https://github.com/rgcgithub/regenie/releases/tag/v1.0.5.8-newest"], "applicationSubCategory": ["GWAS", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Joelle Mbatchou", "softwareVersion": ["v1.1"], "dateModified": 1648045276, "dateCreated": 1618329629, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/RNA-SeQC/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/RNA-SeQC/10", "applicationCategory": "CommandLineTool", "name": "RNA-SeQC", "description": "RNA-SeQC is a tool that computes quality control metrics for RNA-seq data. It takes BAM files as input and outputs HTML reports and files of metrics data. The HTML pages are exposed as different outputs so that they are visible on the SBG platform.\n\n**Pre-run Checklist**   \n1. Are the contig names consistent between BAM(s), Reference and the GTF file?  \n2. Are BAMs coordinate-sorted and have read-group information? \n3. Are BAMs indexed? (.bai files  in Project Files)\n4. Is Reference indexed? (.fai file is in Project Files)  \n5. BAM files should have SampleID and LibraryID metadata fields populated for naming the sample(s)/dir(s) in report and for setting notes for samples.\n\n**NOTE**: GC stratification options are currently unsupported due to errors encountered while executing RNA-SeQC 1.1.8 with these options set according to specification.", "input": [{"name": "BAM files", "encodingFormat": "application/x-bam"}, {"name": "GTF file defining transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Reference genome in FASTA format", "encodingFormat": "application/x-fasta"}, {"name": "Downsample"}, {"name": "Transcripts end length"}, {"name": "Top transcripts number"}, {"name": "No depth of coverage"}, {"name": "No read counting"}, {"name": "Single-end aligned reads"}, {"name": "Strict mode"}, {"name": "Transcript details"}, {"name": "Transcript type"}, {"name": "rRNA downsample target"}, {"name": "Plot gap length distribution"}, {"name": "GATK flags"}, {"name": "Correlation GCT file"}, {"name": "GCT expression values"}, {"name": "rRNA estimation type"}, {"name": "rRNA estimation file", "encodingFormat": "application/x-fasta"}, {"name": "Java '-Xmx' allocated memory in megabytes"}], "output": [{"name": "All reports and output files as archive", "encodingFormat": "application/x-tar"}, {"name": "Overall report"}, {"name": "Report for highly expressed transcripts"}, {"name": "Report for medium expressed transcripts"}, {"name": "Report for low expressed transcripts"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649686803, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rsem-calculate-expression-1-3-3/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rsem-calculate-expression-1-3-3/1", "applicationCategory": "CommandLineTool", "name": "RSEM Calculate Expression", "description": "**RSEM Calculate Expression** estmates transcript abundances using the **Expectation Maximization (EM)** procedure. \n\nIf provided with FASTQ files, this tool aligns input reads against a reference transcriptome with a specified aligner and calculates expression values using the alignments. It is based on the Expectation-Maximization algorithm for quantifying abundances of the transcripts from single-end or paired-end RNA-Seq data. It fractionally assigns reads (also correctly handles multi-reads) mapped to a transcriptome for estimation of isoform expression levels; these are later further used to estimate gene expression levels. If provided with a BAM file, the alignment step is skipped and the EM procedure is directly performed [1].\n\nThe aligners that **RSEM** can internally use are **Bowtie 1.2.3**, **Bowtie2 2.4.1**, **STAR 2.7.3a** and **HISAT2 2.2.0**. \nFor sorting BAM file, **RSEM** uses **SAMtools 1.3**. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n1. **RSEM Prepare Reference** must be run with the appropriate reference before using this tool. The resulting TAR archive, containing the necessary index files, can then be provided to this tool on the **Archive of all files outputed by RSEM prepare reference** input. \n\n2. **RSEM Calculate Expression** can be run in two modes: from FASTQ files (in which case alignment is first performed with the **Aligner** of choice, the default being **Bowtie**, followed by the EM procedure), or from a BAM file (in which case alignment is skipped, and EM is directly performed). Note that the BAM file must be in transcript coordinates and without gapped alignment. \n\n3. For single-end data, it is strongly recommended that the user provides the fragment length distribution parameters - **Mean fragment length** and **Fragment length standard deviation** (`--fragment-length-mean` and `--fragment-length-sd`, respectively).  For paired-end data, **RSEM** will automatically learn a fragment length distribution from the data.\n\n4. Some aligner parameters have default values different from their original settings.\n\n5. With the **Calculate posterior mean estimates** (`--calc-pme`) option, posterior mean estimates will be calculated in addition to maximum likelihood estimates.\n\n6. With the **Calculate credibility intervals** (`--calc-ci`) option, 95% credibility intervals and posterior mean estimates will be calculated in addition to maximum likelihood estimates.\n\n7. The temporary directory and all intermediate files will be removed when **RSEM** finishes unless **Keep temporary files generated by RSEM** (`--keep-intermediate-files`) is specified (specify this if you want to keep **STAR** logs, for example). \n\n### Common issues and important notes\n \n\n1. In case of paired-end FASTQs files, it is crucial to set the **Paired End** metadata field to 1/2.\n\n2. For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID** / **Library ID** / **Platform unit ID** / **File segment number**).\n\n3. If running **RSEM Calculate Expression** from BAM files, the BAM needs to be in transcript coordinates and without gapped alignment. Typically, if **STAR** is first run, turning the option `--quantMode TranscriptomeSAM` will produce a BAM file in transcript coordinates, which is suited for **RSEM**. \n\n### Changes Introduced by Seven Bridges\n\n1. All output files will be prefixed by the input **Sample ID**. \n\n2. The options to choose aligners have been merged into a single enum (**Aligner** option), instead of being four separate boolean options. The default is left to be **Bowtie**. \n\n3. If running **RSEM Calculate Expression** from BAM files, **SAMtools** will be run first to determine whether the BAM file came from paired-end or single-end data, thus eliminating the need for the user for specify so, and allowing for batch running of samples. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, and different input file types (FASTQ or aligned BAM), with the STAR aligner, executed on the on-demand AWS cloud instances:\n\n\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq - FASTQ     |  2 x 590 MB |     Yes    |     5M     |     101     |   16min   | $0.43| c5.9xlarge |\n|     RNA-Seq - FASTQ    |  2 x 1.9 GB |     Yes    |     16M     |     101     |   22min   | $0.59| c5.9xlarge |\n|     RNA-Seq - FASTQ     |  2 x 5.6 GB |     Yes    |     50M     |     101     |   43min   | $1.15| c5.9xlarge |\n|     RNA-Seq - FASTQ    | 2 x 18.5 GB |     Yes    |     163M     |     101     |   1h49min  | $2.91 | c5.9xlarge |\n|     RNA-Seq - BAM     |  636 MB |     Yes    |     5M     |     101     |   6min   | $0.16| c5.9xlarge |\n|     RNA-Seq - BAM    |  1.8 GB |     Yes    |     16M     |     101     |   11min   | $0.29| c5.9xlarge |\n|     RNA-Seq - BAM    | 5.3 GB |     Yes    |     50M     |     101     |   24min  | $0.64 | c5.9xlarge | \n|     RNA-Seq - BAM    | 17.5 GB |     Yes    |     163M     |     101     |   1h01min  | $1.63 | c5.9xlarge |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [RSEM manual](http://deweylab.biostat.wisc.edu/rsem/README.html)", "input": [{"name": "Archive of all files outputed by RSEM prepare reference", "encodingFormat": "application/x-tar"}, {"name": "Read files", "encodingFormat": "application/x-sam"}, {"name": "Append names"}, {"name": "No BAM output"}, {"name": "Output genome BAM"}, {"name": "Sampling for BAM"}, {"name": "Random number generator seed"}, {"name": "Single cell prior"}, {"name": "Calculate posterior mean estimates"}, {"name": "Calculate credibility intervals"}, {"name": "SAM header info"}, {"name": "Seed length"}, {"name": "Tag name"}, {"name": "Bowtie N"}, {"name": "Bowtie E"}, {"name": "Bowtie M"}, {"name": "Bowtie chunk MBs"}, {"name": "Quality scores"}, {"name": "Bowtie 2 mismatch rate"}, {"name": "Bowtie 2 K"}, {"name": "Bowtie 2 sensitivity level"}, {"name": "Output STAR genome BAM"}, {"name": "Sort BAM by read name"}, {"name": "Minimum fragment length"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Estimate RSPD"}, {"name": "Number of bins in the RSPD"}, {"name": "Gibbs burn-in"}, {"name": "Gibbs number of samples"}, {"name": "Gibbs sampling gap"}, {"name": "Credibility intervals credibility level"}, {"name": "Credibility intervals memory"}, {"name": "Credibility intervals number of samples per count vector"}, {"name": "Keep temporary files generated by RSEM"}, {"name": "Time"}, {"name": "Sort BAM by coordinate"}, {"name": "Strandedness"}, {"name": "Number of threads"}, {"name": "Sort BAM memory per thread"}, {"name": "Output prefix"}, {"name": "Aligner"}, {"name": "Memory per job"}], "output": [{"name": "RSEM plot model folder", "encodingFormat": "application/x-tar"}, {"name": "Isoform level expression estimates"}, {"name": "Gene level expression estimates"}, {"name": "Allele level expression estimates"}, {"name": "BAM in transcript coordinates", "encodingFormat": "application/x-bam"}, {"name": "STAR log files"}, {"name": "BAM in genome coordinates", "encodingFormat": "application/x-bam"}, {"name": "STAR genome BAM", "encodingFormat": "application/x-bam"}, {"name": "Time information"}, {"name": "STAR splice junctions"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/deweylab/RSEM", "https://github.com/deweylab/RSEM/archive/v1.3.3.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Quantification", "CWL1.0"], "project": "SBG Public Data", "creator": "Bo Li, Colin Dewey", "softwareVersion": ["v1.0"], "dateModified": 1590513120, "dateCreated": 1590512891, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rsem-plot-model-1-3-3/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rsem-plot-model-1-3-3/1", "applicationCategory": "CommandLineTool", "name": "RSEM Plot Model", "description": "**RSEM Plot Model** generates plots for visualising the model learned by **RSEM**.  \n\nThe plots depend on read type and user configuration and may include fragment length distribution, read length distribution, read start position distribution, quality score vs observed quality given a reference base, position vs percentage of sequencing error given a reference base, and histogram of reads with different number of alignments [1].\n\nAlignment statistics include a histogram and a pie chart. For the histogram, the x-axis shows the number of isoform-level alignments a read has and the y-axis provides the number of reads with that many alignments. The \"inf\" on x-axis means the number of reads filtered due to too many alignments. For the pie chart, four categories of reads --- unalignable, unique, isoform-level-multi-mapping and filtered -- are plotted and their percentages are noted. In both the histogram and the piechart, numbers belong to unalignable, unique, multi-mapping and filtered are colored as green, blue, gray and red. \n\n### Common Use Cases\n\n* In order for this tool to work properly, just supply it with an **RSEM plot model folder** (in the form of a TAR archive), as outputted by the **RSEM Calculate Expression** tool. \n\n### Common issues and important notes\n\nNone\n\n### Changes Introduced by Seven Bridges\n\n* The directory containing the model files is outputted as a TAR bundle by the **RSEM Calculate Expression** tool. This bundle can then be provided to the **RSEM Plot Model** tool, which will automatically take care of untarring it and preparing it to run successfully without further issues. \n\n### Performance Benchmarking\n\n**RSEM Plot Model** is just an R script that plots models, so because of that, tasks running this tool will finish in around 2 minutes, usually costing less than $0.02 on the c4.2xlarge on-demand AWS instance. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [RSEM manual](http://deweylab.biostat.wisc.edu/rsem/README.html)", "input": [{"name": "RSEM plot model folder", "encodingFormat": "application/x-tar"}], "output": [{"name": "PDF plot model file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/deweylab/RSEM", "https://github.com/deweylab/RSEM/archive/v1.3.3.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Quantification", "CWL1.0"], "project": "SBG Public Data", "creator": "Bo Li, Colin Dewey", "softwareVersion": ["v1.0"], "dateModified": 1590513120, "dateCreated": 1590512891, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rsem-prepare-reference-1-3-3/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rsem-prepare-reference-1-3-3/1", "applicationCategory": "CommandLineTool", "name": "RSEM Prepare Reference", "description": "**RSEM Prepare Reference** extracts/preprocesses reference sequences for **RSEM** [1].  \n\nIt can optionally build **Bowtie** indices and/or **Bowtie 2** indices using their default parameters. It can also optionally build **STAR** indices using parameters from **ENCODE3's STAR-RSEM** pipeline [2], as well as **HISAT2** indices on the transcriptome according to **Human Cell Atlas (HCA) SMART-Seq2** pipeline [3]. This program is used in conjunction with the **RSEM Calculate Expression** tool [1].\n\nThe aligners that **RSEM** can internally call are **Bowtie 1.2.3**, **Bowtie2 2.4.1**, **STAR 2.7.3a** and **HISAT2 2.2.0**. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n* **RSEM Prepare Reference** generates one set of files per each FASTA/GTF combination. Once produced, these files (in the form of a TAR archive) could be used as long as the FASTA/GTF combination stays the same. Also, **RSEM Prepare Reference** which produced these files and **RSEM Calculate Expression** tool using them must be of the same toolkit version.\n\n* If the index archive for a desired FASTA/GTF pair has already been generated, make sure to supply the resulting TAR bundle to the tool input if you are using this tool in a workflow in order to skip unnecessary indexing and speed up the whole workflow process.\n\n* **RSEM Prepare Reference** can generate **Bowtie/Bowtie2/STAR/HISAT2** index files in as well, if **RSEM Calculate Expression** is later run from FASTQ files, by selecting any of the boolean options to do so - **Bowtie** (`--bowtie`), **Bowtie2** (`--bowtie2`), **STAR** (`--star`) or **HISAT2** (`--hisat2-hca`). If you plan to run **RSEM Calculate Expression** from BAM files, there is no need to turn these options on. \n\n* In addition to the TAR archive needed by **RSEM Calculate Expression**, this tool also outputs a FASTA file in trancript coordinates. This file can be used by any other bioinformatics tool that requires a transcriptome FASTA (tools like **Salmon**, **Kallisto**, ...).\n\n### Common issues and important notes\n\n* The next tool in the suite, **'RSEM Calculate Expression**, does alignment with the **Bowtie** aligner by default, so unless you want to use one of the other possible aligners (**Bowtie2/STAR/HISAT2**) or you are supplying your own aligned files, please turn the **Bowtie** option on.\n\n\n### Changes Introduced by Seven Bridges\n\n* The directory containing the index files will be outputted as a TAR bundle (the **RSEM prepare reference archive** output). This bundle can then be provided to the **RSEM Calculate Expression** tool, which will automatically take care of untarring it and preparing it to run successfully without further issues. \n\n\n\n### Performance Benchmarking\n\nSince **RSEM Prepare reference** is run with a FASTA/GTF combination, the runtime of this tool will be pretty much constant across a number of different genomes. For the human reference genome, the tool is expected to finish in around 35 minutes, costing around $0.90 on the c5.9xlarge on-demand AWS instance, if **STAR** or **Bowtie** indices are built. Building the **Bowtie2** or **HISAT2** indices, **RSEM Prepare Reference** will finish in around 10 minutes on the c5.9xlarge on-demand AWS instance, costing around $0.30. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [RSEM manual](http://deweylab.biostat.wisc.edu/rsem/README.html)\n[2] [ENCODE3's STAR-RSEM pipeline] (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\n[3] [Human Cell Atlas (HCA) SMART-Seq2 pipeline](https://data.humancellatlas.org/pipelines/smart-seq2-workflow)", "input": [{"name": "Reference fasta file(s) or RSEM index archive", "encodingFormat": "application/x-fasta"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "GFF3 RNA patterns"}, {"name": "Trusted sources"}, {"name": "Transcript to gene map", "encodingFormat": "text/plain"}, {"name": "Allele to gene map", "encodingFormat": "text/plain"}, {"name": "Poly-A tail"}, {"name": "Poly-A tail length"}, {"name": "No Poly-A subset", "encodingFormat": "text/plain"}, {"name": "Bowtie"}, {"name": "Bowtie 2"}, {"name": "STAR"}, {"name": "HISAT2"}, {"name": "STAR splice junction database overhang"}, {"name": "GFF3 genes as transcripts"}, {"name": "Memory per job"}], "output": [{"name": "RSEM prepare reference archive", "encodingFormat": "application/x-tar"}, {"name": "Reference transcript FASTA"}, {"name": "GTF converted from GFF3", "encodingFormat": "application/x-gtf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/deweylab/RSEM", "https://github.com/deweylab/RSEM/archive/v1.3.3.tar.gz"], "applicationSubCategory": ["Transcriptomics", "Quantification", "CWL1.0"], "project": "SBG Public Data", "creator": "Bo Li, Colin Dewey", "softwareVersion": ["v1.0"], "dateModified": 1590513120, "dateCreated": 1590512891, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/rseqc-junction-saturation/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/rseqc-junction-saturation/1", "applicationCategory": "CommandLineTool", "name": "RSeQC - Junction Saturation", "description": "__RSeQC - Junction Saturation__ determines if the current sequencing depth is sufficient to perform alternative splicing analyses by resampling (jackknifing) the total mapped reads. The splice junctions are detected for each re-sampled subset of reads, and the number of detected splice junctions will increase as the resample percentage increases before finally reaching a fixed value. This module checks for saturation by resampling 5%, 10%, 15%, ..., 95% of total alignments from BAM or SAM file, and then detects splice junctions from each subset and compares them to reference gene model. The junction saturation test is very important for alternative splicing analysis, as using an unsaturated sequencing depth would miss many rare splice junctions.\n\n__Common issues__ \nChromosome names (ie. chr1/1) should match in the alignment and the BED file in order to get a proper output.\nBED file should be in the bed12 format.", "input": [{"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Refgene BED", "encodingFormat": "text/x-bed"}, {"name": "Percentile low bound"}, {"name": "Percentile upper bound"}, {"name": "Percentile step"}, {"name": "Minimum intron size"}, {"name": "Minimum splice read"}, {"name": "Minimum mapping quality"}], "output": [{"name": "PDF file"}, {"name": "R plot file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Plotting"], "project": "SBG Public Data", "creator": "Liguo Wang, Shengqin Wang, Wei Li", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649686386, "dateCreated": 1457546447, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/saige-createsparsegrm-0-39-cwl1-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/saige-createsparsegrm-0-39-cwl1-0/4", "applicationCategory": "CommandLineTool", "name": "SAIGE createSparseGRM", "description": "**SAIGE createSparseGRM** is used to create a sparse GRM for gene- and region-based SAIGE-GENE tests [2,3].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* **SAIGE create Sparse GRM** prepares a sparse genetic relationship matrix, starting from a set of Plink genotype files for a cohort (BED, BIM and FAM, input parameter **Plink files for creating the GRM**, `--plinkFile=`). This step is only executed once per dataset. As long as all samples of interest are included, the output matrix can be reused for different phenotypes [2].\n\n\n### Changes Introduced by Seven Bridges\n\n* If **Output file name prefix** is not specified, the outputs are named using the **Plink files for creating the GRM** base name.\n\n### Common Issues and Important Notes\n\n* Input **Plink files for creating the GRM** is required and should be a BED, BIM and FAM file, with the same base name.\n* The created GRM **Output sparse GRM** can be inspected using the readMM function of the R package Matrix.\n\n### Performance Benchmarking\nPerformance depends on the size and complexity (number of samples and number of variants) of the input Plink datasets.\n\nDataset 1: 629 samples, 25488488 variants\n\nDataset 2 (simulated): 50000 samples, 990100 variants\n\nDataset 3 (1000g-derived GRCh38): 2548 samples, 73257632 variants\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1 | 19 min | $0.13 + $0.04 | c4.2xlarge - 1000 GB EBS | \n| Dataset 2 | 24 min | $0.16 + $0.06 | c4.2xlarge - 1000 GB EBS | \n| Dataset 3 | 96 min | $1.23 + $0.22 | m5.4xlarge - 1000 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [SAIGE publication](https://www.nature.com/articles/s41588-018-0184-y)\n\n[2] [SAIGE documentation](https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE)\n\n[3] [SAIGE-GENE publication](https://www.nature.com/articles/s41588-020-0621-6)", "input": [{"name": "Plink files for creating the GRM", "encodingFormat": "text/x-bed"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Memory chunk size [Gb]"}, {"name": "Output file name prefix"}, {"name": "Number of random markers for sparse GRM relatedness"}, {"name": "Relatedness cutoff"}, {"name": "Set GRM diagonal elements to one"}, {"name": "Minimum MAF of markers used for GRM"}], "output": [{"name": "Output sparse GRM"}, {"name": "Output GRM sample IDs", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/weizhouUMICH/SAIGE/blob/master/README.md", "https://github.com/weizhouUMICH/SAIGE", "https://github.com/weizhouUMICH/SAIGE/archive/246f8d059d732e5f66dea4a66914ffa94cc11178.zip", "https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "Wei Zhou", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1612364153, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/saige-step1-fitnullglmm-0-39-cwl1-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/saige-step1-fitnullglmm-0-39-cwl1-0/5", "applicationCategory": "CommandLineTool", "name": "SAIGE step1_fitNULLGLMM", "description": "**SAIGE step1_fitNULLGLMM** tool is used to fit the null logistic/linear mixed model for SAIGE analyses [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* **SAIGE step1_fitNULLGLMM** can be used to prepare a model file for single variant association tests for binary and quantitative traits and gene- and region-based tests.\n* Type of trait analyzed should be specified via the **Trait type** input parameter. For binary traits, a null logistic mixed model will be fitted, whereas if quantitative traits are analyzed, a null linear mixed model will be fitted [2]. **Inverse normalization for quantitative trait** input parameter should be used for quantitative traits.\n* For SAIGE-GENE tests, **Use sparse kin matrix** (`--isSparseKin=`) must be set to TRUE.\n* The inputs are a set of Plink genotype files (BED, BIM, FAM) with a matching base name, provided via the **Plink files**  input parameter (`--plinkFile=`) and a phenotype file with non-genetic covariates (input parameter **Phenotype file**, `--phenoFile=`). The phenotype file should have a header and columns for sample IDs and phenotypes (space or tab separated).\n* As optional inputs, for gene- and region-based tests, the tool accepts a sparse genetic relationship matrix and sample IDs file (**Sparse GRM file** and **Sparse GRM sample ID file**, respectively). If not provided, these files will be generated by the tool if **Use sparse kin matrix** (`--isSparseKin=`) is toggled on, based on **Relatedness cutoff** (`--relatednessCutoff=`) used.\n* Model fitting can be skipped if a previously generated RDA model file is supplied (**RDA file if skipping model fitting**). This allows the reuse of a single variant association model file in SAIGE-GENE tests.\n* The outputs include a model RDA file (**SAIGE RDA file**), a variance ratio file (**Variance ratio file**), association results for a randomly selected subset of markers **SAIGE results file**, and if requested a sparse sigma MTX file (**Sparse sigma file**).\n\nPlease consult the tool official documentation [2] for a detailed breakdown of inputs, outputs, parameters and common use cases.\n\n\n### Changes Introduced by Seven Bridges\n\n* If **Output file name prefix** is not supplied, outputs will be named using the base name of **Plink files**.\n* Input parameter `--IsOverwriteVarianceRatioFile=` has been omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* Inputs **Plink files** and **Phenotype file** are required.\n* **Plink files** should be a set of Plink BED, BIM and FAM files, sharing the same base name.\n* **Phenotype file** should contain a header. Binary trait columns should have values '0' and '1'.\n* Sparse GRM and categorical variance ratios are not needed for single variant association tests. The variance ratio is estimated with a subset of randomly selected markers (with MAC >= 20) [2].\n* Categorical covariates with more than two categories are not supported [2].\n* For SAIGE-GENE tests, **Use sparse kin matrix** (`--isSparseKin=`) must be set to TRUE.\n* If precalculated GRM files are used, tested samples should be a subset of those present in those files.\n\n\n\n### Performance Benchmarking\n\nPerformance depends on the size and complexity (number of samples and number of variants) of the input datasets, the selected traits and input parameters.\n\nDataset 1: 629 samples, 25488488 variants, binary trait\n\nDataset 2 (simulated): 50000 samples, 990100 variants, binary trait\n\nDataset 3 (1000g-derived GRCh38): 2548 samples, 73257632 variants, binary trait\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1 | 79 min | $0.52 + $0.18 | c4.2xlarge - 1000 GB EBS | \n| Dataset 2 | 141 min | $0.93 + $0.32 | c4.2xlarge - 1000 GB EBS | \n| Dataset 3 | 73 min | $1.86 + $0.17 |  c5.9xlarge - 1000 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [SAIGE publication](https://www.nature.com/articles/s41588-018-0184-y)\n\n[2] [SAIGE documentation](https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE)\n\n[3] [SAIGE-GENE publication](https://www.nature.com/articles/s41588-020-0621-6)", "input": [{"name": "Plink files", "encodingFormat": "text/x-bed"}, {"name": "Phenotype file", "encodingFormat": "text/plain"}, {"name": "Phenotype column"}, {"name": "Trait type"}, {"name": "Inverse normalization for quantitative trait"}, {"name": "Covariate columns"}, {"name": "Sample ID column in phenotype file"}, {"name": "Tolerance for fitting the null GLMM"}, {"name": "Maximum number of iterations used to fit the null GLMM"}, {"name": "Tolerance for PCG to converge"}, {"name": "Maximum number of iterations for PCG"}, {"name": "Number of threads"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "SPA cutoff"}, {"name": "Number of random markers for variance ratio"}, {"name": "Skip model fitting"}, {"name": "Memory chunk size [Gb]"}, {"name": "Initial values for tau"}, {"name": "Use LOCO approach"}, {"name": "Trace CV cutoff"}, {"name": "Ratio CV cutoff"}, {"name": "Output file names prefix"}, {"name": "Output prefix variance ratio"}, {"name": "Use sparse kin matrix"}, {"name": "Sparse GRM file"}, {"name": "Sparse GRM sample ID file", "encodingFormat": "text/plain"}, {"name": "Number of random markers to identify related samples for sparse GRM"}, {"name": "Estimate variance ratio on MAC categories"}, {"name": "Relatedness cutoff"}, {"name": "Lower bound of MAC for MAC categories"}, {"name": "Higher bound of MAC for MAC categories"}, {"name": "Use qr transformation on non-genetic covariates"}, {"name": "Set the diagonal elements in GRM to one"}, {"name": "Use sparse GRM to speed up the PCG"}, {"name": "Use sparse sigma to estimate initial tau"}, {"name": "Minimum MAF of markers used for GRM"}, {"name": "Minimum binary covariate count"}, {"name": "Allow non-autosomal markers for variance ratio"}, {"name": "RDA file if skipping model fitting"}], "output": [{"name": "SAIGE results", "encodingFormat": "text/plain"}, {"name": "Variance ratio file", "encodingFormat": "text/plain"}, {"name": "SAIGE RDA file"}, {"name": "Sparse sigma matrix"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/weizhouUMICH/SAIGE/blob/master/README.md", "https://github.com/weizhouUMICH/SAIGE", "https://github.com/weizhouUMICH/SAIGE/archive/246f8d059d732e5f66dea4a66914ffa94cc11178.zip", "https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "Wei Zhou", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1612364153, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/saige-step2-spatests-0-39-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/saige-step2-spatests-0-39-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "SAIGE step2_SPAtests", "description": "**SAIGE step2_SPAtests** is used for single-variant and gene- and region-based association testing with SAIGE [1-3].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* **SAIGE step2_SPAtests** can be used to account for case-control imbalance for binary traits, using saddle point approximation (SPA) [2]. This tool should be run after **SAIGE step1_fitnullGLMM**. For SAIGE-GENE tests, optionally **SAIGE createSparseGRM** should also be executed and its outputs used as inputs to **SAIGE step1_fitnullGLMM**. Both single variant and gene- and region-based association tests can be performed, either separately or in a single run.\n* The tool accepts different input formats for genotypes and dosages: VCF.GZ (with TBI index), BGEN (with BGI index) and SAV (with S1R index). The inputs and index files are supplied separately, using: **VCF file**, **VCF TBI index**, **BGEN file**, **BGEN BGI index file**, **SAV file** and **SAV S1R index file** input parameters.\n* For VCF and SAV inputs, both genotypes and dosages are supported by toggling the value of **VCF field** input parameter (`--vcfField=`) to 'GT' and 'DS', respectively.\n* A subset of markers can be tested by supplying either variant IDs or ranges (for BGEN inputs) or position ranges (VCF and SAV inputs).\n* Sparse sigma matrix input (**Sparse sigma**, `--sparseSigmaFile=`) is specific to SAIGE-GENE and can be created with **SAIGE step1_fitnullGLMM CWL1.0** tool.\n* Output **SAIGE results** contains associations results relative to allele 2. The output format is fully described in the official tool documentation [2].\n* For binary traits, allele frequencies, sample sizes and heterozygous and homozygous counts for cases and controls can be output with the appropriate input parameters.\n* An additional input **Group information file for gene-based tests** is required for SAIGE-GENE tests. For detailed description of the file format, please see the official tool documentation [2].\n* Conditional analysis can be performed (both for single variant and SAIGE-GENE association tests) by specifying **Conditional analysis markers** input parameters.\n\n\n\nPlease consult the official tool documentation [2] for a detailed breakdown of inputs, outputs, parameters and common use cases.\n\n\n### Changes Introduced by Seven Bridges\n\n* If **Output file name for assoc test results** is not supplied, the outputs will be named based on genotype/dosage file inputs.\n\n### Common Issues and Important Notes\n\n* A genotypes/dosage input file with its associated index must be supplied.\n* For VCF and SAV inputs, **Chromosome to test from VCF** input string must match the chromosome string from the VCF/SAV input file.\n* **Drop samples with missing dosages** (`--IsDropMissingDosages=`) option has not been fully tested. If FALSE, missing values will be mean-imputed.\n* **Sample IDs for the dosage file** should not be supplied for VCF files. When supplied for BGEN files, the file should not contain a header.\n* For VCF/SAV inputs, **Group information file for gene-based tests** should contain markers sorted by chromosome and position.\n* The order of markers in the **Group information file for gene-based tests** should match the order in the source inputs.\n* If **Specify customized weights for group test markers** (`--weightsIncludeinGroupFile=`) is TRUE, the weights should be specified with the **Weights for conditioning markers** input parameter.\n* **Minimum marker minor allele count to test** value should be set to at least 3, to filter results. With lower values, SPA always returns p-values close to 0 [2].\n* Marker ID lists used to subset BGEN inputs must not contain duplicate entries [2].\n* If \"Error in setgeno(genofile, subSampleInGeno, memoryChunk) : vector::_M_range_check\" error is encountered, please consider reducing the **Memory chunk size [Gb]** value [2].\n* **Apply leave-one-chromosome-out approach** (`--LOCO=`) should not be used for non-autosomal variants [2].\n* If **Output effect sizes for burden tests** (`--IsOutputBETASEinBurdenTest=`) option is used, please note that the magnitude of the effect size cannot be easily interpreted [2].\n* The tool itself uses only one CPU per job, however, in some scenarios, it internally uses code which may request more CPUs. If necessary, the number of CPUs requested for task execution can be increased with the **Number of CPUs per job** input parameter.\n\n\n### Performance Benchmarking\n\nPerformance depends on the size and complexity (number of samples and number of variants) of the input datasets, the selected traits and input parameters.\n\nDataset 1 (simulated): 50000 samples, 990100 variants\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1 | 77 min | $0.52 + $0.18 | c4.2xlarge - 1000 GB EBS | \n \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n### References\n\n[1] [SAIGE publication](https://www.nature.com/articles/s41588-018-0184-y)\n\n[2] [SAIGE documentation](https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE)\n\n[3] [SAIGE-GENE publication](https://www.nature.com/articles/s41588-020-0621-6)", "input": [{"name": "VCF file", "encodingFormat": "application/x-vcf"}, {"name": "VCF TBI index file"}, {"name": "VCF field"}, {"name": "BGEN file"}, {"name": "BGEN BGI index file"}, {"name": "SAV file"}, {"name": "SAV S1R index file"}, {"name": "Variant IDs to exclude", "encodingFormat": "text/plain"}, {"name": "Variant IDs to include", "encodingFormat": "text/plain"}, {"name": "Genome regions to exclude", "encodingFormat": "text/plain"}, {"name": "Genome regions to include", "encodingFormat": "text/plain"}, {"name": "Chromosome to test from VCF"}, {"name": "Start genome position in VCF to test"}, {"name": "End genome position in VCF to test"}, {"name": "Drop samples with missing dosages"}, {"name": "Minimum marker minor allele frequency to test"}, {"name": "Minimum marker minor allele count to test"}, {"name": "Max MAF for markers tested in group test"}, {"name": "Minimum info for markers to be tested"}, {"name": "Sample IDs for the dosage file", "encodingFormat": "text/plain"}, {"name": "GLMM model file"}, {"name": "Variance ratio file", "encodingFormat": "text/plain"}, {"name": "Output file name for assoc test results"}, {"name": "Number of markers to output"}, {"name": "Use sparsity of genotype vector for binary traits"}, {"name": "SPA cutoff"}, {"name": "Output AF in cases and controls for dichotomous traits"}, {"name": "Output N in cases and controls for dichotomous traits"}, {"name": "Output het and hom counts in cases and controls"}, {"name": "Apply leave-one-chromosome-out approach"}, {"name": "Conditional analysis markers"}, {"name": "Sparse sigma file"}, {"name": "Group information file for gene-based tests", "encodingFormat": "text/plain"}, {"name": "Kernel"}, {"name": "Method for gene-based test p-values"}, {"name": "Beta distribution parameters for rare variants"}, {"name": "Beta distribution parameters for common variants"}, {"name": "MAF cutoff for beta distribution weights"}, {"name": "R corr"}, {"name": "Perform single variant assoc tests for gene-based tests markers"}, {"name": "Lower bound of MAC for MAC categories"}, {"name": "Higher bound of MAC for MAC categories"}, {"name": "Dosage zeroed cutoff"}, {"name": "Output binary traits group test p-value with case control imbalance"}, {"name": "Account for case-control imbalance in group test"}, {"name": "Specify customized weights for group test markers"}, {"name": "Weights for conditioning markers"}, {"name": "Output effect sizes for burden tests"}, {"name": "Memory per job [MB]"}, {"name": "Number of CPUs per job"}], "output": [{"name": "SAIGE results", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/weizhouUMICH/SAIGE/blob/master/README.md", "https://github.com/weizhouUMICH/SAIGE", "https://github.com/weizhouUMICH/SAIGE/archive/246f8d059d732e5f66dea4a66914ffa94cc11178.zip", "https://github.com/weizhouUMICH/SAIGE/wiki/Genetic-association-tests-using-SAIGE"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "Wei Zhou", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1612364153, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-alevin-1-2-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-alevin-1-2-0/3", "applicationCategory": "CommandLineTool", "name": "Salmon Alevin", "description": "**Alevin** is a tool integrated with the **Salmon** software that performs quantification and analysis of 3\u2019 tagged-end single-cell sequencing data. Currently, **Alevin** supports the following two major droplet-based single-cell protocols: \n- Drop-seq\n- 10x-Chromium v1/2/3\n- CEL-Seq and CEL-Seq2\n- Quartz-Seq2 v3.2\n- CITE-seq feature barcoding\n\n**Alevin** uses the same reference index as **Salmon** and consumes the set of FASTA/Q files(s) containing the Cellular Barcode (CB) and Unique Molecule identifier (UMI) in one read file and the read sequence in the other. Given just the transcriptome and the raw read files, alevin generates a cell-by-gene count matrix \n[1].\n\n###Common Use Cases\n**Alevin** requires the following minimal set of necessary input parameters:\n- **Library type** (`-l`) ISR is set by default. It is the recommended type for both Drop-seq and 10x-v2 chemistry.\n- **FASTQ read files** (`-1`, `-2`) the FASTQ file containing CB+UMI and raw read sequences. **Alevin** also supports parsing of data from multiple files as long as the files have required metadata fields set.\n- **Single Cell protocol** (`--dropseq / --chromium / --citeseq / --celseq2 / quartzseq2`) single-cell protocol type of the input sequencing-library where `--chromium` is set by default. \n- **Salmon index** (`-i`)  index of the reference transcriptome, generated with **Salmon** index.\n- **Transcript to gene map** (`--tgMap`) a TSV file without header, containing two columns. The first column contains the names of the transcripts and the second one contains the corresponding gene names.\n\nSee the bottom of the page for a detailed description of all parameters.\n\n###Changes Introduced by Seven Bridges\n- The **Output prefix** option could be used for adding a prefix to the output file names. See the Outputs tab in the table at the bottom of the page for a complete list of all output files.\n- Output directory containing log and metadata files is compressed and saved as a TAR.GZ formatted file.\n\n### Common Issues and Important Notes\n- Due to a known [issue](https://github.com/COMBINE-lab/salmon/issues/336) with Ensembl reference files please use the resources from [Genecode project](https://www.gencodegenes.org/) to build Salmon index.\n- Transcript to gene map file needs to be created from the GTF annotation file. For more information see [here](https://combine-lab.github.io/alevin-tutorial/2018/setting-up-resources/).\n- Paired-end metadata values need to be set for all files provided to the **FASTQ read files** input port. Also, it is required to set the Sample ID metadata when analyzing multiple FASTQ files.\n\n###Performance Benchmarking \nThe main advantage of the **Salmon** software is that it is not computationally challenging, as alignment in the traditional sense is not performed. Below is a table describing the runtimes and task costs for a couple of samples with different file sizes. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n\nProtocol | Dataset | Fastq file 1 size(.gz) | Fastq file 2 size(.gz) | Duration | Cost | Instance type (AWS) |\n|--------|---------|---------------|-----------------|-----------|--------|-----|\n| 10x Chromium v3| 1k_v3 | 1.2 GB | 2.9 GB | 7 min. | $0.20 | c5.9xlarge |\n| 10x Chromium v2 | 4k_v2 | 7.6 GB | 22.4 GB | 28 min. | $0.80 | c5.9xlarge |\n| 10x Chromium v2| 8k_v2 | 12.9 GB | 49.8 GB | 1h1min. |$1.72 | c5.9xlarge |\n| 10x Chromium v3| 10k_v3 | 11.4 GB | 27.3 GB | 49 min. | $1.38 | c5.9xlarge |\n\n###References\n[1] [Alevin documentation](https://salmon.readthedocs.io/en/latest/alevin.html)", "input": [{"name": "FASTQ read", "encodingFormat": "text/fastq"}, {"name": "Library type"}, {"name": "Unmated reads", "encodingFormat": "text/fastq"}, {"name": "Single Cell protocol"}, {"name": "Salmon index", "encodingFormat": "application/x-tar"}, {"name": "Transcript to gene map"}, {"name": "Secondary input point", "encodingFormat": "text/plain"}, {"name": "White-list barcodes", "encodingFormat": "text/plain"}, {"name": "Do not run quantification"}, {"name": "Generate the mean and variance of the count matrix"}, {"name": "Number of cells"}, {"name": "Expected number of cells"}, {"name": "mtRNA genes", "encodingFormat": "text/plain"}, {"name": "rRNA genes", "encodingFormat": "text/plain"}, {"name": "Use correlation"}, {"name": "Keep CB fraction"}, {"name": "Dump barcode modified FASTQ"}, {"name": "Dump the big hash"}, {"name": "Dump UMI graph"}, {"name": "Dump features"}, {"name": "Dump CSV Counts"}, {"name": "Minimum Number of CB"}, {"name": "Maximum Number of Barcodes"}, {"name": "Output prefix"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Number of threads"}, {"name": "Cell-Barcodes end"}, {"name": "UMI length"}, {"name": "Barcode length"}, {"name": "Do not use EM"}, {"name": "Barcodes frequency threshold"}, {"name": "UMI edit distance"}, {"name": "Feature barcode start"}, {"name": "Feature barcode length"}, {"name": "Dump arborescences"}], "output": [{"name": "Output directory"}, {"name": "Compressed count matrix"}, {"name": "Compressed output directory", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0"], "applicationSubCategory": ["RNA-Seq", "Single Cell"], "project": "SBG Public Data", "creator": "Rob Patro, Avi Srivastava", "softwareVersion": ["v1.0"], "dateModified": 1648468513, "dateCreated": 1586797701, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-index-1-2-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-index-1-2-0/5", "applicationCategory": "CommandLineTool", "name": "Salmon Index", "description": "**Salmon Index** tool builds an index necessary for the **Salmon Quant** and **Salmon Alevin** tools. To create an index, it uses a transcriptome reference file in FASTA format. Additionally, one can provide genome reference along with transcriptome to create a hybrid index compatible with the improved mapping algorithm named **Selective Alignment (SA)**.\n\n**SA** is designed to remain as fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nA **Transcript FASTA or Salmon Index** needs to be provided as an input to the tool.\nIt is recommended to provide **Genome FASTA** to be used as a *decoy* sequence when generating the index. \nTo create a file containing transcripts to genes mappings, required for scRNA-seq analysis with **Salmon Alevin** and gene-level abundance estimation with **Salmon Quant** tools, provide a GTF annotation file to the **GTF annotation** input port.\n\n### Changes Introduced by Seven Bridges\n\n - An already generated **Salmon index archive** can be provided to the **Salmon Index** tool (**Transcript FASTA or Salmon Index Archive** input), to skip indexing and reduce processing time if this tool is a part of a workflow.\n - Included bash scripts for extracting target names from genome reference, concatenation of transcriptome and genome reference and for creating transcripts to genes mappings file (if any ERCC contigs are present in the genome file, they will not be included in the list of decoy sequences).\n - Parameter `--gencode` will be automatically included in the command line if the transcriptome fasta file contains `gencode` string in the filename.\n\n### Common Issues and Important Notes\n\nThere are no common issues and important notes concerning this tool.\n\n### Performance Benchmarking\n\nWhen provided with transcriptome reference only, the **Salmon Index** tool builds the index structure for **Salmon** in a short time. The expected time of task execution in such cases is under  5 minutes, with a cost of $0.05 on the default c5.2xlarge instance (AWS).\nOn the other hand, if genome reference is provided along with transcriptome, the expected time of task execution can be as long as 40 minutes with the cost of $0.66 on the c5.4xlarge instance (AWS) having 16 CPUs.\n\n*Note:* We used human reference genome and transcriptome for benchmarking purposes. Execution time and cost can vary for other genomes and transcriptomes.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)", "input": [{"name": "Transcript FASTA or Salmon Index", "encodingFormat": "application/x-tar"}, {"name": "K-mer length"}, {"name": "Gencode FASTA"}, {"name": "Sparse"}, {"name": "Keep duplicates"}, {"name": "Genome FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Number of threads"}, {"name": "Filter size"}, {"name": "GTF annotation", "encodingFormat": "application/x-gtf"}, {"name": "Output file prefix"}, {"name": "Do not create transcripts to genes mappings"}, {"name": "Reference in the TSV format"}, {"name": "Keep fixed fasta"}], "output": [{"name": "Salmon index archive", "encodingFormat": "application/x-tar"}, {"name": "Transcript to gene mappings"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0"], "applicationSubCategory": ["RNA-Seq", "Indexing"], "project": "SBG Public Data", "creator": "Rob Patro, Avi Srivastava", "softwareVersion": ["v1.0"], "dateModified": 1648468513, "dateCreated": 1586797701, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-quant-alignment-1-2-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-quant-alignment-1-2-0/5", "applicationCategory": "CommandLineTool", "name": "Salmon Quant - Alignment", "description": "**Salmon Quant - Alignments** infers transcript abundance estimates from aligned **RNA-seq data**, using the **Expectation Maximization** algorithm [1, 2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n- The main inputs to the tool are aligned BAM files, which should be provided to the **Alignments** input node (`--alignments`). \n- **Salmon Quant - Alignment** expects that the alignment files provided are with respect to the transcripts given in the corresponding fasta file. That is, **Salmon** expects that the reads have been aligned directly to the transcriptome (like RSEM, eXpress, etc.) rather than to the genome (as done by, e.g. Cufflinks). If you have reads that have already been aligned to the genome, there are currently 3 options for converting them for use with **Salmon**. First, you could convert the SAM/BAM file to a FAST{A/Q} file and then use the quasi-mapping-based mode of **Salmon**. Second, given the converted FASTA{A/Q} file, you could re-align these converted reads directly to the transcripts with your favorite aligner and run **Salmon** in alignment-based mode as described above. Third, you could use a tool like sam-xlate to try and convert the genome-coordinate BAM files directly into transcript coordinates. This avoids the necessity of having to re-map the reads.\n- A **Transcriptome FASTA file** (`--transcripts`) also needs to be provided, in addition to an optional **Gene map** (`--geneMap`) file (which should be of the same annotations as in the **Transcriptome FASTA file** - usually a GTF file can be provided here) if gene-level abundance results are desired. \n- The tool will generate transcript abundance estimates in plaintext format, and an optional file containing gene abundance estimates, if the input **Gene map** (`--geneMap`) file is provided. \n- In addition to the default output (abundance estimates), additional outputs can be produced if you turn on the proper options for them (e.g. **PostSample BAM** by setting `--sampleOut`, **Bootstrap data** by setting `--numBootstraps` or `--numGibbsSamples`...).\n- The **GC bias correction** option (`--gcBias`) will correct for GC bias and improve quantification accuracy, but at the cost of increased runtime (a rough estimate would be a **double** increase in runtime per sample).  \n- The use of *data-driven likelihood factorization* is achieved with the **Range factorization bins** parameter (`--rangeFactorizationBins`) coupled with the **Use error model** parameter (`--useErrorModel`) and can be used to bring an increase in accuracy at a very small increase in runtime [3]. \n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from metadata if existent, or from filename otherwise), instead of having identical names between runs. \n- If the transcriptome FASTA file is not provided, it can be automatically generated if the GTF and genome FASTA files are provided to the **Gene map** input, using the built in **RSEM Prepare Reference 1.3.3** module. \n\n### Common Issues and Important Notes\n\n- The GTF and FASTA files need to have compatible transcript IDs. \n\n### Performance Benchmarking\n\n**Salmon Quant - Alignments** performs just the EM algorithm and not the full alignment, thus getting rid of the computationally most expensive part of the quantification procedure. Hence, the tool's execution time is very low. \nFor a BAM file of 13.8GB, the tool executes within 7 minutes on the default instance (c4.2xlarge on AWS), costing around $0.07. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Salmon paper](biorxiv.org/content/biorxiv/early/2016/08/30/021592.full.pdf) \n\n[2] [Rapmap paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908361/) \n\n[3] [Data-driven likelihood factorization](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977)", "input": [{"name": "Alignments", "encodingFormat": "application/x-sam"}, {"name": "Alternative initialization mode"}, {"name": "Bias speed sample"}, {"name": "Bootstrap reproject"}, {"name": "Dump equivalence class counts"}, {"name": "Dump equivalence class weights"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Forgetting factor"}, {"name": "GC bias correction"}, {"name": "GC size sample"}, {"name": "Gencode"}, {"name": "Gene map", "encodingFormat": "application/x-fasta"}, {"name": "Incompatible prior probability"}, {"name": "Initialize uniform parameters"}, {"name": "Library type"}, {"name": "Mapping cache memory limit"}, {"name": "Maximum read occurence"}, {"name": "Meta"}, {"name": "Minimum assigned fragments"}, {"name": "No bias length threshold"}, {"name": "No effective length correction"}, {"name": "No error model"}, {"name": "No fragment length distribution"}, {"name": "No gamma draw"}, {"name": "No length correction"}, {"name": "Number of auxiliary model samples"}, {"name": "Number of bias samples"}, {"name": "Number of bootstraps"}, {"name": "Num error bins"}, {"name": "Number of Gibbs samples"}, {"name": "Number of pre auxiliary model samples"}, {"name": "Per transcript prior"}, {"name": "Position bias"}, {"name": "Range factorization bins"}, {"name": "Reduce GC memory"}, {"name": "PostSample BAM"}, {"name": "Sequence-specific bias correction"}, {"name": "Significant digits"}, {"name": "Thinning factor"}, {"name": "Transcriptome FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Use Variational Bayesian optimization"}, {"name": "VBEM prior"}, {"name": "Write unmapped names"}, {"name": "Aux dir"}, {"name": "Discard Orphans"}, {"name": "Skip Quant"}, {"name": "Sample Unaligned"}, {"name": "Use EM"}, {"name": "Write Orphan Links"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Number of threads"}, {"name": "Equivalence class file."}, {"name": "Maximum occurrences per hit"}, {"name": "Skip fragment length estimate for SE reads"}, {"name": "Auxiliary targets", "encodingFormat": "text/plain"}, {"name": "Downgrade sub-optimal alignments factor"}], "output": [{"name": "Gene-level quantification file"}, {"name": "Quantification file"}, {"name": "Salmon Quant archive", "encodingFormat": "application/x-tar"}, {"name": "Salmon meta info"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0"], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Rob Patro, Avi Srivastava", "softwareVersion": ["v1.1"], "dateModified": 1648468513, "dateCreated": 1586797701, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/salmon-quant-reads-1-2-0/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/salmon-quant-reads-1-2-0/3", "applicationCategory": "CommandLineTool", "name": "Salmon Quant - Reads", "description": "**Salmon Quant - Reads** infers transcript abundance estimates from **RNA-seq data**, using the **Selective Alignment (SA)** algorithm. \n\n**SA** is designed to remain as fast as quasi-mapping while simultaneously eliminating many of its mapping errors. It relies upon alignment scoring to help differentiate between mapping loci that would otherwise be indistinguishable due to the multiple exact matches along the reference. Also, the improved mapping algorithm addresses the failure of direct alignment against the transcriptome, compared to spliced alignment to the genome by identifying and extracting sequence-similar decoy regions or using the entire genome as a decoy. The Salmon index is then augmented with these decoy sequences, which are handled in a special manner during mapping and alignment scoring, leading to a reduction of false mappings [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n- **FASTQ read files** is the required input port that accepts raw sequencing reads. \n- **Salmon index archive** is also required and accepts a salmon index file.\n- **Gene annotation** file can be used for gene-level aggregation of quantification results.\n- The workflow will generate transcript abundance estimates in plaintext format (**Transcript-level quantifications**), and an optional file containing **Gene-level quantifications** if the **Gene annotation** input is provided. \n- In addition to the quantification outputs, additional outputs can be produced if the proper options are selected. These files will be accessible in the TAR archive on the **Salmon Quant archive** output port. \n\n### Changes Introduced by Seven Bridges\n\n- The input sample ID will prefix all output files (inferred from **Sample ID** metadata if existent, or from filename otherwise), instead of having identical names between runs. \n\n### Common Issues and Important Notes\n\n- For paired-end read files, it is important to properly set the **Paired End** metadata field on your read files.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number)**.\n- The GTF and FASTA files need to have compatible transcript IDs. \n\n### Performance Benchmarking\n\nThe main advantage of the Salmon software is that it is not computationally challenging, as alignment in the traditional sense is not performed. \nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   8min   | $0.21| c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M     |     101     |   19min  | $0.5 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Salmon paper](https://www.biorxiv.org/content/10.1101/657874v2)   \n[2] [Towards selective-alignment](https://www.biorxiv.org/content/10.1101/138800v2)", "input": [{"name": "Alternative initialization mode"}, {"name": "Bias speed sample"}, {"name": "Bootstrap reproject"}, {"name": "Consensus slack"}, {"name": "Discard orphans in Quasi-mapping mode"}, {"name": "Dump equivalence class counts"}, {"name": "Dump equivalence class weights"}, {"name": "Maximum fragment length"}, {"name": "Mean fragment length"}, {"name": "Fragment length standard deviation"}, {"name": "Forgetting factor"}, {"name": "The value given to a gap extension in an alignment"}, {"name": "The value given to a gap opening in an alignment"}, {"name": "GC bias correction"}, {"name": "GC size sample"}, {"name": "Gene map", "encodingFormat": "application/x-gtf"}, {"name": "Incompatible prior probability"}, {"name": "Initialize uniform parameters"}, {"name": "Library type"}, {"name": "Maximum read occurence"}, {"name": "Meta"}, {"name": "Minimum assigned fragments"}, {"name": "Minimum score fraction"}, {"name": "No bias length threshold"}, {"name": "No effective length correction"}, {"name": "No fragment length distribution"}, {"name": "No gamma draw"}, {"name": "No length correction"}, {"name": "Number of auxiliary model samples"}, {"name": "Number of bias samples"}, {"name": "Number of bootstraps"}, {"name": "Number of Gibbs samples"}, {"name": "Number of pre auxiliary model samples"}, {"name": "Per transcript prior"}, {"name": "Position bias"}, {"name": "Range factorization bins"}, {"name": "FASTQ Read files", "encodingFormat": "text/fastq"}, {"name": "The value given to a match between read and reference nucleotides in an alignment"}, {"name": "The value given to a mismatch between read and reference nucleotides in an alignment"}, {"name": "Reduce GC memory"}, {"name": "Salmon index archive", "encodingFormat": "application/x-tar"}, {"name": "Sequence-specific bias correction"}, {"name": "Significant digits"}, {"name": "Use the EM algorithm"}, {"name": "Use Variational Bayesian optimization"}, {"name": "VBEM prior"}, {"name": "Aux dir"}, {"name": "Write Mappings"}, {"name": "Bandwidth"}, {"name": "Allow Dovetail"}, {"name": "Recover Orphans"}, {"name": "Mimic BT2"}, {"name": "Mimic Strict BT2"}, {"name": "Hard Filter"}, {"name": "Skip Quant"}, {"name": "Write Orphan Links"}, {"name": "Write Unmapped Names"}, {"name": "Thinning Factor"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Number of threads"}, {"name": "Softclip overhangs"}, {"name": "Full length alignment [selective-alignment mode only]"}, {"name": "Hit filter policy"}, {"name": "Maximum occurrences per hit"}, {"name": "Skip fragment length estimate for SE reads"}, {"name": "Nucleotide level prior"}, {"name": "Auxiliary targets", "encodingFormat": "text/plain"}, {"name": "Downgrade sub-optimal alignments factor"}, {"name": "Disable chaining heuristic"}, {"name": "Decoy threshold"}, {"name": "Enable soft-clipping (experimental)"}, {"name": "Filter low probability alignments"}], "output": [{"name": "Gene-level quantifications"}, {"name": "Transcript-level quantifications"}, {"name": "Salmon Quant archive", "encodingFormat": "application/x-tar"}, {"name": "Salmon quant log"}, {"name": "Salmon meta info"}, {"name": "Fragment length distribution", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/COMBINE-lab/salmon", "https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0"], "applicationSubCategory": ["RNA-Seq", "Quantification"], "project": "SBG Public Data", "creator": "Rob Patro, Carl Kingsford, Steve Mount, Mohsen Zakeri", "softwareVersion": ["v1.0"], "dateModified": 1648468513, "dateCreated": 1586797701, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-flagstat-0-8-1-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-flagstat-0-8-1-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Sambamba Flagstat", "description": "**Sambamba Flagstat** generates statistics from read flags in a BAM file [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba Flagstat** can be used to output read flag statistics from a BAM file (**Input alignments**).\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If the **Output file name prefix** input parameter is not provided, the output will be named based on the **Sample ID** metadata field of the **Input alignments** file. If this field is not populated, the base name of the **Input alignments** file will be used as the output file name prefix.\n\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) - 1 thread | 23 min | $0.15 + $0.06 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - 8 threads | 15 min | $0.10 + $0.04 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba Flagstat** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Number of threads for decompression"}, {"name": "Show progress bar in STDERR"}, {"name": "Output in CSV format"}], "output": [{"name": "Read flags statistics", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["SAM/BAM Processing", "Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208360, "dateCreated": 1648208360, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-index-0-8-1-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-index-0-8-1-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Sambamba Index", "description": "**Sambamba Index** indexes a BAM or FASTA file [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba Index** creates a BAI or FAI index for the provided **Data file to index**. \n\n### Changes Introduced by Seven Bridges\n\n* This wrapper does not support custom locations for the created index file. The file is always created in the current working directory of the task.\n* **Return indexed data file** input parameter was added to allow the wrapper to output the indexed data file alongside its index. This option is useful for passing indexed data to downstream tools within a workflow.\n\n### Common Issues and Important Notes\n\n* **Data file to index** input is required.\n* If the **Data file to index** is in FASTA format, the file should be decompressed and **Input is in FASTA format** parameter should be set to `True`.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) 1 thread | 23 min | $0.15 + $0.06 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) 8 threads | 15 min | $0.10 + $0.04 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba Index** was tested with cwltool version 3.1.20210628163208. The `in_data_source` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Data file to index", "encodingFormat": "application/x-fasta"}, {"name": "Number of threads to use for decompression"}, {"name": "Show progress bar"}, {"name": "Check bins"}, {"name": "Input is in FASTA format"}, {"name": "Return indexed data file"}], "output": [{"name": "Index file"}, {"name": "Indexed data file", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["Indexing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208360, "dateCreated": 1648208360, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-markdup-0-8-1-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-markdup-0-8-1-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Sambamba Markdup", "description": "**Sambamba Markdup** identifies or removes duplicate reads [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba Markdup** can be used to mark or remove duplicate reads from an input BAM file (**Input alignments**).\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--tmpdir`, `--help`.\n* If the **Output file name prefix** input parameter is not provided, the output file is named based on the **Sample ID** metadata field of the first provided **Input alignments** file. If this field is not populated, the base name of the file is used as the output file name prefix.\n\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) - 1 thread | 207 min | $1.37 + $0.48 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - 8 threads | 71 min | $0.47 + $0.17 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba Markdup** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Remove duplicates"}, {"name": "Number of threads to use"}, {"name": "Compression level"}, {"name": "Show progress"}, {"name": "Hash table size"}, {"name": "Overflow list size"}, {"name": "Sort buffer size [MB]"}, {"name": "IO buffer size"}], "output": [{"name": "Deduplicated alignments", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208361, "dateCreated": 1648208361, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-merge-0-8-1-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-merge-0-8-1-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Sambamba Merge", "description": "**Sambamba Merge** merges alignments in BAM format [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba Merge** merges alignments in BAM format (**Input alignments**). The provided files should have the same sorting order.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If the **Output file name prefix** input parameter is not provided, the merged BAM output file will be named based on the **Sample ID** metadata field of the first provided input file, or, if this field is not populated, on the base name of this file.\n\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required. The provided files should have the same sorting order.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS (19 x 2-5 GB) 1 thread | 118 min | $0.78 + $0.27 | c4.2xlarge - 1024 GB EBS | \n| WGS (19 x 2-5 GB) 8 threads | 59 min | $0.39 + $0.14 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba Merge** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Number of compression/decompression threads"}, {"name": "Compression level"}, {"name": "Output merged header only"}, {"name": "Show progress bar in STDERR"}, {"name": "Filter for reads"}, {"name": "Output file name prefix"}], "output": [{"name": "Merged alignments", "encodingFormat": "application/x-bam"}, {"name": "Optional merged header output", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208360, "dateCreated": 1648208359, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-slice-0-8-1-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-slice-0-8-1-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Sambamba Slice", "description": "**Sambamba Slice** copies a slice of the input file [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba Slice** can be used to copy a slice (region) of the coordinate sorted and indexed input file in BAM or FASTA format (**Input data source**). Records overlapping the supplied **Regions** are copied to the output file. **Input is in FASTA format** input parameter should be turned on to process data in FASTA format.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`\n* The output file (`--output-filename`) is always generated. If the **Output file name prefix** input parameter is not provided, the output file will be named using the base name of the **Input data source**.\n\n\n### Common Issues and Important Notes\n\n* **Input data source** and **Regions** inputs are required. **Input data source** file should be indexed and coordinate-sorted.\n* Please note that tool failures were observed on some FASTA files during testing, possibly related to the presence of decoy or HLA contigs.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) - chr20 slice | 8 min | $0.05 + $0.02 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - chrs 1, 5, 10, 15, 20 | 12 min | $0.08 + $0.03 | c4.2xlarge - 1024 GB EBS | \n\n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba Slice** was tested with cwltool version 3.1.20210628163208. The `in_data_source` and `regions` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Output file name prefix"}, {"name": "Input data source", "encodingFormat": "application/x-fasta"}, {"name": "Regions to copy"}, {"name": "Regions file", "encodingFormat": "text/x-bed"}, {"name": "Input is in FASTA format"}], "output": [{"name": "Extracted regions", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208360, "dateCreated": 1648208360, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-sort-0-8-1-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-sort-0-8-1-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Sambamba Sort", "description": "**Sambamba Sort** sorts alignments in BAM format [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba Sort** can be used to sort BAM files (**Input alignments**). Coordinate and query-name (lexicographical `--sort-by-name`, Samtools `--natural-sort` and Picard `--sort-picard` corresponding to **Sort by name - lexicographical order**, **Natural sort - Samtools order** and **Sort by query name - Picard order** input parameters, respectively) sorting orders are supported. By default, the output will be coordinate-sorted.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--tmpdir` and `--help`.\n* If the **Output file name prefix** input parameter is not provided, the sorted BAM output file will be named based on the **Sample ID** metadata field of the **Input alignments** file, or, if this field is not populated, on the base name of this file.\n\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n* If the output should be sorted by query name, only one of the **Sort by name - lexicographical order**, **Natural sort - Samtools order** and **Sort by query name - Picard order** input parameters should be specified, as these parameters are mutually exclusive.\n* **Match mates** (`--match-mates`) input parameter should only be used when query-name sorting with **Sort by name - lexicographical order** or **Natural sort - Samtools order** input parameters.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) - coordinate sort - 1 thread | 213 min | $1.41 + $0.49 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - coordinate sort - 8 threads | 72 min | $0.48 + $0.17 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - lexicographical sort - 8 threads | 91 min | $0.60 + $0.21 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - Picard query name sort - 8 threads | 92 min | $0.61 + $0.21 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - natural (samtools) sort - 8 threads | 142 min | $0.94 + $0.33 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba Sort** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Number of threads"}, {"name": "Compression level"}, {"name": "Show progress bar in STDERR"}, {"name": "Filter for reads"}, {"name": "Memory limit"}, {"name": "Output file name prefix"}, {"name": "Sort by name - lexicographical order"}, {"name": "Sort by query name - Picard order"}, {"name": "Natural sort - Samtools order"}, {"name": "Match mates"}, {"name": "Write uncompressed chunks"}], "output": [{"name": "Sorted alignments", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208361, "dateCreated": 1648208360, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sambamba-view-0-8-1-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sambamba-view-0-8-1-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Sambamba View", "description": "**Sambamba View** inspects and filters alignments in SAM/BAM format [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sambamba View** accepts alignments in BAM or SAM format (**Input alignments**) and outputs data in a user-specified format (**Output format**). CRAM files are not supported.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) - 1 thread | 153 min | $1.01 + $0.35 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - 8 threads | 153 min | $1.01 + $0.35 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) - 15 threads | 64 min | $0.73 + $0.15 | c5.4xlarge - 1024 GB EBS | \n\nValue of the **Maximum number of threads to use** (`--nthreads`) input parameter did not affect task duration on a c4.2xlarge instance (1 and 8 were the values tested). The same analysis completed significantly faster on a c5.4xlarge instance, with 15 allocated threads.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Sambamba View** was tested with cwltool version 3.1.20210628163208. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Sambamba documentation](https://lomereiter.github.io/sambamba/docs/sambamba-view.html)\n\n[2] [Sambamba publication](https://academic.oup.com/bioinformatics/article/31/12/2032/214758)", "input": [{"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Regions"}, {"name": "Filter for alignments"}, {"name": "Filter flag bits"}, {"name": "Output format"}, {"name": "Output file name prefix"}, {"name": "Print header before the reads"}, {"name": "Output only header"}, {"name": "Output only reference info"}, {"name": "Regions file", "encodingFormat": "text/x-bed"}, {"name": "Output only counts of matching records"}, {"name": "Output only valid alignments"}, {"name": "Input is in SAM format"}, {"name": "Reference sequence for writing", "encodingFormat": "application/x-fasta"}, {"name": "Show progress bar"}, {"name": "Compression level"}, {"name": "Maximum number of threads to use"}, {"name": "Subsampling fraction"}, {"name": "Subsampling seed"}], "output": [{"name": "Output alignments", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/biod/sambamba", "https://github.com/biod/sambamba/releases/tag/v0.8.1"], "applicationSubCategory": ["SAM/BAM Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Artem Tarasov and Pjotr Prins", "softwareVersion": ["v1.2"], "dateModified": 1648208360, "dateCreated": 1648208360, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samplot-plot-1-3-0-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samplot-plot-1-3-0-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Samplot Plot", "description": "**Samplot Plot** creates visualizations for SV calls [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Samplot Plot** takes alignment files (**Input alignments**, `--bams`) and coordinates for a region containing the SV call of interest (**Chromosome**, **Start position** and **End position**) and creates a plot of the SV region.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--output_dir`, `--help`, `--print-args` and `--json-only`.\n\n### Common Issues and Important Notes\n\n* **Input alignments** (`--bams`), **Start position** (`--start`), **End position** (`--end`) and **Chromosome** (`--chrom`) inputs are required.\n* If used, **Titles** (`--titles`) should match the order of the provided **Input alignments** files.\n* **Reference sequence** (`--reference`) input is required if CRAM files are used as **Input alignments**.\n* If **Transcripts GFF3 file** (`--transcript_file`) or **Annotation files** (`--annotation_files`) are provided, please make sure that the corresponding TBI or CSI indices are available for all files.\n\n### Performance Benchmarking\n\n**Samplot Plot** is a very fast tool, with typical execution times of <2 minutes. However, task duration can be longer with large input files, as these have to be transferred to the requested instance before execution. For example, the actual tool execution time in the task illustrated in the table below was 14s - the remaining time was spent on inputs transfer.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS CRAM (66 GB) | 12 min | $0.08 + $0.03 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Samplot Plot** was tested with cwltool version 3.1.20211107152837. The `in_alignments`, `chrom`, `start`, `sv_type`, `titles` and `end` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Samplot documentation](https://github.com/ryanlayer/samplot)\n\n[2] [Samplot publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02380-5)", "input": [{"name": "Titles"}, {"name": "Reference sequence for CRAM inputs", "encodingFormat": "application/x-fasta"}, {"name": "Number of stdevs from the mean"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Output file name"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Start position"}, {"name": "End position"}, {"name": "Chromosome"}, {"name": "Window size"}, {"name": "Maximum number of normal pairs to plot"}, {"name": "SV type"}, {"name": "Transcripts GFF3 file"}, {"name": "Name for the transcripts track"}, {"name": "Number of coverage axis points to plot"}, {"name": "Annotation files", "encodingFormat": "text/x-bed"}, {"name": "Names for tracks in annotation files"}, {"name": "Type of track for low MAPQ coverage plot"}, {"name": "Print arguments to a JSON file"}, {"name": "Plot height"}, {"name": "Plot width"}, {"name": "Minimum mapping quality of reads to plot"}, {"name": "Minimum mapping quality to plot separately"}, {"name": "Confidence intervals of the first SV breakpoint"}, {"name": "Confidence intervals of the end SV breakpoint"}, {"name": "Minimum length of a long read"}, {"name": "Ignore HP tag in alignment files"}, {"name": "Minimum event size in long-read CIGAR"}, {"name": "X axis labels font size"}, {"name": "Y axis labels font size"}, {"name": "Legend font size"}, {"name": "Annotation font size"}, {"name": "Set common insert size for all plots"}, {"name": "Hide annotation labels"}, {"name": "Hide all reads and show only coverage"}, {"name": "Maximum coverage cutoff to apply"}, {"name": "Same y-axis scales"}, {"name": "Marker size"}, {"name": "Dots per inch"}, {"name": "Scaling factor for optional tracks"}, {"name": "Zoom region around breakpoints to show"}, {"name": "Print debug statements"}], "output": [{"name": "SV plot"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ryanlayer/samplot", "https://github.com/ryanlayer/samplot/tree/master/samplot", "https://github.com/ryanlayer/samplot/releases/tag/v1.3.0", "https://github.com/ryanlayer/samplot/blob/master/README.md"], "applicationSubCategory": ["Plotting", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ryan Layer", "softwareVersion": ["v1.2"], "dateModified": 1648205393, "dateCreated": 1648204581, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samplot-vcf-1-3-0-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samplot-vcf-1-3-0-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Samplot Vcf", "description": "**Samplot Vcf** plots SVs from a VCF file [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Samplot Vcf** can be used to create visualizations of structural variant calls from a VCF file (**VCF with structural variants**, `--vcf`) with information added from the corresponding BAM or CRAM files (**Input alignments**).\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--command_file`, `--help`, `--manual_run`  and `--out-dir`.\n* Samplot GitHub [1] indicates that optional arguments for **Samplot Plot** can be passed on to **Samplot Vcf**, however as the usage print out of **Samplot Vcf** does not include these arguments, they were omitted from the wrapper. If supported, additional arguments can be added to the wrapper on request.\n* The pop-up display of plots within the HTML page is not directly viewable on Seven Bridges platforms, as the **HTML page** and **Samplot plots** are returned separately. If the plots and the HTML table are downloaded into the same directory, the pop-up view functionality will be available locally. The other interactive elements of the generated HTML table (**HTML page** output) can be used directly on Seven Bridges platforms.\n* **Return a TGZ archive with plots** input parameter was added to allow the users to receive all plots and the HTML table packed in a single archive. This parameter is useful when a large number of output images is expected.\n\n### Common Issues and Important Notes\n\n* **VCF file with structural variants** (`--vcf`) and **Input alignments** (`--bams`) inputs are required.\n* **Sample IDs** (`--sample_ids`) must be provided in the same order as the **Input alignments** (`--bams`) files to which they correspond. If **Sample IDs** are not provided, **Input alignments** must have the RG tag.\n* **Pedigree file** (`--ped`) input is required if **Plot only putative denovo variants** (`--dn_only`) parameter is used.\n* If a **Pedigree file** is provided, alignment files (**Input alignments**, `--bams`) should be provided for all sample IDs in the pedigree file.\n* Please note that this tool will create a visualization (image file in the user-specified format) for every SV event in the VCF file which matches the provided filters and input criteria. You may wish to limit the number of SV events in the **VCF file with structural variants** to only events of interest, or use **Return a TGZ archive with plots** to pack all outputs together.\n\n### Performance Benchmarking\n\n**Samplot Vcf** performance greatly depends on the number of SVs which should be processed and visualized. The tool generates visualizations very quickly with a small number of variants to process, but can take hours to process unfiltered data (processing a WGS trio with no filters and >16000 SVs and generating 7084 images took ~18 h).\n\nThe task listed in the table below was executed with \"DHFFC < 0.7 & SVTYPE == 'DEL'\" and \"DHFFC > 1.3 & SVTYPE == 'DUP'\" and the outputs returned as a TAR.GZ archive.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS trio, >16000 variant lines, 5557 images  | 406 min | $2.70 + $1.04 | c4.2xlarge - 1024 GB EBS | \n\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Samplot Vcf** was tested with cwltool version 3.1.20211107152837. The `in_alignments`, `in_variants` and `sample_ids` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Samplot documentation](https://github.com/ryanlayer/samplot)\n\n[2] [Samplot publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02380-5)", "input": [{"name": "VCF file with structural variants", "encodingFormat": "application/x-vcf"}, {"name": "Pedigree file"}, {"name": "Plot only putative denovo variants"}, {"name": "Minimum variant call rate"}, {"name": "Filters for samples"}, {"name": "Output file type"}, {"name": "Maximum heterozygotes to plot variants"}, {"name": "Minimum sample entries"}, {"name": "Maximum heterozygotes to plot"}, {"name": "Maximum variant length [Mb]"}, {"name": "Minimum variant length [bases]"}, {"name": "Important regions", "encodingFormat": "text/x-bed"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Sample IDs"}, {"name": "FORMAT fields to include in plot title"}, {"name": "Genomic regions"}, {"name": "Number of normal reads/pairs to plot"}, {"name": "Plot all samples and variants"}, {"name": "Debug mode for skipped entries"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Return a TGZ archive with plots"}], "output": [{"name": "Samplot plots"}, {"name": "HTML page", "encodingFormat": "text/html"}, {"name": "TAR.GZ archive with plots", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ryanlayer/samplot", "https://github.com/ryanlayer/samplot/tree/master/samplot", "https://github.com/ryanlayer/samplot/releases/tag/v1.3.0", "https://github.com/ryanlayer/samplot/blob/master/README.md"], "applicationSubCategory": ["Variant Filtration", "Plotting", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ryan Layer", "softwareVersion": ["v1.2"], "dateModified": 1648205393, "dateCreated": 1648204580, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-collate-1-6/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-collate-1-6/7", "applicationCategory": "CommandLineTool", "name": "SAMtools Collate", "description": "**SAMtools Collate** tool shuffles and groups reads together by their names. A faster alternative to a full query name sort, **SAMtools Collate** ensures that reads of the same name are grouped together in contiguous groups, but doesn't make any guarantees about the order of read names between groups. The output from this command should be suitable for any operation that requires all reads from the same template to be grouped together. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Collate** is a faster alternative to a full query name sort. The output from this tool should be suitable for any operation that requires all reads from the same template to be grouped together. Some of the tools that require such input are **SAMtools Fixmate** and **SAMtools FASTQ** (not required but it is recommended for further processing of FASTQ files). If you want to use additional threads, consider using **SAMtools Sort** with parameter **Sort by read name** (`-n`) set to True because it may perform faster than **SAMtools Collate**. Please check **Performance Benchmarking** section of both tools for more information.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Output to stdout** (`-O`) was excluded from wrapper since it was inapplicable on the platform. \n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- Memory consumption depends on the size of **Input BAM/SAM/CRAM file** and **Number of temporary files** (`-n`). For default  **Number of temporary files** (`-n`), it should not exceed 0.2 \\* size(**Input BAM/SAM/CRAM file**). Memory does not depend on number of threads.  \n- When using parameter **Number of temporary files** (`-n`), it is recommended to use values greater than or equal to 64 (default value). If the value is less than 64, it may result in task failure caused by insufficient memory.\n- When specifying output filename (**Output filename prefix**), only prefix of the filename should be used (without extension). \n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools Collate** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | Paired-end | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | Yes |  71.5M | 76 | 1 | 19min. | \\$0.13 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 1 | 39min. | \\$0.26 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 1 | 59min. | \\$0.39 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 1 | 3h 43min. | \\$1.48 | c4.2xlarge |\n| BAM | 5.26 GB | Yes | 71.5M | 76 | 8 | 12min. | \\$0.08 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 8 | 23min. | \\$0.15 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 8 | 34min. | \\$0.23 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 8 | 2h 10min. | \\$0.86 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-bam"}, {"name": "Output filename prefix"}, {"name": "Uncompressed BAM output"}, {"name": "Number of temporary files"}, {"name": "Input file format option"}, {"name": "Output format"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Number of threads"}], "output": [{"name": "Output BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-collate-1-9-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-collate-1-9-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Samtools Collate CWL1.0", "description": "**SAMtools Collate** tool shuffles and groups reads together by their names. A faster alternative to a full query name sort, **SAMtools Collate** ensures that reads of the same name are grouped together in contiguous groups, but doesn't make any guarantees about the order of read names between groups. The output from this command should be suitable for any operation that requires all reads from the same template to be grouped together [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Collate** is a faster alternative to a full query name sort. The output from this tool should be suitable for any operation that requires all reads from the same template to be grouped together. Some of the tools that require such input are **SAMtools Fixmate** and **SAMtools FASTQ** (not required but it is recommended for further processing of FASTQ files). If you want to use additional threads, consider using **SAMtools Sort** with parameter **Sort by read name** (`-n`) set to True because it may perform faster than **SAMtools Collate**. Please check **Performance Benchmarking** section of both tools for more information.\n\n###Changes Introduced by Seven Bridges\n\n- Parameters **Output to stdout** (`-O`)  and **Write output to FILE** (`-o`) were excluded from the wrapper since they were inapplicable on the platform. \n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set the number of additional threads.\n\n###Common Issues and Important Notes\n\n- Memory consumption depends on the size of **Input BAM/SAM/CRAM file** and **Number of temporary files** (`-n`). For the default  **Number of temporary files** (`-n`), it should not exceed 0.2 \\* size(**Input BAM/SAM/CRAM file**). Memory does not depend on the number of threads.  \n- When using parameter **Number of temporary files** (`-n`), it is recommended to use values greater than or equal to 64 (default value). If the value is less than 64, it may result in task failure caused by insufficient memory.\n- When specifying output filename (**Output filename prefix**), only prefix of the filename should be used (without extension). \n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools Collate** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | Paired-end | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | Yes |  71.5M | 76 | 1 | 18min. | \\$0.16 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 1 | 41min. | \\$0.37 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 1 | 1h 2min. | \\$0.56 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 1 | 3h 55min. | \\$2.12 | c4.2xlarge |\n| BAM | 5.26 GB | Yes | 71.5M | 76 | 8 | 7min. | \\$0.06 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 8 | 17min. | \\$0.15 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 8 | 23min. | \\$0.21 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 8 | 1h 32min. | \\$0.82 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-bam"}, {"name": "Output filename prefix"}, {"name": "Uncompressed BAM output"}, {"name": "Number of temporary files"}, {"name": "Input file format option"}, {"name": "Output format"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Number of threads"}, {"name": "Fast mode"}, {"name": "Number of reads to store in memory"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-coverage-1-10-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-coverage-1-10-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Samtools Coverage CWL1.0", "description": "**SAMtools Coverage** prints a tabular format of the average coverage and percent coverage for each reference sequence. The tool prints a number of aligned reads, average mapping quality and base quality. It can also plot a histogram of coverage across the genome. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n\n###Common Use Cases\n\n**SAMtools Coverage** calcultates and visualizes the sequence coverage across each chromosome or contig using input BAM/SAM/CRAM file. Specific region (eg. 1:100-300000) can be additionally specified for obtaining coverage only in that region.  It is possible to provide one or more BAM/SAM/CRAM files on the input. Coverage on the output can be presented in the form of a table (default) or histogram.\n\n\n###Changes Introduced by Seven Bridges\n\n\n- Parameters **Input BAM files list** (`--bam-list`) and **Verbosity level** (`--verbosity`) were excluded from the wrapper since they are not applicable on our platform.\n\n###Common Issues and Important Notes\n\n- Using parameter **Show specified region** (`-r/--region`) requires the BAM/CRAM/SAM files to be indexed.\n- When CRAM file is provided on the **Input BAM/SAM/CRAM file** input, it is recommended to provide the reference file on the **Reference file** input. If reference file is not provided, the tool will try to get reference from external database, which can prolong execution time, or even fail the task if it has a problem accessing the database.\n- When multiple BAM/SAM/CRAM files are provided on the input, the tool will provide only one output file with combined coverage from all input files.\n\n\n###Performance benchmarking\n\n**SAMtools Coverage** is singlethreaded. **Benchmark of SAMtools Coverage showed that the size of the output file and execution duration are proportional to the size of the input file.** In the following table you can find some examples of **SAMtools Coverage** running time and cost. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length | Duration | Cost | Instance (AWS) |\n|---------------|--------------|---------------|------------------|-------------------|--------------|--------|-------------|\n| BAM | 5.26 GB | 71.5M | 76 | 5min. | \\$0.04 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 |  13min. | \\$0.12 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | 1h 3min. | \\$0.57 | c4.2xlarge |\n\n\n\n###References\n[1] [SAMtools GitHub](https://github.com/samtools/samtools/releases/tag/1.10)", "input": [{"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Minimum read length"}, {"name": "Minimum base quality"}, {"name": "Minimum mapping quality"}, {"name": "Required flags"}, {"name": "Filter flags"}, {"name": "Histogram"}, {"name": "ASCII characters in histogram"}, {"name": "Output filename"}, {"name": "Don't print a header in tabular mode"}, {"name": "Number of bins in histogram"}, {"name": "Show specified region"}, {"name": "Input file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "Number of CPUs"}], "output": [{"name": "Coverage results"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.1"], "dateModified": 1649155729, "dateCreated": 1582563777, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-depth-1-3/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-depth-1-3/9", "applicationCategory": "CommandLineTool", "name": "SAMtools Depth", "description": "**SAMtools depth** computes the depth at each position or region.", "input": [{"name": "Alignment input file", "encodingFormat": "application/x-sam"}, {"name": "Total memory"}, {"name": "BED file", "encodingFormat": "text/x-bed"}, {"name": "Set limit coverage"}, {"name": "Output all positions"}, {"name": "Output absolutely all positions"}, {"name": "Ignore short reads"}, {"name": "Truncate reported depth"}, {"name": "Base quality minimum"}, {"name": "Mapping quality minimum"}], "output": [{"name": "Coverage per position"}, {"name": "Average coverage"}, {"name": "Percentage of coverage larger than limit"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li/Sanger Institute,  Bob Handsaker/Broad Institute, James Bonfield/Sanger Institute,", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649415456, "dateCreated": 1509554702, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-dict-1-6/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-dict-1-6/5", "applicationCategory": "CommandLineTool", "name": "SAMtools Dict", "description": "**SAMtools Dict** tool is used to create a sequence dictionary file from a FASTA file. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases \n\n- When using this tool as a standalone tool, **Input DICT file** should not be provided. This input is given as an option that is convenient to use in workflows.\n- When using this tool in a workflow, **Input DICT file** can be provided. In case it is provided, the tool execution will be skipped and it will just pass the inputs through. This is useful for workflows which use tools that require dictionary file when it is not known in advance if the input FASTA file will have accompanying dictionary file present in the project. If the next tool in the workflow requires dictionary file as a secondary file, parameter **Output reference data file** should be set to True. This will provide FASTA file at **FASTA file** output port along with its dictionary file as secondary file.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Output reference data file** and file input **Input DICT file** are added to provide additional options for integration with other tools within a workflow. \n- Default value for parameter **URI for the UR tag** (`--uri/-u`) is set to filename of the **Input FASTA file** instead of a complete path.\n- Parameter for output filename (`-o/--output`) is not exposed to the user. Output filename is *reference.dict* for input reference named *reference.fasta/reference.fa/reference.fasta.gz/reference.fa.gz*.\n\n###Common Issues and Important Notes\n\n- **When using this tool in a workflow, if the next tool in the workflow requires dictionary file as a secondary file, parameter Output reference data file should be set to True. This will provide FASTA file at FASTA file output port along with its dictionary file as secondary file.**\n\n###Performance Benchmarking\n\nThe execution time for creating dictionary file takes several minutes on the default instance; the price is negligible (~ $0.03). Unless specified otherwise, the default instance used to run the **SAMtools Dict** tool will be c4.2xlarge (AWS).\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Input FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Specify the assembly for the AS tag"}, {"name": "Do not print header"}, {"name": "Species"}, {"name": "URI for the UR tag"}, {"name": "Input DICT file"}, {"name": "Output reference data file"}], "output": [{"name": "Sequence dictionary file"}, {"name": "FASTA file", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "FASTA Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-dict-1-9-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-dict-1-9-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Samtools Dict CWL1.0", "description": "**SAMtools Dict** tool is used to create a sequence dictionary file from a FASTA file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases \n\n- When using this tool as a standalone tool, **Input DICT file** should not be provided. This input is given as an option that is convenient to use in workflows.\n- When using this tool in a workflow, **Input DICT file** can be provided. In case it is provided, tool execution will be skipped and it will just pass the inputs through. This is useful for workflows which use tools that require a dictionary file when it is not known in advance if the input FASTA file will have an accompanying dictionary file present in the project. If the next tool in the workflow requires a dictionary file as a secondary file, the parameter **Output reference data file** should be set to True. This will provide a FASTA file at the **FASTA file** output port along with its dictionary file as the secondary file.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Output reference data file** and file input **Input DICT file** are added to provide additional options for integration with other tools within a workflow. \n- Default value for parameter **URI for the UR tag** (`--uri/-u`) is set to filename of the **Input FASTA file** instead of a complete path.\n- Parameter for output filename (`-o/--output`) is not exposed to the user. Output filename is *reference.dict* for input reference named *reference.fasta/reference.fa/reference.fasta.gz/reference.fa.gz*.\n\n###Common Issues and Important Notes\n\n- **When using this tool in a workflow, if the next tool in the workflow requires dictionary file as a secondary file, the parameter Output reference data file should be set to True. This will provide a FASTA file at the FASTA file output port along with its dictionary file as the secondary file.**\n\n###Performance Benchmarking\n\nThe execution time for creating dictionary file is several minutes on the default instance; the price is negligible (~ $0.03). Unless specified otherwise, the default instance used to run the **SAMtools Dict** tool will be c4.2xlarge (AWS).\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Input FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Specify the assembly for the AS tag"}, {"name": "Do not print header"}, {"name": "Species"}, {"name": "URI for the UR tag"}, {"name": "Input DICT file"}, {"name": "Output reference data file"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Sequence dictionary file"}, {"name": "FASTA file", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "FASTA Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-faidx-1-6/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-faidx-1-6/5", "applicationCategory": "CommandLineTool", "name": "SAMtools Faidx", "description": "**SAMtools Faidx** tool is used to index reference sequence in the FASTA format or extract subsequence from indexed reference sequence. The input file can be compressed in the BGZF format. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases \n\nThis tool works in two modes of operation:\n\n1. **Indexing FASTA file** - If no region is specified, the tool will index **Input FASTA file**. If the FASTA file is already indexed, its index can be provided at **Input index file** input port. In this case, the tool execution will be skipped and it will just pass the inputs through.\n2. **Extracting region subsequences** - If regions are specified, subsequences will be provided at **Region FASTA file** output port. In this mode, **Input index file** can be provided, but it is not mandatory. If it is not provided, the tool will first index the file and then extract subsequences.  \n\nWhen using this tool in a workflow as indexer, **Input index file** can be provided. In case it is provided, the tool execution will be skipped and it will just pass the inputs through. This is useful for workflows which use tools that require index file as a secondary file when it is not known in advance if the input FASTA file will have accompanying index file present in the project. If the next tool in the workflow requires index file as a secondary file, parameter **Output indexed data file** should be set to True. This will provide FASTA file at **Indexed FASTA file** output port along with its index file (FAI) as secondary file.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Output indexed data file** is added to provide additional options for integration with other tools within a workflow. \n\n###Common Issues and Important Notes\n\n- The sequences in the input file should all have different names. If they do not, **SAMtools Faidx** will emit a warning about duplicate sequences and retrieval will only produce subsequences from the first sequence with the duplicated name. [1]\n- If you want to **extract region from FASTA.GZ or FA.GZ file, do not provide FAI file** at **Input index file** input port. If FAI file is present, the tool will not index **Input FASTA file** and it will try to read it but since there is no GZI file present, the task will fail. If you provide only FASTA.GZ/FA.GZ file, the tool will first generate both FAI and GZI index and then extract subsequences.\n- **When using this tool in a workflow as indexer, if the next tool in the workflow requires index file as a secondary file, parameter Output indexed data file should be set to True. This will provide FASTA file at Indexed FASTA file output port along with its index file (FAI) as secondary file.**\n\n###Performance Benchmarking\n\nThe execution time for indexing FASTA file takes several minutes on the default instance; the price is negligible (~ $0.04). Unless specified otherwise, the default instance used to run the **SAMtools Faidx** tool will be c4.2xlarge (AWS).\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Input FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Input index file"}, {"name": "Region"}, {"name": "Output indexed data file"}], "output": [{"name": "Indexed FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Generated FAI index"}, {"name": "Generated GZI index"}, {"name": "Region FASTA file", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "FASTA Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-faidx-1-9-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-faidx-1-9-cwl1-0/7", "applicationCategory": "CommandLineTool", "name": "Samtools Faidx CWL1.0", "description": "**SAMtools Faidx** tool is used to index FASTA reference sequence or extract subsequences. The input file can be compressed in the BGZF format [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases \n\nThis tool works in two modes of operation:\n\n1. **Indexing FASTA file** - If no region is specified, the tool will index **Input FASTA file**. If the FASTA file is already indexed, its index can be provided at the **Input index file** input port. In this case, tool execution will be skipped and it will just pass the inputs through.\n2. **Extracting region subsequences** - If regions are specified, subsequences will be provided at the **Region FASTA file** output port. In this mode, **Input index file** can be provided, but it is not mandatory. If it is not provided, the tool will first index the file and then extract subsequences.  \n\nWhen using this tool in a workflow as the indexer, **Input index file** can be provided. In case it is provided, tool execution will be skipped and it will just pass the inputs through. This is useful for workflows which use tools that require an index file as a secondary file when it is not known in advance if the input FASTA file will have an accompanying index file present in the project. If the next tool in the workflow requires an index file as a secondary file, parameter **Output indexed data file** should be set to True. This will provide a FASTA file at the **Indexed FASTA file** output port along with its index file (FAI) as the secondary file.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Output indexed data file** is added to provide additional options for integration with other tools within a workflow. \n- Parameter for output filename (`-o/--output`) is not exposed to the user. Output filename is *reference.fai* for input reference named *reference.fasta/reference.fa/reference.fasta.gz/reference.fa.gz*.\n\n###Common Issues and Important Notes\n\n- The sequences in the input file should all have different names. If they do not, **SAMtools Faidx** will emit a warning about duplicate sequences and retrieval will only produce subsequences from the first sequence with the duplicated name [1].\n- **When using this tool in a workflow as the indexer, if the next tool in the workflow requires an index file as a secondary file, parameter Output indexed data file should be set to True. This will provide a FASTA file at the Indexed FASTA file output port along with its index file (FAI) as the secondary file.**\n\n###Performance Benchmarking\n\nThe execution time for indexing a FASTA file takes several minutes on the default instance; the price is negligible (~ $0.04). Unless specified otherwise, the default instance used to run the **SAMtools Faidx** tool will be c4.2xlarge (AWS).\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Input FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Input index file"}, {"name": "Region"}, {"name": "Output indexed data file"}, {"name": "Length of FASTA sequence line"}, {"name": "Continue working if a non-existant region is requested"}, {"name": "Read regions from a file", "encodingFormat": "text/plain"}, {"name": "Read FASTQ files and output extracted sequences in FASTQ format. Same as using samtools fqidx."}, {"name": "Output the sequence as the reverse complement"}, {"name": "Append strand indicator to sequence name"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Indexed FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Generated FAI index"}, {"name": "Generated GZI index"}, {"name": "Region FASTA file", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "FASTA Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244164, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-fasta-1-6/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-fasta-1-6/8", "applicationCategory": "CommandLineTool", "name": "SAMtools FASTA", "description": "**SAMtools FASTA** tool converts a BAM or SAM into FASTA format. The FASTA files will be automatically compressed if the filenames have a .gz or .bgzf extention. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n- Default use case provides two FASTA files as outputs (**Paired-end FASTA files**). If no parameter is set, this will be the case. The tool will output file with reads that are not properly flagged (**Unflagged reads FASTA file**) only in case this file is not empty. \n- If single FASTA file (with both paired end reads) is required, it should be specified by setting boolean parameter **Single FASTA file** to True. \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Single FASTA file** was added to parameter list to provide option for outputting single FASTA file with all the reads.\n- Parameter **Single FASTA filename** was added to parameter list to specify filename when **Single FASTA file** is set to True. This parameter is not mandatory. If **Single FASTA file** is set to True and **Single FASTA filename** is not specified, default value will be used (*input.fasta* for input **BAM/SAM file** named *input.bam*).\n- CRAM input was excluded from wrapper since it did not work in the cloud.\n- Input **Reference file** (`--reference`) was excluded from wrapper since it did not work with CRAM input in the cloud.\n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n   \n- We had problem running **SAMtools FASTA** with CRAM input files in the cloud. If you need to use this tool with CRAM input files, please contact our support.   \n- When specifying output filenames, complete names should be used (including extensions). If the extension is .fasta.gz or .fasta.bgzf, output will be compressed. The tool does not validate extensions. If the extension is not valid, the task will not fail.\n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **SAMtools FASTA** running time and cost. Parameter **Number of threads** (`--threads/-@`) can decrease running time (10% - 30% with 8 threads on c4.2xlarge instance (AWS)). \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | Paired-end | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | Yes | 71.5M | 76 | 1 | 8min. | \\$0.05 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 1 | 13min. | \\$0.09 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 1 | 16min. | \\$0.11 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 1 | 1h 5min. | \\$0.43 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "BAM/SAM file", "encodingFormat": "application/x-sam"}, {"name": "Unflagged reads filename"}, {"name": "Filename for BAM_READ1 flaged reads"}, {"name": "Filename for BAM_READ2 flaged reads"}, {"name": "Include reads with all of these flags"}, {"name": "Exclude reads with any of these flags"}, {"name": "Exclude reads with all of these flags"}, {"name": "Don't append /1 and /2 to the read name"}, {"name": "Always append /1 and /2 to the read name"}, {"name": "Singleton reads filename"}, {"name": "Copy RG, BC and QT tags to the FASTA header line"}, {"name": "Taglist to copy to the FASTA header line"}, {"name": "Single FASTA filename"}, {"name": "Single FASTA file"}, {"name": "Input file format option"}, {"name": "Number of threads"}], "output": [{"name": "Paired-end FASTA files", "encodingFormat": "application/x-fasta"}, {"name": "Unflagged reads FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Singleton FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Single FASTA with all reads", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-fasta-1-9-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-fasta-1-9-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Samtools FASTA CWL1.0", "description": "**SAMtools FASTA** tool converts a BAM or SAM into FASTA format. The FASTA files will be automatically compressed if the filenames have a .gz or .bgzf extension [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n- Default use case provides two FASTA files as outputs (**Paired-end FASTA files**). If no parameter is set, this will be the case. The tool will output file with reads that are not properly flagged (**Unflagged reads FASTA file**) only in case this file is not empty. \n- If a single FASTA file (with both paired end reads) is required, it should be specified by setting the boolean parameter **Single FASTA file** to True. \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Single FASTA file** was added to parameter list to provide the option for outputting a single FASTA file with all the reads.\n- Parameter **Single FASTA filename** was added to parameter list to specify the filename when **Single FASTA file** is set to True. This parameter is not mandatory. If **Single FASTA file** is set to True and **Single FASTA filename** is not specified, default value will be used (*input.fasta* for input **BAM/SAM file** named *input.bam*).\n- CRAM input was excluded from the wrapper since it did not work in the cloud.\n- Input **Reference file** (`--reference`) was excluded from the wrapper since it did not work with CRAM input in the cloud.\n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n   \n- We had problem running **SAMtools FASTA** with CRAM input files in the cloud. If you need to use this tool with CRAM input files, please contact our support.   \n- When specifying output filenames, complete names should be used (including extensions). If the extension is .fasta.gz or .fasta.bgzf, the output will be compressed. The tool does not validate extensions. If the extension is not valid, the task will not fail.\n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **SAMtools FASTA** running time and cost. Parameter **Number of threads** (`--threads/-@`) can decrease running time (10% - 30% with 8 threads on a c4.2xlarge instance (AWS)). \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | Paired-end | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | Yes | 71.5M | 76 | 1 | 6min. | \\$0.05 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 1 | 13min. | \\$0.12 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 1 | 16min. | \\$0.14 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 1 | 1h 25min. | \\$0.76 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "BAM/SAM file", "encodingFormat": "application/x-sam"}, {"name": "Unflagged reads filename"}, {"name": "Filename for BAM_READ1 flaged reads"}, {"name": "Filename for BAM_READ2 flaged reads"}, {"name": "Include reads with all of these flags"}, {"name": "Exclude reads with any of these flags"}, {"name": "Exclude reads with all of these flags"}, {"name": "Don't append /1 and /2 to the read name"}, {"name": "Always append /1 and /2 to the read name"}, {"name": "Singleton reads filename"}, {"name": "Copy RG, BC and QT tags to the FASTA header line"}, {"name": "Taglist to copy to the FASTA header line"}, {"name": "Single FASTA filename"}, {"name": "Single FASTA file"}, {"name": "Input file format option"}, {"name": "Number of threads"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Paired-end FASTA files", "encodingFormat": "application/x-fasta"}, {"name": "Unflagged reads FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Singleton FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Single FASTA with all reads", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038712, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-fastq-1-8/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-fastq-1-8/5", "applicationCategory": "CommandLineTool", "name": "SAMtools FASTQ", "description": "**SAMtools FASTQ** tool converts a BAM or CRAM into FASTQ format. The FASTQ files will be automatically compressed if the filenames have a .gz or .bgzf extention. [1]\n\n**SAMtools FASTQ does not perform any preprocessing of the input.** If you want to use output FASTQ files with alignment tools, please make sure that **BAM/SAM/CRAM file** is sorted by read name (or collated). Otherwise, alignment tools will fail. Supplementary and secondary alignments are filtered out regardless of filtering parameters. \n\nParameter **Index format for parsing barcode and quality tags** (`--index-format`) is a string used to describe how to parse the barcode and quality tags. For example:  \n\n- i14i8 - the first 14 characters are index 1, the next 8 characters are index 2\n- n8i14 - ignore the first 8 characters, and use the next 14 characters for index 1  \n- If the tag contains a separator, then the numeric part can be replaced with '\\*' to mean 'read until the separator or end of tag', for example:  \nn\\*i\\* - ignore the left part of the tag until the separator, then use the second part. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n- Default use case provides two FASTQ files as outputs (**Paired-end FASTQ files**). If no parameter is set, this will be the case. The tool will output file with reads that are not properly flagged (**Unflagged reads FASTQ file**) only in case this file is not empty. **SAMtools FASTQ does not perform any preprocessing of the input.** If you want to use output FASTQ files with alignment tools, please make sure that **BAM/SAM/CRAM file** is sorted by read name (or collated). Otherwise, alignment tools will fail. Supplementary and secondary alignments are filtered out regardless of filtering parameters.   \n- If single FASTQ file (with both paired end reads) is required, it should be specified by setting boolean parameter **Single FASTQ file** to True. \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Single FASTQ file** was added to parameter list to provide option for outputting single FASTQ file with all the reads.\n- Parameter **Single FASTQ filename** was added to parameter list to specify filename when **Single FASTQ file** is set to True. This parameter is not mandatory. If **Single FASTQ file** is set to True and **Single FASTQ filename** is not specified, default value will be used (*input.fastq* for input **BAM/SAM/CRAM file** named *input.bam*).\n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- If parameters **First index reads filename** (`--i1`) and/or **Second index reads filename** (`--i2`) are specified, **Index format for parsing barcode and quality tags** (`--index-format`) should be specified too and this format should match a number of index files required. \n- When specifying output filenames, complete names should be used (including extensions). If the extension is .fastq.gz or .fastq.bgzf, output will be compressed. The tool does not validate extensions. If the extension is not valid, the task will not fail.\n- Parameter **Number of threads** (`--threads/-@`) does not decrease running time significantly (not more than 10% with 8 threads on c4.2xlarge instance (AWS)).\n- **SAMtools FASTQ** does not perform any preprocessing of the input. If you want to use output FASTQ files with alignment tools, please make sure that **BAM/SAM/CRAM file** is sorted by read name (or collated). Otherwise, alignment tools will fail. Supplementary and secondary alignments are filtered out regardless of filtering parameters. \n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **SAMtools FASTQ** running time and cost. Adding additional threads does not decrease running time significantly (not more than 10% with 8 threads on c4.2xlarge instance (AWS)).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | Paired-end | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS) |\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | Yes | 71.5M | 76 | 1 | 7min. | \\$0.05 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 1 | 15min. | \\$0.10 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 1 | 17min. | \\$0.11 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 1 | 1h 31min. | \\$0.60 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Unflagged reads filename"}, {"name": "Filename for BAM_READ1 flaged reads"}, {"name": "Filename for BAM_READ2 flaged reads"}, {"name": "Include reads with all of these flags"}, {"name": "Exclude reads with any of these flags"}, {"name": "Exclude reads with all of these flags"}, {"name": "Don't append /1 and /2 to the read name"}, {"name": "Always append /1 and /2 to the read name"}, {"name": "Output quality in the OQ tag if present"}, {"name": "Singleton reads filename"}, {"name": "Copy RG, BC and QT tags to the FASTQ header line"}, {"name": "Taglist to copy to the FASTQ header line"}, {"name": "Default quality score if not given in file"}, {"name": "Add Illumina Casava 1.8 format entry to header"}, {"name": "Compression level [0..9]"}, {"name": "First index reads filename"}, {"name": "Second index reads filename"}, {"name": "Barcode tag"}, {"name": "Quality tag"}, {"name": "Index format for parsing barcode and quality tags"}, {"name": "Single FASTQ filename"}, {"name": "Single FASTQ file"}, {"name": "Input file format option"}, {"name": "Number of threads"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Paired-end FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Unflagged reads FASTQ file", "encodingFormat": "text/fastq"}, {"name": "Singleton FASTQ file", "encodingFormat": "text/fastq"}, {"name": "Single FASTQ with all reads", "encodingFormat": "text/fastq"}, {"name": "First and second index reads FASTQ", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["File Format Conversion", "FASTQ Processing", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155968, "dateCreated": 1538151204, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-fastq-1-9-cwl1-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-fastq-1-9-cwl1-0/5", "applicationCategory": "CommandLineTool", "name": "Samtools FASTQ CWL1.0", "description": "**SAMtools FASTQ** tool converts a BAM or CRAM into FASTQ format. The FASTQ files will be automatically compressed if the filenames have a .gz or .bgzf extension [1].\n\n**SAMtools FASTQ does not perform any preprocessing of the input.** If you want to use output FASTQ files with alignment tools, please make sure that the **BAM/SAM/CRAM file** is sorted by read name (or collated). Otherwise, alignment tools will fail. Supplementary and secondary alignments are filtered out regardless of filtering parameters. \n\nParameter **Index format for parsing barcode and quality tags** (`--index-format`) is a string used to describe how to parse the barcode and quality tags. For example:  \n\n- i14i8 - the first 14 characters are index 1, the next 8 characters are index 2\n- n8i14 - ignore the first 8 characters, and use the next 14 characters for index 1  \n- If the tag contains a separator, then the numeric part can be replaced with '\\*' to mean 'read until the separator or end of tag', for example:  \nn\\*i\\* - ignore the left part of the tag until the separator, then use the second part [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n- Default use case provides two FASTQ files as outputs (**Paired-end FASTQ files**). If no parameter is set, this will be the case. The tool will output file with reads that are not properly flagged (**Unflagged reads FASTQ file**) only in case this file is not empty. **SAMtools FASTQ does not perform any preprocessing of the input.** If you want to use output FASTQ files with alignment tools, please make sure that the **BAM/SAM/CRAM file** is sorted by read name (or collated). Otherwise, alignment tools will fail. Supplementary and secondary alignments are filtered out regardless of filtering parameters.   \n- If a single FASTQ file (with both paired end reads) is required, it should be specified by setting the boolean parameter **Single FASTQ file** to True. \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Single FASTQ file** was added to parameter list to provide the option for outputting a single FASTQ file with all the reads.\n- Parameter **Single FASTQ filename** was added to parameter list to specify the filename when **Single FASTQ file** is set to True. This parameter is not mandatory. If **Single FASTQ file** is set to True and **Single FASTQ filename** is not specified, default value will be used (*input.fastq* for input **BAM/SAM/CRAM file** named *input.bam*).\n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- If parameters **First index reads filename** (`--i1`) and/or **Second index reads filename** (`--i2`) are specified, **Index format for parsing barcode and quality tags** (`--index-format`) should be specified too and this format should match the number of index files required. \n- When specifying output filenames, complete names should be used (including extensions). If the extension is .fastq.gz or .fastq.bgzf, the output will be compressed. The tool does not validate extensions. If the extension is not valid, the task will not fail.\n- Parameter **Number of threads** (`--threads/-@`) does not decrease running time significantly (not more than 10% with 8 threads on a c4.2xlarge instance (AWS)).\n- **SAMtools FASTQ** does not perform any preprocessing of the input. If you want to use output FASTQ files with alignment tools, please make sure that **BAM/SAM/CRAM file** is sorted by read name (or collated). Otherwise, alignment tools will fail. Supplementary and secondary alignments are filtered out regardless of filtering parameters. \n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **SAMtools FASTQ** running time and cost. Adding additional threads does not decrease running time significantly (not more than 10% with 8 threads on a c4.2xlarge instance (AWS)).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | Paired-end | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS) |\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | Yes | 71.5M | 76 | 1 | 7min. | \\$0.06 | c4.2xlarge |\n| BAM | 11.86 GB | Yes | 161.2M | 101 | 1 | 18min. | \\$0.16 | c4.2xlarge |\n| BAM | 18.36 GB | Yes | 179M | 76 | 1 | 21min. | \\$0.19 | c4.2xlarge |\n| BAM | 58.61 GB | Yes | 845.6M | 150 | 1 | 2h 7min. | \\$1.14 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Unflagged reads filename"}, {"name": "Filename for BAM_READ1 flaged reads"}, {"name": "Filename for BAM_READ2 flaged reads"}, {"name": "Include reads with all of these flags"}, {"name": "Exclude reads with any of these flags"}, {"name": "Exclude reads with all of these flags"}, {"name": "Don't append /1 and /2 to the read name"}, {"name": "Always append /1 and /2 to the read name"}, {"name": "Output quality in the OQ tag if present"}, {"name": "Singleton reads filename"}, {"name": "Copy RG, BC and QT tags to the FASTQ header line"}, {"name": "Taglist to copy to the FASTQ header line"}, {"name": "Default quality score if not given in file"}, {"name": "Add Illumina Casava 1.8 format entry to header"}, {"name": "Compression level [0..9]"}, {"name": "First index reads filename"}, {"name": "Second index reads filename"}, {"name": "Barcode tag"}, {"name": "Quality tag"}, {"name": "Index format for parsing barcode and quality tags"}, {"name": "Single FASTQ filename"}, {"name": "Single FASTQ file"}, {"name": "Input file format option"}, {"name": "Number of threads"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Paired-end FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Unflagged reads FASTQ file", "encodingFormat": "text/fastq"}, {"name": "Singleton FASTQ file", "encodingFormat": "text/fastq"}, {"name": "Single FASTQ with all reads", "encodingFormat": "text/fastq"}, {"name": "First and second index reads FASTQ", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-fixmate-1-6/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-fixmate-1-6/5", "applicationCategory": "CommandLineTool", "name": "SAMtools Fixmate", "description": "**SAMtools Fixmate** tool is used to fill in mate coordinates, ISIZE and mate related flags from a name-sorted alignment. [1]\n\n**SAMtools Fixmate** requires input grouped/sorted by queryname.\n\nOutput file format is set by the parameter **Output format** (`--output-fmt/-O`). If this parameter is not set, format is recognized from the filename defined by the parameter **Output filename** (`-o`). If the filename is not set or the format can not be deduced, BAM format will be selected.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Fixmate** tool is used to fill in mate information and mate related flags from a name-sorted alignment file. It is required as preprocessing step for **SAMtools Markdup** tool with **Add mate score tag** (`-m`) set to True. **SAMtools Sort** (with **Sort by read name** (`-n`) set to True) can be used for sorting input file by name prior to use of **SAMtools Fixmate**.   \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- **SAMtools Fixmate** requires input grouped/sorted by queryname.\n- Input CRAM files require **Reference file** (`--reference`) to be provided. If it is not provided, the task will not fail, it will generate output file containing only header.  \n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools Fixmate** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n       \n| Input type | Input size | Paired-end | # of reads | Read length | # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-------------------------------|-------------|--------|-------------|\n| BAM | 7.06 GB | Yes | 71.5M | 76 | 1 | 17min. | \\$0.11 | c4.2xlarge |\n| BAM | 16.21 GB | Yes | 161.2M | 101 | 1 | 46min. | \\$0.31 | c4.2xlarge |\n| BAM | 24.05 GB | Yes | 179M | 76 | 1 | 1h 14min. | \\$0.49 | c4.2xlarge |\n| BAM | 89.56 GB | Yes | 845.6M | 150 | 1 | 4h 23min. | \\$1.74 | c4.2xlarge |\n| BAM | 7.06 GB | Yes | 71.5M | 76 | 8 | 9min. | \\$0.06 | c4.2xlarge |\n| BAM | 16.21 GB | Yes | 161.2M | 101 | 8 | 15min. | \\$0.10 | c4.2xlarge |\n| BAM | 24.05 GB | Yes | 179M | 76 | 8 | 23min. | \\$0.15 | c4.2xlarge |\n| BAM | 89.56 GB | Yes | 845.6M | 150 | 8 | 1h 17min. | \\$0.51 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Remove unmapped reads and secondary alignments"}, {"name": "Disable FR proper pair check"}, {"name": "Add template cigar ct tag"}, {"name": "Add mate score tag"}, {"name": "Input file format option"}, {"name": "Output format"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Number of threads"}, {"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Output filename"}], "output": [{"name": "Fixed file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-fixmate-1-9-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-fixmate-1-9-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Samtools Fixmate CWL1.0", "description": "**SAMtools Fixmate** tool is used to fill in mate coordinates, ISIZE and mate related flags from a name-sorted alignment [1].\n\n**SAMtools Fixmate** requires input grouped/sorted by queryname.\n\nOutput file format is set by the parameter **Output format** (`--output-fmt/-O`). If this parameter is not set, format is recognized from the filename defined by the parameter **Output filename** (`-o`). If the filename is not set or the format can not be deduced, BAM format will be selected.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Fixmate** tool is used to fill in mate information and mate-related flags from a name-sorted alignment file. It is required as a preprocessing step for **SAMtools Markdup** with the **Add mate score tag** (`-m`) set to True. **SAMtools Sort** (with **Sort by read name** (`-n`) set to True) can be used for sorting the input file by name prior to use of **SAMtools Fixmate**.   \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set the number of additional threads.\n\n###Common Issues and Important Notes\n\n- **SAMtools Fixmate** requires input grouped/sorted by queryname.\n- Input CRAM files require **Reference file** (`--reference`) to be provided. If it is not provided, the task will fail.  \n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting the parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools Fixmate** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n       \n| Input type | Input size | Paired-end | # of reads | Read length | # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-------------------------------|-------------|--------|-------------|\n| BAM | 7.06 GB | Yes | 71.5M | 76 | 1 | 17min. | \\$0.15 | c4.2xlarge |\n| BAM | 16.21 GB | Yes | 161.2M | 101 | 1 | 46min. | \\$0.41 | c4.2xlarge |\n| BAM | 24.05 GB | Yes | 179M | 76 | 1 | 1h 19min. | \\$0.71 | c4.2xlarge |\n| BAM | 89.56 GB | Yes | 845.6M | 150 | 1 | 4h 42min. | \\$2.54 | c4.2xlarge |\n| BAM | 7.06 GB | Yes | 71.5M | 76 | 8 | 7min. | \\$0.06 | c4.2xlarge |\n| BAM | 16.21 GB | Yes | 161.2M | 101 | 8 | 16min. | \\$0.14 | c4.2xlarge |\n| BAM | 24.05 GB | Yes | 179M | 76 | 8 | 25min. | \\$0.22 | c4.2xlarge |\n| BAM | 89.56 GB | Yes | 845.6M | 150 | 8 | 1h 28min. | \\$0.79 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Remove unmapped reads and secondary alignments"}, {"name": "Disable FR proper pair check"}, {"name": "Add template cigar ct tag"}, {"name": "Add mate score tag"}, {"name": "Input file format option"}, {"name": "Output format"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Number of threads"}, {"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Output filename"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Fixed file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-index-1-6/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-index-1-6/7", "applicationCategory": "CommandLineTool", "name": "SAMtools Index", "description": "**SAMtools Index** tool is used to index a coordinate-sorted BAM or CRAM file for fast random access. (Note that this does not work with SAM files even if they are bgzip compressed \u2014 to index such files, use tabix instead.) This index is needed when region arguments are used to limit **SAMtools View** and similar commands to particular regions of interest. For a CRAM file aln.cram, index file aln.cram.crai will be created; for a BAM file aln.bam, either aln.bam.bai or aln.bam.csi will be created, depending on the index format selected. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases \n\n- When using this tool as a standalone tool, **Input index file** should not be provided. This input is given as an option that is convenient to use in workflows. \n- When using this tool in a workflow, **Input index file** can be provided. In case it is provided, the tool execution will be skipped and it will just pass the inputs through. This is useful for workflows which use tools that require index file when it is not known in advance if the input BAM/CRAM file will have accompanying index file present in the project. If the next tool in the workflow requires index file as a secondary file, parameter **Output indexed data file** should be set to True. This will provide BAM/CRAM file at **Indexed data file** output port along with its index file (BAI/CSI/CRAI) as secondary file.\n- If a CRAM file is provided at **BAM/CRAM input file** port, the tool will generate CRAI index file. If a BAM file is provided, the tool will generate BAI or CSI index file depending on parameter **Format of index file (for BAM files)** (`-b/-c`). If no value is set, the tool will generate BAI index file. Setting a parameter **Minimum interval size (2^INT)** (`-m`) will force CSI format regardless of the value of the parameter **Format of index file (for BAM files)**.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter output filename is omitted from the wrapper. For a CRAM file aln.cram, output filename will be aln.cram.crai; for a BAM file aln.bam, it will be either aln.bam.bai or aln.bam.csi, depending on the index format selected.\n- Parameter **Output indexed data file** and file input **Input index file** are added to provide additional options for integration with other tools within a workflow. \n\n###Common Issues and Important Notes\n\n- **BAM/CRAM input file** should be sorted by coordinates, not by name. Otherwise, the task will fail.\n- **When using this tool in a workflow, if the next tool in the workflow requires index file as a secondary file, parameter Output indexed data file should be set to True. This will provide BAM/CRAM file at Indexed data file output port along with its index file (BAI/CSI/CRAI) as secondary file.**\n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`-@`). In the following table you can find estimates of **SAMtools Index** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|---------------|------------------|---------------------|-------------|--------|-------------|\n|  BAM | 5.26 GB | 71.5M | 76 | 1 | 5min. | \\$0.03 | c4.2xlarge |\n|  BAM | 11.86 GB | 161.2M | 101| 1 | 8min. | \\$0.05 | c4.2xlarge |\n|  BAM | 18.36 GB | 179M | 76 | 1 | 12min. | \\$0.08 | c4.2xlarge |\n|  BAM | 58.61 GB | 845.6M | 150 | 1 | 30min. | \\$0.20 | c4.2xlarge |\n|  BAM | 5.26 GB | 71.5M | 76 | 8 | 4min. | \\$0.03 | c4.2xlarge |\n|  BAM | 11.86 GB | 161.2M | 101| 8 | 6min. | \\$0.04 | c4.2xlarge |\n|  BAM | 18.36 GB | 179M | 76 | 8 | 12min. | \\$0.08 | c4.2xlarge |\n|  BAM | 58.61 GB | 845.6M | 150 | 8 | 20min. | \\$0.13 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "BAM/CRAM input file", "encodingFormat": "application/x-bam"}, {"name": "Output indexed data file"}, {"name": "Input index file"}, {"name": "Minimum interval size (2^INT)"}, {"name": "Format of index file (for BAM files)"}, {"name": "Number of threads"}], "output": [{"name": "Indexed data file", "encodingFormat": "application/x-bam"}, {"name": "Generated index file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-index-1-9-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-index-1-9-cwl1-0/7", "applicationCategory": "CommandLineTool", "name": "Samtools Index CWL1.0", "description": "**SAMtools Index** tool is used to index a coordinate-sorted BAM or CRAM file for fast random access. Note that this does not work with SAM files even if they are bgzip compressed \u2014 to index such files, use tabix instead. This index is needed when region arguments are used to limit **SAMtools View** and similar commands to particular regions of interest. For a CRAM file aln.cram, index file aln.cram.crai will be created; for a BAM file aln.bam, either aln.bam.bai or aln.bam.csi will be created, depending on the index format selected [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases \n\n- When using this tool as a standalone tool, **Input index file** should not be provided. This input is given as an option that is convenient to use in workflows. \n- When using this tool in a workflow, **Input index file** can be provided. In case it is provided, tool execution will be skipped and it will just pass the inputs through. This is useful for workflows which use tools that require an index file when it is not known in advance if the input BAM/CRAM file will have accompanying index file present in the project. If the next tool in the workflow requires an index file as a secondary file, parameter **Output indexed data file** should be set to True. This will provide a BAM/CRAM file at the **Indexed data file** output port along with its index file (BAI/CSI/CRAI) as the secondary file.\n- If a CRAM file is provided at the **BAM/CRAM input file** port, the tool will generate a CRAI index file. If a BAM file is provided, the tool will generate a BAI or CSI index file depending on the parameter **Format of index file (for BAM files)** (`-b/-c`). If no value is set, the tool will generate a BAI index file. Setting the parameter **Minimum interval size (2^INT)** (`-m`) will force the CSI format regardless of the value of the parameter **Format of index file (for BAM files)**.\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Output filename** is omitted from the wrapper. For a CRAM file aln.cram, output filename will be aln.cram.crai; for a BAM file aln.bam, it will be either aln.bam.bai or aln.bam.csi, depending on the index format selected.\n- Parameter **Output indexed data file** and file input **Input index file** are added to provide additional options for integration with other tools within a workflow. \n\n###Common Issues and Important Notes\n\n- **BAM/CRAM input file** should be sorted by coordinates, not by name. Otherwise, the task will fail.\n- **When using this tool in a workflow, if the next tool in the workflow requires index file as a secondary file, parameter Output indexed data file should be set to True. This will provide BAM/CRAM file at Indexed data file output port along with its index file (BAI/CSI/CRAI) as secondary file.**\n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`-@`). In the following table you can find estimates of **SAMtools Index** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|---------------|------------------|---------------------|-------------|--------|-------------|\n|  BAM | 5.26 GB | 71.5M | 76 | 1 | 4min. | \\$0.04 | c4.2xlarge |\n|  BAM | 11.86 GB | 161.2M | 101| 1 | 10min. | \\$0.09 | c4.2xlarge |\n|  BAM | 18.36 GB | 179M | 76 | 1 | 12min. | \\$0.11 | c4.2xlarge |\n|  BAM | 58.61 GB | 845.6M | 150 | 1 | 36min. | \\$0.32 | c4.2xlarge |\n|  BAM | 5.26 GB | 71.5M | 76 | 8 | 3min. | \\$0.03 | c4.2xlarge |\n|  BAM | 11.86 GB | 161.2M | 101| 8 | 9min. | \\$0.08 | c4.2xlarge |\n|  BAM | 18.36 GB | 179M | 76 | 8 | 11min. | \\$0.10 | c4.2xlarge |\n|  BAM | 58.61 GB | 845.6M | 150 | 8 | 30min. | \\$0.27 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "BAM/CRAM input file", "encodingFormat": "application/x-bam"}, {"name": "Output indexed data file"}, {"name": "Input index file"}, {"name": "Minimum interval size (2^INT)"}, {"name": "Format of index file (for BAM files)"}, {"name": "Number of threads"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Indexed data file", "encodingFormat": "application/x-bam"}, {"name": "Generated index file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-markdup-1-6/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-markdup-1-6/6", "applicationCategory": "CommandLineTool", "name": "SAMtools Markdup", "description": "**SAMtools Markdup** tool is used to mark duplicate alignments from a coordinate sorted file that has been run through **SAMtools Fixmate** with the option **Add mate score tag** (`-m`). This program relies on the MC and ms tags that **SAMtools Fixmate** provides. [1]\n\nOutput file format is set by the parameter **Output format** (`--output-fmt/-O`). If this parameter is not set, format is recognized from the filename defined by the parameter **Output filename** (`-o`). If the filename is not set or the format can not be deduced, BAM format will be selected.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Markdup** tool is used to mark duplicate alignments. **Input BAM/SAM/CRAM file** should be coordinate sorted and run through **SAMtools Fixmate** with the option **Add mate score tag** (`-m`). Note that **SAMtools Fixmate** requires query name sorted input while **SAMtools Markdup** requires coordinate sorted file. Preprocessing should follow these steps:\n\n- **SAMtools Sort** with **Sort by read name** (`-n`) set to True (if the file is not already sorted by query name)\n- **SAMtools Fixmate** with **Add mate score tag** (`-m`) set to True \n- **SAMtools Sort** with **Sort by read name** (`-n`) set to False (or None for default settings)\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- The input file has to be sorted by coordinates and run through **SAMtools Fixmate** with the option **Add mate score tag** (`-m`) on.\n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools Markdup** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n       \n| Input type | Input size | Paired-end | # of reads | Read length | # of threads | Duration | Cost | Instance (AWS) |\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.43 GB | Yes | 71.5M | 76 | 1 | 15min. | \\$0.10 | c4.2xlarge |\n| BAM | 12.7 GB | Yes | 161.2M | 101 | 1 | 38min. | \\$0.25 | c4.2xlarge |\n| BAM | 18.52 GB | Yes | 179M | 76 | 1 | 1h 1min. | \\$0.40 | c4.2xlarge |\n| BAM | 61.35 GB | Yes | 845.6M | 150 | 1 | 3h 33min. | \\$1.41 | c4.2xlarge |\n| BAM | 5.43 GB | Yes | 71.5M | 76 | 8 | 6min. | \\$0.04 | c4.2xlarge |\n| BAM | 12.7 GB | Yes | 161.2M | 101 | 8 | 14min. | \\$0.09 | c4.2xlarge |\n| BAM | 18.52 GB | Yes | 179M | 76 | 8 | 18min. | \\$0.12 | c4.2xlarge |\n| BAM | 61.35 GB | Yes | 845.6M | 150 | 8 | 1h | \\$0.40 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Remove duplicate reads"}, {"name": "Max read length"}, {"name": "Report stats"}, {"name": "Output format"}, {"name": "Number of threads"}, {"name": "Input file format option"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Output filename"}], "output": [{"name": "Output deduplicated file", "encodingFormat": "application/x-sam"}, {"name": "Stats file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027916, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-markdup-1-9-cwl1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-markdup-1-9-cwl1-0/6", "applicationCategory": "CommandLineTool", "name": "Samtools Markdup CWL1.0", "description": "**SAMtools Markdup** tool is used to mark duplicate alignments from a coordinate-sorted file.  The coordinate sorted file has to be run through **SAMtools Fixmate** with the option **Add mate score tag** (`-m`) before running this tool. This program relies on the MC and ms tags that **SAMtools Fixmate** provides [1].\n\nOutput file format is set by the parameter **Output format** (`--output-fmt/-O`). If this parameter is not set, format is recognized from the filename defined by the parameter **Output filename** (`-o`). If the filename is not set or the format can not be deduced, BAM format will be selected.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Markdup** tool is used to mark duplicate alignments. **Input BAM/SAM/CRAM file** should be coordinate sorted and run through **SAMtools Fixmate** with the option **Add mate score tag** (`-m`). Note that **SAMtools Fixmate** requires query name sorted input while **SAMtools Markdup** requires coordinate sorted file. Preprocessing should follow these steps:\n\n- **SAMtools Sort** with **Sort by read name** (`-n`) set to True (if the file is not already sorted by query name)\n- **SAMtools Fixmate** with **Add mate score tag** (`-m`) set to True \n- **SAMtools Sort** with **Sort by read name** (`-n`) set to False (or None for default settings)\n\n###Changes Introduced by Seven Bridges\n\n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- The input file has to be sorted by coordinates and run through **SAMtools Fixmate** with the option **Add mate score tag** (`-m`) on.\n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting the parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools Markdup** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n       \n| Input type | Input size | Paired-end | # of reads | Read length | # of threads | Duration | Cost | Instance (AWS) |\n|---------------|--------------|-----------------|---------------|------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.43 GB | Yes | 71.5M | 76 | 1 | 14min. | \\$0.13 | c4.2xlarge |\n| BAM | 12.7 GB | Yes | 161.2M | 101 | 1 | 38min. | \\$0.34 | c4.2xlarge |\n| BAM | 18.52 GB | Yes | 179M | 76 | 1 | 1h 4min. | \\$0.58 | c4.2xlarge |\n| BAM | 61.35 GB | Yes | 845.6M | 150 | 1 | 3h 50min. | \\$2.07 | c4.2xlarge |\n| BAM | 5.43 GB | Yes | 71.5M | 76 | 8 | 5min. | \\$0.04 | c4.2xlarge |\n| BAM | 12.7 GB | Yes | 161.2M | 101 | 8 | 14min. | \\$0.13 | c4.2xlarge |\n| BAM | 18.52 GB | Yes | 179M | 76 | 8 | 20min. | \\$0.18 | c4.2xlarge |\n| BAM | 61.35 GB | Yes | 845.6M | 150 | 8 | 1h 6min. | \\$0.59 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Remove duplicate reads"}, {"name": "Max read length"}, {"name": "Report stats"}, {"name": "Output format"}, {"name": "Number of threads"}, {"name": "Input file format option"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Output filename"}, {"name": "Name prefix for temporary files"}, {"name": "Mark supplementary reads of duplicates as duplicates"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output deduplicated file", "encodingFormat": "application/x-sam"}, {"name": "Stats file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-mpileup-1-6/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-mpileup-1-6/5", "applicationCategory": "CommandLineTool", "name": "SAMtools Mpileup", "description": "**SAMtools Mpileup** tool generates VCF, BCF or PILEUP for one or multiple BAM files. Alignment records are grouped by sample (SM) identifiers in @RG header lines. If sample identifiers are absent, each input file is regarded as one sample. [1]\n\nIn the pileup format, each line represents a genomic position, consisting of chromosome name, 1-based coordinate, reference base, the number of reads covering the site, read bases, base qualities and alignment mapping qualities. Information on match, mismatch, indel, strand, mapping quality and start and end of a read are all encoded at the read base column. At this column, a dot stands for a match to the reference base on the forward strand, a comma for a match on the reverse strand, a '>' or '<' for a reference skip, 'ACGTN' for a mismatch on the forward strand and `acgtn' for a mismatch on the reverse strand. A pattern '\\\\+[0-9]+[ACGTNacgtn]+' indicates there is an insertion between this reference position and the next reference position. The length of the insertion is given by the integer in the pattern, followed by the inserted sequence. Similarly, a pattern '-[0-9]+[ACGTNacgtn]+' represents a deletion from the reference. The deleted bases will be presented as '*' in the following lines. Also at the read base column, a symbol '^' marks the start of a read. The ASCII of the character following '^' minus 33 gives the mapping quality. A symbol '$' marks the end of a read segment. [1]\n\nNote that there are two orthogonal ways to specify locations in the input file; via **Region in which pileup is generated** (`--region/-r`) and **Region BED file or position list file** (`--positions/-l`). The former uses (and requires) an index to do random access while the latter streams through the file contents filtering out the specified regions, requiring no index. The two may be used in conjunction. For example a BED file containing locations of genes in chromosome 20 could be specified using **Region in which pileup is generated** = 20, **Region BED file or position list file** = chr20.bed (`-r 20 -l chr20.bed`), meaning that the index is used to find chromosome 20 and then it is filtered for the regions listed in the bed file. [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Mpileup** tool generates VCF, BCF or PILEUP for one or multiple BAM files. **Input BAM files** should be sorted by coordinates before using **SAMtools Mpileup**. **SAMtools Sort** can be used for sorting. Output file can be in PILEUP, BCF or VCF format. BCF and VCF outputs are compressed by default. For uncompressed BCF or VCF set parameter **Generate uncompressed VCF/BCF output** (`-u/--uncompressed`) to True. \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **List of input BAM files** (`--bam-list/-b`) was excluded from wrapper since it is not useful on the platform.\n\n###Common Issues and Important Notes\n\n- BAM files should be sorted by coordinates before using **SAMtools Mpileup**.\n- Position list files contain two columns (chromosome and position) and start counting from 1. BED files contain at least 3 columns (chromosome, start and end position) and are 0-based half-open. \nWhile it is possible to mix both position-list and BED coordinates in the same file, this is strongly ill advised due to the differing coordinate systems. [1]\n- Using parameter **Region in which pileup is generated** (`--region/-r`) requires the BAM files to be indexed.\n- VCF and BCF output is bgzip-compressed by default. If you need uncompressed file, use parameter **Generate uncompressed VCF/BCF output** (`--uncompressed/-u`). Uncompressed files are preferred for piping (in workflows).  \n- If no **Output format** (`--VCF/-v/--BCF/-g`) is selected, default format is PILEUP. If no format is selected and **Generate uncompressed VCF/BCF output** (`-u/--uncompressed`) is set to True, output format will be uncompressed BCF.\n\n###Performance Benchmarking\n\n**SAMtools Mpileup** is singlethreaded. **Benchmark of SAMtools Mpileup showed that the size of the output file and execution duration are not proportional to the size of input file.** In the following table you can find some examples of **SAMtools Mpileup** running time and cost. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length | Output format | Duration | Cost | Instance (AWS) |\n|---------------|--------------|---------------|------------------|-------------------|--------------|--------|-------------|\n| BAM | 5.26 GB | 71.5M | 76 | PILEUP | 10min. | \\$0.07 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | PILEUP | 2h 43min. | \\$1.08 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | PILEUP | 29min. | \\$0.19 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150| PILEUP | 2h 38min. | \\$1.05 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | VCF.GZ | 42min. | \\$0.28 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | VCF.GZ | 7h 50min. | \\$3.12 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | VCF.GZ | 2h 7min. | \\$0.84 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150| VCF.GZ | 14h 32min. | \\$5.78 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Output mapping quality"}, {"name": "Output base positions on reads"}, {"name": "Comma-delimited list of platforms for INDELs"}, {"name": "Phred-scaled gap open sequencing error probability"}, {"name": "Minimum gapped reads for indel candidates"}, {"name": "Maximum per-file depth for INDEL calling"}, {"name": "Do not perform INDEL calling"}, {"name": "Coefficient for modeling homopolymer errors"}, {"name": "Minimum fraction of gapped reads"}, {"name": "Phred-scaled gap extension sequencing error probability"}, {"name": "Input BAM files", "encodingFormat": "application/x-sam"}, {"name": "Quality is in the Illumina-1.3+ encoding"}, {"name": "Do not skip anomalous read pairs"}, {"name": "Disable probabilistic realignment for the BAQ computation"}, {"name": "Adjust mapping quality"}, {"name": "Maximum per-file depth"}, {"name": "Recalculate BAQ on the fly"}, {"name": "Ignore RG tags"}, {"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Exclude read groups listed in file", "encodingFormat": "text/plain"}, {"name": "Generate uncompressed VCF/BCF output"}, {"name": "Region in which pileup is generated"}, {"name": "Region BED file or position list file", "encodingFormat": "text/plain"}, {"name": "Required flags"}, {"name": "Filter flags"}, {"name": "Disable read-pair overlap detection"}, {"name": "Comma-separated list of FORMAT and INFO tags to output"}, {"name": "Apply -m and -F thresholds per sample"}, {"name": "Output format"}, {"name": "Reference index file"}, {"name": "Output filename"}, {"name": "Output read names"}, {"name": "Output all positions"}, {"name": "Output absolutely all positions"}, {"name": "Index files"}, {"name": "Input file format option"}], "output": [{"name": "Output PILEUP, VCF, or BCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-mpileup-1-9-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-mpileup-1-9-cwl1-0/7", "applicationCategory": "CommandLineTool", "name": "Samtools Mpileup CWL1.0", "description": "**SAMtools Mpileup** tool generates VCF, BCF or PILEUP for one or multiple BAM files. Alignment records are grouped by sample (SM) identifiers in @RG header lines. If sample identifiers are absent, each input file is regarded as one sample [1].\n\nIn the pileup format, each line represents a genomic position, consisting of chromosome name, 1-based coordinate, reference base, the number of reads covering the site, read bases, base qualities and alignment mapping qualities. Information on match, mismatch, indel, strand, mapping quality and start and end of a read are all encoded at the read base column. In this column, a dot stands for a match to the reference base on the forward strand, a comma for a match on the reverse strand, a '>' or '<' for a reference skip, 'ACGTN' for a mismatch on the forward strand and `acgtn' for a mismatch on the reverse strand. A pattern '\\\\+[0-9]+[ACGTNacgtn]+' indicates there is an insertion between this reference position and the next reference position. The length of the insertion is given by the integer in the pattern, followed by the inserted sequence. Similarly, a pattern '-[0-9]+[ACGTNacgtn]+' represents a deletion from the reference. The deleted bases will be presented as '*' in the following lines. Also at the read base column, a symbol '^' marks the start of a read. The ASCII of the character following '^' minus 33 gives the mapping quality. A symbol '$' marks the end of a read segment [1].\n\nNote that there are two orthogonal ways to specify locations in the input file; via **Region in which pileup is generated** (`--region/-r`) and **Region BED file or position list file** (`--positions/-l`). The former uses (and requires) an index to do random access while the latter streams through the file contents filtering out the specified regions, requiring no index. The two may be used in conjunction. For example a BED file containing locations of genes in chromosome 20 could be specified using **Region in which pileup is generated** = 20, **Region BED file or position list file** = chr20.bed (`-r 20 -l chr20.bed`), meaning that the index is used to find chromosome 20 and then it is filtered for the regions listed in the bed file [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n**SAMtools Mpileup** tool generates VCF, BCF or PILEUP for one or multiple BAM files. **Input BAM files** should be sorted by coordinates before using **SAMtools Mpileup**. **SAMtools Sort** can be used for sorting. Output file can be in PILEUP, BCF or VCF format. BCF and VCF outputs are compressed by default. For uncompressed BCF or VCF set parameter **Generate uncompressed VCF/BCF output** (`-u/--uncompressed`) to True. \n\n###Changes Introduced by Seven Bridges\n\n- Parameter **List of input BAM files** (`--bam-list/-b`) was excluded from the wrapper since it is not useful on the Platform.\n\n###Common Issues and Important Notes\n\n- BAM files should be sorted by coordinates before using **SAMtools Mpileup**.\n- Position list files contain two columns (chromosome and position) and start counting from 1. BED files contain at least 3 columns (chromosome, start and end position) and are 0-based half-open. \nWhile it is possible to mix both position-list and BED coordinates in the same file, this is strongly ill advised due to the differing coordinate systems [1].\n- Using parameter **Region in which pileup is generated** (`--region/-r`) requires the BAM files to be indexed.\n- VCF and BCF output is bgzip-compressed by default. If you need uncompressed file, use parameter **Generate uncompressed VCF/BCF output** (`--uncompressed/-u`). Uncompressed files are preferred for piping (in workflows).  \n- If no **Output format** (`--VCF/-v/--BCF/-g`) is selected, default format is PILEUP. If no format is selected and **Generate uncompressed VCF/BCF output** (`-u/--uncompressed`) is set to True, output format will be uncompressed BCF.\n\n###Performance Benchmarking\n\n**SAMtools Mpileup** is singlethreaded. **Benchmark of SAMtools Mpileup showed that the size of the output file and execution duration are not proportional to the size of the input file.** In the following table you can find some examples of **SAMtools Mpileup** running time and cost. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length | Output format | Duration | Cost | Instance (AWS) |\n|---------------|--------------|---------------|------------------|-------------------|--------------|--------|-------------|\n| BAM | 5.26 GB | 71.5M | 76 | PILEUP | 10min. | \\$0.09 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | PILEUP | 3h 24min. | \\$1.84 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | PILEUP | 32min. | \\$0.29 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150| PILEUP | 3h 16min. | \\$1.76 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | VCF.GZ | 45min. | \\$0.40 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | VCF.GZ | 7h 58min. | \\$4.30 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | VCF.GZ | 2h 16min. | \\$1.22 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150| VCF.GZ | 14h 26min. | \\$7.80 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Output mapping quality"}, {"name": "Output base positions on reads"}, {"name": "Comma-delimited list of platforms for INDELs"}, {"name": "Phred-scaled gap open sequencing error probability"}, {"name": "Minimum gapped reads for indel candidates"}, {"name": "Maximum per-file depth for INDEL calling"}, {"name": "Do not perform INDEL calling"}, {"name": "Coefficient for modeling homopolymer errors"}, {"name": "Minimum fraction of gapped reads"}, {"name": "Phred-scaled gap extension sequencing error probability"}, {"name": "Input BAM files", "encodingFormat": "application/x-sam"}, {"name": "Quality is in the Illumina-1.3+ encoding"}, {"name": "Do not skip anomalous read pairs"}, {"name": "Disable probabilistic realignment for the BAQ computation"}, {"name": "Adjust mapping quality"}, {"name": "Maximum per-file depth"}, {"name": "Recalculate BAQ on the fly"}, {"name": "Ignore RG tags"}, {"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Exclude read groups listed in file", "encodingFormat": "text/plain"}, {"name": "Generate uncompressed VCF/BCF output"}, {"name": "Region in which pileup is generated"}, {"name": "Region BED file or position list file", "encodingFormat": "text/plain"}, {"name": "Required flags"}, {"name": "Filter flags"}, {"name": "Disable read-pair overlap detection"}, {"name": "Comma-separated list of FORMAT and INFO tags to output"}, {"name": "Apply -m and -F thresholds per sample"}, {"name": "Output format"}, {"name": "Reference index file"}, {"name": "Output filename"}, {"name": "Output read names"}, {"name": "Output all positions"}, {"name": "Output absolutely all positions"}, {"name": "Index files"}, {"name": "Input file format option"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output PILEUP, VCF, or BCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/reheader-1-3/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/reheader-1-3/7", "applicationCategory": "CommandLineTool", "name": "SAMtools Reheader", "description": "SAMtools Reheader replaces the header in in.bam with the header in in.header.sam. This command is much faster than replacing the header with a BAM->SAM->BAM conversion.", "input": [{"name": "SAM input file", "encodingFormat": "application/x-sam"}, {"name": "Input BAM file", "encodingFormat": "application/x-bam"}, {"name": "No @pg header line"}, {"name": "Header edit in-place"}], "output": [{"name": "Output BAM file", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li/Sanger Institute,  Bob Handsaker/Broad Institute, James Bonfield/Sanger Institute,", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039728, "dateCreated": 1460640040, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-sort-1-6/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-sort-1-6/7", "applicationCategory": "CommandLineTool", "name": "SAMtools Sort", "description": "**SAMtools Sort** tool is used to sort alignments by leftmost coordinates, or by read name when `-n` is used. An appropriate @HD-SO sort order header tag will be added or an existing one updated if necessary. [1]\n\nThe sorted output is written to the file specified by the parameter **Output filename** (`-o`) or default filename (input_file.sorted.bam for input input_file.bam). \n\nThis command will also create temporary files tmpprefix.%d.bam as needed when the entire alignment data cannot fit into memory (as controlled via parameter **Memory per thread** (`-m`)). [1] \n\nOutput file format is set by the parameter **Output format** (`--output-fmt/-O`). If this parameter is not set, format is recognized from the filename defined by the parameter **Output filename** (`-o`). If the filename is not set or no format can be deduced, BAM format will be selected. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n####Ordering Rules\n\nThe following rules are used for ordering records. If the parameter **Sort by vaue of tag** (`-t`) is used, records are first sorted by the value of the given alignment tag, and then by position or name (if the parameter **Sort by read name** (`-n`) is set to True). For example, `-t RG` will make read group the primary sort key. The rules for ordering by tag are:  \n- Records that do not have the tag are sorted before ones that do.\n- If the types of the tags are different, they will be sorted so that single character tags (type A) come before array tags (type B), then string tags (types H and Z), then numeric tags (types f and i).\n- Numeric tags (types f and i) are compared by value. Note that comparisons of floating-point values are subject to issues of rounding and precision.\n- String tags (types H and Z) are compared based on the binary contents of the tag using the C strcmp function.\n- Character tags (type A) are compared by binary character value.\n- No attempt is made to compare tags of other types \u2014 notably type B array values will not be compared.\n\nWhen the parameter **Sort by read name** (`-n`) is set to True, records are sorted by name. Names are compared so as to give a \u201cnatural\u201d ordering \u2014 i.e. sections consisting of digits are compared numerically while all other sections are compared based on their binary representation. This means \u201ca1\u201d will come before \u201cb1\u201d and \u201ca9\u201d will come before \u201ca10\u201d. Records with the same name will be ordered according to the values of the READ1 and READ2 flags.  \nWhen the parameter **Sort by read name** (`-n`) is not set to True, reads are sorted by reference (according to the order of the @SQ header records), then by position in the reference, and then by the REVERSE flag. [1]\n\n###Common Use Cases\n\n**SAMtools Sort** is often used as preprocessing step for other tools that require coordinate or query name sorted BAM/CRAM/SAM file. Some of the tools that require coordinate sorted input are **SAMtools Index**, **SAMtools View** when using **Regions array** parameter, **SAMtools Markdup**. Some of the tools that require query name sorted input are **SAMtools Fixmate** and **SAMtools FASTQ** (not required but it is recommended for further processing of FASTQ files). \n\n###Changes Introduced by Seven Bridges\n\n- Parameter for temporary files prefix (`-T`) is omitted from the wrapper because it is not applicable on the platform.\n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- Parameter **Memory per thread** (`-m`) should be specified either in bytes or with a K, M, or G suffix. The number that precedes suffix should be an integer. To prevent **SAMtools Sort** from creating a huge number of temporary files, it enforces a minimum value of 1M for this setting.\n\n###Performance Benchmarking\n\nBy default, **SAMtools Sort** is single-threaded. Multithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). Parameter **Memory per thread** (`-m`) does not affect execution time but larger values of this parameter will affect instance type selection which can lead to longer execution and/or larger cost. Instance type also depends on parameter **Number of threads** (`--threads/-@`). Recommended values of these parameters for optimal cost are **Number of threads** = 8, **Memory per thread** = 1G (or None for default settings). \n\nIn the following table you can find estimates of **SAMtools Sort** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n   \n| Input type | Input size | # of reads | Read length | Sort by read name |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|---------------|------------------|---------------------------|------------------|-------------|--------|-------------|\n| BAM | 7.06 GB | 71.5M | 76 | False | 1 | 17min. | \\$0.11 | c4.2xlarge |\n| BAM | 16.21 GB | 161.2M | 101 | False | 1 | 44min. | \\$0.29 | c4.2xlarge |\n| BAM | 24.05 GB | 179M | 76 | False | 1 | 1h 16min. | \\$0.50 | c4.2xlarge |\n| BAM | 89.56 GB | 845.6M | 150 | False | 1 | 4h 47min. | \\$1.90 | c4.2xlarge |\n| BAM | 7.06 GB | 71.5M | 76 | False | 8 | 8min. | \\$0.05 | c4.2xlarge |\n| BAM | 16.21 GB | 161.2M | 101 | False | 8 | 16min. | \\$0.11 | c4.2xlarge |\n| BAM | 24.05 GB | 179M | 76 | False | 8 | 25min. | \\$0.17 | c4.2xlarge |\n| BAM | 89.56 GB | 845.6M | 150 | False | 8 | 1h 33min. | \\$0.62 | c4.2xlarge |\n| BAM | 7.06 GB | 71.5M | 76 | False | 16 | 4min. | \\$0.05 | c4.4xlarge |\n| BAM | 16.21 GB | 161.2M | 101 | False | 16 | 10min. | \\$0.13 | c4.4xlarge |\n| BAM | 24.05 GB | 179M | 76 | False | 16 | 17min. | \\$0.23 | c4.4xlarge |\n| BAM | 89.56 GB | 845.6M | 150 | False | 16 | 58min. | \\$0.77 | c4.4xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | True | 1 | 24min. | \\$0.16 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | True | 1 | 1h 1min. | \\$0.40 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | True | 1 | 1h 40min. | \\$0.66 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | True | 1 | 6h 16min. | \\$2.49 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | True | 8 | 9min. | \\$0.06 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | True | 8 | 19min. | \\$0.13 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | True | 8 | 28min. | \\$0.19 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | True | 8 | 2h 1min. | \\$0.80 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | True | 16 | 5min. | \\$0.07 | c4.4xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | True | 16 | 14min. | \\$0.19 | c4.4xlarge |\n| BAM | 18.36 GB | 179M | 76 | True | 16 | 19min. | \\$0.25 | c4.4xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | True | 16 | 1h 13min. | \\$0.97 | c4.4xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Number of threads"}, {"name": "Compression level"}, {"name": "Sort by read name"}, {"name": "Output format"}, {"name": "Memory per thread"}, {"name": "Sort by vaue of tag"}, {"name": "Output filename"}, {"name": "Input file format option"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Sorted file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039470, "dateCreated": 1521027917, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-sort-1-9-cwl1-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-sort-1-9-cwl1-0/8", "applicationCategory": "CommandLineTool", "name": "Samtools Sort CWL1.0", "description": "**SAMtools Sort** tool is used to sort alignments by leftmost coordinates, or by read name when `-n` is used. An appropriate @HD-SO sort order header tag will be added or an existing one updated if necessary [1].\n\nThe sorted output is written to the file specified by the parameter **Output filename** (`-o`) or default filename (in_alignments.sorted.bam for input in_alignments.bam). \n\nThis command will also create temporary files tmpprefix.%d.bam as needed when the entire alignment data cannot fit into memory (as controlled via parameter **Memory per thread** (`-m`)) [1]. \n\nOutput file format is set by the parameter **Output format** (`--output-fmt/-O`). If this parameter is not set, format is recognized from the filename defined by the **Output filename** (`-o`) parameter. If the filename is not set or no format can be deduced, BAM format will be selected. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n####Ordering Rules\n\nThe following rules are used for ordering records. If the parameter **Sort by value of tag** (`-t`) is used, records are first sorted by the value of the given alignment tag, and then by position or name (if the parameter **Sort by read name** (`-n`) is set to True). For example, `-t RG` will make read group the primary sort key. The rules for ordering by tag are:  \n- Records that do not have the tag are sorted before the ones that do.\n- If the types of the tags are different, they will be sorted so that single character tags (type A) come before array tags (type B), then string tags (types H and Z), then numeric tags (types f and i).\n- Numeric tags (types f and i) are compared by value. Note that comparisons of floating-point values are subject to issues of rounding and precision.\n- String tags (types H and Z) are compared based on the binary contents of the tag using the C strcmp function.\n- Character tags (type A) are compared by binary character value.\n- No attempt is made to compare tags of other types \u2014 notably type B array values will not be compared.\n\nWhen the parameter **Sort by read name** (`-n`) is set to True, records are sorted by name. Names are compared so as to give a \u201cnatural\u201d ordering \u2014 i.e. sections consisting of digits are compared numerically while all other sections are compared based on their binary representation. This means \u201ca1\u201d will come before \u201cb1\u201d and \u201ca9\u201d will come before \u201ca10\u201d. Records with the same name will be ordered according to the values of the READ1 and READ2 flags.  \nWhen the parameter **Sort by read name** (`-n`) is not set to True, reads are sorted by reference (according to the order of the @SQ header records), then by position in the reference, and then by the REVERSE flag [1].\n\n###Common Use Cases\n\n**SAMtools Sort** is often used as a preprocessing step for other tools that require coordinate or query name sorted BAM/CRAM/SAM file. Some of the tools that require coordinate-sorted input are **SAMtools Index**, **SAMtools View** when using the **Regions array** parameter, **SAMtools Markdup**. Some of the tools that require query name sorted input are **SAMtools Fixmate** and **SAMtools FASTQ** (not required but it is recommended for further processing of FASTQ files). \n\n###Changes Introduced by Seven Bridges\n\n- Parameter for temporary files prefix (`-T`) is omitted from the wrapper because it is not applicable on the platform.\n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- Parameter **Memory per thread** (`-m`) should be specified either in bytes or with a K, M, or G suffix. The number that precedes suffix should be an integer. To prevent **SAMtools Sort** from creating a huge number of temporary files, it enforces a minimum value of 1M for this setting.\n\n###Performance Benchmarking\n\nBy default, **SAMtools Sort** is single-threaded. Multithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). Parameter **Memory per thread** (`-m`) does not affect execution time but larger values of this parameter will affect instance type selection which can lead to longer execution and/or larger cost. Instance type also depends on parameter **Number of threads** (`--threads/-@`). Recommended values of these parameters for optimal cost are **Number of threads** = 8, **Memory per thread** = 1G (or None for default settings). \n\nIn the following table you can find estimates of **SAMtools Sort** running time and cost.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n   \n| Input type | Input size | # of reads | Read length | Sort by read name |  # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|---------------|------------------|---------------------------|------------------|-------------|--------|-------------|\n| BAM | 7.06 GB | 71.5M | 76 | False | 1 | 20min. | \\$0.18 | c4.2xlarge |\n| BAM | 16.21 GB | 161.2M | 101 | False | 1 | 48min. | \\$0.43 | c4.2xlarge |\n| BAM | 24.05 GB | 179M | 76 | False | 1 | 1h 24min. | \\$0.76 | c4.2xlarge |\n| BAM | 89.56 GB | 845.6M | 150 | False | 1 | 5h 23min. | \\$2.90 | c4.2xlarge |\n| BAM | 7.06 GB | 71.5M | 76 | False | 8 | 8min. | \\$0.07 | c4.2xlarge |\n| BAM | 16.21 GB | 161.2M | 101 | False | 8 | 16min. | \\$0.14 | c4.2xlarge |\n| BAM | 24.05 GB | 179M | 76 | False | 8 | 26min. | \\$0.23 | c4.2xlarge |\n| BAM | 89.56 GB | 845.6M | 150 | False | 8 | 1h 37min. | \\$0.87 | c4.2xlarge |\n| BAM | 7.06 GB | 71.5M | 76 | False | 16 | 5min. | \\$0.07 | c5.4xlarge |\n| BAM | 16.21 GB | 161.2M | 101 | False | 16 | 12min. | \\$0.17 | c5.4xlarge |\n| BAM | 24.05 GB | 179M | 76 | False | 16 | 16min. | \\$0.23 | c5.4xlarge |\n| BAM | 89.56 GB | 845.6M | 150 | False | 16 | 56min. | \\$0.8 | c5.4xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | True | 1 | 26min. | \\$0.23 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | True | 1 | 1h 8min. | \\$0.61 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | True | 1 | 1h 50min. | \\$0.99 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | True | 1 | 7h 11min. | \\$3.88 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | True | 8 | 10min. | \\$0.09 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | True | 8 | 21min. | \\$0.19 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | True | 8 | 31min. | \\$0.28 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | True | 8 | 2h 5min. | \\$1.12 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | True | 16 | 5min. | \\$0.07 | c5.4xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | True | 16 | 12min. | \\$0.17 | c5.4xlarge |\n| BAM | 18.36 GB | 179M | 76 | True | 16 | 16min. | \\$0.23 | c5.4xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | True | 16 | 56min. | \\$0.8 | c5.4xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Number of threads"}, {"name": "Compression level"}, {"name": "Sort by read name"}, {"name": "Output format"}, {"name": "Memory per thread"}, {"name": "Sort by value of tag"}, {"name": "Output filename"}, {"name": "Input file format option"}, {"name": "Output file format option"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Sorted file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-view-1-6/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-view-1-6/5", "applicationCategory": "CommandLineTool", "name": "SAMtools View", "description": "**SAMtools View** tool when used with no options or regions specified, prints all alignments in the specified input alignment file (in SAM, BAM, or CRAM format) to output file in SAM format (with no header). You may specify one or more space-separated region specifications to restrict output to only those alignments which overlap the specified region(s). Use of region specifications requires a coordinate-sorted and indexed input file (in BAM or CRAM format). [1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n####Regions\n\nRegions can be specified as: RNAME[:STARTPOS[-ENDPOS]] and all position coordinates are 1-based. \n\n**Important note:** when multiple regions are given, some alignments may be output multiple times if they overlap more than one of the specified regions.\n\nExamples of region specifications:\n\n- **chr1**  - Output all alignments mapped to the reference sequence named `chr1' (i.e. @SQ SN:chr1).\n\n- **chr2:1000000** - The region on chr2 beginning at base position 1,000,000 and ending at the end of the chromosome.\n\n- **chr3:1000-2000** - The 1001bp region on chr3 beginning at base position 1,000 and ending at base position 2,000 (including both end positions).\n\n- **'\\*'** - Output the unmapped reads at the end of the file. (This does not include any unmapped reads placed on a reference sequence alongside their mapped mates.)\n\n- **.** - Output all alignments. (Mostly unnecessary as not specifying a region at all has the same effect.) [1]\n\n###Common Use Cases\n\nThis tool can be used for: \n\n- Filtering BAM/SAM/CRAM files - options set by following parameters and input files: **Include reads with all of these flags** (`-f`), **Exclude reads with any of these flags** (`-F`), **Exclude reads with all of these flags** (`-G`), **Read group** (`-r`), **Minimum mapping quality** (`-q`), **Only include alignments in library** (`-l`), **Minimum number of CIGAR bases consuming query sequence** (`-m`), **Subsample fraction** (`-s`), **Read group list** (`-R`), **BED region file** (`-L`)\n- Format conversion between SAM/BAM/CRAM formats - set by following parameters: **Output format** (`--output-fmt/-O`), **Fast bam compression** (`-1`), **Output uncompressed BAM** (`-u`)\n- Modification of the data which is contained in each alignment - set by following parameters: **Collapse the backward CIGAR operation** (`-B`), **Read tags to strip** (`-x`)\n- Counting number of alignments in SAM/BAM/CRAM file - set by parameter **Output only count of matching records** (`-c`)\n\n###Changes Introduced by Seven Bridges\n\n- Parameters **Output BAM** (`-b`) and **Output CRAM** (`-C`) were excluded from wrapper since they are redundant with parameter **Output format** (`--output-fmt/-O`).\n- Parameter **Input format** (`-S`) was excluded from wrapper since it is ignored by the tool (input format is auto-detected).\n- Input file **Index file** was added to the wrapper to enable operations that require index file for BAM/CRAM files.\n- Parameter **Number of threads** (`--threads/-@`) specifies total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set number of additional threads.\n\n###Common Issues and Important Notes\n\n- When multiple regions are given, some alignments may be output multiple times if they overlap more than one of the specified regions. [1]\n- Use of region specifications requires a coordinate-sorted and indexed input file (in BAM or CRAM format). [1]\n- Option **Output uncompressed BAM** (`-u`) saves time spent on compression/decompression and is thus preferred when the output is piped to another SAMtools command. [1]\n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools View** running time and cost. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length | Output format | # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | 71.5M | 76 | BAM | 1 | 14min. | \\$0.09 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | BAM | 1 | 35min. | \\$0.23 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | BAM | 1 | 59min. | \\$0.39 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | BAM | 1 | 3h 17min. | \\$1.31 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | BAM | 8 | 7min. | \\$0.05 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | BAM | 8 | 12min. | \\$0.08 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | BAM | 8 | 18min. | \\$0.12 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | BAM | 8 | 55min. | \\$0.36 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | SAM | 8 | 12min. | \\$0.08 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | SAM | 8 | 18min. | \\$0.12 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | SAM | 8 | 26min. | \\$0.17 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | SAM | 8 | 1h 44min. | \\$0.69 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.6.html)", "input": [{"name": "Fast BAM compression"}, {"name": "Output uncompressed BAM"}, {"name": "Include the header in the output"}, {"name": "Output the header only"}, {"name": "Output only count of matching records"}, {"name": "List of reference names and lengths", "encodingFormat": "text/plain"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "BED region file", "encodingFormat": "text/x-bed"}, {"name": "Read group"}, {"name": "Read group list", "encodingFormat": "text/plain"}, {"name": "Minimum mapping quality"}, {"name": "Only include alignments in library"}, {"name": "Minimum number of CIGAR bases consuming query sequence"}, {"name": "Include reads with all of these flags"}, {"name": "Read tags to strip"}, {"name": "Collapse the backward CIGAR operation"}, {"name": "Subsample fraction"}, {"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Regions array"}, {"name": "Index file"}, {"name": "Number of threads"}, {"name": "Output format"}, {"name": "Exclude reads with any of these flags"}, {"name": "Exclude reads with all of these flags"}, {"name": "Input file format option"}, {"name": "Output file format option"}, {"name": "Output filename"}, {"name": "Filename for reads not selected by filters"}], "output": [{"name": "Output BAM, SAM, or CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Reads not selected by filters", "encodingFormat": "application/x-sam"}, {"name": "Alignment count", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648039469, "dateCreated": 1521027916, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/samtools-view-1-9-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/samtools-view-1-9-cwl1-0/7", "applicationCategory": "CommandLineTool", "name": "Samtools View CWL1.0", "description": "**SAMtools View** tool prints all alignments from a SAM, BAM, or CRAM file to an output file in SAM format (headerless). You may specify one or more space-separated region specifications to restrict output to only those alignments which overlap the specified region(s). Use of region specifications requires a coordinate-sorted and indexed input file (in BAM or CRAM format) [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n####Regions\n\nRegions can be specified as: RNAME[:STARTPOS[-ENDPOS]] and all position coordinates are 1-based. \n\n**Important note:** when multiple regions are given, some alignments may be output multiple times if they overlap more than one of the specified regions.\n\nExamples of region specifications:\n\n- **chr1**  - Output all alignments mapped to the reference sequence named `chr1' (i.e. @SQ SN:chr1).\n\n- **chr2:1000000** - The region on chr2 beginning at base position 1,000,000 and ending at the end of the chromosome.\n\n- **chr3:1000-2000** - The 1001bp region on chr3 beginning at base position 1,000 and ending at base position 2,000 (including both end positions).\n\n- **'\\*'** - Output the unmapped reads at the end of the file. (This does not include any unmapped reads placed on a reference sequence alongside their mapped mates.)\n\n- **.** - Output all alignments. (Mostly unnecessary as not specifying a region at all has the same effect.) [1]\n\n###Common Use Cases\n\nThis tool can be used for: \n\n- Filtering BAM/SAM/CRAM files - options set by the following parameters and input files: **Include reads with all of these flags** (`-f`), **Exclude reads with any of these flags** (`-F`), **Exclude reads with all of these flags** (`-G`), **Read group** (`-r`), **Minimum mapping quality** (`-q`), **Only include alignments in library** (`-l`), **Minimum number of CIGAR bases consuming query sequence** (`-m`), **Subsample fraction** (`-s`), **Read group list** (`-R`), **BED region file** (`-L`)\n- Format conversion between SAM/BAM/CRAM formats - set by the following parameters: **Output format** (`--output-fmt/-O`), **Fast bam compression** (`-1`), **Output uncompressed BAM** (`-u`)\n- Modification of the data which is contained in each alignment - set by the following parameters: **Collapse the backward CIGAR operation** (`-B`), **Read tags to strip** (`-x`)\n- Counting number of alignments in SAM/BAM/CRAM file - set by parameter **Output only count of matching records** (`-c`)\n\n###Changes Introduced by Seven Bridges\n\n- Parameters **Output BAM** (`-b`) and **Output CRAM** (`-C`) were excluded from the wrapper since they are redundant with parameter **Output format** (`--output-fmt/-O`).\n- Parameter **Input format** (`-S`) was excluded from wrapper since it is ignored by the tool (input format is auto-detected).\n- Input file **Index file** was added to the wrapper to enable operations that require an index file for BAM/CRAM files.\n- Parameter **Number of threads** (`--threads/-@`) specifies the total number of threads instead of additional threads. Command line argument (`--threads/-@`) will be reduced by 1 to set the number of additional threads.\n\n###Common Issues and Important Notes\n\n- When multiple regions are given, some alignments may be output multiple times if they overlap more than one of the specified regions [1].\n- Use of region specifications requires a coordinate-sorted and indexed input file (in BAM or CRAM format) [1].\n- Option **Output uncompressed BAM** (`-u`) saves time spent on compression/decompression and is thus preferred when the output is piped to another SAMtools command [1].\n\n###Performance Benchmarking\n\nMultithreading can be enabled by setting parameter **Number of threads** (`--threads/-@`). In the following table you can find estimates of **SAMtools View** running time and cost. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n| Input type | Input size | # of reads | Read length | Output format | # of threads | Duration | Cost | Instance (AWS)|\n|---------------|--------------|-----------------|---------------|------------------|-------------------|-----------------|-------------|--------|-------------|\n| BAM | 5.26 GB | 71.5M | 76 | BAM | 1 | 13min. | \\$0.12 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | BAM | 1 | 33min. | \\$0.30 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | BAM | 1 | 60min. | \\$0.54 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | BAM | 1 | 3h 25min. | \\$1.84 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | BAM | 8 | 5min. | \\$0.04 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | BAM | 8 | 11min. | \\$0.10 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | BAM | 8 | 19min. | \\$0.17 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | BAM | 8 | 61min. | \\$0.55 | c4.2xlarge |\n| BAM | 5.26 GB | 71.5M | 76 | SAM | 8 | 14min. | \\$0.13 | c4.2xlarge |\n| BAM | 11.86 GB | 161.2M | 101 | SAM | 8 | 23min. | \\$0.21 | c4.2xlarge |\n| BAM | 18.36 GB | 179M | 76 | SAM | 8 | 35min. | \\$0.31 | c4.2xlarge |\n| BAM | 58.61 GB | 845.6M | 150 | SAM | 8 | 2h 29min. | \\$1.34 | c4.2xlarge |\n\n###References\n\n[1] [SAMtools documentation](http://www.htslib.org/doc/samtools-1.9.html)", "input": [{"name": "Index file"}, {"name": "Output format"}, {"name": "Fast BAM compression"}, {"name": "Output uncompressed BAM"}, {"name": "Include the header in the output"}, {"name": "Output the header only"}, {"name": "Collapse the backward CIGAR operation"}, {"name": "Include reads with all of these flags"}, {"name": "Exclude reads with any of these flags"}, {"name": "Exclude reads with all of these flags"}, {"name": "Read group"}, {"name": "Minimum mapping quality"}, {"name": "Only include alignments in library"}, {"name": "Minimum number of CIGAR bases consuming query sequence"}, {"name": "Read tags to strip"}, {"name": "Output only count of matching records"}, {"name": "Input file format option"}, {"name": "Output file format option"}, {"name": "Subsample fraction"}, {"name": "Number of threads"}, {"name": "Filename for reads not selected by filters"}, {"name": "Output filename"}, {"name": "BED region file", "encodingFormat": "text/x-bed"}, {"name": "Read group list", "encodingFormat": "text/plain"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "List of reference names and lengths", "encodingFormat": "text/plain"}, {"name": "Input BAM/SAM/CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Regions array"}, {"name": "Use the multi-region iterator"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output BAM, SAM, or CRAM file", "encodingFormat": "application/x-sam"}, {"name": "Reads not selected by filters", "encodingFormat": "application/x-sam"}, {"name": "Alignment count", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/samtools", "https://github.com/samtools/samtools/wiki"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "Heng Li (Sanger Institute), Bob Handsaker (Broad Institute), Jue Ruan (Beijing Genome Institute), Colin Hercus, Petr Danecek", "softwareVersion": ["v1.0"], "dateModified": 1648038713, "dateCreated": 1576244126, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-ampliconqc-amplicon-coverage/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-ampliconqc-amplicon-coverage/1", "applicationCategory": "CommandLineTool", "name": "SBG AmpliconQC: Amplicon Coverage", "description": "AmpliconQC - Amplicon Coverage counts the number of reads that cover each target region (amplicon). It assigns each read to only one, closest target region that is at the same time not further than the specified threshold value. The result is saved in BED format.", "input": [{"name": "Bam", "encodingFormat": "application/x-bam"}, {"name": "Interval bed", "encodingFormat": "text/x-bed"}, {"name": "Max distance"}], "output": [{"name": "Results"}], "applicationSubCategory": ["SAM/BAM-Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270162, "dateCreated": 1476270162, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-ampliconqc-coverage-heatmap/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-ampliconqc-coverage-heatmap/1", "applicationCategory": "CommandLineTool", "name": "SBG AmpliconQC: Coverage Heatmap", "description": "AmpliconQC - Coverage Heatmap renders a heatmap plot of coverage per amplicon. The plot is based on Coverage Matrix input produced by AmpliconQC Merge Coverage BEDs tool.", "input": [{"name": "Data matrix"}, {"name": "Max depth"}, {"name": "Min depth"}], "output": [{"name": "Results"}], "applicationSubCategory": ["Plotting-and-Rendering"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270162, "dateCreated": 1471862397, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-ampliconqc-mark-failed-amplicons/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-ampliconqc-mark-failed-amplicons/1", "applicationCategory": "CommandLineTool", "name": "SBG AmpliconQC: Mark Failed Amplicons", "description": "AmpliconQC - Mark Failed Samples/Targets creates files that list failed samples, targets and sample-target pairs. This tool takes as input a file created by AmpliconQC Merge Coverage BEDs tool.", "input": [{"name": "Coverage matrix"}, {"name": "Read depth threshold"}, {"name": "Failed samples threshold"}, {"name": "Samples use percentage"}, {"name": "Failed targets threshold"}, {"name": "Targets use percentage"}], "output": [{"name": "Results"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Quality-Control"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270162, "dateCreated": 1476270162, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-ampliconqc-merge-coverage-beds/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-ampliconqc-merge-coverage-beds/1", "applicationCategory": "CommandLineTool", "name": "SBG AmpliconQC: Merge Coverage BEDs", "description": "AmpliconQC - Merge Coverage BEDs creates the coverage matrix file from a list of BED files containing per amplicon coverage. This file is a TSV text file with rows of samples and columns of read-depth coverage, one per target region (amplicon). This file is usually produced by AmpliconQC Amplicon Coverage tool and is required for AmpliconQC Mark Failed Samples/Targets and AmpliconQC Coverage Heatmap tools.", "input": [{"name": "Beds", "encodingFormat": "text/x-bed"}], "output": [{"name": "Results"}], "applicationSubCategory": ["BED-Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270162, "dateCreated": 1471862491, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-bedpe4oncofuse/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-bedpe4oncofuse/7", "applicationCategory": "CommandLineTool", "name": "SBG Bedpe4Oncofuse", "description": "SBG Bedpe4Oncofuse is a simple one-liner that prepares ChimeraScan BEDPE output for Oncofuse analysis. There are four pre-built libraries, corresponding to the tissue types that are supported by Oncofuse : EPI (epithelial origin), HEM (hematological origin), MES (mesenchymal origin) and AVG (average expression, if tissue source is unknown) and this parameter has to be set for Oncofuse to work properly with ChimeraScan resluts.", "input": [{"name": "Bedpe file"}, {"name": "tissue"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Nevena Ilic Raicevic, Seven Bridges Genomics, <nevena.ilic.raicevic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799474, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-chimerascan4circos/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-chimerascan4circos/2", "applicationCategory": "CommandLineTool", "name": "SBG ChimeraScan4Circos", "description": "SBG ChimeraScan4Circos is a tool that extracts fusion names and links for Circos from ChimeraScan BEDPE file.", "input": [{"name": "Bedpe file"}], "output": [{"name": "Circos links"}, {"name": "Circos names"}], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Vladan Arsenijevic, Seven Bridges Genomics, <vladan.arsenijevic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799178, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-compressor-1-0/19", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-compressor-1-0/19", "applicationCategory": "CommandLineTool", "name": "SBG Compressor", "description": "SBG Compressor performs the archiving(and/or compression) of the files provided on the input. The format of the output can be selected. \n\tSupported formats are:\n\t\t1. TAR\n\t\t2. TAR.GZ \n\t\t3. TAR.BZ2\n\t\t4. GZ\n\t\t5. BZ2\n\t\t6. ZIP\nFor formats TAR, TAR.GZ, TAR.BAZ2 and ZIP, a single archive will be created on the output. For formats GZ and BZ2, one archive per file will be created.\nThe tool takes all hard links files from current working dir and compresses them.", "input": [{"name": "Input files"}, {"name": "Output format"}, {"name": "Output name"}, {"name": "Number of processes"}], "output": [{"name": "Output archives", "encodingFormat": "application/zip"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Marko Petkovic, Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1543949493, "dateCreated": 1453798830, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-compressor-cwl1-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-compressor-cwl1-0/10", "applicationCategory": "CommandLineTool", "name": "SBG Compressor CWL1.0", "description": "Compress input files or folders.\n\n**SBG Compressor** performs archiving (and/or compression) of files and folders provided on the input. The format of the output can be selected. \n\tSupported formats are:\n- TAR\n- TAR.GZ \n- TAR.BZ2\n-  GZ\n- BZ2\n- ZIP\n\nFor TAR, TAR.GZ, TAR.BZ2 and ZIP formats, a single archive will be created on the output. For GZ and BZ2 formats, one archive per file will be created.\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n###Common use cases\n\nThis tool is used to create archives of files and folders. It can be used in workflows to compress and pass on containing files or folders.\n\n###Common Issues and Important Notes\n\nGZ and BZ2 formats are not applicable to folders and can only be used to compress files.  \n\nIf an already zipped file is passed on the input, along with another non-compressed file, and ZIP is selected as the output format, more than one file will be created on the output. It should be noted that if the zipped file is passed as the first file on the input the tool will throw an error.  \n\nIf two input files have the same **Sample ID** metadata field and the selected output format is TAR, TAR.GZ, TAR.BZ2 or ZIP, the output name of the compressed files will be the same as their **Sample ID**. However if they have different **Sample ID**'s and the output name is not chosen, the default value will be \"output_archives\". \n\nThe tool takes all hard linked files from the current working dir and compresses them.\n\n###Performance Benchmarking\n\n| Output Type \t| Input Size \t| Duration \t| Cost \t| Instance (AWS) \t|\n|-\t|-\t|-\t|-\t|-\t|\n| TAR.GZ \t| 150MB \t| 2min \t| $0.01 \t| c4.2xlarge \t|\n| TAR.GZ \t| 1GB \t| 2min \t| $0.01 \t| c4.2xlarge \t|\n| TAR.GZ \t| 25GB \t| 27min \t| $0.18 \t| c4.2xlarge \t|\n| TAR.GZ \t| 150GB \t| 2h,45min \t| $1.1 \t| c4.2xlarge \t|\n| TAR.BZ2 \t| 150MB \t| 1min \t| $0.01 \t| c4.2xlarge \t|\n| TAR.BZ2 \t| 1GB \t| 2min \t| $0.01 \t| c4.2xlarge \t|\n| TAR.BZ2 \t| 25GB \t| 46min \t| $0.30 \t| c4.2xlarge \t|\n| TAR.BZ2 \t| 150GB \t| 4h,57min \t| $2.01 \t| c4.2xlarge \t|\n| TAR \t| 150MB \t| 1min \t| $0.01 \t| c4.2xlarge \t|\n| TAR \t| 1GB \t| 2min \t| $0.01 \t| c4.2xlarge \t|\n| TAR \t| 25GB \t| 16min \t| $0.1 \t| c4.2xlarge \t|\n| TAR \t| 150GB \t| 1h,35min \t| $0.65 \t| c4.2xlarge \t|\n| ZIP \t| 150MB \t| 1min \t| $0.01 \t| c4.2xlarge \t|\n| ZIP \t| 1GB \t| 3min \t| $0.02 \t| c4.2xlarge \t|\n| ZIP \t| 25GB \t| 28min \t| $0.2 \t| c4.2xlarge \t|\n| ZIP \t| 150GB \t| 2h,42min \t| $1.1 \t| c4.2xlarge \t|\n| GZ \t| 150MB \t| 1min \t| $0.01 \t| c4.2xlarge \t|\n| GZ \t| 1GB \t| 2min \t| $0.01 \t| c4.2xlarge \t|\n| GZ \t| 25GB \t| 17min \t| $0.12 \t| c4.2xlarge \t|\n| GZ \t| 150GB \t| 1h,36min \t| $0.65 \t| c4.2xlarge \t|\n| BZ2 \t| 150MB \t| 1min \t| $0.01 \t| c4.2xlarge \t|\n| BZ2 \t| 1GB \t| 2min \t| $0.01 \t| c4.2xlarge \t|\n| BZ2 \t| 25GB \t| 25min \t| $0.17 \t| c4.2xlarge \t|\n| BZ2 \t| 150GB \t| 2h,36min \t| $1.05 \t| c4.2xlarge \t|\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Input files"}, {"name": "Output format"}, {"name": "Output name"}, {"name": "Number of processes"}, {"name": "Input folders"}], "output": [{"name": "Output archives", "encodingFormat": "application/zip"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Marko Petkovic, Seven Bridges Genomics", "softwareVersion": ["v1.0"], "dateModified": 1615275708, "dateCreated": 1591664120, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-concat-gvcfs/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-concat-gvcfs/1", "applicationCategory": "CommandLineTool", "name": "SBG Concat GVCFs", "description": "SBG Concat GVCFs concatenates GVCF files in the order that is being received by taking the header from the first file.", "input": [{"name": "Array of GVCFs"}], "output": [{"name": "Concat GVCF"}], "softwareRequirements": ["CreateFileRequirement", "ExpressionEngineRequirement"], "project": "SBG Public Data", "creator": "Vladimir Kovacevic / SBG", "softwareVersion": ["sbg:draft-2"], "dateModified": 1545924489, "dateCreated": 1501857800, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-convert-sra-bam-to-fastq/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-convert-sra-bam-to-fastq/8", "applicationCategory": "CommandLineTool", "name": "SBG convert SRA/BAM to FASTQ", "description": "**SBG convert SRA/BAM to FASTQ** converts SRA data and/or a SAM, BAM or CRAM file into FASTQ format.\n\nThis unified tool converts SRA/BAM formats to FASTQ, using **SRA-Fasterq Dump 2.10.8** [1] and **Biobambam2 Bam2Fastq 2.0.146** [2].\n\n**NOTE: SRA tool requires an *interactive configuration* since version 2.10.1 [3]. Running this tool on the platform triggers the configuration *automatically*!  Please find more information in the *'Changes introduced by Seven Bridges'* section.**\n\nThe **SRA fasterq-dump** tool converts SRA data into FASTQ format while using temporary files and multi-threading to speed up the extraction. With aligned data, NCBI uses Compression by Reference, which only stores the differences in base pairs between sequence data and the segment it aligns to [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n###Common Use Cases\n\n- **SBG convert SRA/BAM to FASTQ** accepts both **SRA/BAM/FASTQ file** and **SRA accession** as its input. If you wish to use just one of these inputs, leave the other port empty. \n- Output file name is first set by the **Output file name** parameter. In case this value is not provided, the output file name will be the same as the **Sample ID** metadata field, if the **Sample ID** metadata exists. Otherwise, output file name will be inferred from the input file name (or from the **SRA accession** port value).\n- If **Compress output value** is set to True, output file(s) will be in gzip format (fastq.gz).  \n- Output files will be the same as inputs, If FASTQ files are provided as input files.\n  \n\n\n### Common issues and important notes\n\n* Please don't use the **Output file name** parameter when you have both SRA file and SRA accession, because it will not affect the output file names. Otherwise, if you have both  SAM/BAM/CRAM file and SRA accession, or any single input, this option will give correct output file names. \n\n* Output FASTQ files obtained from **SRA accession**  will have **Sample ID** and **Paired end** metadata set. **Sample ID** metadata field will be set according to the SRA accession, while **Paired end** metadata is set to 1 or 2 if and only if \"_1\" and \"_2\" exist in the dumped file's filename. Otherwise, output FASTQ files obtained from the file on the **Input SRA/BAM file** port  will inherit all metadata fields from the input file, while the **Sample ID** metadata field (if it's not defined ) and **Paired end** metadata will be set according to output file's name.\n\n\n* Reference must be provided if the input file used is in CRAM format without an embedded reference.\n\n\n\n###Changes introduced by Seven Bridges\n- About the **SRA-Fasterq Dump** tool:  In order to access even the public data, the tool needs to be configured [3]. The authors have provided an interactive solution to this problem, but since this solution is not perfect for environments such as the Seven Bridges Platform and many others, further solutions are being discussed [4][5][6]. The current solution on the Platform entails that the blank configuration file containing just the UUID (universally unique identifier) is created inside a specific folder in the root directory. This approach is taken from the authors' [Dockerfile](https://github.com/ncbi/sra-tools/blob/master/build/docker/Dockerfile).\n\n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, and different input file types (SRA or BAM), executed on the on-demand AWS cloud instances:\n\n\n\n| Input type |  Input size | Paired-end | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     SRA     |  32.1 MB |     No     |   2min   | $0.01| c4.2xlarge |\n|     BAM    |  2.5 GB |     No    |   4min   | $0.02| c4.2xlarge |\n|     BAM     |  11.9 GB |     Yes    |   11min   | $0.31| c5.9xlarge |\n|     BAM    | 159.1 GB |     Yes    |   2h11min  | $3.64 | c5.9xlarge |\n|     \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [NCBI SRA Toolkit documentation](https://ncbi.github.io/sra-tools/)\n\n[2] [BioBamBam2 GitLab Page](https://gitlab.com/german.tischler/biobambam2)\n\n[3] [Instructions for SRA toolkit installation and configuration](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration)\n\n[4] [SRA Github issue #282](https://github.com/ncbi/sra-tools/issues/282)\n\n[5] [SRA Github issue #291](https://github.com/ncbi/sra-tools/issues/291)\n\n[6] [SRA Github issue #310](https://github.com/ncbi/sra-tools/issues/310)", "input": [{"name": "Accession (SRA accession)"}, {"name": "Output file name"}, {"name": "CRAM reference", "encodingFormat": "application/x-fasta"}, {"name": "Input SRA/BAM/FASTQ file", "encodingFormat": "text/fastq"}, {"name": "Compress output value"}, {"name": "Memory per job"}, {"name": "CPUs per job"}], "output": [{"name": "Out reads", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["Utilities", "FASTQ Processing", "SAM/BAM Processing", "SRA"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["v1.1"], "dateModified": 1649156263, "dateCreated": 1618326851, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-coverage/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-coverage/2", "applicationCategory": "CommandLineTool", "name": "SBG Coverage", "description": "SBG Coverage calculates the depth and breadth of coverage of exons defined in a BED file versus reads defined in a BAM file. It uses BEDTools 2.22.1 and exports the summary file.", "input": [{"name": "Input BAM or BED"}, {"name": "Input BED file"}, {"name": "Strandness"}, {"name": "Depth per base"}, {"name": "Counts"}, {"name": "Split"}, {"name": "Input is a BAM file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Utilities", "Quality Control"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453799488, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-create-rsem-tpm-counts-matrix/40", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-create-rsem-tpm-counts-matrix/40", "applicationCategory": "CommandLineTool", "name": "SBG Create Expression Matrix", "description": "**SBG Create Expression Matrix** is a tool which creates aggregated matrices from various types of inputs. \nA typical example would be creating expression matrices from abundance estimates produced by tools like **RSEM**, **Salmon**, or **Kallisto**. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* The main input for this tool would be results of a certain analysis, produced for multiple samples for which the aggregation is to be performed. Since the most common use case is aggregating expression data, the text that follows will always refer to this specific example. \n\n* After providing the expression results for multiple samples to the **Abundance estimates** input, the tool will generate expression matrices based on the **Column name** parameter that the user specifies. \nFor example, **Salmon's** results come in a file which shows expressions for all genes use din the experiment, and multiple columns are available in that file, including the TPM column, and the NumReads column. If the **Column name** is specified to be TPM, the resulting expression matrix will contain TPM values for all samples found in the **Abundance estimates** input. The default value for the **Column name**  is *tpm*. \n\n* The output matrix will always have an extension named after the column for which the aggregation is performed (i.e. *expression_matrix.tpm* or *expression_matrix.fpkm*). A TSV suffix can be appended via the **Add TSV suffix** option. \n\n### Common Issues and Important Notes\n* If aggregating **STAR** count data, the **STAR input files** boolean option needs to be set to True, due to the specific nature of the STAR count file format. After setting this option to True, you can choose to aggregate over *unstranded*, *1st_strand*, or *2nd_strand* columns, via the **Column name** option. \n* Sometimes, if the number of input files used for aggregation is too large, Linux command line length issues might occur. In order to avoid this, just set the **Read inputs from file** option to True (which will essentially write all the input file paths to a file, and then read from that file instead of reading from the command line). \n\n### Changes Introduced by Seven Bridges\n* None\n\n### Performance Benchmarking\n**SBG Expression Matrix**  is a simple table manipulation tool, and hence the execution time mostly negligible compared to other analyses, where the tool is expected to finish in under 5 minutes (even with a big number of samples), costing less than $0.05", "input": [{"name": "Abundance estimates"}, {"name": "Output file name"}, {"name": "Column name"}, {"name": "STAR input files"}, {"name": "Feature name"}, {"name": "Add TSV suffix"}, {"name": "Read inputs from file"}], "output": [{"name": "Expression matrix"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Utilities", "SBGTools", "Other"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1618328870, "dateCreated": 1618327393, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-create-rsem-tpm-counts-matrix-cwl1/30", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-create-rsem-tpm-counts-matrix-cwl1/30", "applicationCategory": "CommandLineTool", "name": "SBG Create Expression Matrix CWL1.1", "description": "**SBG Create Expression Matrix** is a tool which creates aggregated matrices from various types of inputs. \nA typical example would be creating expression matrices from abundance estimates produced by tools like **RSEM**, **Salmon**, or **Kallisto**. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n* The main input for this tool would be results of a certain analysis, produced for multiple samples for which the aggregation is to be performed. Since the most common use case is aggregating expression data, the text that follows will always refer to this specific example. \n\n* After providing the expression results for multiple samples to the **Abundance estimates** input, the tool will generate expression matrices based on the **Column name** parameter that the user specifies. \nFor example, **Salmon's** results come in a file which shows expressions for all genes use din the experiment, and multiple columns are available in that file, including the TPM column, and the NumReads column. If the **Column name** is specified to be TPM, the resulting expression matrix will contain TPM values for all samples found in the **Abundance estimates** input. The default value for the **Column name**  is *tpm*. \n\n* The output matrix will always have an extension named after the column for which the aggregation is performed (i.e. *expression_matrix.tpm* or *expression_matrix.fpkm*). A TSV suffix can be appended via the **Add TSV suffix** option. \n\n### Common Issues and Important Notes\n* If aggregating **STAR** count data, the **STAR input files** boolean option needs to be set to True, due to the specific nature of the STAR count file format. After setting this option to True, you can choose to aggregate over *unstranded*, *1st_strand*, or *2nd_strand* columns, via the **Column name** option. \n* Sometimes, if the number of input files used for aggregation is too large, Linux command line length issues might occur. In order to avoid this, just set the **Read inputs from file** option to True (which will essentially write all the input file paths to a file, and then read from that file instead of reading from the command line). \n\n### Changes Introduced by Seven Bridges\n* None\n\n### Performance Benchmarking\n**SBG Expression Matrix**  is a simple table manipulation tool, and hence the execution time mostly negligible compared to other analyses, where the tool is expected to finish in under 5 minutes (even with a big number of samples), costing less than $0.05", "input": [{"name": "Column name"}, {"name": "Output file name"}, {"name": "Abundance estimates"}, {"name": "STAR input files"}, {"name": "Feature name."}, {"name": "Add TSV suffix"}, {"name": "Read inputs from file"}], "output": [{"name": "Expression matrix"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["Utilities", "SBGTools", "Other"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["v1.1"], "dateModified": 1618328870, "dateCreated": 1618327392, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-decompressor-1-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-decompressor-1-0/8", "applicationCategory": "CommandLineTool", "name": "SBG Decompressor", "description": "SBG Decompressor performs the extraction of the input archive file. \nSupported formats are:\n1. TAR\n2. TAR.GZ (TGZ)\n3. TAR.BZ2 (TBZ2)\n4. GZ\n5. BZ2\n6. ZIP\n\nIf the archive contains folder structure, it is going to be flatten because CWL doesn't support folders at the moment. In that case the output would contain all the files from all the folders from the archive.", "input": [{"name": "Input archive file", "encodingFormat": "application/zip"}], "output": [{"name": "Output files"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["SBGTools", "Utilities", "File Format Conversion"], "project": "SBG Public Data", "creator": "Marko Petkovic, Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035758, "dateCreated": 1453799111, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-decompressor-cwl1-0/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-decompressor-cwl1-0/15", "applicationCategory": "CommandLineTool", "name": "SBG Decompressor CWL1.0", "description": "The **SBG Decompressor** performs extraction of files from an input archive file. \n\nThe supported formats are:\n1. TAR\n2. TAR.GZ (TGZ)\n3. TAR.BZ2 (TBZ2)\n4. GZ\n5. BZ2\n6. ZIP\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n\n###Common use cases\n\nThis tool can be used to extract necessary files from input archives, or in workflows to uncompress and pass on contained files. \n\nThe two modes of work include outputting archive contents with preserved folder structure, and outputting extracted files as a list.\n\n* Select the mode by setting the parameter **Flatten outputs**. Setting the parameter to **True** extracts all files from the archive and outputs them to a list. \n* To preserve the folder structure of the archive, set the **Flatten outputs** parameter to **False** (default is **True**).\n\n###Common Issues and Important Notes\n\nThis tool cannot extract archives of different types than those noted above.\n\n###Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for different file sizes:\n\n| Input Archive Type | Input Archive Size | Duration | Cost   | Instance (AWS) |\n|--------------------|--------------------|----------|--------|----------------|\n| TAR.GZ             | 100MB              | 2min     | $0.006 | c4.2xlarge     |\n| TAR.GZ             | 1GB                | 8min     | $0.05  | c4.2xlarge     |\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Input Archive File", "encodingFormat": "application/zip"}, {"name": "Flatten Outputs"}], "output": [{"name": "Output Files"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["Utilities", "SBGTools", "File Format Conversion"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["v1.0"], "dateModified": 1648035758, "dateCreated": 1584385297, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-deep-learning-prediction-1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-deep-learning-prediction-1-0/6", "applicationCategory": "CommandLineTool", "name": "SBG Deep Learning Prediction", "description": "**SBG Deep Learning Prediction** is an image classifier tool that classifies unlabeled images based on labeled data. It relies on the transfer learning approach to compose and train the model.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n### Common Use Cases\n\n**SBG Deep Learning Prediction** is intended as a final step after the **SBG Deep Learning Image Classification Exploratory Workflow.** Testing different configurations in parallel with the exploratory workflow and finding the best model configuration for the given dataset, then using **SBG Deep Learning Prediction** with that configuration and all available labeled images as the training data provides the optimal training conditions which lead to the best classification results.\n\n**Labeled images** are one of the two required inputs of the tool. One or multiple directories containing labeled data should be provided - usually train, test and validation directories previously used in the **SBG Deep Learning Image Classification Exploratory Workflow**. This way, the training process is performed on the largest labeled  dataset available. Another required input is **Unlabeled images**. All unlabeled images should be in a single input directory and provided on this input.\n\nHyperparameter is a parameter whose value is used to control the learning process. The user can provide a **Configuration string**, which is a combination of the hyperparameter values of the best tested model. This string can easily be copied from the first column of the HTML report or CSV table which are available as outputs of the **SBG Deep Learning Image Classification Exploratory Workflow** task. Additionally, separate hyperparameter values can also be set, the same as in **SBG Deep Learning Image Classification Exploratory Workflow**, except that only a single configuration can be trained.\n\n**SBG Deep Learning Prediction** builds the model according to the set configuration and trains it using all available labeled data from one or more directories provided on the **Labeled images** input. Finally, the trained model is used to predict classes (labels) for the provided unlabeled images. The tool generates a prediction table in CSV format on its output. For each image from the **Unlabeled images** input directory, the prediction table contains its name, predicted label(s) and corresponding prediction probability.\n\n\n\n### Common Issues and Important Notes\n* **SBG Deep Learning Prediction** supports multiple classification types - binary, multi-class and multi-class multi-label classification. The classification type is determined automatically.\n* Supported image formats are PNG, JPG/JPEG, SVS, DCIM, DICOM and TIFF.\n* Directories containing labeled images should be organized in one of two ways:\n    * Under each of the dataset directories, there can be subdirectories, one for each class where the actual images are placed.\n    * All the images are under the main directory, together with a single CSV file that has only two columns, where the first column has all the image names, and the second column contains corresponding classes.\n* **SBG Deep Learning Prediction** relies on AWS GPU instances, available for projects located on AWS. To use this tool, please run it from an AWS project.\n\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size | Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|---|\n|1692| 500 KiB |1h 20m 55s|$1.19|p2.xlarge|\n|650| 20 MiB |1h 30m|$1.32|p2.xlarge|\n|5856|200 KiB|2h 47m 6s|$2.43|p2.xlarge|\n|650| 90 MiB |7h 59m 56s|$17.52|p2.xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Unlabeled images"}, {"name": "Resolution for conversion"}, {"name": "Labeled images"}, {"name": "Batch size"}, {"name": "Image size"}, {"name": "Pre-trained model"}, {"name": "Dense layers"}, {"name": "Dropout layer"}, {"name": "Learning rate"}, {"name": "Epochs - initial"}, {"name": "Epochs - additional"}, {"name": "Configuration string"}, {"name": "Output prefix"}], "output": [{"name": "Output prediction table"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["Imaging", "SBGTools", "Machine Learning"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648468627, "dateCreated": 1628085203, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fasta-indices/29", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fasta-indices/29", "applicationCategory": "CommandLineTool", "name": "SBG FASTA Indices", "description": "Create indices for FASTA file.\n\n###**Overview**  \n\nTool allows creating FASTA dictionary and index simultaneously which is necessary for running GATK tools. This version of tool for indexing uses SAMtools faidx command (toolkit version 1.9), while for the FASTA dictionary is used CreateFastaDictionary (GATK toolkit version 4.1.0.0).\n\n\n###**Inputs**  \n\n- FASTA file \n\n###**Output**  \n\n- FASTA Reference file\n- FASTA Index file\n- FASTA Dictionary file\n\n\n###**Changes made by Seven Bridges**\n\nCreateFastaDictionary function creates a DICT file describing the contents of the FASTA file. Parameter -UR was added to the command line that sets the UR field to just the Reference file name, instead of the whole path to file. This allows Memoisation feature of the platform to work.", "input": [{"name": "FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Memory per job"}], "output": [{"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "FASTA Index"}, {"name": "FASTA Dictionary"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["SBGTools", "Indexing"], "project": "SBG Public Data", "creator": "Sanja Mijalkovic, Seven Bridges Genomics, <sanja.mijalkovic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035758, "dateCreated": 1453799686, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastqc-beautifier/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastqc-beautifier/2", "applicationCategory": "CommandLineTool", "name": "SBG FastQC Beautifier", "description": "FastQC Beautifier is a simple re-packer for FastQC reports. Adds additional files in order for a nicer HTML render to be produced. The resulting archive will contain repack.html.", "input": [{"name": "Report"}], "output": [{"name": "New report"}], "applicationSubCategory": ["Plotting"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453798969, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastqc-extract/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastqc-extract/2", "applicationCategory": "CommandLineTool", "name": "SBG FastQC Extract", "description": "Extract FASTQC BQ extracts the base quality by read position values from multiple files and places them into a single tab-separated file.", "input": [{"name": "Zip"}], "output": [{"name": "Base qualities"}], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1453799244, "dateCreated": 1453799241, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastq-merge/20", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastq-merge/20", "applicationCategory": "CommandLineTool", "name": "SBG FASTQ Merge", "description": "SBG FASTQ Merge is a tool that concatenates FASTQ files based on chosen grouping parameter and file metadata: Sample ID, Library ID, Platform unit ID, File segment number and Paired End information.\n\nMetadata fields that are uniquely defining one FASTQ pair are Sample ID, Library ID, Platform unit ID and File segment number. Listed order of metadata fields is also representing their hierarchy in the metadata structure. Not all of these four metadata fields are required, but the present set has to be sufficient to create unique combinations for each pair of FASTQ files.", "input": [{"name": "Fastq", "encodingFormat": "text/fastq"}, {"name": "Group by level"}, {"name": "Compress output"}, {"name": "CPU"}, {"name": "Memory"}], "output": [{"name": "Merged", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["FASTQ Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799541, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastq-merge-cwl1-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastq-merge-cwl1-0/10", "applicationCategory": "CommandLineTool", "name": "SBG FASTQ Merge CWL1.0", "description": "**SBG FASTQ Merge** is a tool that concatenates FASTQ files based on their matching metadata fields. Metadata fields that are considered are: Sample ID, Library ID, Platform unit ID, File segment number and Paired End information. Priority of the merge is defined by a grouping parameter that can be assigned one of three values that represent the hierarchy in the metadata structure:\n- Sample - all files from the same sample will be merged into one file for each end for paired-end reads;\n- Library - all files from the same sample and library will be merged into one file for each end for paired-end reads;\n- Platform unit - all files from the same sample, library and platform unit will be merged into one file for each end for paired-end reads.\n\nIf no value is set, the grouping parameter will assume the value \u2018Sample\u2019.\n\nNot all of these metadata fields are required, but the present set has to be sufficient to create unique combinations for each pair of FASTQ files.\n\nThe order in which the input files are appended is determined by the File segment number (if it exists), or by alphabetical order of the input file names.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n\n\n###Common use cases\n\nThis tool is used to merge FASTQ files with matching metadata fields.\n\n[![code2flow-v5.png](https://i.postimg.cc/1XRjXLzR/code2flow-v5.png)](https://postimg.cc/KkVNspLC)\n\n It can also be used in workflows to reassemble previously split files, based on File segment number.\n\n\n###Common Issues and Important Notes\n\nSupported input formats are:\n- FASTQ\n- FASTQ.GZ\n- FQ\n- FQ.GZ\n\nIn order to be merged, all input files have to be either zipped or uncompressed. \n\nThis tool works successfully for single end, as well as the interleaved files.\n\nFiles with no metadata can be merged as long as all the files in question don\u2019t have filled in  metadata fields.  \n\n\n###Performance Benchmarking\n\n| Input\t| No. of input files |Size per file (uncompressed)\t|Total size| Duration \t| Cost \t| \n|-\t|-\t|-\t|-\t|-\t|-\t|\n| FASTQ\t| 2\t| 500MB \t|1000MB\t|2m 28s\t| $0.02\t| \n| FASTQ.GZ(156MB) \t| 2\t| 500MB \t|1000MB\t|2m 3s\t| $0.02\t|\n| FASTQ\t| 2\t| 10GB \t|20GB\t|17m 22s\t| $0.15\t|\n| FASTQ.GZ (2.7GB) \t|2\t| 10GB \t|20GB\t|11m 39s\t| $0.10\t|\n| FASTQ \t| 2\t| 50GB \t|100GB\t|1h 14m 21s\t| $0.66\t|\n| FASTQ.GZ (12.5GB) \t| 2\t| 50GB \t|100GB\t|52m 18s\t| $0.47\t|\n\n*All given values are for the c4.2xlarge instance.*\n\n*Cost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*", "input": [{"name": "Input FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Group by level"}, {"name": "Compress output"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "Merged files", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["FASTQ Processing"], "project": "SBG Public Data", "softwareVersion": ["v1.0"], "dateModified": 1649156263, "dateCreated": 1612270384, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastq-quality-adjuster/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastq-quality-adjuster/16", "applicationCategory": "CommandLineTool", "name": "SBG FASTQ Quality Adjuster", "description": "This app detects quality score format used in input FASTQ file. FASTQ quality score is then converted to standard Sanger quality score if conversion is required. It is basically a compact merged version of \"SBG Fastq Quality Detector\" and \"SBG Fastq Quality Converter\", created to speed up the execution of pipelines. Supported source formats are: Solexa, Illumina 1.3, Illumina 1.5 and Illumina 1.8.", "input": [{"name": "Fastq file", "encodingFormat": "text/fastq"}, {"name": "Total memory [GB]"}, {"name": "Used quality scale"}], "output": [{"name": "Result", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["FASTQ Processing", "File Format Conversion"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1471539420, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastq-quality-converter/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastq-quality-converter/4", "applicationCategory": "CommandLineTool", "name": "SBG FASTQ Quality Converter", "description": "This app converts FASTQ quality scores from a given, source format, to standard Sanger quality scores. Supported source formats are: Solexa, Illumina 1.3, Illumina 1.5 and Illumina 1.8.", "input": [{"name": "Fastq"}, {"name": "Source"}], "output": [{"name": "Result", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["File Format Conversion", "FASTQ Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241352, "dateCreated": 1453799688, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastq-quality-detector/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastq-quality-detector/17", "applicationCategory": "CommandLineTool", "name": "SBG FASTQ Quality Detector", "description": "FASTQ Quality Scale Detector detects which quality encoding scheme was used in your reads and automatically enters the proper value in the \"Quality Scale\" metadata field.", "input": [{"name": "Fastq", "encodingFormat": "text/fastq"}], "output": [{"name": "Result", "encodingFormat": "text/fastq"}, {"name": "Detected Quality Scale"}], "softwareRequirements": ["CreateFileRequirement"], "applicationSubCategory": ["FASTQ Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799955, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-fastq-split/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-fastq-split/8", "applicationCategory": "CommandLineTool", "name": "SBG FASTQ Split", "description": "FASTQ Splitter is a tool that can split FASTQ files into smaller chunks. Chunks can be defined either by number of new files or by number of reads per new file. If both files and reads parameters are set, fastq will be split based on selected number of files. Tool can process either one or two fastq input files. if two files are specified (eg. both paired-ends), tool will group outputs based on file segment metadata field (eg. every nth chunk from paired-end 1 is grouped with nth chunk from paired-end 2).", "input": [{"name": "Fastq", "encodingFormat": "text/fastq"}, {"name": "Split by"}, {"name": "Threads"}, {"name": "CPU"}, {"name": "Memory"}], "output": [{"name": "Results", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["SBGTools", "FASTQ Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648035758, "dateCreated": 1453799776, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-filterfasta/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-filterfasta/2", "applicationCategory": "CommandLineTool", "name": "SBG FilterFasta", "description": "SBG-FilterFasta is a tool that filters your reference FASTA by chromosomes/contigs and gathers them in a gzipped tar file.", "input": [{"name": "List of custom fasta files"}, {"name": "Output fastas"}, {"name": "List of pre-defined fasta files"}, {"name": "Fasta file."}, {"name": "Reference genome build"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["FASTA Processing"], "project": "SBG Public Data", "creator": "Vladan Arsenijevic, Seven Bridges Genomics, <vladan.arsenijevic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799225, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-flatten/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-flatten/4", "applicationCategory": "CommandLineTool", "name": "SBG Flatten", "description": "SBG Flatten is initially wrapped for usage together with SBG Group Input app. It is now updated to support any number of nested lists as well as any number of inputs (connections from the previous apps). All connections of interest should be passed to the \"Nested\" SBG Flatten input in order to produce the output as one, flattened list of files.\n\n***Example***:\n\n`output_app1` - [file1, file2]\n\n`output_app2` - [[file3, null], [null, file4]] - which can happen in some cases if the app was scattered\n\n\n`nested` - [[file1, file2], [[file3, null], [null, file4]]]\n\n`flatten` - [file1, file2, file3, file4]", "input": [{"name": "Nested"}], "output": [{"name": "All grouped files"}], "softwareRequirements": ["CreateFileRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Ana Damljanovic/ Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1545924489, "dateCreated": 1453799658, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-flattenlists/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-flattenlists/7", "applicationCategory": "CommandLineTool", "name": "SBG FlattenLists", "description": "SBG FlattenLists is used as a proprietary tool for combining different types of File and arrays of File into a single array of File type.\n\nThe **SBG FlattenLists** combines all the inputs passed on to its #input_list port and flattens all the array type inputs to a single array on the output. \n\n### Common Use Cases\n\n**SBG FlattenLists** is mainly used inside workflows in order to format the type into an array type, allowing tools expecting array of File on the input to process the input, or tools that expect a File type to be scattered across this input.\n\nExample usage:\n- A single File type will be converted to an array of a single File.\n- One File type input and one array of File type input will combine into one array of File containing all of the elements of the array and the single File appended to it.\n- Two array of File types will convert into a single array containing elements of both.\n- Any multi-array type (array of array, which can sometimes be produced by scatter) will be processed and converted into a single array.\n\n### Common Issues and Important Notes\n\n* When used inside a workflow, keep in mind that linking outputs from tools or inner workflows on the same input as an input node might lead to the input node's contents not being passed on to the tool. For this reason, it is advisable to process the input node links with an additional **SBG FlattenLists** tool.\n\n### Performance \n\n**SBG FlattenLists** spends the most of the execution on download and upload of files. The flatten functionality is done through JavaScript expressions in OutputEval of the CWL, which is evaluated even before the tool execution.", "input": [{"name": "Input list of files and lists"}], "output": [{"name": "Output list of files"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1545924489, "dateCreated": 1459789092, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-gc-content/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-gc-content/3", "applicationCategory": "CommandLineTool", "name": "SBG GC Content", "description": "This tool computes GC percentages for amplicon sequences. It takes a reference file in FASTA format and a BED file with amplicon regions on input and outputs a BED file with GC percentages.", "input": [{"name": "Fasta"}, {"name": "Bed"}], "output": [{"name": "Result", "encodingFormat": "text/x-bed"}], "applicationSubCategory": ["Analysis"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1453799656, "dateCreated": 1453799652, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-gc-coverage/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-gc-coverage/3", "applicationCategory": "CommandLineTool", "name": "SBG GC Coverage", "description": "GC Coverage takes a TSV file with coverage per amplicon per sample, and a BED file with GC content per amplicon on input, and outputs a file with GC percentages and coverage.", "input": [{"name": "TSV"}, {"name": "Bed"}], "output": [{"name": "Coverage"}], "applicationSubCategory": ["Analysis"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1453799038, "dateCreated": 1453799031, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-genome-coverage/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-genome-coverage/7", "applicationCategory": "CommandLineTool", "name": "SBG Genome Coverage", "description": "SBG Genome Coverage extends BEDTools Genome Coverage. The Genome Coverage calculates histograms, per-base reports and BedGraph summaries of feature coverage (aligned sequences for example) for a given genome. This extended version additionally extracts and creates a text file containing summary coverage stats.\n\nNote: This tool should be used for genome data only.", "input": [{"name": "Bam", "encodingFormat": "application/x-bam"}, {"name": "Fasta", "encodingFormat": "application/x-fasta"}, {"name": "Format"}, {"name": "Report Z"}, {"name": "GZipped"}, {"name": "Split"}, {"name": "Strand"}, {"name": "Coverage interval"}, {"name": "Scale"}, {"name": "Additional track"}, {"name": "UCSC track line"}], "output": [{"name": "Per interval"}, {"name": "Summary"}, {"name": "Bed graph"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Utilities", "WGS", "Quality Control"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037019, "dateCreated": 1453799748, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-group-input/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-group-input/7", "applicationCategory": "CommandLineTool", "name": "SBG Group Input", "description": "SBG Group Input accepts list of files that need to be grouped and sets the metadata field that indicates these files belong to the same group. This app should be included in the pipeline once for each group and used together with SBG Flatten. Each SBG Group Input output should be passed to SBG Flatten \"Nested\" input.", "input": [{"name": "Input files"}, {"name": "Group name"}], "output": [{"name": "Grouped Files"}], "softwareRequirements": ["CreateFileRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Ana Damljanovic/ Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1512559198, "dateCreated": 1453798988, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-histology-whole-slide-image-preprocessing-1-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-histology-whole-slide-image-preprocessing-1-0/5", "applicationCategory": "CommandLineTool", "name": "SBG Histology Whole Slide Image Preprocessing", "description": "**SBG Histology Whole Slide Image Preprocessing** obtains parts of the images that consist of at least 90% tissue. \n\nIt takes SVS histopathology images as its input, removes various artifacts, and outputs the desired number of best quality tiles in PNG format that consist of at least 90% tissue.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n\n### Common Use Cases\n\nTissue identification in whole slide images can be an important precursor to deep learning. Deep learning is computationally expensive and medical whole slide images are enormous. Typically, a large portion of a slide isn\u2019t useful, such as the background, shadows, water, smudges, and pen marks, which are removed by this tool. \n\n**SBG Histology Whole Slide Image Preprocessing** steps include: \n1. scaling down whole slide images, \n2. applying filters to these scaled-down images for tissue segmentation, \n3. breaking the slides into tiles, scoring the tiles, \n4. and finally, retrieving the top tiles based on their scores.\n\nThese image modifications can lead to faster, more accurate model training.\n\n### Common Issues and Important Notes\n- **SBG Histology Whole Slide Image Preprocessing** is written in Python and based on IBM\u2019s guidelines for tissue identification in whole slide images[1].\n- Output images represent tiles from the original image that contain a large percent of tissue, no additional filtering is performed on the tiles.\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size |Tile size (pixels)|Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|---|\n|170|150 MB|1000x1000|37min|$2.6|c5.24xlarge|\n|170|150 MB|3000x3000|1h 23min|$5.83|c5.24xlarge|\n|170|150 MB|10000x10000|4h 15min|$17.93|c5.24xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n[1] [IBM Developer - Whole slide image preprocessing in Python](https://developer.ibm.com/technologies/data-science/articles/an-automatic-method-to-identify-tissues-from-big-whole-slide-images-pt1/)", "input": [{"name": "Whole slide images"}, {"name": "Number of tiles per image"}, {"name": "Tile height in pixels"}, {"name": "Tile width in pixels"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output tiles"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["SBGTools", "Imaging", "File Format Conversion"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035174, "dateCreated": 1628085435, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-html2b64/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-html2b64/13", "applicationCategory": "CommandLineTool", "name": "SBG Html2b64", "description": "Tool for converting HTML reports of FastQC, SnpEff, MultiQC (simple report only) and ChimeraScan to b64html so it can easily be displayed on SBG platform.", "input": [{"name": "Input file", "encodingFormat": "text/html"}], "output": [{"name": "B64html", "encodingFormat": "text/html"}], "softwareRequirements": ["CreateFileRequirement"], "applicationSubCategory": ["Plotting", "File Format Conversion"], "project": "SBG Public Data", "creator": "Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799039, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/loci-snapshoter/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/loci-snapshoter/5", "applicationCategory": "CommandLineTool", "name": "SBG Loci Snapshoter", "description": "**SBG Loci Snapshoter** generates screenshots of specific regions across all aligned files provided as inputs. It utilizes the IGV [1] batch functionality [2] to create PNG images of desired loci across multiple samples. \n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\n\n**SBG Loci Snapshoter** can be used to get screenshots of one or multiple regions from one or more aligned files. Regions can be provided either as a TXT file via the **Loci file** input, or as a list of strings via **Loci to capture**. \nIn case that regions are provided using a **Loci file**, each line in the file must include a single locus. Locus can be a position, in **chr:position** or **chr_position** format, or a region, in **chr:start-end** or **chr_start-end** format, where **position, start** and **end** can use comma as thousands separator, or be without separators. Note that these formats, as well as locus types can be mixed (positions and regions), and that both ways of providing inputs can be used, in which case outputs will contain images of both loci provided in a file and as list of strings. In case that loci are provided as positions, screenshot will flank the position with 20bp on each side, resulting in a region of length 41.\n\n### Common Issues and Important notes\n * Loci must be provided in specific format, as described in **Common Use Cases**. In case a locus is provided in an invalid format, the task will not fail, but the image for the locus in question will be empty. At least one locus must be provided.\n * Chromosome must be in the same format in all files provided (alignment and reference)  as well as in loci list (either file or string or both). This refers to chromosome nomenclature which differs between assemblies (i.e. chr1 vs 1).\n\n### Performance Benchmarking \nWith all tested cases - up to 10 loci, **Loci Snapshoter** executes in less than 3 minutes and costs less than $0.02.\n\n### References\n[1] [IGV](http://software.broadinstitute.org/software/igv/home) \\\n[2] [IGV batch functionality](https://software.broadinstitute.org/software/igv/batch)", "input": [{"name": "In alignments", "encodingFormat": "application/x-bam"}, {"name": "Loci file", "encodingFormat": "text/plain"}, {"name": "Genome"}, {"name": "Genome FASTA", "encodingFormat": "application/x-fasta"}, {"name": "Loci to capture"}, {"name": "Output name prefix"}, {"name": "Track view"}], "output": [{"name": "Output screenshots"}, {"name": "Output HTML", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/igvteam/igv/blob/master/license.txt"], "applicationSubCategory": ["Plotting"], "project": "SBG Public Data", "creator": "SBG", "softwareVersion": ["v1.1"], "dateModified": 1649156263, "dateCreated": 1608732174, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-medical-image-convert-1-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-medical-image-convert-1-0/8", "applicationCategory": "CommandLineTool", "name": "SBG Medical Image Convert", "description": "**SBG Medical Image Convert** performs medical image format conversion. If the input data are medical images in a non-standard format  (e.g. SVS, TIFF, DCM or DICOM), **SBG Medical Image Convert** converts them to PNG format. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.* \n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n### Common Use Cases\n- **SBG Medical Image Convert** is used to convert medical image formats into PNG format and thus enable the preview of the images on the platform.\n- It can be used to prepare images for other tools that don't support non-standard formats, like **SBG Stain Normalization**.\n\n### Common Issues and Important Notes\n- Currently supported image formats are SVS, TIFF, DCM and DICOM. \n- **Size for conversion** parameter is used to define the output image width, and the height of the output image is scaled accordingly. If the parameter is not defined, the output image will retain its original size.\n- The conversion of whole slide images can sometimes encounter an insufficient memory memory issue, which can be overcome by choosing a larger instance or by resizing the image.\n- The tool is written in python and uses slideio[1] and pydicom[2] libraries that allow this tool to read medical image formats.\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size |Size for conversion(pixel)|Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|---|\n|20|700 MB|default |8 h 18 min|$18.86|r4.8xlarge|\n|20|700 MB|3000 |4 min|$0.15|r4.8xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [slideio](http://www.slideio.com/)\n\n[2] [pydicom](https://pydicom.github.io/)", "input": [{"name": "Medical  images"}, {"name": "Size for conversion"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "Converted medical images"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["SBGTools", "Imaging", "File Format Conversion"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035174, "dateCreated": 1628086467, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-oncofuse4circos/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-oncofuse4circos/13", "applicationCategory": "CommandLineTool", "name": "SBG Oncofuse4Circos", "description": "An R script that extracts fusion links and names needed from the Oncofuse output and prepares it for Circos tool.", "input": [{"name": "Oncofuse output", "encodingFormat": "text/plain"}], "output": [{"name": "Circos links", "encodingFormat": "text/plain"}, {"name": "Circos names", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Nevena Ilic Raicevic, Seven Bridges Genomics, <nevena.ilic.raicevic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799498, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-pair-contigs/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-pair-contigs/14", "applicationCategory": "CommandLineTool", "name": "SBG Pair Contigs", "description": "This tool is specifically designed to handle output file of the SAMtools Mpileup parallel workflow. Main goal of this tool is to make a tumor-normal pair for each of the contigs regardless of the output file format specified in the SAMtools Mpileup parallel workflow. Output of this tool is a list of tumor-normal pairs for each contig which is necessary to run the VarScan2 Somatic tool in parallel mode.\nAlso, tool will automatically filter out contig pairs where at least one of the contig files is empty. This situation is common when the chromosomes are not present or covered at all. In this situations, mpileup will produce empty pileup file for such a contig and VarScan2 Somatic tool will not be able to handle it. Empty contig will be listed in the Empty_contigs.txt file.\n\n**This tool shouldn't be used outside of this context (VarScan2 WF)!**", "input": [{"name": "Array of sample2 contigs", "encodingFormat": "application/x-vcf"}, {"name": "Array of sample1 contigs", "encodingFormat": "application/x-vcf"}], "output": [{"name": "List of empty contigs", "encodingFormat": "text/plain"}, {"name": "List of the tumor-normal pairs"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["File Format Conversion"], "project": "SBG Public Data", "creator": "Djordje Klisic, Seven Bridges, <djordje.klisic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799992, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-pair-fastqs-by-metadata/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-pair-fastqs-by-metadata/17", "applicationCategory": "CommandLineTool", "name": "SBG Pair FASTQs by Metadata", "description": "Tool accepts list of FASTQ files groups them into separate lists. This grouping is done using metadata values and their hierarchy (Sample ID > Library ID > Platform unit ID > File segment number) which should create unique combinations for each pair of FASTQ files. Important metadata fields are Sample ID, Library ID, Platform unit ID and File segment number. Not all of these four metadata fields are required, but the present set has to be sufficient to create unique combinations for each pair of FASTQ files. Files with no paired end metadata are grouped in the same way as the ones with paired end metadata, generally they should be alone in a separate list. Files with no metadata set will be grouped together. \n\nIf there are more than two files in a group, this might create errors further down most pipelines and the user should check if the metadata fields for those files are set properly.", "input": [{"name": "List of FASTQ files", "encodingFormat": "text/fastq"}], "output": [{"name": "List of grouped FASTQ files", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["FASTQ Processing"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799739, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-pass-contigs/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-pass-contigs/5", "applicationCategory": "CommandLineTool", "name": "SBG Pass Contigs", "description": "Purpose of this tool is to extract all contigs (chromosomes) present in the BAM header file and to pass them to the SAMtools Mpileup tool as a list of files that will be used for the parallelization of the process. Each file will be named as the contig which is going to be used later in the SAMtools Mpileup tool.", "input": [{"name": "BAM or list of BAM files", "encodingFormat": "application/x-bam"}], "output": [{"name": "List of file per chromosome"}, {"name": "BAM header", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["SAM/BAM Processing"], "project": "SBG Public Data", "creator": "DJordje Klisic, Seven Bridges Genomics, <djordje.klisic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799022, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-pass-intervals/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-pass-intervals/3", "applicationCategory": "CommandLineTool", "name": "SBG Pass Intervals", "description": "Primary function of this tool is parallelization preparation for the Seven Bridges Whole Genome Pipeline. On the input this tool accepts BED file with the intervals for the whole genome. Each line of the BED file is converted to a string in the format chromosome:start-end. This string is used as the filename for the array of files necessary for the parallelization (scatter option) on the Seven Bridges platform. Also, these strings are going to be used for the intervals parameter in the downstream GATK tools.", "input": [{"name": "Intervals BED file", "encodingFormat": "text/plain"}], "output": [{"name": "List of interval files", "encodingFormat": "text/plain"}], "softwareRequirements": ["CreateFileRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Djordje Klisic, Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1473260896, "dateCreated": 1453799743, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-ped-file-creator/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-ped-file-creator/1", "applicationCategory": "CommandLineTool", "name": "SBG PED File Creator", "description": "Function of this tool is to create pedigree file in a specific format that can be used by other downstream tools in the Trio analysis pipelines. It is necessary to set family ID as well as the sample IDs for each member of the trio. Also, the gender of the proband has to be set as well. Output f this tools is a pedigree (PED) file.", "input": [{"name": "Family ID"}, {"name": "Father ID"}, {"name": "Mother ID"}, {"name": "Proband ID"}, {"name": "Proband gender"}], "output": [{"name": "Pedigree file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Djordje Klisic, Seven Bridges <djordje.klisic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1459789093, "dateCreated": 1459789093, "contributor": ["sevenbridges"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-picard-gatherbamfiles-sorted/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-picard-gatherbamfiles-sorted/1", "applicationCategory": "CommandLineTool", "name": "SBG Picard GatherBamFiles Sorted", "description": "This tool is developed only to be used in WGS pipeline, so it should not be used outside of it. It gets BAM files on input, reads the contigs from first BAM header and orders the BAM files in contigs order, according to their filename. For merging BAM files we use Picard GatherBAMFiles.", "input": [{"name": "Input BAM files"}, {"name": "Validation stringency"}, {"name": "Compression level"}, {"name": "Create Index"}, {"name": "Memory Per Job"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["SAM/BAM-Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1471539421, "dateCreated": 1459852872, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-prepare-for-gsea/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-prepare-for-gsea/5", "applicationCategory": "CommandLineTool", "name": "SBG Prepare for GSEA", "description": "**SBG Prepare for GSEA** is an app which consists of python and R scripts which parse `gene_exp.diff` output from Cuffdiff tool and produce files needed for Enrichment Analysis in Cytoscape. R script is using goseq R library to calculate p-values for GO terms. \n\n###Limitations###\nThe scripts are written specifically for the human GO annotations and currently can't be used with any other files except `goa_human.gaf` and `gene_sets_file.gmt` which are available from **Public Reference Files**. Also, reference gene annotations file (GTF) should have UniProtKB identifiers as gene IDs (as in [human\\_hg19\\_genes\\_2015.gtf](https://igor.sbgenomics.com/public/files/578cf94a507c17681a3117e5/)).", "input": [{"name": "Reference Annotations", "encodingFormat": "application/x-gtf"}, {"name": "Cuffdiff zipped output", "encodingFormat": "application/zip"}, {"name": "GO Annotation"}, {"name": "Gene Sets"}], "output": [{"name": "Expression Data", "encodingFormat": "text/plain"}, {"name": "Enrichment Results", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Differential Expression", "Functional Characterization"], "project": "SBG Public Data", "creator": "Ana Damljanovic/Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1501081658, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-prepare-gatk-variantrecalibrator-resource/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-prepare-gatk-variantrecalibrator-resource/14", "applicationCategory": "CommandLineTool", "name": "SBG Prepare GATK VariantRecalibrator Resource", "description": "Prepare VQSR resource is a tool for preparing resource datasets and arguments to use with VQSR. It sets a metadata for a list of sites for which to apply a prior probability of being correct, but which aren't used by the algorithm (training and truth sets are required to run).", "input": [{"name": "VCF File", "encodingFormat": "application/x-vcf"}, {"name": "Label"}, {"name": "Known"}, {"name": "Training"}, {"name": "Truth"}, {"name": "Prior"}], "output": [{"name": "Prepared VCF"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453798906, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-prepare-intervals/100", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-prepare-intervals/100", "applicationCategory": "CommandLineTool", "name": "SBG Prepare Intervals", "description": "Depending on selected Split Mode value, output files are generated in accordance with description below:\n\n1. File per interval - The tool creates one interval file per line of the input BED(FAI) file.\nEach interval file contains a single line (one of the lines of BED(FAI) input file).\n\n2. File per chr with alt contig in a single file - For each contig(chromosome) a single file\nis created containing all the intervals corresponding to it .\nAll the intervals (lines) other than (chr1, chr2 ... chrY or 1, 2 ... Y) are saved as\n(\"others.bed\").\n\n3. Output original BED - BED file is required for execution of this mode. If mode 3 is applied input is passed to the output.\n\n4. File per interval with alt contig in a single file - For each chromosome a single file is created for each interval.\nAll the intervals (lines) other than (chr1, chr2 ... chrY or 1, 2 ... Y) are saved as\n(\"others.bed\").\n\n##### Common issues: \nDo not use option 1 (File per interval) with exome BED or a BED with a lot of GL contigs, as it will create a large number of files.", "input": [{"name": "Input BED file", "encodingFormat": "text/x-bed"}, {"name": "Input FAI file"}, {"name": "Split mode"}, {"name": "Interval format"}], "output": [{"name": "Intervals", "encodingFormat": "text/x-bed"}, {"name": "Output file names"}, {"name": "String output"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["BED Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1473165018, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/push-to-solvebio/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/push-to-solvebio/4", "applicationCategory": "CommandLineTool", "name": "SBG Push2Solvebio", "description": "Tool to push VCF and JSON file(s) to the SolveBio knowledge database. This tool is a component of the SolveBio Integration Workflow.\n\n***Required Inputs***\n\n1. Variants file - Input variants file [VCF or JSON].\n\n***Required Parameters:***\n\n1. API Token - User-specific SolveBio API token (must create account at SolveBio to obtain).\n2. Dataset - Name of the SolveBio dataset to append to or create. Should follow the formatting {Depository}/{Version}/{Dataset}. Can be existing or new.\n3. Genome Ref - Genome reference used for variant calling. It must also match the name of SnpEff database file.\n\n***Optional Parameters:***\n\n1. Auto Approve - Automatically approve the VCF/JSON upload to SolveBio. Default set to TRUE.\n2. Output Filename - Name of a B64HTML output file to capture links to the \"job\" and \"dataset\" pages. Will be appended with '_solve_push.txt'.  Default output name is generated from input file(s).\n\n***Outputs:***\n\n1. B64HTML Out - B64HTML file containing links to the \"dataset\" and \"job\" pages on SolveBio [B64HTML].", "input": [{"name": "VCF(s)"}, {"name": "SolveBio API token"}, {"name": "SolveBio dataset"}, {"name": "Genome reference"}, {"name": "Output filename prefix"}, {"name": "Auto approve"}], "output": [{"name": "B64HTML output file"}], "softwareRequirements": ["CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["integrations"], "project": "SBG Public Data", "creator": "Jack DiGiovanna / Seven Bridges", "softwareVersion": ["sbg:draft-2"], "dateModified": 1502804268, "dateCreated": 1479307146, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-report-renderer-2/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-report-renderer-2/6", "applicationCategory": "CommandLineTool", "name": "SBG Report Renderer 2", "description": "SBG Report Renderer 2 is a powerful tool based on Jinja2 and wkhtmltopdf that can be used to render custom reports in PDF file format.\n\n**Tool manual:**\n\n---\nConfig file is a variable mapping configuration file. It can contain variable mappings in the format one per line. The same mapping format is used if variables are supplied on the command line.\n\nVariable mapping format: **NAME,TYPE,VALUE**\nFirst parameter argument is the name of the variable, as used in the template. Second is the parameter type and the third is it's value.\n\n**Available types and its expected values:**\n*TEXT*: any textual string\n\nDATE: no value\n\n*TIME*: no value\n\n*SAMPLE*: Alternative sample name\n\n*FILE*: file name glob\n\n*TXT-CONTENT*: file name glob\n\n*TSV-CONTENT*: file name glob\n\n**Variable content as returned in the template script:**\n*TEXT*: text supplied as it's value\n\n*DATE*: current date\n\n*TIME*: current time\n\n*SAMPLE*: sample name as set in metadata or if sample name is missing, alternative text if provided\n\n*FILE*: list of all files supplied on input that match given glob\n\n*TXT-CONTENT*: list containing file name of the matched file as the first item and file's textual content as the second item. If more files match the glob they will follow in identical fashion\n\n*TSV-CONTENT*: list containing file name of the matched file as the first item and file's content divided in rows and columns (as a double array) as the next item. If more files match the glob they will follow in identical fashion.", "input": [{"name": "Template"}, {"name": "Index file name"}, {"name": "File"}, {"name": "Output"}, {"name": "Config"}, {"name": "variable"}, {"name": "Header"}, {"name": "Footer"}, {"name": "Portrait"}, {"name": "Grayscale"}, {"name": "Page size"}, {"name": "Margin left"}, {"name": "Margin right"}, {"name": "Margin top"}, {"name": "Margin bottom"}, {"name": "Table of content"}], "output": [{"name": "PDF report"}, {"name": "HTML"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Plotting"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799158, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-samtools-mpileup-merge-out/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-samtools-mpileup-merge-out/16", "applicationCategory": "CommandLineTool", "name": "SBG SAMtools Merge Mpileup", "description": "Main function of this tool is to provide option to the user whether to merge list of files from SAMtools Mpileup tool or to pass list of files for each contig to the output. \n\nIf MERGE option is selected, regardless of the type, files will be merged following the contig order from the reference file index file or BED file. This order is provided using input contig_order_names.  So, this tool will receive list of pileup, vcf or BC files and list of files names of BED files used in creating input files. Pairing and finding the correct order will be done using metadata field  ScatteredUsing.\n\n####Common issue:\nThis tools is built specially for mpileup parallel and Varscan pipelines, so please don't use it outside these pipeline unless you are sure how to use it. Contact our support team for information.", "input": [{"name": "Input file list", "encodingFormat": "application/x-vcf"}, {"name": "merging_order"}, {"name": "Output state"}], "output": [{"name": "output_file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Text Processing", "VCF Processing"], "project": "SBG Public Data", "creator": "Mohamed Marouf, Seven Bridges Genomics, <mohamed.marouf@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241352, "dateCreated": 1453799746, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-single-cell-object-convertor/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-single-cell-object-convertor/8", "applicationCategory": "CommandLineTool", "name": "SBG single cell object convertor", "description": "This tool performs conversion of single cell data object type for commonly used formats: Seurat [1], AnnotatedData [2] and SingleCellExperiment [3]. This CWL tool is a wrapper around the script based on Seurat 4.0.4, Seurat Disk 0.0.0.9019 and SingleCellExperiment 1.14.1 libraries.\n\n_A list of  **all inputs and parameters**  with corresponding descriptions can be found at the bottom of this page._\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\nPossible use-cases for this tool are:\n - Transforming an input Seurat object to a SingleCellExperiment or AnnData object. Main Seurat Assay can be specified by using the **Seurat assay** input. If this input is not specified, default assay in the Seurat object will be used. The input Seurat object should be provided in the form of a .RDS file. Output SingleCellExperiment object will be outputted as a .RDS file while the AnnData object is outputted in the h5ad format.\n - Transforming an input SingleCellExperiment object to a Seurat or AnnData object. Counts assay (contains raw counts) in SingleCellExperiment can be specified by using the  **SingleCellExperiment counts assay** input while logcounts assay (contains normalized counts) can be specified by using the  **SingleCellExperiment logcounts assay** input. The input SingleCellExperiment object should be provided in the form of a .RDS file. Output Seurat object will be outputted as a .RDS file while the AnnData object is outputted in the h5ad format.\n\n### Changes Introduced by Seven Bridges\n\nNo changes to the original libraries were introduced by Seven Bridges.\n   \n### Common Issues and Important Notes\nIn order to avoid conversion issues when converting from Seurat object type, the following object attributes are removed prior to conversion:\n - Seurat \"tools\" attribute is removed prior to conversion to any object type.\n - Seurat \"scale.data\" attribute is removed prior to conversion to SingleCellExperiment object type.\n\nIn order to avoid conversion issues when converting from SingleCellExperiment type, \"altExp\" is removed from SingleCellExperiment object prior to conversion.\n\nAdditional implementation details regarding different transformation types:\n - When transforming Seurat to SingleCellExperiment, single assay from\n   Seurat data is used to generate a Single Cell Experiment object. \n - When transforming Seurat to Annotated Data, all Seurat assays are   \n   used for conversion.\n - When transforming SingleCellExperiment to any object type, only two  \n   matrices are used to generate the new object - one matrix representing   \n   raw counts and another matrix representing normalized counts   \n   (logcounts).\n\n### Performance Benchmarking\nExecution time and price do not depend on input file size. Object type conversion usually takes around 5 minutes with an approximate cost of 0.05$ on a r5.xlarge instance.\n\n### Portability\n\n**SBG single cell object convertor**  was tested with cwltool 3.1.20220124184855.\n\n### References\n\n[1] [Seurat documentation](https://satijalab.org/seurat/)\n\n[2] [Annotated Data documentation](https://anndata.readthedocs.io/en/latest/)\n\n[3] [Single Cell Experiment bioconductor page](https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html)", "input": [{"name": "Input object type"}, {"name": "Output object type"}, {"name": "SingleCellExperiment counts assay"}, {"name": "Input Single Cell object"}, {"name": "Seurat assay"}, {"name": "SingleCellExperiment logcounts assay"}], "output": [{"name": "Converted single cell data"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/mojaveazure/seurat-disk"], "applicationSubCategory": ["Single Cell"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1646862515, "dateCreated": 1646833451, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-split-bed/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-split-bed/5", "applicationCategory": "CommandLineTool", "name": "SBG Split BED", "description": "Splits input BED file into two BED files, one with small regions (less than 1000000 nucleotides) and the other with large regions.", "input": [{"name": "Input BED file", "encodingFormat": "text/x-bed"}], "output": [{"name": "Small bed regions", "encodingFormat": "text/x-bed"}, {"name": "Large regions BED", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["BED Processing"], "project": "SBG Public Data", "creator": "Bogdan Gavrilovic, Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1468402324, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-splitfasta/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-splitfasta/3", "applicationCategory": "CommandLineTool", "name": "SBG SplitFasta", "description": "SBG-SplitFasta is a tool that splits input FASTA file per chromosome. The output is given as a compressed .tar.gz file.", "input": [{"name": "Fasta file", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Compressed fastas"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["FASTA Processing"], "project": "SBG Public Data", "creator": "Vladan Arsenijevic, Seven Bridges Genomics, <vladan.arsenijevic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453798785, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-split-folders-1-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-split-folders-1-0/5", "applicationCategory": "CommandLineTool", "name": "SBG Split Folders", "description": "**SBG Split Folders** organizes an image directory into the train and test subdirectory structure. \n\nThese directories are necessary inputs for **SBG Deep Learning Image Classification Exploratory Workflow** and **SBG Deep Learning Prediction**.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly***\n\n\n### Common Use Cases\n- **SBG Deep Learning Image Classification Exploratory Workflow** and **SBG Deep Learning Prediction** require train and test directories with labeled images as inputs, while the validation directory is optional. If the user has all their images inside one directory, the images can be split into train and test directories by using the **SBG Split Folders** tool beforehand. \n- Number of files in each subdirectory can be selected by setting the **Split ratio** parameter, which will determine what percentage of images will be separated into the test directory, while the rest will be put in the train directory.\n\n### Common Issues and Important Notes\n- The tool is written in python and uses the split-folders v0.4.3 library[1], which allows this tool to work on any file type, the files get shuffled, while a defined seed makes splits reproducible.\n- **Input directory** can be organized in one of two ways:\n     1. Images are placed into corresponding subdirectories of **Input directory**, one for each class.\n     2. Images are under the main **Input directory**, together with a single CSV file that has two columns - the first column containing all the image names and the second column containing corresponding labels (classes).\n\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size |Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|\n|5840|1 MB|9 min|$0.08|c4.2xlarge|\n|171|100 MB|14 min|$0.13|c4.2xlarge|\n|151|1000 MB|1 h 31 min|$0.81|c4.2xlarge|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1] Pypi.org - [Split folders](https://pypi.org/project/split-folders/) python library", "input": [{"name": "Input directory"}, {"name": "Split ratio"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Seed value for shuffling the items"}], "output": [{"name": "Output directory"}], "softwareRequirements": ["ShellCommandRequirement", "LoadListingRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["Utilities", "SBGTools"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648468627, "dateCreated": 1628085982, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-stain-normalization-1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-stain-normalization-1-0/7", "applicationCategory": "CommandLineTool", "name": "SBG Stain Normalization", "description": "**SBG Stain Normalization** involves casting an array of images in the stain colors of a target image. \n\nStain normalization is used as a histopathology image preprocessing step to reduce the color and intensity variations present in stained images obtained from different laboratories.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\nHistopathology involves a manual staining procedure for preparing tissues prior to microscopic imaging for cancer diagnosis. This non-quanti\ufb01ed procedure may cause a considerable variation in color characteristics of tissue samples. However, computer-aided diagnosis (CAD) is affected by the variations in color and intensity of the images. To compensate for these effects in a CAD system, stain color normalization is a common practice[1].\n\n### Common Issues and Important Notes\n- Normalization will be performed on the whole dataset based on one target image, which is provided on the **Target image** input port.\n- Stain normalization can be performed on PNG, JPEG and JPG images whose size does not exceed 1GB. If using large SVS whole slide images, a reduction in size and conversion has to be done first by using **SBG Medical Image Convert**.\n- This tool is written in python and uses the StainTools[2] library which is used for tissue image stain normalization and augmentation in Python3.\n\n### Performance Benchmarking\n\nBelow is a table describing the run times and task costs on on-demand AWS instances for a set of samples with different file sizes.\n\n|Number of images|Average image size |Execution time| Price| Instance (AWS)|\n|---|---|---|---|---|\n|10|180 MB|16 min|$1.37|c5.24xlarge|\n|5|800 MB|48 min|$3.38|c5.24xlarge|\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n[1] Andreaa Anghel et al. [A High-Performance System for Robust Stain Normalization of Whole-Slide Images in Histopathology](https://www.frontiersin.org/articles/10.3389/fmed.2019.00193/full) Front. Med., 30 September 2019 \n\n[2] [StainTools](https://hackmd.io/@peter554/staintools)", "input": [{"name": "Target image"}, {"name": "Input images"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Stain extractor method"}], "output": [{"name": "Normalized images"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "applicationSubCategory": ["SBGTools", "Imaging"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1648035174, "dateCreated": 1628086237, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-text2html/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-text2html/5", "applicationCategory": "CommandLineTool", "name": "SBG Text2Html", "description": "This is a simple R script that takes a TEXT file and converts it into an HTML file. It uses just one function from 'sjPlot' library; it allows sorting of rows based on a chosen column given its name (header) or index number.\n\n### Inputs\n**text_file** - file to be converted to HTML\n\n**sortcolumn** - chose column for sorting \n\n### Common issues\nIf **sortcolumn** is misspelled task will faill. Configuration input **sortcolumn** is case sensitive, provided string has to mach  column one chooses, other way task will fail.", "input": [{"name": "TEXT file", "encodingFormat": "text/plain"}, {"name": "Sort based on the column"}, {"name": "Header"}], "output": [{"name": "HTML file.", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Nevena Ilic Raicevic, Seven Bridges Genomics, <nevena.ilic.raicevic@sbgenomics.com>", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648468219, "dateCreated": 1453799824, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-ucsc-b37-bed-converter/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-ucsc-b37-bed-converter/2", "applicationCategory": "CommandLineTool", "name": "SBG UCSC-B37 BED Converter", "description": "Converts a BED file from hg (USCS) to GRCh (NCBI) format and vice versa.", "input": [{"name": "Bed"}, {"name": "Direction"}], "output": [{"name": "Converted", "encodingFormat": "text/x-bed"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["BED Processing"], "project": "SBG Public Data", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799130, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-unpack-fastqs-1-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-unpack-fastqs-1-0/5", "applicationCategory": "CommandLineTool", "name": "SBG Unpack TCGA FASTQs", "description": "**SBG Unpack TCGA FASTQs** performs the extraction of the input archive, containing TCGA FASTQ files. \nThis tool also sets the \"paired_end\" metadata field. It assumes that FASTQ file names are formatted in this manner:\nfirst pair reads FASTQ file        -  *1.fastq\nsecond pair reads FASTQ file  -  * 2.fastq. \nwhere * represents any string.\n**This tool is designed to be used for paired-end metadata with above mentioned name formatting only.**\nSupported formats are:\n1. TAR\n2. TAR.GZ (TGZ)\n3. TAR.BZ2 (TBZ2)\n4. GZ\n5. BZ2\n6. ZIP", "input": [{"name": "Input archive file", "encodingFormat": "application/zip"}], "output": [{"name": "Output FASTQ files", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Marko Petkovic, Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1513857723, "dateCreated": 1453799523, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-untar-fasta/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-untar-fasta/12", "applicationCategory": "CommandLineTool", "name": "SBG Untar fasta", "description": "SBG Untar fasta is primarily used for extracting FASTA file from TAR with more different files. For example, extracting FASTA from BWA-MEM's index TAR.\n\n**Inputs**\n\nTAR (containing FASTA).\n\n**Outputs**\n\nFA/FASTA/FA.GZ/FASTA.GZ from TAR.\n\n\n\n**Important: TAR.GZ format is not supported. \nIf only decompress operation is required use SBG Decompressor tool.**", "input": [{"name": "Input archive file with fasta"}], "output": [{"name": "Unpacked fasta file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "applicationSubCategory": ["Other"], "project": "SBG Public Data", "creator": "Vladimir Kovacevic, Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1500474352, "dateCreated": 1466173965, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-vcf2json/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-vcf2json/1", "applicationCategory": "CommandLineTool", "name": "SBG Vcf2Json", "description": "This tool is built from a Python script that converts an input VCF file to JSON format; compresses resulting output JSON file (GZIP). This tool is a component of the SolveBio Integration workflow.", "input": [{"name": "VCF input file", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Output file in JSON format"}], "softwareRequirements": ["CreateFileRequirement", "ExpressionEngineRequirement"], "codeRepository": ["https://github.com/solvebio/solvebio-python", "https://github.com/solvebio/solvebio-python/blob/master/solvebio/contrib/vcf_parser/vcf_parser.py"], "applicationSubCategory": ["Converters"], "project": "SBG Public Data", "creator": "David Caplan, Dandan Xu/SolveBio", "softwareVersion": ["sbg:draft-2"], "dateModified": 1479307146, "dateCreated": 1479307146, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sbg-vcf-reorder/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sbg-vcf-reorder/2", "applicationCategory": "CommandLineTool", "name": "SBG VCF Reorder", "description": "VCF Reorder is a tool for re-ordering variants in VCF files by chromosome.", "input": [{"name": "Vcf"}, {"name": "Reference"}, {"name": "Species"}, {"name": "Memory"}], "output": [{"name": "Reordered VCF", "encodingFormat": "application/x-vcf"}], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241353, "dateCreated": 1453799484, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/seq2hla-2-3-cwl-1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/seq2hla-2-3-cwl-1-0/7", "applicationCategory": "CommandLineTool", "name": "seq2HLA CWL1.0", "description": "**seq2HLA** tool performs HLA typing from RNA-seq reads [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nHLA typing using paired-end RNA-seq data in FASTQ or FASTQ.GZ format.\n\n### Changes Introduced by Seven Bridges\n\n- Parameter `--runName` is now not required. If a value for it is not provided by the user, the tool will try to assign it based on the **Sample ID** metadata field value of input FASTQ files (**Paired-end FASTQ files**). If **Sample ID** metadata field is not filled, string \"seq2HLA_typing\" is used as the default value. \n- The following parameters have been excluded from the Seven Bridges version of the tool:\n    * `--help`\n    * `--version`\n- Input FASTQ files are now supplied as a list (**Paired-end FASTQ files**) and the command line is built from there (`-1` and `-2` input parameters) based on the values of the **Paired-end** metadata field.\n\n### Common Issues and Important Notes\n \n* Input **Paired-end FASTQ files** is required. These files should be paired-end FASTQ or FASTQ.GZ files and the **Paired-end** metadata field should be filled.\n* Input **Bowtie number of parallel threads** determines the number of CPUs the tool requests (and therefore influences the choice of instance on which the task will be run).\n* **Biobambam2 bamtofastq** is recommended for obtaining input FASTQ files from alignment files (BAM, CRAM).\n* HLA alleles reported with a trailing apostrophe in the typing output files correspond to alleles reported in the **Typing ambiguities** output. If more than one allele is fully consistent with the RNA-seq and allele database information, **seq2HLA** will choose and report the most probable allele in the typing files, but list all valid matches in the **Typing ambiguities** output.\n* Please note that the tool uses its original HLA alleles database, which prohibits the identification of newer alleles.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of **seq2HLA** running time and cost.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n                   \n| Inputs | Duration | Cost | Instance (AWS) |\n| -------------------- | ----------------- | ------------ | ------------- |\n| 3 GB FASTQ  | 10 min  |  $0.07  |   c4.2xlarge   |\n| 2.8 GB FASTQ.GZ  |  80 min  | $0.53 |   c4.2xlarge  |\n| 2.8 GB FASTQ.GZ  |  44 min  | $0.58 |   c4.4xlarge  |\n| 2.8 GB FASTQ.GZ  |  30 min  | $0.79 |   c4.8xlarge  |\n\n\n### Portability\n\n**seq2HLA** was tested with cwltool version 3.1.20211107152837. The `in_reads` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [seq2HLA GitHub page](https://github.com/TRON-Bioinformatics/seq2HLA)\n[2] [seq2HLA paper](https://genomemedicine.biomedcentral.com/articles/10.1186/gm403)", "input": [{"name": "Bowtie number of parallel threads"}, {"name": "Bowtie number of bases to trim from 3` end"}, {"name": "Name of this HLA typing run"}, {"name": "Paired-end FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Expression of class I alleles"}, {"name": "Class I 2 digit results"}, {"name": "Typing ambiguities"}, {"name": "Class II 4 digit results"}, {"name": "Class II 2 digit results"}, {"name": "Class I 4 digit results"}, {"name": "Expression of class II alleles"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/TRON-Bioinformatics/seq2HLA", "https://github.com/TRON-Bioinformatics/seq2HLA/blob/master/seq2HLA.py", "https://github.com/TRON-Bioinformatics/seq2HLA/blob/master/README.txt"], "applicationSubCategory": ["HLA Typing", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Sebastian Boegel", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1582564167, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/shapeit-4-4-2-1-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/shapeit-4-4-2-1-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "SHAPEIT 4", "description": "**SHAPEIT 4** is a phasing tool for sequencing and SNP array data  [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**SHAPEIT 4** can be used to estimate haplotypes starting from SNP array or sequencing data (**Genotypes to phase**). The tool phases a specified genomic region (**Target region**) and requires a genomic map (**Genomic map**). Genomic maps for humans are [available at the tool GitHub repository](https://github.com/odelaneau/shapeit4/tree/master/maps). \n\n### Changes Introduced by Seven Bridges\n\n* Parameter `--help` was omitted from the wrapper.\n* Parameter `--output` is no longer required, but the output file type for the phased haplotypes must be provided instead (**Phased haplotypes output file type**).\n\n### Common Issues and Important Notes\n\n* Inputs **Genotypes to phase**, **Genetic map**, **Target region** and **Phased haplotypes output file type**  are required.\n* **Genotypes to phase** input should be in BCF or VCF format, with the accompanying index (CSI or TBI). The authors recommend indexing this file with **BCFtools Index**.\n* If used, the **Reference panel** or **Scaffold of haplotypes** inputs should be in BCF/VCF format with the accompanying index (CSI or TBI).\n* Genetic maps for humans are available [at the tool GitHub repository](https://github.com/odelaneau/shapeit4/tree/master/maps). Please untar the archives before use and provide the chromosome file matching the other inputs.\n\n### Performance Benchmarking\n\nChromosome 20 of the 1000 genomes dataset (GRCh38 coordinates, 1647102 variants, 3202 individuals) was used for testing (chr20:10000000-30000000 and the entire chromosome) with the default tool parameters, except for `--sequencing` and `--thread`.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| 20 Mb - 15 cores  | 272 min | $3.08 + $0.06 |  c5.4xlarge 100 GB EBS |\n| 20 Mb - 30 cores  | 181 min | $4.61 + $0.08 |  c5.9xlarge 200 GB EBS |\n| chr20 - 15 cores  | 838 min | $10.73 + $0.39 |  m5.4xlarge 200 GB EBS |\n| chr20 - 30 cores  | 485 min | $12.37 + $0.22 |  c5.9xlarge 200 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**SHAPEIT 4** was tested with cwltool version 3.1.20211107152837. The `in_genotypes`, `in_map`, `region`, `suffix` and `thread` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [SHAPEIT 4 publication](https://www.nature.com/articles/s41467-019-13225-y)\n\n[2] [SHAPEIT 4 documentation](https://odelaneau.github.io/shapeit4/)", "input": [{"name": "Random number generator seed"}, {"name": "Number of threads"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Genotypes to phase", "encodingFormat": "application/x-vcf"}, {"name": "Reference panel", "encodingFormat": "application/x-vcf"}, {"name": "Scaffold of haplotypes", "encodingFormat": "application/x-vcf"}, {"name": "Genetic map"}, {"name": "Target region"}, {"name": "PS field to use"}, {"name": "Parameter settings for sequencing data"}, {"name": "Iteration scheme of the MCMC"}, {"name": "Pruning threshold for genotype graphs"}, {"name": "PBWT modulo"}, {"name": "PBWT depth"}, {"name": "PBWT MAC"}, {"name": "PBWT MDR"}, {"name": "Disable initialization by PBWT sweep"}, {"name": "Minimal size of the phasing window in cM"}, {"name": "Effective size of the population"}, {"name": "Output file name prefix"}, {"name": "Phased haplotypes output file type"}, {"name": "File name for phased haplotypes in BIN format"}, {"name": "Log file name"}], "output": [{"name": "Phased haplotypes", "encodingFormat": "application/x-vcf"}, {"name": "Optional BIN phased haplotypes file"}, {"name": "Optional output log file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/odelaneau/shapeit4", "https://github.com/odelaneau/shapeit4/releases/tag/v4.2.1"], "applicationSubCategory": ["Phasing", "VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Olivier Delaneau", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618329189, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/single-cell-rna-seq-gene-set-enrichment-analysis/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/single-cell-rna-seq-gene-set-enrichment-analysis/3", "applicationCategory": "CommandLineTool", "name": "Single-Cell RNA-Seq Gene Set Enrichment Analysis", "description": "This tool is based on **fgsea**, an R-package used for fast preranked GSEA, adjusted for single cell RNA-Seq data. Preranked gene set enrichment analysis (GSEA) is a widely used method for analyzing gene expression data. It allows selection from an a priori defined list of gene sets, those which have non-random behavior in a considered experiment. This package allows quick and accurate calculation of arbitrarily low GSEA p-values for a collection of gene sets [1]. **Single-Cell RNA-Seq Gene Set Enrichment Analysis** tool can handle both MSig and Reactome databases, and it supports human and mouse samples.\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n### Common Use Cases\nThe tool has one required input, **Input Seurat object**, which contains the results of the clustering process, with gene markers identified in each cluster. It is recommended to use the output of **Clustering and Gene Marker Identification with Seurat 3.2.2 workflow**, available in our public gallery. \n**Single-Cell RNA-Seq Gene Set Enrichment Analysis** performs fast GSEA on the dataset provided on the **Input Seurat object** port. Since the information of all genes is needed for the GSEA, the differential expression test is performed using the Wilcoxon sum rank test, for which *presto* R package was used. Since Reactome database is compatible with Entrez IDs, Gene IDs from the input Seurat object are first translated to Entrez IDs (using *org.Hs.eg.db* or *org.Mm.eg.db* R packages, depending on the input sample) [2]. If MSigDb is being used, Gene IDs from the input Seurat object are first translated to Gene Symbols (using Ensembl based annotation R packages *EnsDb.Hsapiens.v86* and *EnsDb.Mmusculus.v79*, depending on the input sample) [3].\n**fgsea** package works only with lists of genes that are previously sorted based on their significance in the experiment. Therefore, two options for gene ranking are available: *auc* parameter, which is provided as a result of the Wilcoxon test, and well-established *metric2*, defined as: sign(log2FoldChange) * (-log(pvalue-adj)). Then the tool performs GSEA on each cluster compared to all others. By defining certain parameters, one of the two fgsea functions will be used. If the **Number of permutations parameter** (which sets the number of permutations for preliminary estimation of p-values) is defined, then **fgseaSimple** will be called by default. If the said parameter is not defined, **fgseaMultilevel**  will be used. More information about the aforementioned functions can be found in [1]. \nFollowing the GSEA analysis, only statistically significant pathways are kept (having p-value below 0.1, as advised in [4]). An additional option is the use of the **CollapsePathways** function, which filters out the redundant pathways. \n\n\nThe tool produces the following outputs:\n**Most enriched pathways for all clusters** - a single file in TXT format, containing a few most enriched pathways in each cluster (the number of the most enriched clusters can be defined through the parameter *The number of the most enriched pathways*);\n**All pathways** - a list of files in TSV format, containing 10 most up- and down-regulated pathways in each cluster;\n**Top pathways** - file in PDF format, containing GSEA table plots for ten most and least enriched pathways in each cluster.\n\n\n\n### Changes Introduced by Seven Bridges\n* **Single-Cell RNA-Seq Gene Set Enrichment Analysis** tool is set to work on 8 cores (the fgsea parameter *nproc* is set to 8, and is not exposed), as it can only be applied to the first function in the analysis (fgseaSimple/fgseaMultilevel), whose duration is negligible compared to the task execution time when the Collapse Pathways option is selected.\n\n\n\n### Common Issues and Important Notes\n\n* **Single-Cell RNA-Seq Gene Set Enrichment Analysis** can take only one input Seurat object, which can be produced by **Clustering and Gene Marker Identification with Seurat 3.2.2 workflow**. If some other Seurat object is used, please make sure to use Gene IDs as gene markers in a clusterization and gene marker identification result.\n* It is recommended to define the input file metadata (sample ID), due to its use in the output PDF report. \n* When working with MSigDB, **MSigDB category** and **MSigDB subcategory** must be defined and the chosen options must match.\n* The tool supports only human and mouse samples.\n* If **Output files prefix** is not defined, the tool will use the input file sample ID as a prefix for output files. If sample ID is not defined, all files will have the *Analyzed_sample* prefix. \n\n\n### Performance Benchmarking\n\n* Four datasets containing different numbers of cells were used for benchmarking purposes. The selection of the ranking metric does not affect the execution time, nor does the database selection. However, when the **Collapse Pathways** options is used with MSigDB, the execution time is greatly prolonged.\n     \n| Dataset (number of cells)  | Database / Metric for gene ranking / Collapse pathways | Duration | Cost | Instance (AWS on-demand)|\n|--------------------------------|---------------------------------------|--------------------------|-----------------------|------------------------------|\n| Human PBMCs (11700) | Reactome / auc / CollapsePathways | 32m | $0.28 | c4.2xlarge | \n| Human PBMCs (11700) | MSigDb / auc / CollapsePathways | 1d 12h 24m | $19.01 | c4.2xlarge | \n| Human PBMCs (4600) | MSigDb / auc / CollapsePathways | 19h 48m | $10.59 | c4.2xlarge | \n| Human PBMCs (8700) | Reactome / metric2 / CollapsePathways | 19m | $0.43 | c5.9xlarge | \n| Human PBMCs (11700) | Reactome/ auc | 5m | $0.05 | c4.2xlarge | \n| Human PBMCs (9700) | MSigDb / auc / CollapsePathways | 20h 11m | $9.61 | c5.9xlarge | \n| Human PBMCs (1200) | MSigDb / auc / CollapsePathways | 7h 33m | $4.03 | c4.2xlarge | \n|Human PBMCs (1200) | MSigDb / auc / CollapsePathways | 5h 39m | $2.40 | c5.2xlarge | \n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n\n###References\n[1] [fgsea paper](https://www.biorxiv.org/content/10.1101/060012v2.full)\\\n[2] [OrgDb annotation package](https://bioconductor.org/packages/release/data/annotation/html/org.Hs.eg.db.html)\\\n[3] [Ensemble annotation package](https://bioconductor.org/packages/release/data/annotation/html/EnsDb.Hsapiens.v86.html)\\\n[4] [fgsea tutorial](https://www.bioconductor.org/packages/release/bioc/vignettes/fgsea/inst/doc/fgsea-tutorial.html)", "input": [{"name": "Seurat object"}, {"name": "The number of the most enriched pathways"}, {"name": "Species"}, {"name": "Database"}, {"name": "MSigDb category"}, {"name": "MSigDb subcategory"}, {"name": "Collapse pathways"}, {"name": "Minimal size of a gene set to test"}, {"name": "Maximal size of a gene set to test"}, {"name": "The size of a random set of genes to test."}, {"name": "Number of permutations"}, {"name": "GSEA parameter"}, {"name": "GSEA score type parameter"}, {"name": "P-value limit"}, {"name": "Metric for gene ranking"}, {"name": "Memory in MB per job"}, {"name": "Number of CPUs per job"}, {"name": "Output files prefix"}], "output": [{"name": "Most enriched pathways for all clusters", "encodingFormat": "text/plain"}, {"name": "All pathways"}, {"name": "Top pathways"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ctlab/fgsea/"], "applicationSubCategory": ["Single Cell"], "project": "SBG Public Data", "creator": "Gennady Korotkevich et al.", "softwareVersion": ["v1.1"], "dateModified": 1648560772, "dateCreated": 1643383471, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/slingshot-and-tradeseq/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/slingshot-and-tradeseq/2", "applicationCategory": "CommandLineTool", "name": "Single cell RNA-seq trajectory analysis with slingshot and tradeSeq", "description": "This tool performs single cell trajectory analysis with slingshot 2.0.0 [1] and differential expression testing on inferred trajectories with tradeSeq 1.6.0 [2]. Slingshot takes advantage of single cell data principal components analysis (PCA) and clustering to infer probable paths of cell development. Trajectories consist of clusters which represent different stages in the cell developmental process. tradeSeq uses generalized additive models to perform differential expression testing within trajectories and between trajectories [3].\n\n**Warning**: Slingshot analysis is always performed while tradeSeq analysis is optional. tradeSeq analysis can be skipped by setting **Run tradeSeq** input to FALSE. If tradeSeq analysis is not performed gene clustering will not be performed as well.\n\n_A list of  **all inputs and parameters**  with corresponding descriptions can be found at the bottom of the page._\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\nTrajectory analysis is used to study and explain cell developmental processes in single cell expression data. In order to perform trajectory analysis, single cell data should be properly  preprocessed. Mandatory preprocessing includes PCA and UMAP dimensionality reduction as well as cell clustering. \n\nFollowing sections contain instructions for running trajectory analysis on the Seven Bridges platform.\n\n#### Input file requirements\nSingle cell data must be provided as a SingleCellExperiment object stored in a .RDS file. **SBG single cell object convertor** CWL tool can be used to transform Seurat objects to SingleCellExperiment objects. Input SingleCellExperiment object must contain PCA and UMAP dimensionality reductions as well as a metadata variable (inside colData) that contains information on cell clustering. Since different tools use different names for clustering, clustering name should be provided to the **Single cell experiment clustering name** input.\n\n#### Ways to preprocess single cell data for trajectory analysis on the Seven Bridges platform\nSingle cell dimensionality reduction and clustering can be performed with the following tools on the Seven Bridges platform :\n**Clustering and Gene Marker Identification with Seurat 3.2.2** performs preprocessing of single cell data as well as dimensionality reduction and clustering. Seurat output object can be transformed to SingleCellExperiment format using **SBG single cell object convertor**. Since this workflow can perform clustering with multiple resolutions, the users can use one of the available clusterings. Clustering names in **Clustering and Gene Marker Identification with Seurat 3.2.2** are formatted as \"SCT_snn_res. + clustering resolution\" (i.e. \"SCT_snn_res.0.4\" for clustering resolution 0.4).\n\n**Single cell velocity with scVelo** performs single cell data preprocessing including PCA & UMAP dimensionality reductions and clustering. A Seurat output object  can be converted to SingleCellExperiment object by using **SBG single cell object convertor**. Default clustering name in the output Seurat object is \"seurat_clusters\".\n\n#### Choosing right clustering for trajectory analysis\nClustering granularity should be chosen so that the clustering reflects cell types (developmental stages) present in the dataset. \n\nTrajectory analysis should not be performed on clusterings that have high granularity (large number of clusters where each cell type is split into multiple clusters) because slingshot may overfit the dataset and generate multiple trajectories without biological meaning. \n\n#### What datasets should trajectory analysis be performed on \nTrajectory analysis should be performed only on single cell datasets where presence of cell developmental processes is expected. If slingshot is run on datasets where no such process is present, slingshot will still produce results, but generated results will have no biological meaning.\n\n**Explanation** : Each trajectory should represent one cell development process and clusters that belong to a single trajectory should represent different cell-development stages. Trajectories are generated based on clustering and between-cluster distances in PCA space. Trajectories consist of clusters that are closest to each other in PCA space. In the case of a dataset that contains a cell development process, clusters that represent consecutive cell stages are close to each other in PCA space and trajectories that consist of such clusters have biological meaning. In the case of a dataset containing unrelated cell types, trajectories will still be generated and will consist of clusters that are closest to each other in PCA space, but such trajectories will consist of unrelated cell types and will not have biological meaning.\n\n#### Dataset subsetting\nOften, a single dataset can contain multiple cell types but cell development happens only in certain cell types. In this case, all non-interesting cell types (clusters) should be removed prior to trajectory analysis. This should be done because slingshot will fit trajectories by using all clusters as explained in the last section. \nThere are two ways for dataset subsetting. The first way is selecting clusters to remove prior to analysis, by using the **Clusters to remove** input. This subsetting method should be used if a minority of dataset clusters should be removed. The second way is selecting clusters that will be used for trajectory analysis by using the **Clusters to keep** input setting; all the clusters except the selected ones will be dropped prior to trajectory analysis. Only one dataset subsetting method can be used; if both **Clusters to remove** and **Clusters to keep** are used at the same time, error will be raised.\n\n#### Selecting start and end clusters\nSlingshot trajectory accuracy is greatly improved when a cluster that represents starting cell population is provided to the **Start cluster** input. Providing a **Start cluster** is highly recommended since automatic detection of the starting cell population is not accurate.\n Additional improvement in performance can be achieved by also providing clusters denoting end cell population(s) by providing one or more cluster names to the **End clusters** input. Automatic detection of end point cell population is accurate.\n\n#### Setting up other slingshot parameters\nNumber of principal components used by slingshot can be set by using the **Number of principal components** input. Default value of 10 is sufficient for most datasets, while it is not recommended to use more than 30 principal components.\nResolution of fitted trajectories is determined by the **Slingshot approx points** input. Default value of 300 is sufficient for most dataset. If **Slingshot approx points** is set to 0, number of points in the trajectory curves will be equal to number of cells in the dataset. [5]\n\n#### Trajectory differential expression testing\nDifferential expression testing is performed using functions from the tradeSeq package. **Number of knots in GAM model** input can be used to set the number of knots in tradeSeq GAM model [3]. Default value of 5 knots is sufficient for most datasets.\n**Trade seq covariate name** can be used to add covariate to the model such as batch effect.  **Trade seq covariate name** can be any field from input SingleCellExperiment object colData.\ntradeSeq performs multiple statistical tests by using the fitted GAM model:\n - associationTest [4] is performed in order to discover genes whose expression is associated with a particular lineage.\n - startVsEndTest [4] is performed in order to discover marker genes of the progenitor/differentiated cell population. \n - In case of multiple lineages diffEndTest [4] is performed in order to discover genes whose expression differs between end points of different lineages.\n - In case of multiple lineages patternTest [4] is also performed to discover genes with different expression patterns along pseudo-time between different lineages.\n\n**Threshold log2fold change** input can be used to set the absolute value of the log2 fold-change cut-off to test against in all the tests. For example, setting **Threshold log2fold change** to 1 (=log2(2)) in case of  startVsEndTest will test which genes have a fold change that is significantly higher than 2 or significantly lower than 1/2 between lineage start and end point. Default value of  0.322 (=log2(1.25)) is set in order to eliminate genes with small expression changes.\n\nIn case when differential expression testing is repeated with different testing parameters (such as different log2fold threshold) but trajectory & GAM fitting parameters remain the same, previously fitted GAM model can be provided to **Trade seq fitted model** input. This will lower execution time since fitting a GAM model consumes a significant amount of time.\n\n#### Clustering genes based on expression patterns\nTop genes from tradeSeq statistical tests will be clustered based on normalized expression patterns in pseudo-time in all the lineages. Output clusters group genes with similar dynamics of normalized expression change in various lineages [4]. \n\nBy default, top genes from startVsEndTest, diffEndTest and patternTest are used for clustering while top genes from associationTest are omitted since associationTest can introduce noisy genes and reduce clustering quality. User can include associationTest top genes by setting **Use associationTest genes for clustering** input to TRUE. When associationTest top genes are used for clustering, we recommend setting  **Threshold log2fold change** to higher values (e.g. 1) in order to eliminate noisy genes.\n\nGene clustering step can be skipped by setting **Cluster genes by expression patterns** to FALSE.\n\n### Changes Introduced by Seven Bridges\nNo changes to the original libraries were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n - When running tradeSeq, fitting a GAM model is responsible for most of the tool execution time. Fitting a GAM model takes more time to execute when a higher number of trajectories are present.\n - Running slingshot with over-clustered data may result in multiple trajectories without biological meaning.\n - Fitting a GAM model for multiple lineages (>6) and a large number of cells(>12k) can consume significant amounts of RAM (>60GB) when running with default app parameters. Memory consumption can be lowered by reducing the number of tradeSeq cores but this would increase the execution time.\n\n### Performance benchmarking\ntradeSeq was executed on 4 cores (default value in the app).\n| # of cells | tradeSeq included  |  Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:----------:|:------------:|\n|       1.2k     |     No   |      2 min   |   $0.03    |    r5.2xlarge    |\n|       4.6k     |     No  |    2 min   |   $0.03    |    r5.2xlarge    |\n|       8.7k     |     No    |   4 min   |   $0.04    |    r5.2xlarge    |\n|       11.7k     |    No    |   5 min   |   $0.6    |    r5.2xlarge    |\n|       1.2k     |     Yes   |      14 min   |   $0.15    |    r5.2xlarge    |\n|       4.6k     |     Yes  |    55 min   |   $0.6    |    r5.2xlarge    |\n|       8.7k     |     Yes    |   1 h 42 min   |   $1.12    |    r5.2xlarge    |\n|       11.7k     |    Yes    |   2 h 19 min   |   $2.69    |    r5.4xlarge    |\n\n### Portability\n\n**Single cell  RNA-seq trajectory analysis with slingshot and tradeSeq**  was tested with cwltool 3.1.20220124184855.\n\n### References\n[1] [Slingshot bioconductor page](https://bioconductor.org/packages/release/bioc/html/slingshot.html)\n\n[2] [tradeSeq bioconductor page](https://www.bioconductor.org/packages/release/bioc/html/tradeSeq.html)\n\n[3] \n[tradeSeq GAM fitting vignette](https://www.bioconductor.org/packages/release/bioc/vignettes/tradeSeq/inst/doc/fitGAM.html)\n\n[4] [tradeSeq workflow vignette](https://statomics.github.io/tradeSeq/articles/tradeSeq.html)\n\n[5] [Slingshot vignette](https://bioconductor.org/packages/devel/bioc/vignettes/slingshot/inst/doc/vignette.html)", "input": [{"name": "Input single cell data"}, {"name": "Trade seq fitted model"}, {"name": "Clusters to remove"}, {"name": "Clusters to keep"}, {"name": "Start cluster"}, {"name": "End clusters"}, {"name": "Number of principal components"}, {"name": "Number of knots in GAM model"}, {"name": "Cluster genes by expression patterns"}, {"name": "Output name prefix"}, {"name": "Single cell experiment clustering name"}, {"name": "Threshold log2fold change"}, {"name": "tradeSeq number of cores"}, {"name": "Slingshot approx points"}, {"name": "Run tradeSeq"}, {"name": "Trade seq covariate name"}, {"name": "Use associationTest genes for clustering"}], "output": [{"name": "Trade seq fitted GAM"}, {"name": "Slingshot object"}, {"name": "Trajectory analysis report", "encodingFormat": "text/html"}, {"name": "Start vs end test results"}, {"name": "Diff end test results"}, {"name": "Pattern test results"}, {"name": "Association test results"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1646862286, "dateCreated": 1646862286, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sleuth-0-30-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sleuth-0-30-0/2", "applicationCategory": "CommandLineTool", "name": "Sleuth", "description": "**Sleuth** is a program for analysis of RNA-Seq experiments for which transcript abundances have been quantified with **Kallisto**. \n\n**Sleuth** performs RNA-seq differential analysis of gene expression data that utilizes bootstrapping in conjunction with response error linear modeling to decouple biological variance from inferential variance. It relies on variance decomposition to identify biological differences in transcript or gene expression, while using a standard strategy of shrinkage to stabilize variance estimates from few samples. \n\n**Sleuth** is an R package developed for accurate differential analysis of both isoform and gene levels, using quantification uncertainty estimates obtained via **Kallisto**. It provides a set of functions for importing data, performing exploratory analysis and finally testing for differential expression. Employing the standard workflow for this type of analysis on the Seven Bridges Platform, this CWL tool is a wrapper around the script presented in [1].\n\n**Sleuth** implements the likelihood ratio test (LRT). This statistical test performs a comparison of two nested models. One model is the full model, which includes all covariates, while the second model is the reduced model, without the covariate of interest. This test compares the likelihood of the full and reduced models, yielding the level of significance for the covariate of interest [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n- The main input file for **Sleuth** are transcript level abundance estimates, obtained from tools like **Kallisto** and **Salmon**. \n- To perform **Sleuth** analysis with **Kallisto** results, H5 files (as outputted by **Kallisto**) need to be provided as inputs. \n- To perform **Sleuth** analysis with **Salmon** results, TAR archives with complete **Salmon** results need to be provided as inputs. \n- In order to run **Sleuth** with **Salmon** data, additional preparation is needed with the **Wasabi** package, but this procedure is fully automated in the **Sleuth** wrapper on the Seven Bridges Platform. \n- All the quantification tools (**Salmon**, **Kallisto**) need to have been run with bootstrap options turned on, in order for **Sleuth** to perform differential expression successfully. This extra bootstrap information (and fragment length distribution information) needs to be present in a subdirectory of quantification results named **aux**. This is, in most cases, the default behaviour of the previously mentioned tools (on the Seven Bridges Platform, this **Sleuth** wrapper can successfully be used with other public wrappers of **Salmon** or **Kallisto**). \n\n- Phenotype information is required in order to fit the generalized linear model for each gene of the provided abundance estimates. The design of the experiment may include one or more covariates to explain the expression levels. Experiments with multiple variables influencing the outcomes can be analyzed using the design formula that includes the additional covariates, with the **Control variables** option. \n\nThere are two different ways to provide phenotype information:\n\n1. By including a CSV file (**Phenotype data** input) that contains a row for each sample, with Sample ID in the first column. These Sample IDs need to match those in input files **Sample ID** metadata. Also, a single-line header with variable names should be included.\n\n2. By indicating API keys for metadata fields that need to be included in the design. Phenotype information will then consist of variables you listed as **Covariate of interest** and **Control variables**.\n\nExample CSV content below:    \n```\nsample_id,batch_number,investigation.\nZR-75-1,3,CCLE-BRCA.\nUACC-812,4,CCLE-BRCA.\nHCC1806,3,CCLE-BRCA.\nHCC1500,4,CCLE-BRCA.\nCOLO829,3,CCLE-SKCM.\nCOLO792,4,CCLE-SKCM.\nCJM,3,CCLE-SKCM.\nA-375,4,CCLE-SKCM\n```\nSupplying a CSV like this, while inputting *condition* for the value of the **Covariate of interest** parameter and *library* in **Control variables**, will test for differential expression between treated and untreated samples, while controlling the effect of library preparation.\n\nThe information about samples belonging to treated or untreated groups can also be kept in the metadata. To use the metadata field for splitting the samples into groups for testing, enter its metadata key for the **Covariate of interest** parameter. All the input files need to have this metadata field populated. To control possible confounders, enter their API keys as **Control variables**.\n\n### Changes Introduced by Seven Bridges\n\n- Although the **Sleuth** package covers different hypothesis tests (Wald and Likelihood Ratio Test (LRT)), the user does not choose the type of test that will be performed. The tool always uses the LRT.\n- The analysis report contains the list of input parameters, phenotype data table, plot projections of samples onto the principal components for a data set, density plot of condition grouping, mean-variance relationship of transcripts plot, histogram of adjusted p-values and a short summary of the results.\n- As previously mentioned, **Salmon** results will automatically be preprocessed with the **Wassabi** package, as long as they are provided in the form of TAR archives (containing all outputs produced by **Salmon**). \n\n\n### Common Issues and Important Notes\n\n- If phenotype data is read from the metadata, any metadata key entered in the **Covariate of interest** or **Control variables** field needs to exist and its field needs to be populated in all the samples provided on the **Expression data** input. \n- If phenotype data is read from the provided CSV file - the **Covariate of interest** keys need to match the column names in the CSV\u2019s header. \n- Keep in mind that metadata keys are usually different to what is seen on the front-end. To match metadata keys to their corresponding values on the front-end please refer to [this table](https://docs.sevenbridges.com/docs/metadata-on-the-seven-bridges-platform). \n- To learn how to add a custom metadata field to expression data files refer to the [following document](https://docs.sevenbridges.com/docs/format-of-a-manifest-file#section-modifying-metadata-via-the-visual-interface).\n- The parameter **Select level of DE analysis** allows the user to choose between gene-level analysis and transcript-level analysis. The default is gene level.\n- If gene-level analysis is selected, then the **Sleuth** analysis can be run either for human or mouse samples. Depending on the species the samples are coming from, the appropriate **BioMart** database needs to be used for adding the information about the genes transcripts are associated with. The parameter **Select species** allows the user to choose between the two options (human and mouse) and **Sleuth** will use this input to collect the necessary information about genes from the corresponding **BioMart** database. The default value for this parameter is **human**. \n- Be careful when choosing covariates - generalized linear model fitting will fail if the model matrix is not a full rank matrix!\n\n\n\n### Performance Benchmarking\n\nBeing an R package for downstream analysis, **Sleuth** is not a computationally heavy tool (like aligners for example), and thus runs for a short period of time. Even for a couple dozen samples, the tool is expected to finish in under half an hour, costing less $0.20 on the default c4.2xlarge instance (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [Differential analysis of RNA-seq incorporating quantification uncertainty](https://www.nature.com/articles/nmeth.4324)    \n[2] [Sleuth vignette](https://pachterlab.github.io/sleuth/)", "input": [{"name": "Analysis title"}, {"name": "Abundances", "encodingFormat": "application/x-tar"}, {"name": "Covariate of interest"}, {"name": "FDR cutoff"}, {"name": "Phenotype data"}, {"name": "Number of CPUs"}, {"name": "Control variables"}, {"name": "Skip Sleuth execution"}, {"name": "Select species"}, {"name": "Select level of DE analysis"}], "output": [{"name": "HTML report", "encodingFormat": "text/html"}, {"name": "Sleuth analysis results"}, {"name": "RData files"}, {"name": "Normalized counts", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pachterlab/sleuth", "https://github.com/pachterlab/sleuth/releases/tag/v0.30.0"], "applicationSubCategory": ["Transcriptomics", "Differential Expression", "CWL1.0"], "project": "SBG Public Data", "creator": "Harold Pimentel, Nicolas Bray, Suzette Puente, P\u00e1ll Melsted, Lior Pachter", "softwareVersion": ["v1.0"], "dateModified": 1584705546, "dateCreated": 1582906180, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smart-variant-filtering/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smart-variant-filtering/13", "applicationCategory": "CommandLineTool", "name": "Smart Variant Filtering", "description": "The basic concept of Smart Variant Filtering (SVF) is to use features from existing Genome In A Bottle (GIAB) variant-called samples (HG001-HG005) to train the model and perform filtering (classification) of variants on the other samples. \n\n#Common use cases#\nSmart Variant Filtering tool can operate in 2 modes - training and filtering.\n## Training a model\nThe process of training a machine learning model (classifier) is performed using the python sklearn library. Tables with features (CHROM, POS, QD, MQ, FS, MQRankSum, ReadPosRankSum, SOR, dbSNPBuildID) together with correctness flag (TP or FP) are passed on input to train a model which will be used for filtering. Several different machine learning models are available: AdaBoost, K-Neighbors, Random Forest, Support Vector Machine, Quadratic Discriminant Analysis, Multi Layer Perceptron. The result of this mode is trained model saved to file for both SNVs and indels.\nCurrently supported classifiers and its parameters are:\n| Classifier | Parameter set                         |\n|------------|---------------------------------------|\n| ADA        | n_estimators, learning_rate,algorithm |\n| KNN        | neighbors,algorithms,p_distance       |\n| SVM        | C,kernels                             |\n| RF         | n_estimators, criterion               |\n| QD         | tol                                   |\n| MLP        | hidden_layer_sizes, activation,solver |\nSmart Variant Filtering tool receives on input configuration consisted of comma separated list of classifier that will be used for training and its required parameters (e.g. MLP,250,logistic,sgd).\n\n## Applying the filtering\nApply filter mode of Smart Variant Filtering tool receives trained model and raw VCF file and outputs filtered VCF with marked variants in Filter field with labels SVF_SNV and SVF_INDEL. In order to speed-up the process VCF can be divided by chromosome and for each part of VCF separate platform job can be created, after which all VCF parts are merged creating a filtered VCF. \n\n#Performance Benchmarking#\nIn order to find the best model and configuration, several available classification algorithms have been tested (AdaBoost, K-Neighbors, Random Forest, Support Vector Machine, Quadratic Discriminant Analysis, MLP) with total 300 different parameter configurations (hyper-parameter space testing). For every configuration cross-validation with 10 folds has been conducted. F-score was selected as a criteria for quality of the model\u2019s configuration and for 10 f-scores obtained for every parameter configuration mean and standard deviation are calculated. Due to intense computational demands the entire process of testing is done on Seven Bridges platform and only for whole exome samples for SNVs and indels. The differences in mean f-score and standard deviation between top parameter configurations are small. Obtained best configurations from the experiment are shown in the Table.\n**Type**|**Algorithm**|**Parameters**|**Stddev f-score**|**Mean f-score**\n:-----:|:-----:|:-----:|:-----:|:-----:\nIndels|MLP|250-logistic-sgd|0.0018|0.96466\nSNVs|MLP|500-tanh-adam|0.0001|0.99856\n\nUsing these model configurations leave one out testing (e.g. train model on HG001-HG004 and test on HG005) for 5 whole exome and 8 whole genome different library preparations has been performed: \n**SNP F-measure**|**Variant Filter**|**Raw**|**SVF MLP**\n:-----:|:-----:|:-----:|:-----:\nHG001|97.8426%|97.2234%|97.4174%\nHG002|99.6576%|99.6850%|99.7644%\nHG003|99.6161%|99.6251%|99.7205%\nHG004|99.6515%|99.7057%|99.7884%\nHG005|99.5745%|99.6502%|99.7151%\n\n**INDEL F-measure**|**Variant Filter**|**Raw**|**SVF MLP**\n:-----:|:-----:|:-----:|:-----:\nHG001|78.709%|73.954%|82.06%\nHG002|91.583%|90.390%|92.11%\nHG003|90.838%|89.394%|91.19%\nHG004|90.951%|89.699%|91.65%\nHG005|94.708%|93.976%|94.62%\n\n**SNP F-measure**|**VQSR**|**Raw**|**SVF MLP**\n:-----:|:-----:|:-----:|:-----:\nHG001-50x|99.8278%|99.8759%|99.9436%\nHG002-50x|99.8058%|99.8741%|99.9280%\nHG003-60x|99.8118%|99.8594%|99.9279%\nHG004-60x|99.5721%|99.8529%|99.9291%\nHG005-60x|99.7673%|99.8779%|99.9163%\nHG001-30x-PCR-FREE|99.6195%|99.8825%|99.8053%\nHG001-30x-Robot|99.4092%|99.7997%|99.8511%\nHG001-150x|99.6277%|99.7400%|99.8442%\n\n**SNP F-measure**|**VQSR**|**Raw**|**SVF MLP**\n:-----:|:-----:|:-----:|:-----:\nHG001-50x|99.8278%|99.8759%|99.9436%\nHG002-50x|99.8058%|99.8741%|99.9280%\nHG003-60x|99.8118%|99.8594%|99.9279%\nHG004-60x|99.5721%|99.8529%|99.9291%\nHG005-60x|99.7673%|99.8779%|99.9163%\nHG001-30x-PCR-FREE|99.6195%|99.8825%|99.8053%\nHG001-30x-Robot|99.4092%|99.7997%|99.8511%\nHG001-150x|99.6277%|99.7400%|99.8442%\n\nExecution times:\n**Mode**|**Number of variants**|**Configuration**|**Duration**|**Cost [$]**|**Instance**\n:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\nTrain|3 215 225|MLP,250,logistic,sgd|8 minutes|0.04|c4.2xlarge\nTrain|50 825|MLP,250,logistic,sgd|3 minutes|0.02|c4.2xlarge\nApply|4 822 453|Scatter|18 minutes|0.15|c4.8xlarge\nApply|4 822 453|No scatter|1 hour, 39 minutes|0.42|c4.2xlarge\n#Important notes#\nVCF file that will be filtered must contain in its info column values for following fields: CHROM, POS, QD, MQ, FS, MQRankSum, ReadPosRankSum, SOR, dbSNPBuildID or its minimal ordered subset defined my the number_of_features parameter. To filter VCF without this features please add them with [GATK Variant Annotator](https://sbgdev.atlassian.net/wiki/spaces/DOCS/pages/679870582/Smart+Variant+Filtering) \n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"perform_training_of_model\": True,\n        \"indel_model_or_table\": \"model_indel.sav\",\n        \"snv_model_or_table\": \"mode_snv.sav\"\n}\n# Creates draft task\ntask = api.tasks.create(name=\"Smart Variant Filtering - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).", "input": [{"name": "VCF to be filtered", "encodingFormat": "application/x-vcf"}, {"name": "Model for filtering indels or table to perform learning"}, {"name": "Model for filtering SNVs or table to perform learning"}, {"name": "Machine learning Algorithm for indels and its params"}, {"name": "Machine learning Algorithm for SNVs and its params"}, {"name": "Train a model"}, {"name": "Reserved memory for the job in MB"}, {"name": "List of SNV features"}, {"name": "Discard existing filters from VCF being filtered"}, {"name": "Keep variants from database"}, {"name": "List of indel features"}, {"name": "Number of threads"}], "output": [{"name": "Filtered VCF", "encodingFormat": "application/x-vcf"}, {"name": "Trained model indel"}, {"name": "Trained model SNV"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/sbg/smart-variant-filtering"], "applicationSubCategory": ["Variant Filtration"], "project": "SBG Public Data", "creator": "Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649159219, "dateCreated": 1522169879, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-annotate-0-2-8-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-annotate-0-2-8-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Smoove Annotate", "description": "**Smoove Annotate** annotates SV calls with SV quality and gene information from GFF3 files [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Annotate** takes a VCF file with SV calls (**Input variants**) and adds gene and SV quality annotations [1]. The source of gene annotations is the provided **Gene annotations GFF3** input.\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n\n### Common Issues and Important Notes\n\n* **Input variants** and **Gene annotations GFF3** inputs are required.\n\n\n### Performance Benchmarking\n\nTypical **Smoove Annotate** task durations are 3-5 minutes (< $0.10) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Annotate** was tested with cwltool version 3.1.20211107152837. The `in_variants` and `in_gff` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Gene annotations GFF3"}, {"name": "Output file name prefix"}], "output": [{"name": "Annotated SV calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648205548, "dateCreated": 1648205548, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-call-0-2-8-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-call-0-2-8-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Smoove Call", "description": "**Smoove Call** calls structural variants with Lumpy and optionally calls svtyper [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Call** takes alignments in BAM or CRAM format (**Input alignments**) and calls structural variation using `lumpy_filter` with additional optional read filters. Optionally, the output can be streamed on to svtyper for genotyping (**Genotype output with svtyper**, `--genotype`). It is recommended to supply a BED file with regions to exclude from the analysis (**BED with regions to exclude** input; `--exclude`).\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* Input parameter `--outdir` was hardcoded to the task working directory (`$PWD`) in the wrapper.\n* If the **Output file name prefix** (`--name`) input parameter is not provided, the outputs will be named based on the **Sample ID** metadata field value or (in its absence) the basename of the first provided **Input alignments** BAM file.\n\n### Common Issues and Important Notes\n\n* **Input alignments** input is required.\n* **Run duphold on the output** (`--duphold`) and **Remove PRPOS and PREND tags from INFO** (`--removepr`) parameters only work if **Genotype output with svtyper** (`--genotype`) is used.\n* if **Input alignments** are in CRAM format, **Reference sequence** (`--fasta`) input should be provided with the corresponding FAI and GZI indices.\n* During testing, task failures were observed when using **Run duphold on the output** (`--duphold`) input parameter on some samples, matching a [known issue](https://github.com/brentp/smoove/issues/132). Please consider running duphold separately if you encounter similar errors. \n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS BAM (48 GB) | 45 min | $0.30 + $0.12 | c4.2xlarge - 1024 GB EBS | \n| WGS BAM (48 GB) with genotyping | 46 min | $0.30 + $0.12 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Call** was tested with cwltool version 3.1.20211107152837. The `in_alignments` and `in_reference` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "BED with regions to exclude", "encodingFormat": "text/x-bed"}, {"name": "Exclude chromosomes"}, {"name": "Number of processes to parallelize"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "No extra filters"}, {"name": "Minimum support for reporting a variant"}, {"name": "Genotype output with svtyper"}, {"name": "Run duphold on the output"}, {"name": "Remove PRPOS and PREND tags from INFO"}], "output": [{"name": "SV calls", "encodingFormat": "application/x-vcf"}, {"name": "Genotyped SV calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648205549, "dateCreated": 1648205549, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-duphold-0-2-8-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-duphold-0-2-8-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Smoove Duphold", "description": "**Smoove Duphold** annotates SV calls [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Duphold** annotates SV calls in the **Input SV VCF** (`--vcf`) file based on information from the provided alignment files (**Input alignments**).\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If the **Output file name prefix** (`--outvcf`) input parameter is not provided, the outputs will be named based on the **Input SV VCF** file.\n\n### Common Issues and Important Notes\n\n* **Input alignments**, **Reference sequence** and **Input SV VCF** inputs are required.\n* If **Input alignments** are in CRAM format, please ensure that FAI and GZI indices are available for the **Reference sequence** input.\n\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS single-sample BAM (48 GB) | 16 min | $0.11 + $0.04 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Duphold** was tested with cwltool version 3.1.20211107152837. The `in_alignments`, `in_reference` and `in_variants` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Number of processes to parallelize"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Input SV VCF", "encodingFormat": "application/x-vcf"}, {"name": "SNPs/indels VCF file for allelic balance", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Duphold-annotated SV calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648205549, "dateCreated": 1648205549, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-genotype-0-2-8-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-genotype-0-2-8-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Smoove Genotype", "description": "**Smoove Genotype** runs svtyper in parallel on provided SV inputs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Genotype** takes alignment files **Input alignments** and a **VCF to genotype** and genotypes the provided SV calls by running svtyper. **VCF to genotype** can contain multiple samples (an example would be a VCF obtained using **Smoove Merge**).\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* Input parameter `--outdir` was hardcoded to the task working directory (`$PWD`) in the wrapper.\n* If the **Output file name prefix** (`--name`) input parameter is not provided, the outputs will be named based on the first **Input alignments** input.\n* Standard input is not supported as an option for **VCF to genotype** (`--vcf`) input parameter in this wrapper.\n\n### Common Issues and Important Notes\n\n* **Input alignments**, **Reference sequence** and **VCF to genotype** inputs are required.\n* Index files (BAI and CRAI for BAM and CRAM files, respectively) should be available for **Input alignments**.\n* If **Input alignments** are in CRAM format, please make sure that FAI and GZI indices are available for the **Reference sequence** input.\n* During testing, task failures were observed when using **Run duphold on the output** (`--duphold`) input parameter on some samples, matching a [known issue](https://github.com/brentp/smoove/issues/132). Please consider running duphold separately if you encounter similar errors. \n\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WGS single sample | 10 min | $0.07 + $0.03 | c4.2xlarge - 1024 GB EBS | \n| WGS single sample with duphold | 18 min | $0.12 + $0.05 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Genotype** was tested with cwltool version 3.1.20211107152837. The `in_alignments`, `in_reference` and `in_variants` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Number of processes to parallelize"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Run duphold on the output"}, {"name": "Remove PRPOS and PREND tags from INFO"}, {"name": "VCF to genotype", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Genotyped SV calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648205549, "dateCreated": 1648205549, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-merge-0-2-8-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-merge-0-2-8-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Smoove Merge", "description": "**Smoove Merge** merges and sorts SV calls from multiple samples [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Merge** merges SV calls from individual **Input variants** files with SV calls and sorts them using svtools [1].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* Input parameter `--outdir` was hardcoded to the task working directory (`$PWD`) in the wrapper.\n* If the **Output file name prefix** (`--name`) input parameter is not provided, the outputs will be named based on the basename of the first provided **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants** and **Reference sequence** inputs are required.\n\n\n### Performance Benchmarking\n\n**Smoove Merge** performance depends on the number and size of the provided **Input variants**. Merging a WGS trio takes 3 minutes (<$0.05) on an on-demand c4.2xlarge AWS instance. \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Merge** was tested with cwltool version 3.1.20211107152837. The `in_variants` and `in_reference` inputs were provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Output file name prefix"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Merged SV calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648205549, "dateCreated": 1648205548, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-paste-0-2-8-cwl1-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-paste-0-2-8-cwl1-2/4", "applicationCategory": "CommandLineTool", "name": "Smoove Paste", "description": "**Smoove Paste** gathers SV calls from multiple samples [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Paste** squares matching SV calls from individual **Input variants** files to a single joint file with final calls [1].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* Input parameter `--outdir` was hardcoded to the task working directory (`$PWD`) in the wrapper.\n* If the **Output file name prefix** (`--name`) input parameter is not provided, the outputs will be named based on the basename of the first provided **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants** and **Reference sequence** inputs are required.\n* Either TBI or CSI indices must be available for the **Input variants** files.\n* All **Input variants** files must have the same number of variants. If this is not the case, a possible solution is to run **Smoove Merge**, re-genotype each sample with **Smoove Genotype** and then obtain a joint genotyped VCF with **Smoove Paste**.\n\n\n### Performance Benchmarking\n\nMerging a WGS trio with **Smoove Paste** took 3 mins (<$0.05) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Paste** was tested with cwltool version 3.1.20211107152837. The `in_variants` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Output file name prefix"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Merged SV calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Structural Variant Calling", "VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648207094, "dateCreated": 1648205549, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/smoove-plot-counts-0-2-8-cwl1-2/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/smoove-plot-counts-0-2-8-cwl1-2/2", "applicationCategory": "CommandLineTool", "name": "Smoove Plot-counts", "description": "**Smoove Plot-counts** plots counts of split and discordant reads [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Smoove Plot-counts** takes a VCF file created by other **Smoove** tools (**Input variants**) and plots counts of split and discordant reads before and after filtering [1].\n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper: `--help`.\n* If the **Output file name prefix** (`--html`) input parameter is not provided, the outputs will be named based on the basename of the **Input variants** file.\n\n### Common Issues and Important Notes\n\n* **Input variants** input is required.\n\n\n### Performance Benchmarking\n\nTypical **Smoove Plot-counts** tasks take a few minutes (<$0.10) on an on-demand c4.2xlarge AWS instance.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**Smoove Plot-counts** was tested with cwltool version 3.1.20211107152837. The `in_variants` input was provided in the job.yaml/job.json file and used for testing. \n\n\n### References\n\n[1] [Smoove documentation](https://github.com/brentp/smoove)", "input": [{"name": "Output file name prefix"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Plot of counts", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/smoove", "https://github.com/brentp/smoove/releases/tag/v0.2.8", "https://github.com/brentp/smoove/blob/master/README.md"], "applicationSubCategory": ["Plotting", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.2"], "dateModified": 1648205548, "dateCreated": 1648205548, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sniffles-1-0-12b-cwl1-1/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sniffles-1-0-12b-cwl1-1/5", "applicationCategory": "CommandLineTool", "name": "Sniffles CWL1.1", "description": "**Sniffles** is a structural variation caller for PacBio or Oxford Nanopore data [1,2].\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**Sniffles** calls structural variation from sorted long read alignments (**Sorted alignments** input parameter). The tool accepts sorted output from **BWA MEM** (use -M and -x parameters), **Minimap2** (SAM file with Cigar & MD string) or **NGMLR**.\n\n### Changes Introduced by Seven Bridges\n\n* Input **Output file type** was added to specify the file type (VCF or BEDPE) of the output (corresponding to parameters `--vcf` and `--bedpe`).\n* Parameter `--tmp-file` was omitted from the wrapper as the current working directory is used by default.\n* Parameters `--genotype` and `--report_seq` were omitted from the wrapper as they are always enabled in this version of the tool.\n\n### Common Issues and Important Notes\n\n* Inputs **Sorted alignments** and **Output file type** are required.\n\n### Performance Benchmarking\n\n * [CHM13 ONT data](https://pubmed.ncbi.nlm.nih.gov/29431738/) (50\u00d7 coverage whole-genome sequencing dataset of the CHM13hTERT human cell line on the Oxford Nanopore GridION) - 140 GB FQ.GZ file, downloaded from: https://s3.amazonaws.com/nanopore-human-wgs/chm13/nanopore/rel2/rel2.fastq.gz, aligned to GRCh38 with **Minimap 2**.\n* [NA12878 ONT data - BAM aligned with NGLMR](http://labshare.cshl.edu/shares/schatzlab/www-data/fsedlaze/Sniffles/NA12878/ont/)\n\nBased on the observed duration of tasks, it is recommended to use 15 or fewer cores when running **Sniffles**, as increasing the number of cores further yielded no significant performance gains.\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| CHM13 - 8 cores | 8 h 21 min |$3.32 + $0.6 | c4.2xlarge 500 GB EBS | \n| CHM13 - 15 cores | 5 h 10 min |$7.9 + $0.36 | c5.9xlarge 500 GB EBS | \n| CHM13 - 30 cores | 5 h 17 min |$8.08 + $0.37 | c5.9xlarge 500 GB EBS | \n| NA12878 - 15 cores | 1 h 54 min |$1.29 + $0.28 | c5.4xlarge 1024 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**Sniffles** was tested with cwltool version 3.1.20211107152837. The `in_alignments` and `in_out_type` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Sniffles publication](https://www.nature.com/articles/s41592-018-0001-7)\n\n[2] [Sniffles documentation](https://github.com/fritzsedlazeck/Sniffles)", "input": [{"name": "Sorted alignments", "encodingFormat": "application/x-bam"}, {"name": "Output file name prefix"}, {"name": "Output file type"}, {"name": "Input VCF file to enable force calling", "encodingFormat": "application/x-vcf"}, {"name": "Minimum number of reads that support a SV"}, {"name": "Maximum number of splits per read"}, {"name": "Maximum distance to group SV together"}, {"name": "Number of threads to use"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Minimum length of SV to be reported"}, {"name": "Minimum mapping quality"}, {"name": "Number of supporting reads to report"}, {"name": "Minimum segment size"}, {"name": "Minimum zmws for PacBio reads"}, {"name": "Scan CS string instead of CIGAR and MD"}, {"name": "Phase SVs occuring on the same reads"}, {"name": "Minimum number of reads supporting SV clustering"}, {"name": "Allele frequency threshold"}, {"name": "Homozygous AF threshold"}, {"name": "Heterozygous AF threshold"}, {"name": "Use TRA instead of BND in VCF output"}, {"name": "Do not report indel sequences in VCF [beta]"}, {"name": "Ignore SD based filtering"}, {"name": "Preset CCS PacBio setting [beta]"}, {"name": "Report STR [alpha testing]"}, {"name": "Skip parameter estimation"}, {"name": "Estimated ratio of deletions per read"}, {"name": "Estimated ratio of insertions per read"}, {"name": "Maximum differences per 100 bp"}, {"name": "Maximum distance between alignment (indel) events"}], "output": [{"name": "Called variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/fritzsedlazeck/Sniffles", "https://github.com/fritzsedlazeck/Sniffles/tree/master/src", "https://github.com/fritzsedlazeck/Sniffles/releases", "https://github.com/fritzsedlazeck/Sniffles/blob/master/README.md"], "applicationSubCategory": ["Long Reads", "Structural Variant Calling", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Fritz Sedlazeck", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612306035, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpeff-4-3t-cwl1-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpeff-4-3t-cwl1-0/11", "applicationCategory": "CommandLineTool", "name": "SnpEff", "description": "**SnpEff** is a variant annotation and effect prediction\u200b tool, which annotates and predicts the effects of variants on genes, such as amino acid changes [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nTypical usage assumes predicted variants (SNPs, insertions, deletions, and MNPs) as input, usually in variant call format (VCF). **SnpEff** analyzes and annotates input variants and calculates the effects they produce on known genes [1]. The output file can be in several file formats, most common being VCF.\n\n**SnpEff** requires an annotation database to run. Official SnpEff annotation databases can be downloaded from [here](https://sourceforge.net/projects/snpeff/files/databases/v4_3/); however, human databases are also hosted on Seven Bridges, in Public Reference Files section (files snpEff_v4_3_GRCh38.86.zip and\tsnpEff_v4_3_GRCh37.75.zip) and can be [imported](https://docs.sevenbridges.com/docs/copy-files-using-the-visual-interface).\n\n### Changes Introduced by Seven Bridges\n\n* Input VCF file (**Input variants file**) is required (as opposed to default input being STDIN).\n* Parameter **Java memory requirement [MB]** which controls the amount of RAM available to **SnpEff** was included in the wrapper.\n* The following parameters have been excluded from the wrapper:  \n    * `-fileList` - Processing multiple files can be achieved by using batch tasks or scatter mode in workflows.\n    * `-dataDir <path>`  - In the wrapper, data directory is always the same and corresponds to the location of the prepared **SnpEff** database.\n    * `-download`  - Supplying a database archive as an input is required. Downloading missing data for a genome from command line is not supported.\n    * `-help`, `-quiet`, `-verbose`, `-debug` and `-version` - These options are not usually included in Seven Bridges wrappers.\n    * **Prefix** input parameter was added to allow setting custom prefixes for output file names.\n\n### Common Issues and Important Notes\n\n* Required inputs are **Input variants file** (a VCF or VCF.GZ file to be annotated), **SnpEff database file** (SnpEff database ZIP archive matching the major version of SnpEff used [2], which is 4.3 for this wrapper; e.g. snpEff_v4_3_GRCh38.86.zip or snpEff_v4_3_GRCh37.75.zip from Public Reference Files section), and **Assembly (genome version)**, which is a string representing genome version/assembly (e.g., GRCh38.86, GRCh37.75, hg19), matching the SnpEff database used (GRCh38.86 and GRCh37.75 should be used for the files in the Public Reference Files section).\n* As **SnpEff** is a java tool, it may be occasionally necessary to increase the amount of allocated RAM (default value: 8192 MB), using the **Java memory requirement [MB]** parameter.\n* A number of **SnpEff** command line options are designed in mutually exclusive pairs (for example `-noStats` and `-stats` or `-lof` and `-noLof`) with some redundancy. These options should not be used together, to avoid task failure.\n* Multithreading parameter **Use multiple threads (implies '-noStats')** (`-t`) will disable statistics. \n* If using VCFs with mitochondrial DNA marked as chrM with GRCh38 database, chromosome not found error can be addressed by renaming chrM to chrMT in the input VCF files, for example using sed: `sed \"s/^chrM/chrMT/g\" input.vcf > input_renamed.vcf`\n* Disabling statistics using **Do not create stats (summary) file** (`-noStats`) will in general speed-up execution.\n\n### Performance Benchmarking\n\nAnnotating NA12878 genome (GRCh38, ~220 Mb as VCF.GZ) with default annotation parameters, 1 CPU, and 8192 MB RAM took 25 minutes with a cost of $0.17 using on-demand default AWS instance.\nBy default, **SnpEff** is allocated 8192 MB of memory. Allocating less memory is not recommended when working with whole genome VCF files.\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n### Portability\n\n**SnpEff** was tested with cwltool version 3.1.20211107152837. The `in_variants`, `database` and `assembly` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [SnpEff documentation](http://snpeff.sourceforge.net/SnpEff_manual.html)\n\n[2] [Official SnpEff 4.3 databases download location](https://sourceforge.net/projects/snpeff/files/databases/v4_3/)", "input": [{"name": "Use HGVS annotations for amino acid sub-field"}, {"name": "Add loss of function (LOF) and nonsense mediated decay (NMD) tags"}, {"name": "Assembly (genome version)"}, {"name": "Perform 'cancer' comparisons (somatic vs. germline)"}, {"name": "Two column TXT file defining 'original and derived' samples", "encodingFormat": "text/plain"}, {"name": "Only use canonical transcripts"}, {"name": "Canon list file"}, {"name": "String to prepend to chromosome names"}, {"name": "Use old style annotations"}, {"name": "Override a config file option (name=value format)"}, {"name": "Configuration file"}, {"name": "Create CSV summary file alongside HTML"}, {"name": "SnpEff database file", "encodingFormat": "application/zip"}, {"name": "Only analyze changes that intersect with the intervals specified in this file (you may use this option many times)"}, {"name": "Use EFF field"}, {"name": "Use gene ID instead of gene name (VCF output)"}, {"name": "Use old HGVS notation"}, {"name": "Use one letter amino acid codes in HGVS"}, {"name": "Use transcript ID in HGVS"}, {"name": "Input format"}, {"name": "Annotate using interactions"}, {"name": "Use a custom intervals in TXT/BED/BigBed/VCF/GFF file (you may use this option many times)", "encodingFormat": "application/x-vcf"}, {"name": "Max TSL"}, {"name": "Annotate using motifs (requires Motif database)"}, {"name": "Annotate using NextProt (requires NextProt database)"}, {"name": "Disable IUB code expansion"}, {"name": "Do not show EffectType"}, {"name": "Do not show downstream changes"}, {"name": "Do not load any genomic database"}, {"name": "Do not add HGVS annotations"}, {"name": "Disable interaction annotations"}, {"name": "Do not show intergenic changes"}, {"name": "Do not show intron changes"}, {"name": "Do not add LOF and NMD annotations"}, {"name": "Disable motif annotations"}, {"name": "Disable NextProt annotations"}, {"name": "Do not shift variants according to HGVS"}, {"name": "Do not show upstream changes"}, {"name": "Do not show 5_PRIME_UTR or 3_PRIME_UTR changes"}, {"name": "Do not create stats (summary) file"}, {"name": "Add OICR tag in VCF file"}, {"name": "Only use transcripts from this file", "encodingFormat": "text/plain"}, {"name": "Only use protein coding transcripts"}, {"name": "Only use regulation tracks"}, {"name": "Output format"}, {"name": "Regulation track to use (this option can be used add several times)"}, {"name": "Use Sequence Ontology terms"}, {"name": "Set size for splice site region within exons"}, {"name": "Set minimum number of bases for splice site region within intron"}, {"name": "Set maximum number of bases for splice site region within intron"}, {"name": "Set size for splice sites (donor and acceptor) in bases"}, {"name": "Name of stats file (summary)"}, {"name": "Only use validated transcripts"}, {"name": "Use multiple threads (implies '-noStats')"}, {"name": "Java memory requirement [MB]"}, {"name": "Upstream downstream interval length"}, {"name": "Input variants file", "encodingFormat": "application/x-vcf"}, {"name": "Prefix"}, {"name": "Memory overhead per job [MB]"}], "output": [{"name": "SnpEff annotated file", "encodingFormat": "text/x-bed"}, {"name": "Summary file", "encodingFormat": "text/html"}, {"name": "Summary", "encodingFormat": "text/plain"}, {"name": "CSV summary file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1576240899, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpeff-download-4-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpeff-download-4-2/4", "applicationCategory": "CommandLineTool", "name": "SnpEff Download", "description": "SnpEff Download allows users to download (wget) the database for the desired organism and genome build.  The database for the download is passed as the URL from snpEff website.\n\n(genome version)(organism)(genome build).(database version)   e.g. GRCh37.75\n\nThe downloaded database is in .ZIP format. The SnpEff tool that requires this database is adjusted to handle ZIP archive as an input, so it is not necessary to unpack the database before feeding it to the SnpEff tool.\n\nA list of all supported databases can be found at this link: https://sourceforge.net/projects/snpeff/files .", "input": [{"name": "Genome URL"}, {"name": "Java Xmx%m requirement"}], "output": [{"name": "SnpEff Database", "encodingFormat": "application/zip"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Utilities"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648045661, "dateCreated": 1461762649, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-annotate-4-3/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-annotate-4-3/3", "applicationCategory": "CommandLineTool", "name": "SnpSift Annotate", "description": "**SnpSift Annotate** uses fields from another VCF file (such as [dbSNP](https://www.ncbi.nlm.nih.gov/projects/SNP/snp_summary.cgi), [1000 genomes](http://www.internationalgenome.org/data), [ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/), or [ExAC](http://exac.broadinstitute.org/downloads)) to annotate input VCF files [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**SnpSift Annotate** is typically used to annotate VCF ID and INFO fields from a \"database\" VCF file, such as dbSNP.\n\n### Changes Introduced by Seven Bridges\n\n* VCF file used as the annotation database (**Database for annotation**) should be compressed and tabix-indexed (to prepare the file, please use **Tabix Bgzip** and **Tabix Index** tools which can be found under Public Apps gallery).\n\n### Common Issues and Important Notes\n\n* Inputs **VCF file that will be annotated** and **Database for annotation** are required.\n* Note that **Database for annotation** has to be a compressed and tabix-indexed VCF file. \n* By default **SnpSift Annotate** adds **all** database INFO fields.\nYou can use the **Annotate using a list of INFO fields** (`-info`) parameter if you want to select only a subset of fields from the database VCF file and **Only annotate ID field** (`-id`) parameter if you only want to annotate using ID field (no INFO fields will be added) [1].\n* Parameters **VCF database is sorted and uncompressed** (`-sorted`) and **Max block size** (`-maxBlockSize <int>`) should be used together and applied only if the annotation database VCF file is uncompressed.\n\n### Performance Benchmarking\n\nAdding dbSNP (v150) annotations (all fields) to SnpEff-annotated NA12878 genome (GRCh38 ~3 GB as VCF) with 15 GB RAM took 2 h 23 minutes (price: $0.95 using on-demand default instance)\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n### References\n\n[1] [SnpSift Annotate documentation](http://snpeff.sourceforge.net/SnpSift.html#annotate)", "input": [{"name": "Java Xmx%m requirement"}, {"name": "Only annotate ID field"}, {"name": "Do not use REF and ALT fields"}, {"name": "Do not annotate ID field"}, {"name": "Annotate using a list of INFO fields"}, {"name": "String prefix for all annotated INFO fields"}, {"name": "VCF file that will be annotated", "encodingFormat": "application/x-vcf"}, {"name": "Annotate fields even if value is absent from the annotation database"}, {"name": "Annotate whether the variant exists in the database"}, {"name": "Max block size"}, {"name": "Do not annotate INFO fields"}, {"name": "VCF database is sorted and uncompressed"}, {"name": "Database type"}, {"name": "Database for annotation", "encodingFormat": "application/x-vcf"}, {"name": "VCF database is tabix-indexed"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648045661, "dateCreated": 1512131914, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-annotate-4-2/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-annotate-4-2/13", "applicationCategory": "CommandLineTool", "name": "SnpSift Annotate ", "description": "SnpSift Annotate uses fields from another VCF file, such as dbSnp, 1000 Genomes projects, ClinVar, or ExAC to annotate input VCF.\n\nThis tool is typically used to annotate ID and INFO fields from a \"database\" VCF file, such as dbSNP.\n\nNote that the database for annotation used has to be VCF file that is compressed and Tabix indexed.\n\nBy default it adds ALL database INFO fields.\nYou can use the '-info' command line option if you only want select only a subset of fields from db.vcf file.\nYou can use the '-id' command line option if you only want to add ID fields (no INFO fields will be added).", "input": [{"name": "Java Xmx%m requirement"}, {"name": "Only annotate ID field"}, {"name": "Do not use REF and ALT fields"}, {"name": "Do not annotate ID field"}, {"name": "Annotate using a list of INFO fields"}, {"name": "Prepend 'str' to all annotated INFO fields"}, {"name": "VCF file that will be annotated", "encodingFormat": "application/x-vcf"}, {"name": "Annotate fields even without database"}, {"name": "Annotate whether the variant exists"}, {"name": "Max block size"}, {"name": "Do not annotate INFO fields"}, {"name": "VCF Database is sorted"}, {"name": "Database type"}, {"name": "Database for annotation", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648045661, "dateCreated": 1461762648, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-annotate-4-3t-cwl1-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-annotate-4-3t-cwl1-0/8", "applicationCategory": "CommandLineTool", "name": "SnpSift Annotate - CWL 1.0", "description": "**SnpSift Annotate** annotates VCF files [1].\n\nThe tool uses VCF files for annotation, such as [dbSNP](https://www.ncbi.nlm.nih.gov/projects/SNP/snp_summary.cgi), [1000 genomes](http://www.internationalgenome.org/data), [ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/), or [ExAC](http://exac.broadinstitute.org/downloads).\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**SnpSift Annotate** is typically used to annotate VCF ID and INFO fields from a \"database\" VCF file, such as dbSNP.\n\n### Changes Introduced by Seven Bridges\n\n* VCF file used as the annotation database (**Database for annotation**) should be compressed and tabix-indexed (to prepare the file, please use **Tabix Bgzip** and **Tabix Index** tools which can be found under Public Apps gallery).\n\n### Common Issues and Important Notes\n\n* Inputs **VCF file that will be annotated** and **Database for annotation** are required.\n* Note that **Database for annotation** has to be a compressed and tabix-indexed VCF file.\n* By default **SnpSift Annotate** adds **all** database INFO fields.\nYou can use the **Annotate using a list of INFO fields** (`-info`) parameter if you want to select only a subset of fields from the database VCF file and **Only annotate ID field** (`-id`) parameter if you only want to annotate using ID field (no INFO fields will be added) [1].\n* Parameters **VCF database is sorted and uncompressed** (`-sorted`) and **Max block size** (`-maxBlockSize <int>`) should be used together and applied only if the annotation database VCF file is uncompressed.\n\n### Performance Benchmarking\n\nAdding dbSNP (v150) annotations (all fields) to SnpEff-annotated NA12878 genome (GRCh38 ~3 GB as VCF) with 15 GB RAM took 2 h 23 minutes (price: $0.95 using on-demand default AWS instance).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n### Portability\n\n**SnpSift Annotate** was tested with cwltool version 3.1.20211107152837. The `in_variants` and `database_for_annotation` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [SnpSift Annotate documentation](http://snpeff.sourceforge.net/SnpSift.html#annotate)", "input": [{"name": "String prefix for all annotated INFO fields"}, {"name": "Database for annotation", "encodingFormat": "application/x-vcf"}, {"name": "Database type"}, {"name": "Do not annotate INFO fields"}, {"name": "Annotate fields even if value is absent from the annotation database"}, {"name": "Only annotate ID field"}, {"name": "Annotate using a list of INFO fields"}, {"name": "VCF file that will be annotated", "encodingFormat": "application/x-vcf"}, {"name": "Max block size"}, {"name": "Java Xmx%m requirement"}, {"name": "Do not annotate ID field"}, {"name": "Do not use REF and ALT fields"}, {"name": "VCF database is tabix-indexed"}, {"name": "Annotate whether the variant exists in the database"}, {"name": "VCF database is sorted and uncompressed"}, {"name": "Memory overhead per job [MB]"}], "output": [{"name": "Annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1576240899, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-dbnsfp-4-3/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-dbnsfp-4-3/3", "applicationCategory": "CommandLineTool", "name": "SnpSift dbNSFP", "description": "**SnpSift dbNSFP** allows annotation with [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP) [1], an integrated database of functional predictions from multiple algorithms (SIFT, Polyphen2, LRT and MutationTaster, PhyloP and GERP++, etc.) [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nAdding **dbNSFP** annotations to VCF files.\n\n### Changes Introduced by Seven Bridges\n\n* To adapt the tool to newer versions of **dbNSFP** (currently 3.5), which have changed some of the fields (column headers) available for annotation, default list of annotation fields used by the tool has been somewhat updated to exclude fields no longer found in the database. Currently used default annotation fields list consists of the following fields:   \n  \n  `Interpro_domain,SIFT_pred, LRT_pred,MutationTaster_pred, GERP++_NR,GERP++_RS, phastCons100way_vertebrate, MutationAssessor_pred, FATHMM_pred, PROVEAN_pred, MetaSVM_pred, 1000Gp3_AC, 1000Gp3_AF, 1000Gp3_AFR_AC, 1000Gp3_AFR_AF, 1000Gp3_EUR_AC, 1000Gp3_EUR_AF, 1000Gp3_AMR_AC, 1000Gp3_AMR_AF, 1000Gp3_EAS_AC, 1000Gp3_EAS_AF, 1000Gp3_SAS_AC, 1000Gp3_SAS_AF, ESP6500_AA_AC, ESP6500_AA_AF, ESP6500_EA_AC, ESP6500_EA_AF, ExAC_AC, ExAC_AF, ExAC_Adj_AC, ExAC_Adj_AF, ExAC_AFR_AC, ExAC_AFR_AF, ExAC_AMR_AC, ExAC_AMR_AF, ExAC_EAS_AC, ExAC_EAS_AF, ExAC_FIN_AC, ExAC_FIN_AF, ExAC_NFE_AC, ExAC_NFE_AF, ExAC_SAS_AC, ExAC_SAS_AF`.\n\nWhen using dbNSFP 2.x files, the above list is modified to exclude `ESP6500_AA_AC,ESP6500_EA_AC` and replace 1000 genomes phase 3 fields with 1000 genomes phase 1 allele frequency fields found in dbNSFP 2.9.x files.\n\n### Common Issues and Important Notes\n\n* Please note that **dbNSFP** database file used for annotation (input **Database for annotation**) should be preprocessed with bgzip and tabix before use. **dbNSFP** database files hosted on the Seven Bridges platform in the Public Reference Files section (dbNSFP_3.3c.gz and dbNSFP2.9.3.txt.gz) and older versions of the database available from SnpSift website links [2] have already been preprocessed. If you wish to preprocess your database files locally, this is one possible way (assumes a unix-based workstation) [3]:\n\n    `unzip dbNSFPv3.x.zip`\n\n    `head -n1 dbNSFP3.x_variant.chr1 ` > ` h`\n\n    `cat dbNSFP3.x_variant.chr* | grep -v ^#chr | sort -k1,1 -k2,2n - | cat h - | bgzip -c ` > ` dbNSFP_3.x.gz`\n\n    `tabix -s 1 -b 2 -e 2 dbNSFP_3.x.gz`\n\n* Please make sure that the **dbNSFP** version used matches the genome assembly version of your data (3.x for GRCh38 and 2.x for GRCh37).\n* Seven Bridges hosts **dbNSFP** files available for commercial use (dbNSFP_3.3c.gz and dbNSFP2.9.3.txt.gz), however please note that additional annotations are available to researchers in academic versions of **dbNSFP** database available on the **dbNSFP** website [1].\n\n### Performance Benchmarking\n\nAnnotating NA12878 genome (GRCh38, ~3 GB as VCF.GZ) with default **dbNSFP** (v3.5c) annotation fields and 8 GB RAM  took 42 minutes on the default instance (price: $0.28 using on-demand instance).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n\n### References\n\n[1] [dbNSFP website](https://sites.google.com/site/jpopgen/dbNSFP)\n\n[2] [SnpSift dbNSFP documentation](http://snpeff.sourceforge.net/SnpSift.html#dbNSFP)\n\n[3] [Ensembl VEP dbNSFP plugin documentation](https://github.com/Ensembl/VEP_plugins/blob/release/90/dbNSFP.pm)", "input": [{"name": "RAM requirement in MB"}, {"name": "Database for annotation", "encodingFormat": "application/x-vcf"}, {"name": "Annotate fields, even if the database has an empty value"}, {"name": "Colapse repeated values"}, {"name": "VCF file that will be annotated", "encodingFormat": "application/x-vcf"}, {"name": "Database fields to add"}, {"name": "Genome version"}, {"name": "Annotate missing entries (with '.')"}, {"name": "Invert 'fields to add' selection"}, {"name": "dbNSFP version"}], "output": [{"name": "DBNSFP annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648045482, "dateCreated": 1512131915, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-dbnsfp-4-3t-cwl-1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-dbnsfp-4-3t-cwl-1-0/7", "applicationCategory": "CommandLineTool", "name": "SnpSift dbNSFP - CWL 1.0", "description": "**SnpSift dbNSFP** allows annotation with [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP) [1], an integrated database of functional predictions from multiple algorithms (SIFT, Polyphen2, LRT and MutationTaster, PhyloP and GERP++, etc.) [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nAdding **dbNSFP** annotations to VCF files.\n\n### Changes Introduced by Seven Bridges\n\n* To adapt the tool to newer versions of **dbNSFP** (such as 3.5), which have changed some of the fields (column headers) available for annotation, default list of annotation fields used by the tool has been somewhat updated to exclude fields no longer found in the database. Currently used default annotation fields list consists of the following fields:   \n  \n  `Interpro_domain,SIFT_pred, LRT_pred,MutationTaster_pred, GERP++_NR,GERP++_RS, phastCons100way_vertebrate, MutationAssessor_pred, FATHMM_pred, PROVEAN_pred, MetaSVM_pred, 1000Gp3_AC, 1000Gp3_AF, 1000Gp3_AFR_AC, 1000Gp3_AFR_AF, 1000Gp3_EUR_AC, 1000Gp3_EUR_AF, 1000Gp3_AMR_AC, 1000Gp3_AMR_AF, 1000Gp3_EAS_AC, 1000Gp3_EAS_AF, 1000Gp3_SAS_AC, 1000Gp3_SAS_AF, ESP6500_AA_AC, ESP6500_AA_AF, ESP6500_EA_AC, ESP6500_EA_AF, ExAC_AC, ExAC_AF, ExAC_Adj_AC, ExAC_Adj_AF, ExAC_AFR_AC, ExAC_AFR_AF, ExAC_AMR_AC, ExAC_AMR_AF, ExAC_EAS_AC, ExAC_EAS_AF, ExAC_FIN_AC, ExAC_FIN_AF, ExAC_NFE_AC, ExAC_NFE_AF, ExAC_SAS_AC, ExAC_SAS_AF`.\n\nWhen using dbNSFP 2.x files, the above list is modified to exclude `ESP6500_AA_AC,ESP6500_EA_AC` and replace 1000 genomes phase 3 fields with 1000 genomes phase 1 allele frequency fields found in dbNSFP 2.9.x files.\n\n### Common Issues and Important Notes\n\n* Please note that **dbNSFP** database file used for annotation (input **Database for annotation**) should be preprocessed with bgzip and tabix before use. **dbNSFP** database files hosted on the Seven Bridges platform in the Public Reference Files section (dbNSFP_3.3c.gz and dbNSFP2.9.3.txt.gz) and older versions of the database available from SnpSift website links [2] have already been preprocessed. If you wish to preprocess your database files locally, this is one possible way (assumes a unix-based workstation) [3]:\n\n    `unzip dbNSFPv3.x.zip`\n\n    `head -n1 dbNSFP3.x_variant.chr1 ` > ` h`\n\n    `cat dbNSFP3.x_variant.chr* | grep -v ^#chr | sort -k1,1 -k2,2n - | cat h - | bgzip -c ` > ` dbNSFP_3.x.gz`\n\n    `tabix -s 1 -b 2 -e 2 dbNSFP_3.x.gz`\n\n* Please make sure that the **dbNSFP** version used matches the genome assembly version of your data (3.x for GRCh38 and 2.x for GRCh37). If using 4.x dbNSFP files, please ensure that they have been preprocessed for your genome assembly version of interest.\n* Seven Bridges hosts **dbNSFP** files available for commercial use (dbNSFP_3.3c.gz and dbNSFP2.9.3.txt.gz), however please note that additional annotations are available to researchers in academic versions of **dbNSFP** database available on the **dbNSFP** website [1].\n\n### Performance Benchmarking\n\nAnnotating NA12878 genome (GRCh38, ~100 MB as VCF.GZ) with default **dbNSFP** (v3.5c) annotation fields and 8 GB RAM  took 42 minutes on the default AWS instance (price: $0.28 using an on-demand instance).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n### Portability\n\n**SnpSift dbNSFP** was tested with cwltool version 3.1.20211107152837. The `in_variants`, `database` and `dbnsfp_version` inputs were provided in the job.yaml/job.json file and used for testing. Cwltool command line parameters `--no-read-only` and `--no-match-user` were required for successful tool executions.\n\n### References\n\n[1] [dbNSFP website](https://sites.google.com/site/jpopgen/dbNSFP)\n\n[2] [SnpSift dbNSFP documentation](http://snpeff.sourceforge.net/SnpSift.html#dbNSFP)\n\n[3] [Ensembl VEP dbNSFP plugin documentation](https://github.com/Ensembl/VEP_plugins/blob/release/90/dbNSFP.pm)", "input": [{"name": "Database fields to add"}, {"name": "Annotate missing entries (with '.')"}, {"name": "Colapse repeated values"}, {"name": "Database for annotation", "encodingFormat": "application/x-vcf"}, {"name": "dbNSFP version"}, {"name": "Annotate fields, even if the database has an empty value"}, {"name": "Genome version"}, {"name": "VCF file that will be annotated", "encodingFormat": "application/x-vcf"}, {"name": "Invert 'fields to add' selection"}, {"name": "RAM requirement in MB"}, {"name": "Memory overhead per job [MB]"}], "output": [{"name": "DBNSFP annotated VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045481, "dateCreated": 1576240898, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-filter-4-3/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-filter-4-3/3", "applicationCategory": "CommandLineTool", "name": "SnpSift Filter", "description": "**SnpSift Filter** filters SnpEff-annotated VCF files using arbitrary expressions, for instance \"(QUAL > 30) | (exists INDEL) | ( countHet() > 2 )\". The actual expressions can be quite complex, allowing for a lot of flexibility [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nThe **SnpSift Filter** tool is used for filtering SnpEff-annotated VCF files to extract variants of interest.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n* Parameter **Filter expression**: Wrapping quotes (\"\") are added automatically by the wrapper and should not be included in the filter expression. For example, filtering expression: \"( CHROM = 'chr1' )\" should be written as `( CHROM = 'chr1' )`.\n* **SnpSift Filter** filtering syntax allows for creation of complex, flexible filters with different operators and functions. Please check the [official documentation](http://snpeff.sourceforge.net/SnpSift.html#filter) for examples of filter syntax and valid filtering fields before using the tool.\n\n### Performance Benchmarking\n\nTypical runs take <10 minutes and cost <$0.1 on an on-demand default instance.\n\n### References\n\n[1] [SnpSift Filter documentation](http://snpeff.sourceforge.net/SnpSift.html#filter)", "input": [{"name": "Memory to use for the task [Mb]"}, {"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Filter expression (without quotes)"}, {"name": "Add a string to FILTER field"}, {"name": "Expression file"}, {"name": "Filter ID"}, {"name": "Inverse"}, {"name": "Pass"}, {"name": "Remove string from FILTER field"}, {"name": "Set File"}, {"name": "Error on missing fields"}, {"name": "Format"}, {"name": "Galaxy"}], "output": [{"name": "Filtered VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Variant Filtration", "VCF Processing"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institue", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648040158, "dateCreated": 1512131915, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/snpsift-filter-4-3t-cwl-1-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/snpsift-filter-4-3t-cwl-1-0/6", "applicationCategory": "CommandLineTool", "name": "SnpSift Filter - CWL 1.0", "description": "**SnpSift Filter** filters SnpEff-annotated VCF files using arbitrary expressions.\n An example expression would be \"(QUAL > 30) | (exists INDEL) | ( countHet() > 2 )\". The actual expressions can be quite complex, allowing for a lot of flexibility [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\nThe **SnpSift Filter** tool is used for filtering SnpEff-annotated VCF files to extract variants of interest.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made. \n\n### Common Issues and Important Notes\n\n* Parameter **Filter expression**: Wrapping quotes (\"\") are added automatically by the wrapper and should not be included in the filter expression. For example, filtering expression: \"( CHROM = 'chr1' )\" should be written as `( CHROM = 'chr1' )`.\n* **SnpSift Filter** filtering syntax allows for creation of complex, flexible filters with different operators and functions. Please check the [official documentation](http://snpeff.sourceforge.net/SnpSift.html#filter) for examples of filter syntax and valid filtering fields before using the tool.\n\n### Performance Benchmarking\n\nTypical runs take <10 minutes and cost <$0.1 on an on-demand default AWS instance.\n\n### Portability\n\n**SnpSift Filter** was tested with cwltool version 3.1.20211107152837. The `in_variants` and `database` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [SnpSift Filter documentation](http://snpeff.sourceforge.net/SnpSift.html#filter)", "input": [{"name": "Add a string to FILTER field"}, {"name": "Error on missing fields"}, {"name": "Expression file"}, {"name": "Filter expression (without quotes)"}, {"name": "Filter ID"}, {"name": "Format"}, {"name": "Galaxy"}, {"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Inverse"}, {"name": "Memory to use for the task [MB]"}, {"name": "Pass"}, {"name": "Remove string from FILTER field"}, {"name": "Set File"}, {"name": "Memory overhead per job [MB]"}], "output": [{"name": "Filtered VCF", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/pcingola/SnpEff"], "applicationSubCategory": ["Variant Filtration", "VCF Processing", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Pablo Cingolani/Broad Institute", "softwareVersion": ["v1.0"], "dateModified": 1648045482, "dateCreated": 1576240899, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-v1-0-5-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-v1-0-5-0/5", "applicationCategory": "CommandLineTool", "name": "SomaticSniper", "description": "The purpose of this program is to identify single nucleotide positions that are different between tumor and normal (or, in theory, any two bam files). \nIt takes a tumor bam and a normal bam and compares the two to determine the differences. It outputs a file in a format very similar to Samtools consensus format.\nIt uses the genotype likelihood model of MAQ (as implemented in Samtools)and then calculates the probability that the tumor and normal genotypes are different. \nThis probability is reported as a somatic sore. The somatic sore is the Phred-scaled probability (between 0 to 255) that the Tumor and Normal genotypes \nare not different where 0 means there is no probability that the genotypes are different and 255 means there is a probability of 1 \u2212 10(255/\u221210) that\nthe genotypes are different between tumor and normal. This is consistent with\nhow the SAM format reports such probabilities.\nThere are two modes, the joint genotyping mode (-J) takes into account the fact that the tumor and normal samples are not entirely independent and also\ntakes into account the prior probability of a somatic mutation. This probability can be scaled to control the sensitivity of the algorithm. An accurate value for\nthis prior would be 0.000001, but this may result in a severe lack of sensitivity at lower depths. A less realistic prior probability will generate more sensitive\nresults at the expense of an increase in the number of false positives. To get a similar sensitivity to the default mode, authors of the tool recommend using a prior of 0.01.\nThe default mode treats the two samples as if they came from two different individuals. This mode uses a less accurate mathematical model, but yields good results, \nespecially if the normal may contain some tumor cells or the tumor is quite impure.", "input": [{"name": "Select output format"}, {"name": "Prior of a difference between two haplotypes"}, {"name": "Number of haplotypes"}, {"name": "Theta in maq consensus"}, {"name": "Prior probability of a somatic mutation"}, {"name": "Use prior probabilities"}, {"name": "disable priors in the somatic calculation"}, {"name": "Do not report Gain of Reference"}, {"name": "Do not report LOH variants"}, {"name": "Filtering somatic SNVs"}, {"name": "Filtering reads"}, {"name": "Normal BAM", "encodingFormat": "application/x-bam"}, {"name": "Tumor BAM", "encodingFormat": "application/x-bam"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Somatic variants file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/somatic-sniper", "https://github.com/genome/somatic-sniper/archive/v1.0.5.0.tar.gz"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649347464, "dateCreated": 1453799634, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-bam-readcount-v1-0-5-0/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-bam-readcount-v1-0-5-0/4", "applicationCategory": "CommandLineTool", "name": "SomaticSniper BAM Readcount", "description": "The purpose of this program is to generate metrics at single nucleotide positions.\nThere are number of metrics generated which can be useful for filtering out false positive calls.", "input": [{"name": "Minimum mapping quality"}, {"name": "Minimum base quality"}, {"name": "Max depth to avoid excessive memory usage"}, {"name": "File containing a list of regions to report readcounts within", "encodingFormat": "text/plain"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Report results by library."}, {"name": "Input BAM", "encodingFormat": "application/x-bam"}, {"name": "Maximum number of warnings"}, {"name": "Generate INDEL centric readcounts"}], "output": [{"name": "Readcount output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/bam-readcount/blob/master/README.textile", "https://github.com/genome/bam-readcount"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649330216, "dateCreated": 1453800035, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-fp-filter-v1-0-5-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-fp-filter-v1-0-5-0/2", "applicationCategory": "CommandLineTool", "name": "SomaticSniper FP Filter", "description": "This program will filter bam-somaticsniper output with a variety of filters as detailed in the VarScan2 paper (http://www.ncbi.nlm.nih.gov/pubmed/22300766). It requires the bam-readcount utility (https://github.com/genome/bam-readcount). This is more convenient than the filtering described in the SomaticSniper paper, but more heuristic. Regardless, the principles are similar and we observe ~5% false negative rate with this filter.\n\nThis filter was calibrated on 100bp PE Illumina reads. It is likely to be overly stringent for longer reads and may be less effective on shorter reads.", "input": [{"name": "Filtered bam-somaticsniper output VCF file", "encodingFormat": "application/x-vcf"}, {"name": "the bam-readcount output", "encodingFormat": "text/plain"}], "output": [{"name": "Mutations that have passed the filter", "encodingFormat": "application/x-vcf"}, {"name": "Mutations that have failed to pass the filter", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/somatic-sniper", "https://github.com/genome/somatic-sniper/archive/v1.0.5.0.tar.gz"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649347464, "dateCreated": 1453799667, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-hc-filter-v1-0-5-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-hc-filter-v1-0-5-0/2", "applicationCategory": "CommandLineTool", "name": "SomaticSniper HC Filter", "description": "This program will filter bam-somaticsniper output into a high confidence set based on mapping quality and somatic score.", "input": [{"name": "The input bam-somaticsniper output file", "encodingFormat": "application/x-vcf"}, {"name": "Min mapping quality"}, {"name": "Minimum somatic score"}], "output": [{"name": "High confidence somatic mutations", "encodingFormat": "application/x-vcf"}, {"name": "Low quality mutations", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/somatic-sniper", "https://github.com/genome/somatic-sniper/archive/v1.0.5.0.tar.gz"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649347465, "dateCreated": 1453799227, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-prepare-for-bam-readcount-v1-0-5-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-prepare-for-bam-readcount-v1-0-5-0/2", "applicationCategory": "CommandLineTool", "name": "SomaticSniper Prepare for BAM Readcount", "description": "Filtered bam-somaticsniper output VCF file. This file can be generated using the \"SNP Filter for SomaticSniper\" tool. Output VCF file is prepared for bam-readcount's -l option.", "input": [{"name": "Filtered bam-somaticsniper output VCF file", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Prepared VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/somatic-sniper", "https://github.com/genome/somatic-sniper/archive/v1.0.5.0.tar.gz"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649347464, "dateCreated": 1453800041, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/somaticsniper-snp-filter-v1-0-5-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/somaticsniper-snp-filter-v1-0-5-0/2", "applicationCategory": "CommandLineTool", "name": "SomaticSniper SNP Filter", "description": "This tool will filter SomaticSniper output VCF with some basic filters inspired by maq.pl SNPfilter.", "input": [{"name": "The input bam-somaticsniper output file", "encodingFormat": "application/x-vcf"}, {"name": "Minimum mapping quality of the reads"}, {"name": "Minimum consensus quality"}, {"name": "Minimum read depth to call a SNP"}, {"name": "Maximum read depth to call a SNP"}, {"name": "Window size for filtering dense SNPs."}, {"name": "Maximum number of SNPs in a sized window"}, {"name": "Check minimum SNP quality"}, {"name": "INDEL File"}, {"name": "Window size of INDEL position"}, {"name": "Minimum samtools INDEL score"}, {"name": "Filter out homozygous ref calls"}], "output": [{"name": "Low quality SNPs", "encodingFormat": "application/x-vcf"}, {"name": "Filtered VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/genome/somatic-sniper", "https://github.com/genome/somatic-sniper/archive/v1.0.5.0.tar.gz"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "The Genome Institute at Washington University School of Medicine", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649347464, "dateCreated": 1453799750, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/spp-1-14/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/spp-1-14/2", "applicationCategory": "CommandLineTool", "name": "SPP 1.14", "description": "**SPP** is an R package for identifying peaks for TF binding sites from ChIP-seq data.**SPP** is an R package for identifying peaks for TF binding sites from ChIP-seq data.\nSPP version 1.14 is a modified version of the original SPP peak-caller package.\nSPP is suggested peak caller for analysing binding sites of transcription factors such as (CTCF, OCT2, SOX4, NANOG, P30, GATA3, CEBP, etc.).\n\nThe app allows the user to choose between two distributed R scripts: run\\_spp.R and run\\_spp\\_nodups.R (option run\\_spp\\_nodups can be set to TRUE or FALSE).\nThe option -outdir is fixed to \".\" to allow the files to be saved into the correct output directory and mapped to output ports. \nThe option -rf is also present to prevent SPP to abort in case output files already exist.\n\n**Outputs:**\n\nThe analysis results consist of the following files:\n\n* ***\\_SPPpeaks.narrowPeak**: a BED6+4 format file which contains the peak locations together with  pvalue and qvalue. This output contains fixed width peaks.\n\n* ***\\_SPPpeaks.regionPeak**: a BED6+4 format file which contains the peak locations together with  pvalue and qvalue.  This output contains variable width peaks with regions of enrichment around peak summits.\n\n* ***\\_SPPmodel.Rdata**: An Rdata object which you can use to access the model and output results produced by SPP. \n\n* ***\\_SPPxcorplot.pdf**: The cross-correlation of stranded read density profiles plot saved in a PDF file.\nSPP version 1.14 is a modified version of the original SPP peak-caller package.\n\nThe app allows the user to choose between two distributed R scripts: run\\_spp.R and run\\_spp\\_nodups.R (option run\\_spp\\_nodups can be set to TRUE or FALSE).\nThe option -outdir is fixed to \".\" to allow the files to be saved into the correct output directory and mapped to output ports. \nThe option -rf is also present to prevent SPP to abort in case output files already exist.\n\n**Outputs:**\n\nThe analysis results consist of the following files:\n\n* ***\\_SPPpeaks.narrowPeak**: a BED6+4 format file which contains the peak locations together with  pvalue and qvalue. This output contains fixed width peaks.\n\n* ***\\_SPPpeaks.regionPeak**: a BED6+4 format file which contains the peak locations together with  pvalue and qvalue.  This output contains variable width peaks with regions of enrichment around peak summits.\n\n* ***\\_SPPmodel.Rdata**: An Rdata object which you can use to access the model and output results produced by SPP. \n\n* ***\\_SPPxcorplot.pdf**: The cross-correlation of stranded read density profiles plot saved in a PDF file.", "input": [{"name": "strand_shift"}, {"name": "fragment_length"}, {"name": "threads"}, {"name": "FDR"}, {"name": "npeak"}, {"name": "tmpdir"}, {"name": "filtchr"}, {"name": "savn"}, {"name": "savd"}, {"name": "savp"}, {"name": "output_name_prefix"}, {"name": "savr"}, {"name": "spp_nodups"}, {"name": "input_bams"}], "output": [{"name": "Output cross correlation PDF"}, {"name": "NarrowPeak file output"}, {"name": "output_RegionPeak"}, {"name": "output_Rdata"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/hms-dbmi/spp", "https://github.com/hms-dbmi/spp/releases"], "applicationSubCategory": ["ChIP-seq"], "project": "SBG Public Data", "creator": "Peter Kharchenko, Anshul Kundaje, Encode consortium", "softwareVersion": ["sbg:draft-2"], "dateModified": 1500041198, "dateCreated": 1500041198, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sra-fasterq-dump-v2-10-8/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sra-fasterq-dump-v2-10-8/7", "applicationCategory": "CommandLineTool", "name": "SRA fasterq-dump", "description": "**SRA Toolkit** from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives format (SRA). \n\n**NOTE: SRA toolkit requires an *interactive configuration* since version 2.10.1 [2]. Running this tool on the platform triggers the configuration *automatically*!  Please find more information in the *'Changes introduced by Seven Bridges'* section.**\n\n**SRA fasterq-dump** tool converts SRA data into FASTQ format while using temporary files and multi-threading to speed up the extraction. With aligned data, NCBI uses Compression by Reference, which only stores the differences in base pairs between sequence data and the segment it aligns to [1].  \n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n###Common Use Cases\n\n- **SRA fasterq-dump** accepts both SRA file and SRA accession as input. To dump from an SRA file, choose your file using the **SRA file** input port, and leave **SRA accession** to empty. If you wish to use SRA accession, leave the **SRA file** port empty, and type the accession in the **SRA accession** field.  \n\n###Changes introduced by Seven Bridges\n\n- In order to access even the public data, the tool needs to be configured [2]. The authors have provided the interactive solution to this problem, but since this solution is not perfect for environments such as Seven Bridges platform and many other, further solutions are being discussed [3][4][5]. The current solution on the platform entails that the blank configuration file containing just the UUID (universally unique identifier) is created inside a specific folder in the root directory. This approach is taken from the authors' [Dockerfile](https://github.com/ncbi/sra-tools/blob/master/build/docker/Dockerfile).\n- Dumped FASTQ files will have **Sample ID** and **Paired end** metadata set. **Sample ID** metadata field will be set according to the SRA accession, while **Paired end** metadata is set to 1 or 2 if and only if \"_1\" and \"_2\" exist in the dumped file's filename. \n- **Due to security issues, *downloading controlled data is not recommended*. The following input ports have been omitted from the tool wrapper: NGC file (--ngc), Permission file (--perm) and Cart file (--cart_file).**\n\n\n###Common issues and important notes\n\n- The **Rerun if failed** (*rerun_switch*) parameter is an option that enables a command rerun in case the download has failed, probably due to a connection error. The command is rerun 10 times before the task is considered failed. It is recommended to set the option to *True* if the tool is used inside a workflow, especially if the tool is scattered based on input accession.\n\n\n###Performance Benchmarking\n\nThe speed and cost of the workflow depend on the number of bases in the SRA file. The following table shows pricing and duration for several use-cases. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n| Assay type | Input                       | Paired-end | Number of bases | Output size | Duration  | Price | Instance type |\n|------------|-----------------------------|------------|-----------------|-------------|-----------|-------|---------------|\n| RNAseq     | single SRA accession        | no         | 900M            | 4GB         | 3 min     | $0.02 | c4.2xlarge    |\n| RNAseq     | multiple SRA accessions (5) | no         | 5 x 400M        | 5 x 1.5GB   | 5 min     | $0.03 | c4.2xlarge    |\n| WES        | single SRA accession        | yes        | 2 x 6.18G       | 2 x 7.5GB   | 11 min    | $0.07 | c4.2xlarge    |\n| WGS        | single SRA accession        | yes        | 2 x 114.3G      | 2 x 137.3GB | 3h 22 min | $1.34 | c4.2xlarge    |\n\n###References\n\n[1] [NCBI SRA Toolkit documentation](https://ncbi.github.io/sra-tools/)\n\n[2] [Instructions for SRA toolkit installation and configuration](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration)\n\n[3] [SRA Github issue #282](https://github.com/ncbi/sra-tools/issues/282)\n\n[4] [SRA Github issue #291](https://github.com/ncbi/sra-tools/issues/291)\n\n[5] [SRA Github issue #310](https://github.com/ncbi/sra-tools/issues/310)", "input": [{"name": "SRA file"}, {"name": "Accession (SRA accesion)"}, {"name": "Split files"}, {"name": "Split spot"}, {"name": "Minimum read leangth"}, {"name": "Skip technical"}, {"name": "Inculde technical"}, {"name": "Table name"}, {"name": "Split 3"}, {"name": "Option file"}, {"name": "Output file name"}, {"name": "Number of threads"}, {"name": "Concatenate reads"}, {"name": "Use row ID as name"}, {"name": "Print read number"}, {"name": "Strict"}, {"name": "Filter by bases"}, {"name": "Location in the cloud"}, {"name": "Rerun if failed"}, {"name": "Memory per job"}], "output": [{"name": "Out reads", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ncbi/ncbi-vdb"], "applicationSubCategory": ["Utilities", "FASTQ Processing"], "project": "SBG Public Data", "creator": "NCBI", "softwareVersion": ["v1.1"], "dateModified": 1648036776, "dateCreated": 1619188016, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sra-fastq-dump-v2-10-8/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sra-fastq-dump-v2-10-8/7", "applicationCategory": "CommandLineTool", "name": "SRA fastq-dump", "description": "**SRA Toolkit** from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives format (SRA). \n\n**NOTE: SRA toolkit requires an *interactive configuration* since version 2.10.1 [2]. Running this tool on the platform triggers the configuration *automatically*!  Please find more information in the *'Changes introduced by Seven Bridges'* section.**\n\n**SRA fastq-dump** tool converts SRA data into FASTQ format. With aligned data, NCBI uses Compression by Reference, which only stores the differences in base pairs between sequence data and the segment it aligns to. The process to restore original data, for example as FASTQ, requires fast access to the reference sequences that the original data was aligned to  [1].\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n###Common Use Cases\n\n- **SRA fastq-dump** accepts both SRA file and SRA accession as input. To dump an SRA file, choose your file using the **SRA file** input port, and leave **SRA accession** empty. If you wish to use SRA accession, leave the **SRA file** port empty, and type the accession in the **SRA accession** field. \n- Output can be FASTA (FASTQ with qualities omitted) by setting **FASTA** to *True* and optionally setting  **FASTA line width**. \n- Output can also be GZIP/BZIP2 archived FASTQ/FASTA by setting **Gzip compress** or **bzip2 compress** to *True*.\n- To format the sequence using color space set **dump CSkey** to True. Specify the cskey using the **CSkey** parameter.\n\n\n###Changes introduced by Seven Bridges\n\n- In order to access even the public data, the tool needs to be configured [2]. The authors have provided the interactive solution to this problem, but since this solution is not perfect for environments such as Seven Bridges platform and many other, further solutions are being discussed [3][4][5]. The current solution on the platform entails that the blank configuration file containing just the UUID (universally unique identifier) is created inside a specific folder in the root directory. This approach is taken from the authors' [Dockerfile](https://github.com/ncbi/sra-tools/blob/master/build/docker/Dockerfile).\n- Dumped FASTQ files will have **Sample ID** and **Paired end** metadata set. **Sample ID** metadata field will be set according to the SRA accession, while **Paired end** metadata is set to 1 or 2 if and only if \"_1\" and \"_2\" exist in the dumped file's filename. \n- **Due to security issues, *downloading controlled data is not recommended*. The following input ports have been omitted from the tool wrapper: NGC file (--ngc), Permission file (--perm) and Cart file (--cart_file).**\n\n\n\n###Common issues and important notes\n\n- The **Rerun if failed** (*rerun_switch*) parameter is an option that enables a command rerun in case the download has failed, probably due to a connection error. The command is rerun 10 times before the task is considered failed. This option should be used only with one accession. It is recommended to set the option to *True* if the tool is used inside a workflow, especially if the tool is scattered based on input accession.\n\n\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the size of the cDNA FASTQ files. The following table showcases the metrics for the task running on the c4.2xlarge on demand AWS instance. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n| Assay type | Input                       | Paired-end | Number of bases | Output size | Duration | Price | Instance type |\n|--------------|-------------------------------|--------------|-------------------|---------------|------------|---------|-----------------|\n| RNAseq     | single SRA accession        | no         | 900M            | 4GB         | 3 min    | $0.02 | c4.2xlarge    |\n| RNAseq     | multiple SRA accessions (5) | no         | 5 x 400M        | 5 x 1.5GB   | 8 min    | $0.05 | c4.2xlarge    |\n| WES        | single SRA accession        | yes        | 2 x 6.18G       | 2 x 7.5GB   | 16 min   | $0.10 | c4.2xlarge    |\n| WGS        | single SRA accession        | yes        | 2 x 114.3G      | 2 x 137.3GB | 4h 4 min | $1.62 | c4.2xlarge    |\n\n### References\n\n[1] [NCBI SRA Toolkit documentation](https://ncbi.github.io/sra-tools/)\n\n[2] [Instructions for SRA toolkit installation and configuration](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration)\n\n[3] [SRA Github issue #282](https://github.com/ncbi/sra-tools/issues/282)\n\n[4] [SRA Github issue #291](https://github.com/ncbi/sra-tools/issues/291)\n\n[5] [SRA Github issue #310](https://github.com/ncbi/sra-tools/issues/310)", "input": [{"name": "SRA file"}, {"name": "Accession (SRA accession)"}, {"name": "Split files"}, {"name": "Split spot"}, {"name": "FASTA only"}, {"name": "Original sequence name"}, {"name": "Dump cskey"}, {"name": "Sequence formatting - base space"}, {"name": "Offset"}, {"name": "Minimum sopt ID"}, {"name": "Maximum spot ID"}, {"name": "Minimum read leangth"}, {"name": "Skip technical"}, {"name": "Dump aligned reads"}, {"name": "Dump unaligned"}, {"name": "Gzip compress"}, {"name": "bzip2 compress"}, {"name": "Replace accession"}, {"name": "Table name"}, {"name": "Spot groups"}, {"name": "Clip adapter sequences"}, {"name": "Split by READ_FILTER"}, {"name": "Quality filter"}, {"name": "Quality filter 1"}, {"name": "Aligned region"}, {"name": "Matepair distance"}, {"name": "Split 3"}, {"name": "Keep empty files"}, {"name": "Suppress quality for cskey"}, {"name": "Read IDs"}, {"name": "Helicos"}, {"name": "Defline format - sequence"}, {"name": "Defline format - qualities"}, {"name": "Location in the cloud"}, {"name": "Option file"}, {"name": "Rerun if failed"}, {"name": "Memory per job"}], "output": [{"name": "Out reads", "encodingFormat": "text/fastq"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ncbi/ncbi-vdb"], "applicationSubCategory": ["Utilities", "FASTQ Processing"], "project": "SBG Public Data", "creator": "NCBI", "softwareVersion": ["v1.1"], "dateModified": 1648036776, "dateCreated": 1619188015, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sra-sam-dump-v2-10-8/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sra-sam-dump-v2-10-8/4", "applicationCategory": "CommandLineTool", "name": "SRA sam-dump", "description": "**SRA Toolkit** from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives format (SRA). \n\n**NOTE: SRA toolkit requires an *interactive configuration* since version 2.10.1 [2]. Running this tool on the platform triggers the configuration *automatically*!  Please find more information in the *'Changes introduced by Seven Bridges'* section.**\n\n**SRA sam-dump** converts SRA data into SAM format. With aligned data, NCBI uses Compression by Reference, which only stores the differences in base pairs between sequence data and the segment it aligns to. The process to restore original data, for example as FASTQ, requires fast access to the reference sequences that the original data was aligned to [1]. \n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n\n###Common Use Cases\n\n- **SRA sam-dump** accepts both SRA file and SRA accession as input. To dump from an SRA file, choose your file using the **SRA file** input port, and set **SRA accession** to \"from_sra_file\". If you wish to use SRA accession, leave the **SRA file** port empty, and enter the accession in the **SRA accession** field. \n- Printing header in the output is controlled via two parameters **Reconstruct header** and **Don't output header**. Set either to *True* for the desired results.\n- Default output format for **SRA sam-dump** is SAM. If you wish the output to be in BAM format, choose either *'BAM'* or *'BAM_with_header'* options in  **Output format** parameter.\n- Output file name is automatically generated from SRA accession or from SRA file name. If you wish to give the output file a custom name, enter the desired string in the **Output file name** parameter.\n\n###Changes introduced by Seven Bridges\n\n- In order to access even the public data, the tool needs to be configured [2].  The authors have provided the interactive solution to this problem, but since this solution is not perfect for environments such as Seven Bridges platform and many others, further solutions are being discussed [3][4][5]. The current solution on the platform entails that the blank configuration file containing just the UUID (universally unique identifier) is created inside a specific folder in the root directory. This approach is taken from the authors' [Dockerfile](https://github.com/ncbi/sra-tools/blob/master/build/docker/Dockerfile).\n- **Due to security issues, *downloading controlled data is not recommended*. The following input ports have been omitted from the tool wrapper: NGC file (--ngc), Permission file (--perm) and Cart file (--cart_file).**\n\n\n###Common issues and important notes\n\n- The **Rerun if failed** (*rerun_switch*) parameter is an option that enables a command rerun in case the download has failed, probably due to a connection error. The command is rerun 10 times before the task is considered failed. It is recommended to set the option to *True* if the tool is used inside a workflow, especially if the tool is scattered based on input accession.\n\n###Performance Benchmarking\n\nThe speed and cost of the workflow depend on the number of bases in the SRA file. The following table shows pricing and duration for several use-cases. The price can be significantly reduced by using spot instances (set by default). Visit [The Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n| Assay type | Input                | Paired-end | Number of bases | Output size | Output format | Duration | Price | Instance type |\n|------------|----------------------|------------|-----------------|-------------|---------------|----------|-------|---------------|\n| WGS        | single SRA accession | yes        | 1.1G            | 404.5 KiB   | SAM           | 2 min    | $0.01 | c4.2xlarge    |\n| WGS        | single SRA accession | yes        | 1.1G            | 78.3 KiB    | BAM           | 2 min    | $0.01 | c4.2xlarge    |\n| WES        | single SRA accession | yes        | 6.18G           | 15GB        | SAM           | 20 min   | $0.13 | c4.2xlarge    |\n| WES        | single SRA accession | yes        | 6.18G           | 2 x 7.5GB   | BAM           | 26 min   | $0.17 | c4.2xlarge    |\n\n### References\n\n[1] [NCBI SRA Toolkit documentation](https://ncbi.github.io/sra-tools/)\n\n[2] [Instructions for SRA toolkit installation and configuration](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration)\n\n[3] [SRA Github issue #282](https://github.com/ncbi/sra-tools/issues/282)\n\n[4] [SRA Github issue #291](https://github.com/ncbi/sra-tools/issues/291)\n\n[5] [SRA Github issue #310](https://github.com/ncbi/sra-tools/issues/310)", "input": [{"name": "SRA file"}, {"name": "Accession (SRA accession)"}, {"name": "Primary alignments"}, {"name": "CIGAR string long"}, {"name": "Reconstruct header"}, {"name": "Don't output header"}, {"name": "Print SEQ_ID"}, {"name": "Hide identical to reference"}, {"name": "Reverse unaligned reads"}, {"name": "RNA splicing"}, {"name": "Output unaligned reads"}, {"name": "Filter by position"}, {"name": "Filter by distance between matepairs"}, {"name": "Unaligned spots only"}, {"name": "Minimal mapping quality"}, {"name": "Gzip compress"}, {"name": "bzip2 compress"}, {"name": "Output format"}, {"name": "Output file name"}, {"name": "Cigar GC"}, {"name": "Header file"}, {"name": "Header comment"}, {"name": "Aligned region"}, {"name": "Matepair distance"}, {"name": "Print SEQ_ID"}, {"name": "SPOT_GROUP"}, {"name": "Prefix QNAME"}, {"name": "Reverse unaligned reads"}, {"name": "CIGAR CG merge"}, {"name": "Output cSRA alignment ID"}, {"name": "Quantization string"}, {"name": "CG evidence"}, {"name": "CG evidence DNB"}, {"name": "CG mappings"}, {"name": "CG SAM"}, {"name": "Cache report"}, {"name": "No mate-cache"}, {"name": "CG names"}, {"name": "RNA splice level"}, {"name": "Print MD flag"}, {"name": "Location in the cloud"}, {"name": "Option file"}, {"name": "Rerun if failed"}, {"name": "Memory per job"}], "output": [{"name": "Out alignments", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/ncbi/ncbi-vdb"], "applicationSubCategory": ["Utilities", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "NCBI", "softwareVersion": ["v1.1"], "dateModified": 1648036776, "dateCreated": 1619188015, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sra-toolkit-fastq-dump-2-8-0/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sra-toolkit-fastq-dump-2-8-0/6", "applicationCategory": "CommandLineTool", "name": "SRA Toolkit fastq-dump", "description": "SRA Toolkit from NCBI is a collection of tools and libraries for using data in the [INSDC](http://www.insdc.org) Sequence Read Archives format (SRA).\n\nTool `fastq-dump` converts SRA data into FASTQ format. Optionally, output could be FASTA (FASTQ with qualities omitted) or GZIP/BZIP2 archived FASTQ/FASTA (option `compress_output`).\n\nFastq-dump accepts File as input, but also could accept accession number, which will be prefetched on-the-fly. Prefetch option should be used when only part of the SRA is to be dumped.\n\n### Common issues ###\nIf paired-end reads are used and the outputs are intended to be used as inputs of BWA MEM aligner, \"split_files\" should be selected in combination with \"no_paired_end\" option in \"read_ids\".", "input": [{"name": "Split reads"}, {"name": "Output fasta (wrap width)"}, {"name": "Read ids"}, {"name": "Original sequence name"}, {"name": "Format sequence"}, {"name": "Offset quality"}, {"name": "Compress output"}, {"name": "Minimum spot ID"}, {"name": "Maximum spot ID"}, {"name": "Skip technical"}, {"name": "Dump aligned"}, {"name": "SRA input file"}, {"name": "SRA accession number"}, {"name": "Minimum read length"}, {"name": "Clipping"}], "output": [{"name": "FASTQ files", "encodingFormat": "text/fastq"}, {"name": "Archived reads"}, {"name": "FASTA files", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/ncbi/sra-tools", "https://github.com/ncbi/sra-tools/wiki"], "applicationSubCategory": ["SRA", "FASTQ Processing"], "project": "SBG Public Data", "creator": "NCBI", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241845, "dateCreated": 1482932223, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/sam-dump/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/sam-dump/1", "applicationCategory": "CommandLineTool", "name": "SRA Toolkit sam-dump", "description": "SRA Toolkit from NCBI is a collection of tools and libraries for using data in the [INSDC](http://www.insdc.org) Sequence Read Archives format (SRA).\n\nTool `sam-dump` converts SRA data to SAM format. Data could be aligned or unaligned (for unaligned data dump select \"unaligned\" option). This tool can also export in BAM format if option \"output_bam\" is selected - internally it uses `samtools 1.3` to convert SAM to BAM.", "input": [{"name": "Output primary alignments"}, {"name": "Output long CIGAR"}, {"name": "Reconstruct header"}, {"name": "No header in output"}, {"name": "SeqID in RNAME"}, {"name": "Hide identical bases"}, {"name": "Reverse unaligned reads"}, {"name": "Modify CIGAR if rna-splicing detected"}, {"name": "Output unaligned reads"}, {"name": "Output BAM"}, {"name": "Filter by position on genome"}, {"name": "SRA input file"}, {"name": "SRA accession number"}], "output": [{"name": "SAM file", "encodingFormat": "application/x-sam"}, {"name": "BAM file", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/ncbi/sra-tools", "https://github.com/ncbi/sra-tools/wiki"], "applicationSubCategory": ["SRA", "SAM/BAM Processing"], "project": "SBG Public Data", "creator": "NCBI", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241843, "dateCreated": 1482932068, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/star-2-7-3a/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/star-2-7-3a/11", "applicationCategory": "CommandLineTool", "name": "STAR", "description": "**STAR** is an ultrafast universal RNA-seq aligner.\n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n**STAR** works with reads starting from lengths ~15 bases up to ~300 bases. In case of having longer reads, the use of **STAR Long** tool is recommended instead.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main purpose of **STAR** is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nSome important notes about this tool are: \n\n- The main input to the tool are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- **Genome files** in the form of a **STAR index archive**, outputted by the **STAR Genome Generate** tool, also need to be provided.\n- It\u2019s generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this tool is the **Aligned reads** output. STAR outputs alignments directly in binary BAM format by default, thus saving time on converting SAM files to BAM. It can also sort BAM files by coordinates (set **Output sorting type** option to **SortedByCoordinate**), which is required by many downstream applications. The **Transcriptome aligned reads** BAM file is produced if the **Quantification mode** (`--quantMode`) parameter is set to **TranscriptomeSAM**. \n- Gene counts are produced if the **Quantification mode** (`--quantMode`) parameter is set to **GeneCounts**.  \n- **STAR** can detect chimeric transcripts, but the parameter **Min chimeric segment** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis(using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads in FASTQ format are outputted on the **Unmapped reads** output if the **Output unmapped reads** (`--outReadsUnmapped`) parameter is set to the **Fastx** value. \n- Unmapped reads can be outputted within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs if the **Write unmapped in SAM** (`--outSAMunmapped`) parameter is set to **Within** or **Within KeepPairs**. \n- A basic **Two-pass mode** can be turned during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass, by setting the **Two-pass mode** (`--twopassMode`) option to **Basic**. \n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end 2 or more then one single-end FASTQ file), the **Paired-end** metadata field must be set. Read Groups will be formed using metadata fields following this hierarchy: **Sample ID/Library ID/Platform Unit ID/File Segment Number**. If **No read groups** parameter is set to **True**, read groups won\u2019t be formed.      \n\nAdditionally, if you have long reads available and wish to map them with STAR, setting the **STARlong** option will run the **STARlong** algorithm instead, which uses a more efficient seed stitching algorithm for long reads (>200b), and also uses different array allocations. Selecting this boolean option will also automatically change the following parameters of STAR to comply with long read alignment best practices:\n`--outFilterMultimapScoreRange 20`\n`--outFilterScoreMinOverLread 0`\n`--outFilterMismatchNmax 1000`\n`--winAnchorMultimapNmax 200`\n`--seedSearchLmax 30`\n`--seedSearchStartLmax 12`\n`--seedPerReadNmax 100000`\n`--seedPerWindowNmax 100`\n`--alignTranscriptsPerReadNmax 100000`\n`--alignTranscriptsPerWindowNmax 10000`\n\nThe **STAR solo** algorithm is turned on with: `--soloType CB_UMI_Simple` (a.k.a. Droplet, one UMI and one Cell Barcode of fixed length in read2, e.g. Drop-seq and 10X Chromium) or `--soloType CB_UMI_Complex` (one UMI of fixed length, but multiple Cell Barcodes of varying length, as well as adapters sequences are allowed in read2 only, e.g. inDrop). Presently, the cell barcode whitelist has to be provided with: `--soloCBwhitelist /path/to/cell/barcode/whitelist`. Please make sure that the whitelist is compatible with the specific version of the 10X chemistry (V1,V2,V3 etc). Importantly, in the `--readFilesIn` option, the 1st FASTQ file has to be cDNA read, and the 2nd FASTQ file has to be the barcode (cell+UMI) read, i.e. `--readFilesIn cDNAfragmentSequence.fastq.gz CellBarcodeUMIsequence.fastq.gz`. To invoke this behavior, please set the **Paired End** metadata on these files to 1 and 2, respectively (1 for the cDNA file and 2 for the barcode file). Feature statistics summaries are recorded in files **<Feature>.stats** where features are those used in the `--soloFeatures` option, e.g. **Gene.stats** [2].  \n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- If you **do not** use STAR Genome Generate for generating the index archive (which is highly suggested), you should specify the path to the index files using **Genome dir name** (`--genomeDir`) parameter. For example, if you have TAR file where index files are within some folder, you need to set **Genome dir name** parameter to name of that folder. If you tar files (that are not in the folder) you should set **Genome dir name** parameter to **./** . \n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in subsequent analysis (i.e. downstream alignment). The option to sort unmapped reads by read ID is added to this wrapper, by setting the **Sort unmapped reads** parameter to True. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The tool can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will be inferred automatically using a built-in **Samtools** script.\n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for four samples with different file sizes, using default options. \n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 383 MB |     Yes    |     1.6M     |     101     |   2min   | $0.08 | c5.9xlarge |\n|     RNA-Seq     |  2 x 2.2 GB |     Yes    |     9.5M     |     101     |   11min   | $0.31 | c5.9xlarge |\n|     RNA-Seq     | 2 x 10.8 GB |     Yes    |     47.5M    |     101     |   20min  | $0.56 | c5.9xlarge |\n|     RNA-Seq     | 2 x 21.5 GB |     Yes    |     95M    |     101     |   38min  | $1.01 | c5.9xlarge |\n\n\nRuntime and task cost for one of previous samples when 2-pass mapping was on (`--twopassMode Basic` option).\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 2.2 GB |     Yes    |     9.5M     |     101     |   13min   | $0.36 | c5.9xlarge |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)\n[2] [STAR manual](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)", "input": [{"name": "Read sequence", "encodingFormat": "application/x-bam"}, {"name": "Reads lengths"}, {"name": "Reads to map"}, {"name": "Junctions max number"}, {"name": "Collapsed junctions max number"}, {"name": "Output unmapped reads"}, {"name": "Quality conversion"}, {"name": "Output format"}, {"name": "Output sorting type"}, {"name": "SAM mode"}, {"name": "Strand field flag"}, {"name": "SAM attributes"}, {"name": "Write unmapped in SAM"}, {"name": "Sorting in SAM"}, {"name": "Primary alignments"}, {"name": "Read ID"}, {"name": "MAPQ value"}, {"name": "OR SAM flag"}, {"name": "AND SAM flag"}, {"name": "SAM header @HD"}, {"name": "SAM header @PG"}, {"name": "Sequencing center"}, {"name": "Library ID"}, {"name": "Median fragment length"}, {"name": "Platform"}, {"name": "Platform unit ID"}, {"name": "Sample ID"}, {"name": "Filtering type"}, {"name": "Multimapping score range"}, {"name": "Max number of mappings"}, {"name": "Max number of mismatches"}, {"name": "Mismatches to *mapped* length"}, {"name": "Mismatches to *read* length"}, {"name": "Min score"}, {"name": "Min score normalized"}, {"name": "Min matched bases"}, {"name": "Min matched bases normalized"}, {"name": "Motifs filtering"}, {"name": "Collapsed junctions reads"}, {"name": "Min overhang SJ"}, {"name": "Min unique count"}, {"name": "Min total count"}, {"name": "Min distance to other donor/acceptor"}, {"name": "Max gap allowed"}, {"name": "Gap open penalty"}, {"name": "Non-canonical gap open"}, {"name": "GC/AG and CT/GC gap open"}, {"name": "AT/AC and GT/AT gap open"}, {"name": "Log scaled score"}, {"name": "Deletion open penalty"}, {"name": "Deletion extension penalty"}, {"name": "Insertion Open Penalty"}, {"name": "Insertion extension penalty"}, {"name": "Max score reduction"}, {"name": "Search start point"}, {"name": "Search start point normalized"}, {"name": "Max seed length"}, {"name": "Filter pieces for stitching"}, {"name": "Max seeds per read"}, {"name": "Max seeds per window"}, {"name": "Max one-seed loci per window"}, {"name": "Min intron size"}, {"name": "Max intron size"}, {"name": "Max mates gap"}, {"name": "Min overhang"}, {"name": "Min overhang: annotated"}, {"name": "Min mapped length"}, {"name": "Min mapped length normalized"}, {"name": "Max windows per read"}, {"name": "Max transcripts per window"}, {"name": "Max transcripts per read"}, {"name": "Alignment type"}, {"name": "Soft clipping"}, {"name": "Max loci anchors"}, {"name": "Bin size"}, {"name": "Max bins between anchors"}, {"name": "Flanking regions size"}, {"name": "Min chimeric segment"}, {"name": "Min total score"}, {"name": "Max drop score"}, {"name": "Min separation score"}, {"name": "Non-GT/AG penalty"}, {"name": "Min junction overhang"}, {"name": "Quantification mode"}, {"name": "Reads to process in 1st step"}, {"name": "Two-pass mode"}, {"name": "Genome dir name"}, {"name": "Save junction files"}, {"name": "Chromosome names"}, {"name": "Set exons feature"}, {"name": "Exons' parents name"}, {"name": "Gene name"}, {"name": "\"Overhang\" length"}, {"name": "Extra alignment score"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Clip 3p bases"}, {"name": "Clip 5p bases"}, {"name": "Clip 3p adapter sequence"}, {"name": "Max mismatches proportions"}, {"name": "Clip 3p after adapter seq"}, {"name": "Chimeric output type"}, {"name": "Genome files", "encodingFormat": "application/x-tar"}, {"name": "Max insert junctions"}, {"name": "Prohibit alignment type"}, {"name": "Limit BAM sorting memory"}, {"name": "Max number of multiple alignment"}, {"name": "Order of multimapping alignment"}, {"name": "IH attribute start value"}, {"name": "Output filter"}, {"name": "Splice junction stich max mismatch"}, {"name": "Chimeric segment gap"}, {"name": "Chimeric filter"}, {"name": "Unmapped output file names"}, {"name": "No read groups"}, {"name": "List of annotated junctions", "encodingFormat": "text/plain"}, {"name": "Number of threads"}, {"name": "Protrusion of alignment ends"}, {"name": "Max main chimeric segment alignments"}, {"name": "Output file name prefix"}, {"name": "BAM remove duplicates type"}, {"name": "BAM remove duplicates mate2 bases"}, {"name": "Output wiggle type"}, {"name": "Output wiggle strand"}, {"name": "Output wiggle reference prefix"}, {"name": "Output wiggle normalization"}, {"name": "Filter intron strands"}, {"name": "Seed split min"}, {"name": "Variation VCF file", "encodingFormat": "application/x-vcf"}, {"name": "Align insertion flush"}, {"name": "Out SAM TLEN"}, {"name": "Output BAM sorting bins"}, {"name": "WASP output mode"}, {"name": "Chimeric multimap max number"}, {"name": "Chimeric multimap score range"}, {"name": "Minimum no-chimeric drop score"}, {"name": "Paired end overlap min bases number"}, {"name": "Paired end overlap max mismatches proportion"}, {"name": "Output BAM compression"}, {"name": "Output BAM sorting threads"}, {"name": "Verbose indexing"}, {"name": "STARlong"}, {"name": "Chimeric file format"}, {"name": "STARsolo activation"}, {"name": "STARsolo cell barcode whitelist", "encodingFormat": "text/plain"}, {"name": "STARsolo features"}, {"name": "STARsolo cell barcode start"}, {"name": "STARsolo cell barcode length"}, {"name": "STARsolo UMI start"}, {"name": "STARsolo UMI length"}, {"name": "STARsolo UMI deduplication"}, {"name": "STARsolo strand"}, {"name": "STARsolo output filenames"}, {"name": "STARsolo barcode read length"}, {"name": "Sort unmapped reads"}, {"name": "STARsolo Cell Filter"}, {"name": "STARsolo UMI filtering"}, {"name": "STARsolo Cell Barcodes to the WhiteList matching"}, {"name": "STARsolo Adapter sequence to anchor barcodes"}, {"name": "STARsolo Max number of mismatches allowed in adapter seq"}, {"name": "STARsolo Position of Cell Barcode(s) on the barcode read"}, {"name": "STARsolo String position of the UMI on the barcode read"}, {"name": "GTF attrbute name for parent gene name"}, {"name": "GTF attrbute name for parent gene type"}, {"name": "Memory per job"}], "output": [{"name": "Aligned SAM/BAM", "encodingFormat": "application/x-bam"}, {"name": "Transcriptome alignments", "encodingFormat": "application/x-bam"}, {"name": "Reads per gene"}, {"name": "Main and Progress logs"}, {"name": "Splice junctions"}, {"name": "Chimeric junctions"}, {"name": "Unmapped reads", "encodingFormat": "text/fastq"}, {"name": "Intermediate genome files", "encodingFormat": "application/x-tar"}, {"name": "Chimeric alignments", "encodingFormat": "application/x-sam"}, {"name": "Wiggle files"}, {"name": "STARsolo outputs"}, {"name": "Summary statistics output"}, {"name": "Summary statistics log"}, {"name": "STARsolo matrix TAR", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/tree/master/source", "https://github.com/alexdobin/STAR/archive/2.7.3a.tar.gz", "https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf"], "applicationSubCategory": ["Alignment", "RNA-Seq"], "project": "SBG Public Data", "creator": "Alexander Dobin/CSHL", "softwareVersion": ["v1.0"], "dateModified": 1648048649, "dateCreated": 1581087147, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/star-fusion-v1-9-0/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/star-fusion-v1-9-0/9", "applicationCategory": "CommandLineTool", "name": "STAR-Fusion", "description": "**STAR-Fusion** uses the **STAR aligner** to identify candidate fusion transcripts supported by Illumina reads.  \n\n**STAR-Fusion** is a component of the Trinity Cancer Transcriptome Analysis Toolkit (CTAT). **STAR-Fusion** processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set [1]. \n\nA reference genome and corresponding protein-coding gene annotation set, including blast-matching gene pairs must be provided to STAR-Fusion in the form of a **CTAT genome lib archive** that can be downloaded from the following [link](https://data.broadinstitute.org/Trinity/CTAT_RESOURCE_LIB/__genome_libs_StarFv1.9/) or built with the **STAR-Fusion Build FusionFilter Dataset** tool. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n**STAR-Fusion** can be run in two modes:\n1. With FASTQ input files, doing alignment by **STAR** internally\n2. With chimeric junctions input files, obtained by running **STAR** beforehand (version 2.7.2b or higher)\n\n- If running with chimeric junctions as input files, the following STAR parameters are recommended by the STAR-Fusion author:\n\n\n| ID                                       | Label                                        | Prefix                             | Value    |\n|------------------------------------------|----------------------------------------------|------------------------------------|----------|\n| chim_segment_min                         | Min chimeric segment                         | --chimSegmentMin                   | 12       |\n| chim_junction_overhang_min               | Min junction overhang                        | --chimJunctionOverhangMin          | 8        |\n| chim_out_junction_format                 | Chimeric file format                         | --chimOutJunctionFormat            | 1        |\n| align_sj_stitch_mismatch_n_max           | Splice junction stich max mismatch           | --alignSJstitchMismatchNmax        | 5 -1 5 5 |\n| pe_overlap_n_bases_min                   | Paired end overlap min bases number          | --peOverlapNbasesMin               | 12       |\n| pe_overlap_mmp                           | Paired end overlap max mismatches proportion | --peOverlapMMp                     | 0.1      |\n| chim_multimap_n_max                      | Chimeric multimap max number                 | --chimMultimapNmax                 | 20       |\n| chim_multimap_score_range                | Chimeric multimap score range                | --chimMultimapScoreRange           | 3        |\n| chim_score_junction_non_gtag             | Non-GT/AG penalty                            | --chimScoreJunctionNonGTAG         | -4       |\n| chim_nonchim_score_drop_min              | Minimum no-chimeric drop score               | --chimNonchimScoreDropMin          | 10       |\n| align_intron_max                         | Max intron size                              | --alignIntronMax                   | 100000   |\n| align_mates_gap_max                      | Max mates gap                                | --alignMatesGapMax                 | 100000   |\n| align_sjdb_overhang_min                  | Min overhang: annotated                      | --alignSJDBoverhangMin             | 10       |\n| align_insertion_flush                    | Align insertion flush                        | --alignInsertionFlush              | Right    |\n| align_spliced_mate_map_l_min_over_l_mate | Min mapped length normalized                 | --alignSplicedMateMapLminOverLmate | 0        |\n| align_spliced_mate_map_l_min             | Min mapped length                            | --alignSplicedMateMapLmin          | 30       |\n| out_sam_unmapped                         | Write unmapped in SAM                        | --outSAMunmapped                   | Within   |\n| align_insertion_flush                    | Align insertion flush                        | --alignInsertionFlush              | 0        |\n| align_spliced_mate_map_l_min             | Min mapped length                            | --alignSplicedMateMapLmin          | 30       |\n| two_pass_mode                            | Two-pass mode                                | --twopassMode                      | Basic    |\n\n- The other important input to **STAR-Fusion** is the **CTAT genome lib archive** (`--genome_lib_dir`), which can be downloaded from the tool's homepage (or via this [link](https://data.broadinstitute.org/Trinity/CTAT_RESOURCE_LIB/__genome_libs_StarFv1.9/)) for the Homo Sapiens Gencode anotation. This archive can also be generated for other organisms and annotations, by following the instructions on this [link](https://github.com/FusionFilter/FusionFilter/wiki/Building-a-Custom-FusionFilter-Dataset), or by using the **STAR-Fusion Build FusionFilter Dataset** app, available on the platform as well.\n- **STAR-Fusion** has an option to run from a sample-sheet instead of specifying the FASTQ on the command line. This way, **STAR-Fusion** processes each sample in the sample-sheet individually. This behavior can be mimicked on the platform just by supplying any number of samples (FASTQ files) on the **Input files** input, and properly setting the **Paired End** and **Sample ID** metadata fields. \n- **FusionInspector** assists in fusion transcript discovery by performing a supervised analysis of fusion predictions, attempting to recover and re-score evidence for such predictions. As of July, 2017, FusionInspector has been included as a component of the STAR-Fusion suite [2]. It is turned on by setting **FusionInspector** (`--FusionInspector`) to 'validate' or 'inspect'.\n\n###Common issues###\n\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- If running with chimeric junctions input files, obtained by running **STAR** beforehand, note that the **STAR** version needs to be 2.7.2b or higher.\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output prefix** option is explicitly specified.\n\n### Performance Benchmarking\n\nSince **STAR-Fusion** can be run in two modes (with and without alignment), the table below shows runtimes and task costs for different file sizes, both with and without alignment (with the input files being either FASTQ files, or chimeric junctions, produced by **STAR**)\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq - FASTQ     |  2 x 1.9 GB |     Yes    |     16M     |     101     |   32min   | $0.81 | c5.9xlarge |\n|     RNA-Seq - FASTQ    |  2 x 5.6 GB |     Yes    |     50M     |     101     |   47min   | $1.20 | c5.9xlarge |\n|     RNA-Seq - FASTQ    | 2 x 9.3 GB |     Yes    |     82M    |     101     |  66min  | $1.68 | c5.9xlarge |\n|     RNA-Seq - FASTQ     |  2 x 18.5 GB |     Yes    |     163M     |     101     |   100min   | $2.55 | c5.9xlarge |\n|     RNA-Seq - chimeric junctions    |  2 x 700 MB |     Yes    |     16M     |     101     |   21min   | $0.14 | c4.2xlarge |\n|     RNA-Seq - chimeric junctions   |   2 x 2 GB |     Yes    |     50M     |     101     |   28min   | $0.19 | c4.2xlarge |\n|     RNA-Seq - chimeric junctions    | 2 x 9.3 GB |     Yes    |     82M    |     101     |   37min  | $0.24 | c4.2xlarge |\n|     RNA-Seq - chimeric junctions    |   2 x 18.5 GB |     Yes    |     163M    |     101     |   53min  | $0.36 | c4.2xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [STAR-Fusion wiki](https://github.com/STAR-Fusion/STAR-Fusion/wiki)\n\n[2] [FusionInspector wiki](https://github.com/FusionInspector/FusionInspector/wiki)", "input": [{"name": "CTAT genome lib archive", "encodingFormat": "application/x-tar"}, {"name": "FusionInspector"}, {"name": "STAR CPU"}, {"name": "STAR Sorted by coordinate"}, {"name": "STAR chimeric multimap max number"}, {"name": "STAR chimeric multimap score range"}, {"name": "STAR minimum no-chimeric drop score"}, {"name": "STAR maximum mate distance"}, {"name": "STAR paired end overlap min bases number"}, {"name": "STAR paired end overlap max mismatches proportion"}, {"name": "STAR use shared memory"}, {"name": "Aggregate novel junction dist"}, {"name": "Denovo reconstruct"}, {"name": "Examine coding effect"}, {"name": "Extract fusion reads"}, {"name": "Input files", "encodingFormat": "text/fastq"}, {"name": "Max promiscuity"}, {"name": "Minimum FFPM"}, {"name": "Min alt pct junction"}, {"name": "Min junction reads"}, {"name": "Min novel junction support"}, {"name": "Minimum non-specific multimapping read percentage"}, {"name": "Min percentage for dominant promiscutiy"}, {"name": "Minimum spanning fragments only"}, {"name": "Min supporting fragments"}, {"name": "No annotation filter"}, {"name": "No filter"}, {"name": "No duplicate removing"}, {"name": "Output prefix"}, {"name": "Require LDAS"}, {"name": "Run STAR only"}, {"name": "Memory per job"}], "output": [{"name": "Fusion predictions"}, {"name": "Fusion predictions abridged"}, {"name": "STAR-Fusion output archive", "encodingFormat": "application/x-tar"}, {"name": "FusionInspector fusion predictions"}, {"name": "FusionIspector HTML fusions summary", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/STAR-Fusion/STAR-Fusion", "https://github.com/STAR-Fusion/STAR-Fusion/releases/tag/STAR-Fusion-v1.8.1", "https://github.com/STAR-Fusion/STAR-Fusion/wiki", "https://github.com/STAR-Fusion/STAR-Fusion"], "applicationSubCategory": ["RNA-Seq", "Gene Fusion"], "project": "SBG Public Data", "creator": "Brian Haas", "softwareVersion": ["v1.1"], "dateModified": 1648036776, "dateCreated": 1587672096, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/star-fusion-build-fusionfilter-dataset-v1-9-0/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/star-fusion-build-fusionfilter-dataset-v1-9-0/2", "applicationCategory": "CommandLineTool", "name": "STAR-Fusion Build FusionFilter Dataset", "description": "**STAR-Fusion Build FusionFilter Dataset** is a tool that builds the **CTAT genome lib archive** necessary for **STAR-Fusion** to run successfully\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n\n### Common Use Cases\n\n- A combination of FASTA/GTF files for any species are the only requirements as inputs, and the tool will take care of building all the necessary files and put them in an archive ready for **STAR-Fusion** to use [1]. \n- If you want to exclude the building of STAR index files inside the STAR-Fusion index archive, make sure to turn on the **Skip STAR** (`--skip_star`) option. \n- **Dfam database** is a required input from version 1.8.0, and it will be pulled from docker image unless provided on input. It is recommended to use an organism specific Dfam databaase. Default Dfam dabase file is downloaded from [link](https://www.dfam.org/releases/Dfam_3.1/families/Dfam.hmm.gz).\n- If you want to include Pfam domain information inside the STAR-Fusion index archive, make sure to provide file from the following [link](ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz) to **Pfam domain file** input port, otherwise default Pfam database downloaded from [link](ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz) will be pulled from docker image.\n\n\n###Common issues\n\n- The FASTA and GTF should be unzipped and have matching chromosome namings (i.e. >chr1, >chr2, ... or >1, >2, ...) \n\n### Changes Introduced by Seven Bridges\n\n- All output files are packed in an archive ready for **STAR-Fusion** (as available on the platform) to use. \n- The **Skip STAR** option is exposed to use, which is hidden in the official documentation of this tool. \n\n### Performance Benchmarking\n\nSince **STAR-Fusion Build FusionFilter Dataset** is run with a FASTA/GTF combination, the runtime of this tool will be dependent on the genome in question, more specifically the comprehensiveness of the annotations. For human genome GRCh38 the tool is expected to finish in around 46 hours, costing around $70 on a dedicated c5.9xlarge AWS instance or around $40 on the spot instance of the same type. For GRCh37  version the tools needs around 38 hours which costs around $58  on a dedicated c5.9xlarge AWS instance or around $30 on the spot instance of same type. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### References\n\n[1] [STAR-Fusion wiki](https://github.com/STAR-Fusion/STAR-Fusion/wiki)", "input": [{"name": "Reference FASTA or TAR archive", "encodingFormat": "application/x-tar"}, {"name": "Fusion annotation library", "encodingFormat": "text/plain"}, {"name": "GTF file", "encodingFormat": "application/x-gtf"}, {"name": "Maximum read length"}, {"name": "Number of threads"}, {"name": "Pfam domain file"}, {"name": "Skip STAR"}, {"name": "dfam_db"}], "output": [{"name": "STAR-Fusion index archive", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/STAR-Fusion/STAR-Fusion", "https://github.com/STAR-Fusion/STAR-Fusion/releases/download/STAR-Fusion-v1.5.0/STAR-Fusion-v1.5.0.FULL.tar.gz", "https://github.com/STAR-Fusion/STAR-Fusion/wiki", "https://github.com/STAR-Fusion/STAR-Fusion"], "applicationSubCategory": ["Gene Fusion", "RNA-Seq"], "project": "SBG Public Data", "creator": "Brian Haas", "softwareVersion": ["v1.0"], "dateModified": 1648036776, "dateCreated": 1587672096, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/star-genome-generate-2-7-3a/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/star-genome-generate-2-7-3a/2", "applicationCategory": "CommandLineTool", "name": "STAR Genome Generate", "description": "**STAR Genome Generate** produces the necessary index files for **STAR** alignment, from an input FASTA and GTF files.  \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n**STAR Genome Generate** is a tool that generates genome index files. One set of files should be generated per each FASTA/GTF combination. Once produced, these files could be used as long as FASTA/GTF combination stays the same. Also, **STAR Genome Generate** which produced these files and **STAR** aligner using them must be of the same toolkit version.\n\n* If the indexes for a desired FASTA/GTF pair have already been generated, make sure to supply the resulting TAR bundle to the tool input if you are using this tool in a workflow in order to skip unnecessary indexing and speed up the whole workflow process.\n* If you are providing a GFF3 file (which are usually downloaded from **NCBI's RefSeq database**) and wish to use **STAR** results for further downstream analysis, a good idea would be to set the **Exon parent name** (`--sjdbGTFtagExonParentTranscript`) option to **Parent**.\n* If you wish to run **STAR** in **multiple samples two-pass mode**, you need to provide the resulting **splice junction** outputs from **STAR** to the **List of annotated junctions** (`--sjdbFileChrStartEnd`) input of **STAR Genome Generate**, and generate the index archive with these, instead of supplying the GTF file [2]. \n\n###Common issues###\n\n* If the reference genome used has a bit number of contig sequences (>5000), a suggestion is to set the **Bins size** (`--genomeChrBinNbits`) parameter to the value of min(18, log2(GenomeLength/NumberOfReferences)). For example, for 3 gigaBase genome with 100,000 chromosomes/scaffolds, this is equal to 15.\n* If the reference genome used is a rather small genome, a suggestion is to set the **Pre-indexing string length** (`--genomeSAindexNbases`) parameter to the value of min(14, log2(GenomeLength)/2 - 1). For example, for 1 megaBase genome, this is equal to 9, for 100 kiloBase genome, this is equal to 7.\n* If **STAR Genome Generates** for some reason fails because of insufficient RAM problem, the **Limit Genome Generate RAM** (`--limitGenomeGenerateRAM`) parameter can be increased to make the RAM demands, though since the default value is 60GB, this should only be happening with extremely large reference files (for example, 30GB is enough for the human reference genome). \n* The GTF and FASTA files need to have compatible transcript IDs and chromosome names.\n\n### Changes Introduced by Seven Bridges\n\n* The directory containing the index files will be outputted as a TAR bundle (the **Genome files** output). This bundle can then be provided to the **STAR** aligner, which will automatically take care of untarring it and preparing it to run successfully without further issues. \n\n### Performance Benchmarking\n\nSince **STAR Genome Generate** is run with a FASTA/GTF combination, the runtime of this tool will be pretty much constant across a number of different genomes. For the human reference genome, the tool is expected to finish in around 30 minutes, costing around $0.75 on the c4.8xlarge AWS instance. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)\n[2] [STAR manual](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)", "input": [{"name": "Reference/Index files", "encodingFormat": "application/x-tar"}, {"name": "Bins size"}, {"name": "Pre-indexing string length"}, {"name": "Suffux array sparsity"}, {"name": "Gene annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Set exons feature"}, {"name": "Exon parent name"}, {"name": "Gene name"}, {"name": "\"Overhang\" length"}, {"name": "Extra alignment score"}, {"name": "Chromosome names"}, {"name": "Maximum genome suffic length"}, {"name": "Limit Genome Generate RAM"}, {"name": "List of annotated junctions", "encodingFormat": "text/plain"}, {"name": "Number of threads"}], "output": [{"name": "Genome Files", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/tree/master/source", "https://github.com/alexdobin/STAR/archive/2.7.3a.tar.gz", "https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf"], "applicationSubCategory": ["Alignment", "RNA-Seq"], "project": "SBG Public Data", "creator": "Alexander Dobin/CSHL", "softwareVersion": ["v1.0"], "dateModified": 1648048649, "dateCreated": 1581087148, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/star-long-2-6-0c/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/star-long-2-6-0c/8", "applicationCategory": "CommandLineTool", "name": "STARlong", "description": "**STARlong** is an ultrafast universal RNA-seq aligner for long read data. \n\n**STAR** (Spliced Transcripts Alignment to a Reference), an ultrafast RNA-seq aligner, is capable of mapping full length RNA sequences and detecting de novo canonical junctions, non-canonical splices, and chimeric (fusion) transcripts. **STAR** employs an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. It is optimized for mammalian sequence reads, but fine tuning of its parameters enables customization to satisfy unique needs [1].\n\n**STARlong** works with reads longer than 300 bases. \nIn the following documentation, **STARlong** will be abbreviated as **STAR**, for simplicity sake.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\nThe main purpose of **STAR** is to generate aligned BAM files (in genome and transcriptome coordinates) from RNA-seq data, which can later be used in further RNA studies, like gene expression analysis for example. \n\nSome important notes about this tool are: \n\n- The main input to the tool are **Reads** (`--readFilesIn`) in FASTQ format (single end or paired end), or unaligned BAM format.\n- **Genome files** in the form of a **STAR index archive**, outputted by the **STAR Genome Generate** tool, also need to be provided.\n- It\u2019s generally a good idea to always provide a GTF file to the inputs, if you want to get the **Transcriptome aligned reads** and **Reads per gene** outputs. \n- The main output of this tool is the **Aligned reads** output in coordinate sorted BAM format. The **Transcriptome aligned reads** BAM file is produced if the **Quantification mode** (`--quantMode`) parameter is set to **TranscriptomeSAM**. \n- Gene counts are produced if the **Quantification mode** (`--quantMode`) parameter is set to **GeneCounts**.  \n- **STAR** can detect chimeric transcripts, but the parameter **Min chimeric segment** (`--chimSegmentMin`) in *Chimeric Alignments*  category must be adjusted to a desired minimum chimeric segment length (12 is a good value, as recommended by the **STAR-Fusion** wiki). This output can later be used in **STAR-Fusion** for further fusion analysis. \n- If you want to use **STAR** results as an input to an RNA-seq differential expression analysis(using the **Cufflinks** app), set the parameter **Strand field flag** (`--outSAMstrandField`) to **intronMotif**.\n- Unmapped reads in FASTQ format are outputted on the **Unmapped reads** output if the **Output unmapped reads** (`--outReadsUnmapped`) parameter is set to the **Fastx** value. \n- Unmapped reads can be outputted within the main out BAM file on the **Aligned reads** and **Transcriptome aligned reads** outputs if the **Write unmapped in SAM** (`--outSAMunmapped`) parameter is set to **Within** or **Within KeepPairs**. \n- A basic **Two-pass mode** can be turned during the alignment step, which means that all the first pass junctions will be inserted into the genome indices for the second pass, by setting the **Two-pass mode** (`--twopassMode`) option to **Basic**. \n\n###Common issues###\n- For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n- For FASTQ reads in multi-file format (i.e. two FASTQ files for paired-end 1 and two FASTQ files for paired-end2), the proper metadata needs to be set (the following hierarchy is valid: **Sample ID/Library ID/Platform Unit ID/File Segment Number**).\n\n### Changes Introduced by Seven Bridges\n\n- All output files will be prefixed by the input sample ID (inferred from the **Sample ID** metadata if existent, of from filename otherwise), unless the **Output file name prefix** option is explicitly specified.\n- **Unmapped reads** in FASTQ format are by default unsorted by read ID. This can induce problems if these files are used in subsequent analysis (i.e. downstream alignment). The option to sort unmapped reads by read ID is added to this wrapper, by setting the **Sort unmapped reads** parameter to True. The suffix for the **Unmapped reads** output can be controlled by the **Unmapped output file names** options (the default is *Unmapped*).\n- The tool can accept uncompressed FASTQ files, as well as GZ and BZ2 compressed FASTQ files, without the user having to specify anything. Also, if unaligned BAM files are used as inputs, the single-end/paired-end flag (SE/PE) needn't be specified - it will inferred automatically using a built-in **Samtools** script. \n\n### Performance Benchmarking\n\nBelow is a table describing the runtimes and task costs for a couple of samples with different file sizes, with the following options in mind - unmapped reads are sorted by read id, output BAM is sorted by coordinate and basic two pass mode is turned on:\n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  Cost |  Instance (AWS) |\n|:---------------:|:-----------:|:----------:|:----------:|:-----------:|:--------:|:-----:|:----------:|\n|     RNA-Seq     |  2 x 230 MB |     Yes    |     1M     |     101     |   18min   | $0.40 | c4.8xlarge |\n|     RNA-Seq     |  2 x 4.5 GB |     Yes    |     20M     |     101     |   30min   | $0.60 | c4.8xlarge |\n|     RNA-Seq     | 2 x 17.4 GB |     Yes    |     76M    |     101     |   64min  | $1.20 | c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [STAR paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/)", "input": [{"name": "Read sequence", "encodingFormat": "application/x-bam"}, {"name": "Reads lengths"}, {"name": "Reads to map"}, {"name": "Junctions max number"}, {"name": "Collapsed junctions max number"}, {"name": "Output unmapped reads"}, {"name": "Quality conversion"}, {"name": "Output format"}, {"name": "Output sorting type"}, {"name": "SAM mode"}, {"name": "Strand field flag"}, {"name": "SAM attributes"}, {"name": "Write unmapped in SAM"}, {"name": "Sorting in SAM"}, {"name": "Primary alignments"}, {"name": "Read ID"}, {"name": "MAPQ value"}, {"name": "OR SAM flag"}, {"name": "AND SAM flag"}, {"name": "SAM header @HD"}, {"name": "SAM header @PG"}, {"name": "Sequencing center"}, {"name": "Library ID"}, {"name": "Median fragment length"}, {"name": "Platform"}, {"name": "Platform unit ID"}, {"name": "Sample ID"}, {"name": "Filtering type"}, {"name": "Multimapping score range"}, {"name": "Max number of mappings"}, {"name": "Max number of mismatches"}, {"name": "Mismatches to *mapped* length"}, {"name": "Mismatches to *read* length"}, {"name": "Min score"}, {"name": "Min score normalized"}, {"name": "Min matched bases"}, {"name": "Min matched bases normalized"}, {"name": "Motifs filtering"}, {"name": "Collapsed junctions reads"}, {"name": "Min overhang SJ"}, {"name": "Min unique count"}, {"name": "Min total count"}, {"name": "Min distance to other donor/acceptor"}, {"name": "Max gap allowed"}, {"name": "Gap open penalty"}, {"name": "Non-canonical gap open"}, {"name": "GC/AG and CT/GC gap open"}, {"name": "AT/AC and GT/AT gap open"}, {"name": "Log scaled score"}, {"name": "Deletion open penalty"}, {"name": "Deletion extension penalty"}, {"name": "Insertion Open Penalty"}, {"name": "Insertion extension penalty"}, {"name": "Max score reduction"}, {"name": "Search start point"}, {"name": "Search start point normalized"}, {"name": "Max seed length"}, {"name": "Filter pieces for stitching"}, {"name": "Max seeds per read"}, {"name": "Max seeds per window"}, {"name": "Max one-seed loci per window"}, {"name": "Min intron size"}, {"name": "Max intron size"}, {"name": "Max mates gap"}, {"name": "Min overhang"}, {"name": "Min overhang: annotated"}, {"name": "Min mapped length"}, {"name": "Min mapped length normalized"}, {"name": "Max windows per read"}, {"name": "Max transcripts per window"}, {"name": "Max transcripts per read"}, {"name": "Alignment type"}, {"name": "Soft clipping"}, {"name": "Max loci anchors"}, {"name": "Bin size"}, {"name": "Max bins between anchors"}, {"name": "Flanking regions size"}, {"name": "Min chimeric segment"}, {"name": "Min total score"}, {"name": "Max drop score"}, {"name": "Min separation score"}, {"name": "Non-GT/AG penalty"}, {"name": "Min junction overhang"}, {"name": "Quantification mode"}, {"name": "Reads to process in 1st step"}, {"name": "Two-pass mode"}, {"name": "Genome dir name"}, {"name": "Save junction files"}, {"name": "Chromosome names"}, {"name": "Set exons feature"}, {"name": "Exons' parents name"}, {"name": "Gene name"}, {"name": "\"Overhang\" length"}, {"name": "Extra alignment score"}, {"name": "Gene annotation file"}, {"name": "Clip 3p bases"}, {"name": "Clip 5p bases"}, {"name": "Clip 3p adapter sequence"}, {"name": "Max mismatches proportions"}, {"name": "Clip 3p after adapter seq"}, {"name": "Chimeric output type"}, {"name": "Genome files", "encodingFormat": "application/x-tar"}, {"name": "Max insert junctions"}, {"name": "Prohibit alignment type"}, {"name": "Limit BAM sorting memory"}, {"name": "Max number of multiple alignment"}, {"name": "Order of multimapping alignment"}, {"name": "IH attribute start value"}, {"name": "Output filter"}, {"name": "Splice junction stich max mismatch"}, {"name": "Chimeric segment gap"}, {"name": "Chimeric filter"}, {"name": "Unmapped output file names"}, {"name": "No read groups"}, {"name": "Sort unmapped reads"}, {"name": "List of annotated junctions"}, {"name": "Number of threads"}, {"name": "Minimum relative coverage"}, {"name": "Minimum number of bases for seeds in a window"}, {"name": "Protrusion of alignment ends"}, {"name": "Max main chimeric segment alignments"}, {"name": "Output file name prefix"}, {"name": "BAM remove duplicates type"}, {"name": "BAM remove duplicates mate2 bases"}, {"name": "Output wiggle type"}, {"name": "Output wiggle strand"}, {"name": "Output wiggle reference prefix"}, {"name": "Output wiggle normalization"}, {"name": "Filter intron strands"}, {"name": "Seed split min"}, {"name": "Variation VCF file"}, {"name": "Align insertion flush"}, {"name": "Out SAM TLEN"}, {"name": "Output BAM sorting bins"}, {"name": "WASP output mode"}, {"name": "Chimeric multimap max number"}, {"name": "Chimeric multimap score range"}, {"name": "Minimum no-chimeric drop score"}, {"name": "Paired end overlap min bases number"}, {"name": "Paired end overlap max mismatches proportion"}, {"name": "Out BAM compression"}, {"name": "Output BAM sorting threads"}, {"name": "Verbose indexing"}], "output": [{"name": "Aligned SAM/BAM"}, {"name": "Transcriptome alignments"}, {"name": "Reads per gene"}, {"name": "Log files"}, {"name": "Splice junctions"}, {"name": "Chimeric junctions"}, {"name": "Unmapped reads", "encodingFormat": "text/fastq"}, {"name": "Intermediate genome files", "encodingFormat": "application/x-tar"}, {"name": "Chimeric alignments", "encodingFormat": "application/x-sam"}, {"name": "Wiggle files"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR", "https://github.com/alexdobin/STAR/archive/2.6.0c.tar.gz", "https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf"], "applicationSubCategory": ["Alignment", "RNA"], "project": "SBG Public Data", "creator": "Alexander Dobin/CSHL", "softwareVersion": ["sbg:draft-2"], "dateModified": 1587671901, "dateCreated": 1526052558, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/strelka2-germline-2-9-10/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/strelka2-germline-2-9-10/5", "applicationCategory": "CommandLineTool", "name": "Strelka2 Germline", "description": "**Strelka2 Germline**  is a variant caller that calls germline small variants from mapped sequencing reads.\n\nIt is optimized for rapid clinical analysis of germline variation in small cohorts. Strelka's germline caller employs a haplotype model to improve call quality and provide short-range read-backed phasing in addition to a probabilistic variant calling model using indel error rates adaptively estimated from each input sample's sequencing data. Both germline and somatic callers include a final empirical variant rescoring step using a random forest model to reflect numerous features indicative of call reliability which may not be represented in the core variant calling probability model.\nStrelka's germline calling model employs a haplotype representation to improve variant call quality and provide short-range read backed phasing of all variants. The haplotype idenfiication method uses both a fast k-mer ranking approach for simple loci and local assembly for more complex or repetitive regions.\nAll methods are optimized by default for whole genome DNA-Seq, but are routinely tested for exome and amplicon inputs. [1]\n\n**Strelka Germline** can be used for finding and reporting variants in a single sample, or to produce multi-sample VCF if several samples are provided on the input **Input BAM file(s)**. Strelka accepts input read mappings from BAM or CRAM files, and optionally candidate and/or forced-call alleles (**Forced Genotyping**) from VCF. It reports all small variant predictions in VCF 4.1 format. Germline variant reporting uses the gVCF conventions to represent both variant and reference call confidence.\nAdditionally, there is an option to use Strelka for RNA-Seq germline variant calling (RNA sequencing should be chosen on the **Type of analysis** parameter), but this type of analysis is still in development and not fully supported [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n* There are different use cases related to the sequencing approach: Whole Exome sequencing (WES) (`--exome`); Targeted sequencing  (`--targeted`) [3]; RNA sequencing (`--rna`); Whole Genome sequencing (WGS) (default). This type of the sequencing samples should be chosen on the parameter **Type of analysis**.\n* Keep in mind that supplying the `--exome`, `--targeted`  or `--rna` flag at configuration time will provide appropriate settings for WES and other regional enrichment analyses, but currently this flag disables all high depth filters, which are designed to exclude pericentromeric reference compressions in the WGS case but cannot be applied correctly to a targeted analysis. For germline analysis, this mode also disables the empirical variant scoring (EVS) model, falling back to a set of simple threshold-based filters instead.\n* In an exome or targeted analysis, it may be desirable to restrict calling to the targeted regions by providing a BED file to the **Region BED file** input option. Note that this option acts independently of choosing **Type of analysis** to be Whole Exome sequencing/Targeted sequencing/RNA sequencing [2].\n\n### Changes Introduced by Seven Bridges\n\n* **Strelka2** requires BED files, if provided, to be coordinate sorted, bgziped compressed and tabix indexed. On the Seven Bridges platform this will happen automatically, so the input files provided on the inputs **Region BED file**  and **BED file with regions without gVCF block compression** should be in BED file format. \n* Output names for genome VCF output(s) per sample are changed to contain **sample_id** as a prefix, or, if **sample_id** is not provided, the prefix would be taken from the input filename. The same renaming convention stands for BAM files, if the parameter **Output realigned BAM file**   is chosen. \n\n###Common Issues and Important Notes\n\n* Setting the parameter **Type of analysis** to \"RNA sequencing\" will provide experimental settings for RNA-Seq variant calling. At present this flag disables all high depth filters which are designed to exclude pericentromeric reference compressions in the WGS case but cannot be applied correctly to RNA-Seq analysis. In addition, the expected allele frequency of heterozygous variants is expanded to account for allele-specific expression and a custom RNA-Seq empirical scoring model is used.\n* When **Region BED file** is specified, the `--exome`/`--targeted`/`--rna` flag is still required for exome or targeted data to get appropriate depth filtration behavior for non-WGS cases.\n* BAM/CRAM files used as inputs should be sorted and indexed. This can be done by using **Samtools Sort** and **Samtools Index** apps available in the public apps gallery. **Reference FASTA file** should be indexed too, which can be done by using SBG FASTA Indices.\n* Input Alignments cannot contain the \"=\" character in the SEQ field.\n* RG (read group) tags are ignored - each BAM file must represent one sample.\n* BAM records with basecall quality values greater than 70 are not supported, it is assumed that this indicates an offset error.\n* **Forced Genotyping** and **Indel Candidates** input VCF file(s) must be bgzip-compressed and tabix-indexed. All indel alleles must be left-shifted/normalized, any unnormalized allele will either be ignored or trigger a runtime error.\n* When using **Forced Genotyping** mode, be aware that in certain cases where a forced allele conflicts with Strelka's internal haplotype model, the forced variant will not be genotyped, but it will still appear in the VCF output with the \"NotGenotyped\" filter. \n* The germline multi-sample variants VCF, variants.vcf.gz, describes all potential variant loci across all analyzed samples. This VCF includes both high-confidence variant loci and lower-confidence potential variant loci, where 'high-confidence variant loci' refer to those that include a variant genotype passing all filters in at least one sample. To ease interpretation of this file, an additional filter NoPassedVariantGTs is appended to the VCF FILTER field at loci lacking any samples with a variant genotype passing all sample-level filters. This allows the high-confidence variant loci to be queried by simply requiring that the VCF FILTER field is set to PASS.\n* Input VCF files are accepted for a number of roles as described. All input VCF records are checked for compatibility with the given reference genome, in addition to role-specific checks described below. If any VCF record's REF field is not compatible with the reference genome, a runtime error will be triggered. \"Compatible with the reference genome\" means that each VCF record's REF base either matches the corresponding reference genome base, that the VCF record's REF base is 'N', or the reference genome base is any ambiguous IUPAC base code (all ambiguous base codes are converted to 'N' while importing the reference).\n* Strelka may have runtime issues while attempting to process a large number of small decoys and unplaced/unlocalized contigs found in (for example) GRCh38 and other reference genomes. This is due to a known issue with read realignment sporadically experiencing substantial slowdowns on very short, high-depth contigs. Until this issue can be resolved, runtime can be improved for such cases by excluding smaller contigs from analysis, which can be done by providing a BED file of all the chromosomes that should be included in the analysis on the **Region BED file** input [2].\n\n#### Known limitation of Strelka:\n\n* Strelka is capable of performing joint germline analysis on a family scale (10s of samples). This is primarily intended to facilitate de-novo variant analysis in families. **Strelka's germline** analysis capabilities are not currently optimized for population analysis and may become unstable or fail to leverage population variant constraints to improve calls at higher sample counts.\n* Strelka is capable of detecting SNVs and indels up to a predefined maximum size, currently defaulting to 49 bases or less.\n* Sequencing reads provided on the **input BAM file(s)** input are expected to come from a paired-end sequencing assay. Any input other than paired-end reads are ignored by default except to double-check for putative somatic variant evidence in the normal sample during somatic variant analysis. Read lengths above ~400 bases are not tested [2].\n\n###Performance Benchmarking\n\nIn the following table you can find estimates of Strelka run times and costs. All samples are aligned against the **human_g1k_v37_decoy** reference. The default number of threads is 8 as the default AWS instance has 8 CPUs and 15GB RAM.\n\n| Threads and CPUs        | Memory          | Input BAM size       | WGS/WES            | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:|------------- | ------------- | ------------- |\n|8|15000| 218MB + 130MB  | WES Joint Calling | 4 minutes| $0.03 |    c4.2xlarge |\n|8|15000| 4.5GB   | WES | 8 minutes| $0.06 |    c4.2xlarge |\n|8|15000| 160GB  | WGS |4 hours |  $1.59  |   c4.2xlarge |\n|36|58000| 160GB  | WGS | 1.5 hour |  $2.4  |    c4.8xlarge |\n|36|58000| 160GB + 150GB  | WGS Joint Calling | 2hours 45minutes|  $4.5 |    c4.8xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References \n\n[1] [Strelka2 GitHub](https://github.com/Illumina/strelka)\n\n[2] [Strelka2 User Guide](https://github.com/Illumina/strelka/blob/v2.9.x/docs/userGuide/README.md)\n\n[3] [Strelka2 Targeted Calling](https://github.com/Illumina/strelka/issues/71)", "input": [{"name": "Configuration file", "encodingFormat": "text/plain"}, {"name": "Input BAM file(s)", "encodingFormat": "application/x-bam"}, {"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Indel candidates", "encodingFormat": "application/x-vcf"}, {"name": "Forced Genotyping", "encodingFormat": "application/x-vcf"}, {"name": "Type of analysis"}, {"name": "Memory for tool execution (in MB)"}, {"name": "Number of threads for tool execution"}, {"name": "Region BED file", "encodingFormat": "text/x-bed"}, {"name": "BED file without block compression", "encodingFormat": "text/x-bed"}, {"name": "Ploidy file in VCF", "encodingFormat": "application/x-vcf"}, {"name": "Output realigned BAM file"}, {"name": "Maximum indel size"}, {"name": "Minimum mapping quality"}, {"name": "Call continuous variant frequencies"}, {"name": "CPU per job"}, {"name": "Memory per job (in MB)"}], "output": [{"name": "Germline VCF", "encodingFormat": "application/x-vcf"}, {"name": "Genome(s) variants", "encodingFormat": "application/x-vcf"}, {"name": "Internal statistics from the variant calling"}, {"name": "Internal statistics from the variant calling in XML format"}, {"name": "Realigned BAM", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Illumina/strelka", "https://github.com/Illumina/strelka/tree/master/docs/userGuide", "https://github.com/Illumina/strelka/releases/tag/v2.9.10"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Chris Saunders, Illumina", "softwareVersion": ["v1.0"], "dateModified": 1649156263, "dateCreated": 1606234281, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/strelka-1-0-15/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/strelka-1-0-15/6", "applicationCategory": "CommandLineTool", "name": "Strelka Workflow", "description": "**Strelka Workflow** is a somatic variant caller (SNVs and INDELs) which performs variant calling on matched tumor - normal BAM files.\n\nThe method uses a novel Bayesian approach which represents continuous allele frequencies for both tumor and normal samples, while leveraging the expected genotype structure of the normal. This is achieved by representing the normal sample as a mixture of germline variation with noise, and representing the tumor sample as a mixture of the normal sample with somatic\nvariation.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\nBAM files used as inputs, **Tumor file** (`--tumor`) and **Normal file** (`--normal`), should be sorted and indexed. This can be done by using **Sambamba Sort**. The **Reference file** (`-ref`) should be indexed too, this can be done by using **SBG FASTA Indices**. \n\nThere are two use cases related to the sequencing approach - one for **Whole Exome (WES) and targeted samples** and one for **Whole Genome samples (WGS)**. Keep in mind the following parameter when working with either of these: **Skip depth filters**:\n\n* When working with WGS samples, this parameter should be set to False in order to apply filtering by read depth\n* When working with WES samples, it should be set to True in order to skip filtering by read depth\n\n###Changes Introduced by Seven Bridges\n\n* Strelka's package comes with three pre-defined configuration files, each corresponding to an aligner used for generating the input BAM files. The configuration file for BWA is integrated into this tool as the starting point for parameters. Individual parameters can be changed through application settings. **It is possible to provide a custom configuration file. If that is the case, all settings are overwritten by it.** In addition, a user can provide one of the two additional pre-defined configuration files (Eland, and Isaac aligner) which can be found in the \ntool's package located [here](https://sites.google.com/site/strelkasomaticvariantcaller/home/download).\n\n* In case of a failed task, error messages are parsed from error files so they can be seen in the job error log (`job.err.log`) file in **View stats & logs** task section.\n\n* Outputs are named based on the **Tumor file** (`--tumor`) **Sample ID** metadata field or input file name in the case that that metadata field is missing.\n\n* The default number of threads is set to 8 as this is the number of CPUs on the default AWS instance.\n\n###Common Issues and Important Notes\n\n- Parameter **Skip depth filters** should be set to True when using WES and targeted samples. This filtering step is skipped when using these samples because their average coverage across genome is by far smaller than the coverage at sequenced intervals.\n- Strelka creates a folder for each chromosome in the reference. However, Strelka does not sanitize chromosome names. For this reason, characters that have special meaning in a linux shell (e.g \\ | $ *) should not be present in chromosome names. This is often an issue with decoy contigs or unassembled contigs\n- According to the [Strelka FAQ](https://sites.google.com/site/strelkasomaticvariantcaller/home/faq) the following BAM records are incompatible with Strelka (quoted):\n  - Alignments which use the match/mismatch (\"=\"/\"X\") CIGAR notation\n  - Records where the \"=\" character is used in the SEQ field\n  - BAM records with basecall quality values greater than 70 (not supported on the assumption that this indicates an offset error)\n- Unless input BAM files (**Tumor file** (`--tumor`) and **Normal file** (`--normal`)) are supplied to Strelka sorted and indexed, Strelka task will fail. We recommend using **Sambamba Sort** app to sort and index BAM files if needed\n- The **Reference file** (--ref) has to be indexed. We recommend using **SBG FASTA Indices** app to index the reference file if needed\n- Strelka separates SNVs and INDELs into two VCF files. If you want to combine them, use a separate tool such as **GATK CombineVariants** with the `assume_identical_samples` option enabled.\nIn addition there are two groups of output files - **all** (**All SNPs** and **All indels**) and **passed** (**Passed somatic SNPs** and **Passed somatic Indels**). File with keyword **all** in its filename contains all variants, even those that failed the filtering steps, while the **passed** one\ncontains those that only passed filtering\n- If there are many variants in the **all** variants VCF and no variants in the *passed* variants VCF when running Strelka on WES samples, please check if **Skip depth filters** (`is_skip_depth_filters`) is set to True\n- There are two ways to configure your Strelka execution:\n  1. You can use your own **Configuration file** and provide it as input. Strelka already provides configuration files for common aligners which are available as a part of Strelka's package\n  which can be found [here](https://sites.google.com/site/strelkasomaticvariantcaller/home/download)\n  2. You can customize all parameters by defining the tool settings. In this case, all settings are set to the **BWA** defaults, unless changed by the user\n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **Strelka** run times and costs. All samples are aligned against a **hg37 human reference**. The default number of threads is 8 as the default AWS instance has 8 CPUs and 15GB RAM.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Read length | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:|:-------------:| ------------- | ------------- | ------------- |\n|8|14000| 2 x 200MB     | WES | 150| 5 minutes| $0.04 |    c4.2xlarge |\n|8|14000| 7GB + 13GB     | WES | 150| 35 minutes| $0.24 |    c4.2xlarge |\n|36|55000| 7GB + 13GB     | WES | 150| 13 minutes| $0.34 |    c4.8xlarge |\n|8|14000| 160GB + 145GB     | WGS | 150| 7 hours | $2.8 |    c4.2xlarge |\n|36|55000| 75GB + 120GB    | WGS | 150| 1 hour 17 minutes | $2.07 |    c4.8xlarge |\n|36|55000| 65GB + 104GB    | WGS | 150| 1 hour | $1.59 |    c4.8xlarge |\n\nStrelka is a fast tool compared to other similar tools. Parameters showing the fastest executions are 36 threads and CPUs with 55000MB of memory.\n\n###References\n\n[1] [Strelka: accurate somatic small-variant calling from sequenced tumor\u2013normal sample pairs](https://academic.oup.com/bioinformatics/article/28/14/1811/218573)", "input": [{"name": "Tumor file", "encodingFormat": "application/x-bam"}, {"name": "Normal file", "encodingFormat": "application/x-bam"}, {"name": "Reference file", "encodingFormat": "application/x-fasta"}, {"name": "Configuration file"}, {"name": "Number of CPU threads"}, {"name": "Memory for job in megabytes"}, {"name": "Skip depth filters"}, {"name": "Max input depth"}, {"name": "Depth filter multiple"}, {"name": "SNV maximum filtered basecall fraction"}, {"name": "SNV maximum spanning deletion fraction"}, {"name": "Indel maximum reference repeat"}, {"name": "Indel maximum window filtered basecall fraction"}, {"name": "Indel maximum interrupted homopolymer length"}, {"name": "Somatic SNV Prior"}, {"name": "Somatic indel prior"}, {"name": "Somatic SNV noise probability"}, {"name": "Somatic indel noise probability"}, {"name": "Somatic SNV noise fraction attributed to strand bias"}, {"name": "Minimum Tier1 MAPQ score"}, {"name": "Minimum Tier2 MAPQ"}, {"name": "Somatic SNV lower bound"}, {"name": "Somatic indel quality lower bound"}, {"name": "Bin size"}, {"name": "Ignore conflicting read names"}, {"name": "Used allele count minimum qscore"}, {"name": "Candidate indel input VCFs", "encodingFormat": "application/x-vcf"}, {"name": "Force output VCF", "encodingFormat": "application/x-vcf"}, {"name": "Minimum small candidate indel read fraction"}, {"name": "Eland compatibility"}, {"name": "Remap input softclip"}, {"name": "Output realigned BAM file"}], "output": [{"name": "Realigned normal BAM file", "encodingFormat": "application/x-bam"}, {"name": "Realigned tumor BAM file", "encodingFormat": "application/x-bam"}, {"name": "Passed somatic SNPs", "encodingFormat": "application/x-vcf"}, {"name": "Passed somatic Indels", "encodingFormat": "application/x-vcf"}, {"name": "All SNPs", "encodingFormat": "application/x-vcf"}, {"name": "All indels", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Christopher Saunders (csaunders@illumina.com)", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155968, "dateCreated": 1520424992, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/stringtie-2-1-3/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/stringtie-2-1-3/7", "applicationCategory": "CommandLineTool", "name": "StringTie", "description": "**StringTie** is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional *de novo* assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only alignments of short reads that can also be used by other transcript assemblers, but also alignments of longer sequences that have been assembled from those reads.  In order to identify differentially expressed genes between experiments, **StringTie**'s output can be processed by specialized software like **Ballgown**, **Cuffdiff** or other programs (**DESeq2**, **edgeR**, etc.) [1].\n\n*A list of __all inputs and parameters__ with corresponding descriptions can be found at the bottom of this page.*\n\n*__Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.__*\n\n### Common Use Cases\n\n* __StringTie__ can be used as a transcriptome assembler. It takes aligned reads as input (note that the provided BAM file has to be sorted by coordinate) and estimates expression level of genes and transcripts as it assembles them [3]. __Reference annotation file__ (`-G`) can be provided to guide the assembly process, though this is optional. If the ultimate goal is differential expression analysis assembled transcripts for each sample have to be merged into unified set of transcripts for all samples using __StringTie Merge__. After the merging step is done additional run of __StringTie__ is required to re-estimate merged transcripts for each sample using __Keep annotated transcripts only__ (`-e`) option that tells __StringTie__ to estimate expression levels only for reference transcripts and the __Create input files for Ballgown and DESeq2__ (`-B`) option that enables creating count-data input files for __Ballgown__ and __DESeq2__. For more details refer to StringTie protocol paper [3].\n* __StringTie__ can be used just for quantification. For this purpose __Reference annotation file__ (`-G`) has to be provided and the __Keep annotated transcripts only__ (`-e`) parameter should be set to True, telling StringTie to only estimate expression levels of genes and transcripts present in the annotation file. This mode should be used in experiments when there is a predefined list of genes or transcripts of interest or in case of re-estimating merged transcripts as described above.\n* __Text feature file__ input loads a list of point-features to guide the transcriptome assembly. Accepted point features are transcription start sites (TSS) and polyadenylation sites (CPAS). Details on how this file should look like can be found in the input's description info.\n* To output a TAB file in which gene abundance estimation will be reported, set the __Output gene abundance__ (`-A`) parameter to True.\n* To output a file with all transcripts in the provided reference file that are fully covered by the reads, set the __Output covered reference transcripts__ (`-C`) parameter to True. This option requires **Reference annotation file** (`-G`) to be provided.\n\n### Changes Introduced by Seven Bridges\n\n* In order to enable __StringTie__ to produce quantification tables tailored for __DESeq2__ or [EdgeR](http://bioconductor.org/packages/release/bioc/html/edgeR.html), the [`prepDE.py`](https://ccb.jhu.edu/software/stringtie/dl/prepDE.py) python script [2] that extracts raw counts from **Ballgown** input tables is embedded within the app. Parameters of the scripts have default values except names of gene and transcript count matrices. To obtain raw counts suitable for __DESeq2__ set **Create input files for Ballgown and DESeq2** (`-B`) to True. This will also produce __Ballgown__ input tables as these tables are needed for obtaining __DESeq2__ raw counts.\n* __Ballgown__ input tables (5 CTAB files) are outputted as an archive TAR file containing all 5 files. This TAR file can be directly fed to __Ballgown__ without any further modification given the metadata field __Sample ID__ is properly set (check the Common Issues and Important Notes section).\n\n### Common Issues and Important Notes\n\n* If you want to perform differential expression analysis using quantification produced by __StringTie__, make sure that all input BAM files on the **Aligned reads** input have metadata field __Sample ID__ properly set. The value of __Sample ID__ metadata field will be used to match corresponding expression (count) and phenotype data by downstream tools for differential expression (__DESeq2__ and __Ballgown__).\n* All BAM files on the **Aligned reads** input need to be sorted by coordinates.\n\n### Performance Benchmarking\n\nThe execution time for quantification/assembly+quantification for human RNA-Seq data (~ 160M of reads/12GB) takes somewhat less than 15 minutes on the default instance; the price is very low (~ 0.05$). Unless specified otherwise, the default instance used to run the __StringTie__ tool will be c4.xlarge (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [StringTie home page](https://ccb.jhu.edu/software/stringtie/index.shtml)\n\n[2] [StringTie manual page - using StringTie with DESeq2 and EdgeR](https://ccb.jhu.edu/software/stringtie/index.shtml?t=manual#deseq)\n\n[3] [HISAT, StringTie, Ballgown protocol paper](http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html)", "input": [{"name": "Number of threads"}, {"name": "Reference annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Transcripts name prefix"}, {"name": "Minimum isoform abundance"}, {"name": "Minimum isoform length"}, {"name": "Output gene abundance"}, {"name": "Output covered reference transcripts"}, {"name": "Minimum junction coverage"}, {"name": "Disable trimming"}, {"name": "Minimum read coverage"}, {"name": "Minimum locus gap separation value"}, {"name": "Create input files for Ballgown and DESeq2"}, {"name": "Keep annotated transcripts only"}, {"name": "Maximum fraction of multiply mapped reads"}, {"name": "Ignore alignments on the specified sequence"}, {"name": "Aligned reads", "encodingFormat": "application/x-bam"}, {"name": "Minimum anchor length for junctions"}, {"name": "Stranded library - firststrand"}, {"name": "Stranded library - secondstrand"}, {"name": "Text feature file"}, {"name": "Minimum coverage for single-exon transcripts"}, {"name": "Conservative mode"}, {"name": "Multi-mapping correction"}, {"name": "Long reads processing"}, {"name": "-L alternative"}, {"name": "Long reads splice sites window"}, {"name": "Viral long reads"}, {"name": "Memory per job [MB]"}, {"name": "Output prefix"}], "output": [{"name": "Assembled transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Gene abundance estimation"}, {"name": "Covered reference transcripts", "encodingFormat": "application/x-gtf"}, {"name": "Archived ballgown input tables", "encodingFormat": "application/x-tar"}, {"name": "DESeq2 gene count matrix"}, {"name": "DESeq2 transcript count matrix"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/gpertea/stringtie"], "applicationSubCategory": ["Quantification", "RNA-Seq"], "project": "SBG Public Data", "creator": "Johns Hopkins University, Center for Computational Biology", "softwareVersion": ["v1.0"], "dateModified": 1648049435, "dateCreated": 1612220557, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/stringtie-merge-2-1-3/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/stringtie-merge-2-1-3/5", "applicationCategory": "CommandLineTool", "name": "StringTie Merge", "description": "The **StringTie Merge** tool takes a list of GTF/GFF files as its input and merges/assembles these transcripts into a non-redundant set of transcripts [1]. This tool is used as an intermediate step in the new Tuxedo differential expression analysis pipeline described in [2] to generate a global, unified set of transcripts (isoforms) across multiple RNA-Seq samples.\n\n*A list of __all inputs and parameters__ with corresponding descriptions can be found at the bottom of this page.*\n\n*__Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.__*\n\n### Common Use Cases\n\n* This tool should be used after __StringTie__ transcript assembling of each sample in the experiment. This tool creates a set of transcripts that is consistent across all samples, so that the transcripts can be compared in subsequent steps of the analysis [2]. For more details refer to the StringTie protocol paper [2].\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made.\n\n### Common Issues and Important Notes\n\n* No common issues specific to the tool's execution on the Seven Bridges Platform have been detected. \n\n### Performance Benchmarking\n\n* The execution time for merging 8 assembled transcripts takes several minutes on the default instance; the price is negligible (~ 0.01$). Unless specified otherwise, the default instance used to run the __StringTie Merge__ tool will be c4.large (AWS).\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [StringTie manual page](http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual)\n\n[2] [HISAT, StringTie, Ballgown protocol paper](http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html)", "input": [{"name": "Reference annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Minimum transcript length"}, {"name": "Minimum transcript coverage"}, {"name": "Minimum transcript FPKM"}, {"name": "Minimum transcript TPM"}, {"name": "Minimum isoform abundance"}, {"name": "Keep transcripts with retained introns"}, {"name": "Transcripts name prefix"}, {"name": "Input GTF/GFF files", "encodingFormat": "application/x-gtf"}, {"name": "Gap length"}, {"name": "Output prefix"}, {"name": "Memory per job [MB]"}, {"name": "CPU per job"}], "output": [{"name": "Merged transcripts", "encodingFormat": "application/x-gtf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/gpertea/stringtie"], "applicationSubCategory": ["Quantification", "RNA-Seq"], "project": "SBG Public Data", "creator": "Johns Hopkins University, Center for Computational Biology", "softwareVersion": ["v1.0"], "dateModified": 1648049435, "dateCreated": 1612220557, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tabix-bgzip-1-11/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tabix-bgzip-1-11/2", "applicationCategory": "CommandLineTool", "name": "Tabix BGZIP", "description": "**Tabix BGZIP** is used for compressing/decompressing (BAM, VCF, BED, ...) any file in BGZF and from BGZF format.\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n###Common Use Cases\n\n**Tabix BGZIP** is used in cases where tools expect inputs which are \u2018BGZF\u2019 formatted. In most of these tools, it is expected that \u2018BGZF\u2019 file is indexed, so **Tabix Index** is called after **Tabix BGZIP**.\n\nThere are three modes for **Tabix BGZIP**:\n 1. it will compress the input if the **decompress** parameter is not set at all or set to *False*,\n 2. it will decompress the input if the **decompress** parameter is set to *True*,\n 3. if the input file has suffix \u2018.gz\u2019 and **decompress** parameter is not set at all or set to *False*, it will decompress the input using the gzip command and then compress it using **Tabix BGZIP**.\n\n###Changes Introduced by Seven Bridges\n**Tabix BGZIP** is extended so that it will work with the given compressed input and the **decompress** parameter not set or set to *False* in the way described in the **Common Use Cases** section, case 3.\nAlso, the wrapper around the tool doesn\u2019t support following parameters:  '-- offset', '--help', '--stdout', '--size' as the tool used with these options outputs the result to stdout. Options '--index', '--index-name', '--reindex', '--rebgzip' are not part of the wrapper as these functions are redundant either with wrapped options of **Tabix BGZIP** or they can be achieved using **Tabix Index**. Option '--force' isn\u2019t a part of the wrapper as it is used by default on the command line.\n\n###Common Issues and Important Notes\nThere aren't any common issues.\n\n###Performance Benchmarking\n**Tabix BGZIP** is not CPU/Memory intensive. The default c4.2 AWS instance can be used.\nCost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.", "input": [{"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Decompress input file"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Compress level"}, {"name": "Number of threads to be used"}], "output": [{"name": "Compressed or decompressed file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/htslib/tree/master"], "applicationSubCategory": ["Utilities", "VCF Processing"], "project": "SBG Public Data", "creator": "Heng Li -  Broad Institue", "softwareVersion": ["v1.1"], "dateModified": 1648038175, "dateCreated": 1612275838, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tabix-bgzip-1-9-cwl1-0/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tabix-bgzip-1-9-cwl1-0/17", "applicationCategory": "CommandLineTool", "name": "Tabix BGZIP CWL1.0", "description": "**Tabix BGZIP** is used for compressing/decompressing (BAM, VCF, BED, ...) any file in BGZF and from BGZF format.\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n###Common Use Cases\n\n**Tabix BGZIP** is used in cases where tools expect inputs which are \u2018BGZF\u2019 formatted. In most of these tools, it is expected that \u2018BGZF\u2019 file is indexed, so **Tabix Index** is called after **Tabix BGZIP**.\n\nThere are three modes for **Tabix BGZIP**:\n 1. it will compress the input if the **decompress** parameter is not set at all or set to *False*,\n 2. it will decompress the input if the **decompress** parameter is set to *True*,\n 3. if the input file has suffix \u2018.gz\u2019 and **decompress** parameter is not set at all or set to *False*, it will decompress the input using the gzip command and then compress it using **Tabix BGZIP**.\n\n###Changes Introduced by Seven Bridges\n**Tabix BGZIP** is extended so that it will work with the given compressed input and the **decompress** parameter not set or set to *False* in the way described in the **Common Use Cases** section, case 3.\nAlso, the wrapper around the tool doesn\u2019t support following parameters:  '-- offset', '--help', '--stdout', '--size' as the tool used with these options outputs the result to stdout. Options '--index', '--index-name', '--reindex', '--rebgzip' are not part of the wrapper as these functions are redundant either with wrapped options of **Tabix BGZIP** or they can be achieved using **Tabix Index**. Option '--force' isn\u2019t a part of the wrapper as it is used by default on the command line.\n\n###Common Issues and Important Notes\nThere aren't any common issues.\n\n###Performance Benchmarking\n**Tabix BGZIP** is not CPU/Memory intensive. The default c4.2 AWS instance can be used.\nCost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.", "input": [{"name": "Input file", "encodingFormat": "application/x-sam"}, {"name": "Decompress input file"}, {"name": "Memory per job"}, {"name": "CPU per job"}, {"name": "Compress level"}, {"name": "Number of threads to be used"}], "output": [{"name": "Compressed or decompressed file", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/htslib/tree/master"], "applicationSubCategory": ["Utilities", "VCF Processing"], "project": "SBG Public Data", "creator": "Heng Li -  Broad Institue", "softwareVersion": ["v1.0"], "dateModified": 1648038175, "dateCreated": 1577455001, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tabix-index-1-11/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tabix-index-1-11/2", "applicationCategory": "CommandLineTool", "name": "Tabix Index", "description": "**Tabix Index** indexes a TAB-delimited genome position file\u00a0IN.TAB.BGZ\u00a0and creates an index file (IN.TAB.BGZ.TBI\u00a0or\u00a0IN.TAB.BGZ.CSI). The input data file must be position sorted and compressed by\u00a0bgzip\u00a0which has a\u00a0gzip-like interface[1].\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n###Common Use Cases\n**Tabix Index** can be used to generate index or to pass-through data and data\u2019s index if the index file is provided as input.\n\nDepending on the usage of the tool, whether it is used as a stand-alone tool or as a part of a workflow, **Don't output data file** parameter is used as follows:\n 1. If the index is expected to be generated inside a workflow, **Don't output data file** should be set to *False* or left unset. \n 2. If the index is generated when the tool is run separately, **Don't output data file** should be set *True*. This way only index will be outputted and there will be no duplication of the files on the output. \n\n###Changes Introduced by Seven Bridges\nPlease note that in this tool, **Tabix** is wrapped only for indexing. Querying and other options are not supported.\n\n###Common Issues and Important Notes\nAs described by tool\u2019s official description, **Select input file format** ('-p', '-- preset STR') should not be used with any of the following options: **Column number for sequence names** ('-s', '-- sequence INT'), **Column number for region start** ('-b', '-- begin INT'), **Column number for region end** ('-e', '-- end INT'), **Skip comment lines starting with character CHAR** ('-c', '-- comment CHAR') and **Specify if the position in the data file is 0 based** ('-0', '-- zero-based'). As a consequence, wrapper will ignore '-s', '-b', '-e', '-c', '-0' options (will not be present in the command-line) if they are used together with '-p'.\n\n###Performance Benchmarking\n**Tabix Index** is not CPU/Memory intensive. The default c4.2 AWS instance can be used.\nCost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n### References\n\n[1] [Tabix](http://www.htslib.org/doc/tabix.html)", "input": [{"name": "Select input file format"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Skip first N lines"}, {"name": "Column number for region end"}, {"name": "Column number for sequence names"}, {"name": "Column number for region start"}, {"name": "Skip comment lines starting with character CHAR"}, {"name": "Specify if the position in the data file is 0 based"}, {"name": "Reserve N MB of RAM"}, {"name": "Index file"}, {"name": "Don't output data file"}, {"name": "Cpus to be used on the platform"}, {"name": "Generate CSI index for VCF"}, {"name": "Set minimal interval size for CSI indices to 2^INT"}], "output": [{"name": "Tabix indexed file", "encodingFormat": "application/x-vcf"}, {"name": "Tabix index"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/htslib/tree/master"], "applicationSubCategory": ["Indexing", "Utilities", "VCF Processing"], "project": "SBG Public Data", "creator": "Heng Li -  Broad Institue", "softwareVersion": ["v1.1"], "dateModified": 1648038175, "dateCreated": 1612275838, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tabix-index-1-9-cwl1-0/18", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tabix-index-1-9-cwl1-0/18", "applicationCategory": "CommandLineTool", "name": "Tabix Index CWL1.0", "description": "**Tabix Index** indexes a TAB-delimited genome position file\u00a0IN.TAB.BGZ\u00a0and creates an index file (IN.TAB.BGZ.TBI\u00a0or\u00a0IN.TAB.BGZ.CSI). The input data file must be position sorted and compressed by\u00a0bgzip\u00a0which has a\u00a0gzip-like interface[1].\n\nA list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.\n\n###Common Use Cases\n**Tabix Index** can be used to generate index or to pass-through data and data\u2019s index if the index file is provided as input.\n\nDepending on the usage of the tool, whether it is used as a stand-alone tool or as a part of a workflow, **Don't output data file** parameter is used as follows:\n 1. If the index is expected to be generated inside a workflow, **Don't output data file** should be set to *False* or left unset. \n 2. If the index is generated when the tool is run separately, **Don't output data file** should be set *True*. This way only index will be outputted and there will be no duplication of the files on the output. \n\n###Changes Introduced by Seven Bridges\nPlease note that in this tool, **Tabix** is wrapped only for indexing. Querying and other options are not supported.\n\n###Common Issues and Important Notes\nAs described by tool\u2019s official description, **Select input file format** ('-p', '-- preset STR') should not be used with any of the following options: **Column number for sequence names** ('-s', '-- sequence INT'), **Column number for region start** ('-b', '-- begin INT'), **Column number for region end** ('-e', '-- end INT'), **Skip comment lines starting with character CHAR** ('-c', '-- comment CHAR') and **Specify if the position in the data file is 0 based** ('-0', '-- zero-based'). As a consequence, wrapper will ignore '-s', '-b', '-e', '-c', '-0' options (will not be present in the command-line) if they are used together with '-p'.\n\n###Performance Benchmarking\n**Tabix Index** is not CPU/Memory intensive. The default c4.2 AWS instance can be used.\nCost can be significantly reduced by using spot instances. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n### References\n\n[1] [Tabix](http://www.htslib.org/doc/tabix.html)", "input": [{"name": "Select input file format"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Skip first N lines"}, {"name": "Column number for region end"}, {"name": "Column number for sequence names"}, {"name": "Column number for region start"}, {"name": "Skip comment lines starting with character CHAR"}, {"name": "Specify if the position in the data file is 0 based"}, {"name": "Reserve N MB of RAM"}, {"name": "Index file"}, {"name": "Don't output data file"}, {"name": "Cpus to be used on the platform"}, {"name": "Generate CSI index for VCF"}, {"name": "Set minimal interval size for CSI indices to 2^INT"}], "output": [{"name": "Tabix indexed file", "encodingFormat": "application/x-vcf"}, {"name": "Tabix index"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/samtools/htslib/tree/master"], "applicationSubCategory": ["Indexing", "Utilities", "VCF Processing"], "project": "SBG Public Data", "creator": "Heng Li -  Broad Institue", "softwareVersion": ["v1.0"], "dateModified": 1648038175, "dateCreated": 1577455001, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tophat2/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tophat2/13", "applicationCategory": "CommandLineTool", "name": "TopHat2", "description": "## TopHat2 (version 2.1.1)\n\n[TopHat2](https://ccb.jhu.edu/software/tophat/manual.shtml) is a program that aligns RNA-Seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml).\n\nTopHat2 can also align reads directly to a transcriptome. If provided with annotation of known transcripts, TopHat2 constructs a virtual transcriptome and uses Bowtie2 to align reads to this reference first. Reads that do not align to the transcriptome are then mapped on the reference genome. The reads that did align on the transcriptome will be converted to genomic mappings (spliced as needed) and merged with the novel mappings and junctions in the final output.\n\nThis version of TopHat only accepts reads in FASTQ format. It is optimized for reads that are at least 75bp long.\n\n**Common issues:**\nOne of the most common issues when running TopHat is incompatibility between reference genome and annotations. Please, make sure that you are using compatible FASTA (from which you have created tar bundle with index files) and GTF files in order to run tasks successfully. If you suspect your task has failed due to this incompatibility, you can check the last line in `job.err.log` file which would look as following if your assumptions are correct: `Error: Couldn't build bowtie index with err = 1`.\n\n**Q&A:**\n\n***Q: What should I do if I already have Bowtie2 index files, not archived as tar bundle?***\n\n***A***: You can provide your *.bt2 files to [SBG Compressor](https://igor.sbgenomics.com/public/apps#admin/sbg-public-data/sbg-compressor-1-0/) app from our public apps and set \"TAR\" as your output format. After the task is finished, **you should assign common prefix of the index files to the `Reference genome` metadata field** and your TAR is ready for use.\n\n***Example:***\nIndexed files: chr20.1.bt2, chr20.2.bt2, chr20.3.bt2, chr20.4.bt2, chr20.rev.1.bt2, chr20.rev.2.bt2\n\nMetadata - `Reference genome`: **chr20**\n\n__Important note: In case of paired-end alignment it is crucial to set metadata 'paired-end' field to 1/2. Sequences specified as mate 1s must correspond file-for-file and read-for-read with those specified for mate 2s. Reads may be a mix of different lengths. In case of unpaired reads, the same metadata field should be set to '-'. Only one type of alignment can be performed at once, so all specified reads should be either paired or unpaired.__", "input": [{"name": "Annotations", "encodingFormat": "application/x-gtf"}, {"name": "Seed extension attempts"}, {"name": "Seed substring length"}, {"name": "Allowed mismatch number"}, {"name": "Max number of re-seed"}, {"name": "Disallow gaps"}, {"name": "Constant A"}, {"name": "Coefficient B"}, {"name": "Function type"}, {"name": "Mismatch penalty"}, {"name": "Constant A"}, {"name": "Coefficient B"}, {"name": "Function type"}, {"name": "Ambiguous character penalty"}, {"name": "Bowtie2 preset"}, {"name": "Read gap penalties"}, {"name": "Reference gap penalties"}, {"name": "Constant A"}, {"name": "Coefficient B"}, {"name": "Function type"}, {"name": "Bowtie -n"}, {"name": "Coverage search"}, {"name": "Deletions", "encodingFormat": "text/plain"}, {"name": "Fusion anchor length"}, {"name": "Fusion ignore chromosomes"}, {"name": "Fusion minimum distance"}, {"name": "Fusion multipairs"}, {"name": "Fusion multireads"}, {"name": "Fusion read mismatches"}, {"name": "Fusion search"}, {"name": "Insertions", "encodingFormat": "text/plain"}, {"name": "Keep FASTA order"}, {"name": "Library type"}, {"name": "Mate inner distance"}, {"name": "Mate standard deviation"}, {"name": "Maximum coverage intron"}, {"name": "Maximum deletion length"}, {"name": "Maximum insertion length"}, {"name": "Maximum intron length"}, {"name": "Maximum multihits"}, {"name": "Maximum segment intron"}, {"name": "Microexon search"}, {"name": "Minimum anchor length"}, {"name": "Minimum coverage intron"}, {"name": "Minimum intron length"}, {"name": "Minimum segment intron"}, {"name": "Disable discordant alignments"}, {"name": "Disable mixed alignments"}, {"name": "No novel indels"}, {"name": "No novel juncs"}, {"name": "Disable BAM sorting"}, {"name": "Prefilter multihits"}, {"name": "Raw junctions"}, {"name": "Read edit distance"}, {"name": "Read gap length"}, {"name": "Read mismatches"}, {"name": "Read realign edit distance"}, {"name": "Bowtie index archive", "encodingFormat": "application/x-tar"}, {"name": "Report secondary alignments"}, {"name": "Segment length"}, {"name": "Segment mismatches"}, {"name": "Splice mismatches"}, {"name": "Transcriptome max hits"}, {"name": "Transcriptome only"}, {"name": "Read sequence", "encodingFormat": "text/fastq"}], "output": [{"name": "Align Summary", "encodingFormat": "text/plain"}, {"name": "Aligned BAM", "encodingFormat": "application/x-bam"}, {"name": "TopHat Deletions", "encodingFormat": "text/x-bed"}, {"name": "TopHat Insertions", "encodingFormat": "text/x-bed"}, {"name": "TopHat Junctions", "encodingFormat": "text/x-bed"}, {"name": "Unmapped reads", "encodingFormat": "application/x-bam"}], "softwareRequirements": ["MemoryRequirement", "ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["RNA-Seq", "Alignment"], "project": "SBG Public Data", "creator": "Daehwan Kim, Steven Salzberg, Cole Trapnell/Johns Hopkins University, University of Washington", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648033584, "dateCreated": 1453799839, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tmap-index/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tmap-index/10", "applicationCategory": "CommandLineTool", "name": "Torrent Mapping Alignment Program Index", "description": "TMAP's Index creates a compact version of the reference genome and associated index. The index is stored as a compressed suffix array using the FM-index and BWT transform. A hash into this index accelerates this lookups of DNA sequences in this index. \n\nIn fact, a second additional index of the reference genome is created that indexes the reverse (but not complimented) reference genome. This second index further speeds up the search time.\n\n**This tool outputs an archive file with all indices and the input FASTA file. If you provide it with an already made archive file, it will not do anything aside from passing it further.** \n\n### Common issues\n\nNo issues were noticed.", "input": [{"name": "Input fasta or archive file"}, {"name": "The occurrence interval"}, {"name": "K-mer occurrence hash width"}, {"name": "Suffix array interval"}, {"name": "BWT construction algorithm"}, {"name": "Do not validate BWT hash"}, {"name": "Print index format and exit"}, {"name": "Pretend that reference build is old"}, {"name": "Memory for job"}], "output": [{"name": "Index archive", "encodingFormat": "application/x-tar"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/TMAP/src/index", "https://github.com/iontorrent/TS", "https://github.com/iontorrent/TS/blob/master/Analysis/TMAP/doc/tmap-book.pdf"], "applicationSubCategory": ["Indexing"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1476270166, "dateCreated": 1457032405, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tmap-map1/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tmap-map1/10", "applicationCategory": "CommandLineTool", "name": "Torrent Mapping Alignment Program Map1", "description": "*TMAP aligner, based on BWA short-read algorithm, suited for Ion Torrent reads less than 150 bases.*\n\nTMAP is an aligner specifically tuned for data from the Ion Torrent sequencers. It uses a series of algorithms to map data to an indexed reference sequence. \n\nThis tool is running TMAP's \"Map1\" algorithm. Map1 is used to quickly map short sequences to a reference genome by intelli-gently enumerating errors. This algorithm is not well suited for longer reads (<150bp) and it is based off of the *BWA short-read* algorithm.\n\n**In order to run this tool you will need reference file's indices that were made using TMAP Indexer.**\n\n### Common issues\n\n* This tool accepts **RAW BAM** files coming from IonTorrent sequencer as these files contain **flow data** required for further analysis. FastQ files are not capable of storing such data.", "input": [{"name": "Index archive", "encodingFormat": "application/x-tar"}, {"name": "BED file for AmpliSeq", "encodingFormat": "text/x-bed"}, {"name": "Reads"}, {"name": "Reads format"}, {"name": "BAM start VFO (virtual file offset)"}, {"name": "BAM end VFO (virtual file offset)"}, {"name": "Score match"}, {"name": "Mismatch penalty"}, {"name": "Pen gap open"}, {"name": "Pen gap extension"}, {"name": "Pen gap long"}, {"name": "Gap long length"}, {"name": "Band width"}, {"name": "Softclip type"}, {"name": "Duplicate window"}, {"name": "Max seed band"}, {"name": "Unroll banding"}, {"name": "Long hit mult"}, {"name": "Score threshold"}, {"name": "Reads queue size"}, {"name": "Num threads"}, {"name": "Aln output mode"}, {"name": "SAM read group"}, {"name": "Bidirectional"}, {"name": "Use seq equal"}, {"name": "Ignore RG from SAM"}, {"name": "Rand read name"}, {"name": "Prefix exclude"}, {"name": "Suffix exclude"}, {"name": "New QV"}, {"name": "Input gz"}, {"name": "Input bz2"}, {"name": "Output type"}, {"name": "End repair"}, {"name": "End repair NR"}, {"name": "Min indel end repair"}, {"name": "Max adapter bases for soft clipping"}, {"name": "Shared memory key"}, {"name": "VSW type"}, {"name": "Do realign"}, {"name": "R mat"}, {"name": "R mis"}, {"name": "R gip"}, {"name": "R gep"}, {"name": "R bw"}, {"name": "R clip"}, {"name": "Stats"}, {"name": "Realignment log file", "encodingFormat": "text/plain"}, {"name": "Do repeat clip"}, {"name": "Context"}, {"name": "Gap scale"}, {"name": "C mat"}, {"name": "C mis"}, {"name": "C gip"}, {"name": "C gep"}, {"name": "C bw"}, {"name": "Context debug log"}, {"name": "Min al len"}, {"name": "Pen flow error"}, {"name": "Softclip key"}, {"name": "SAM flowspace tags"}, {"name": "Ignore flowgram"}, {"name": "Final flowspace"}, {"name": "Pairing"}, {"name": "Strandedness"}, {"name": "Positioning (for same strand strandedness)"}, {"name": "Ins size mean"}, {"name": "Ins size std"}, {"name": "Ins size std max num"}, {"name": "Ins size outlier bound"}, {"name": "Ins size min mapq"}, {"name": "Read rescue"}, {"name": "Read rescue std num"}, {"name": "Read rescue mapq thr"}, {"name": "Seed length"}, {"name": "Seed max diff"}, {"name": "Seed2 length"}, {"name": "Max diff"}, {"name": "Max error rate"}, {"name": "Max mismatches"}, {"name": "Max gap opens"}, {"name": "Max gap extensions"}, {"name": "Max cals deletion"}, {"name": "Indel ends bound"}, {"name": "Max best CALs"}, {"name": "Max nodes"}, {"name": "Min seq length"}, {"name": "Max seq length"}, {"name": "Reference indices"}, {"name": "Memory for job"}], "output": [{"name": "TMAP Map1 mapped file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/TMAP/src/map/map1", "https://github.com/iontorrent/TS", "https://github.com/iontorrent/TS/blob/master/Analysis/TMAP/doc/tmap-book.pdf"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1495632249, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tmap-map2/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tmap-map2/9", "applicationCategory": "CommandLineTool", "name": "Torrent Mapping Alignment Program Map2", "description": "*TMAP aligner, based on BWA long-read algorithm, suited for Ion Torrent reads longer than 150 bases.*\n\nTMAP is an aligner specifically tuned for data from the Ion Torrent sequencers. It uses a series of algorithms to map data to an indexed reference sequence. \n\nThis tool is running TMAP's \"Map2\" algorithm. Map2 is used to quickly map long sequences to a reference genome. This algorithm is well suited for longer reads (\u2265 150bp) and it is based off of the *BWA long-read* algorithm.\n\n**In order to run this tool you will need reference file's indices that were made using TMAP Indexer.**\n\n### Common issues\n\n* This tool accepts **RAW BAM** files coming from IonTorrent sequencer as these files contain **flow data** required for further analysis. FastQ files are not capable of storing such data.", "input": [{"name": "Index archive", "encodingFormat": "application/x-tar"}, {"name": "BED file for AmpliSeq", "encodingFormat": "text/x-bed"}, {"name": "Reads"}, {"name": "Reads format"}, {"name": "BAM start VFO (virtual file offset)"}, {"name": "BAM end VFO (virtual file offset)"}, {"name": "Score match"}, {"name": "Mismatch penalty"}, {"name": "Pen gap open"}, {"name": "Pen gap extension"}, {"name": "Pen gap long"}, {"name": "Gap long length"}, {"name": "Band width"}, {"name": "Softclip type"}, {"name": "Duplicate window"}, {"name": "Max seed band"}, {"name": "Unroll banding"}, {"name": "Long hit mult"}, {"name": "Score threshold"}, {"name": "Reads queue size"}, {"name": "Num threads"}, {"name": "Aln output mode"}, {"name": "SAM read group"}, {"name": "Bidirectional"}, {"name": "Use seq equal"}, {"name": "Ignore RG from SAM"}, {"name": "Rand read name"}, {"name": "Prefix exclude"}, {"name": "Suffix exclude"}, {"name": "New QV"}, {"name": "Input gz"}, {"name": "Input bz2"}, {"name": "Output type"}, {"name": "End repair"}, {"name": "End repair NR"}, {"name": "Min indel end repair"}, {"name": "Max adapter bases for soft clipping"}, {"name": "Shared memory key"}, {"name": "VSW type"}, {"name": "Do realign"}, {"name": "R mat"}, {"name": "R mis"}, {"name": "R gip"}, {"name": "R gep"}, {"name": "R bw"}, {"name": "R clip"}, {"name": "Stats"}, {"name": "Realignment log file", "encodingFormat": "text/plain"}, {"name": "Do repeat clip"}, {"name": "Context"}, {"name": "Gap scale"}, {"name": "C mat"}, {"name": "C mis"}, {"name": "C gip"}, {"name": "C gep"}, {"name": "C bw"}, {"name": "Context debug log"}, {"name": "Min al len"}, {"name": "Pen flow error"}, {"name": "Softclip key"}, {"name": "SAM flowspace tags"}, {"name": "Ignore flowgram"}, {"name": "Final flowspace"}, {"name": "Pairing"}, {"name": "Strandedness"}, {"name": "Positioning (for same strand strandedness)"}, {"name": "Ins size mean"}, {"name": "Ins size std"}, {"name": "Ins size std max num"}, {"name": "Ins size outlier bound"}, {"name": "Ins size min mapq"}, {"name": "Read rescue"}, {"name": "Read rescue std num"}, {"name": "Read rescue mapq thr"}, {"name": "Max seed hits"}, {"name": "Length coef"}, {"name": "Max seed intv"}, {"name": "Z best"}, {"name": "Seeds rev"}, {"name": "Narrow rmdup"}, {"name": "Maximum gap size during chaining"}, {"name": "Min seq length"}, {"name": "Max seq length"}, {"name": "Memory for job"}], "output": [{"name": "TMAP Map2 mapped file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/TMAP/src/map/map2", "https://github.com/iontorrent/TS", "https://github.com/iontorrent/TS/blob/master/Analysis/TMAP/doc/tmap-book.pdf"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1495632251, "dateCreated": 1457032404, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tmap-map3/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tmap-map3/8", "applicationCategory": "CommandLineTool", "name": "Torrent Mapping Alignment Program Map3", "description": "*TMAP aligner, based on SSAHA long-read algorithm, suited for Ion Torrent reads longer than 150 bases.*\n\nTMAP is an aligner specifically tuned for data from the Ion Torrent sequencers. It uses a series of algorithms to map data to an indexed reference sequence. \n\nThis tool is running TMAP's \"Map3\" algorithm. Map3 is used to map sequences to a reference genome. This algorithm is well\nsuited for longer reads (\u2265 150bp) and a simplification of the *SSAHA long-read* algorithm.\n\n**In order to run this tool you will need reference file's indices that were made using TMAP Indexer.**\n\n### Common issues\n\n* This tool accepts **RAW BAM** files coming from IonTorrent sequencer as these files contain **flow data** required for further analysis. FastQ files are not capable of storing such data.", "input": [{"name": "Index archive", "encodingFormat": "application/x-tar"}, {"name": "BED file for AmpliSeq", "encodingFormat": "text/x-bed"}, {"name": "Reads"}, {"name": "Reads format"}, {"name": "BAM start VFO (virtual file offset)"}, {"name": "BAM end VFO (virtual file offset)"}, {"name": "Score match"}, {"name": "Mismatch penalty"}, {"name": "Pen gap open"}, {"name": "Pen gap extension"}, {"name": "Pen gap long"}, {"name": "Gap long length"}, {"name": "Band width"}, {"name": "Softclip type"}, {"name": "Duplicate window"}, {"name": "Max seed band"}, {"name": "Unroll banding"}, {"name": "Long hit mult"}, {"name": "Score threshold"}, {"name": "Reads queue size"}, {"name": "Num threads"}, {"name": "Aln output mode"}, {"name": "SAM read group"}, {"name": "Bidirectional"}, {"name": "Use seq equal"}, {"name": "Ignore RG from SAM"}, {"name": "Rand read name"}, {"name": "Prefix exclude"}, {"name": "Suffix exclude"}, {"name": "New QV"}, {"name": "Input gz"}, {"name": "Input bz2"}, {"name": "Output type"}, {"name": "End repair"}, {"name": "End repair NR"}, {"name": "Min indel end repair"}, {"name": "Max adapter bases for soft clipping"}, {"name": "Shared memory key"}, {"name": "VSW type"}, {"name": "Do realign"}, {"name": "R mat"}, {"name": "R mis"}, {"name": "R gip"}, {"name": "R gep"}, {"name": "R bw"}, {"name": "R clip"}, {"name": "Stats"}, {"name": "Realignment log file", "encodingFormat": "text/plain"}, {"name": "Do repeat clip"}, {"name": "Context"}, {"name": "Gap scale"}, {"name": "C mat"}, {"name": "C mis"}, {"name": "C gip"}, {"name": "C gep"}, {"name": "C bw"}, {"name": "Context debug log"}, {"name": "Min al len"}, {"name": "Pen flow error"}, {"name": "Softclip key"}, {"name": "SAM flowspace tags"}, {"name": "Ignore flowgram"}, {"name": "Final flowspace"}, {"name": "Pairing"}, {"name": "Strandedness"}, {"name": "Positioning (for same strand strandedness)"}, {"name": "Ins size mean"}, {"name": "Ins size std"}, {"name": "Ins size std max num"}, {"name": "Ins size outlier bound"}, {"name": "Ins size min mapq"}, {"name": "Read rescue"}, {"name": "Read rescue std num"}, {"name": "Read rescue mapq thr"}, {"name": "Max seed hits"}, {"name": "Seed length"}, {"name": "Hit frac"}, {"name": "Seed step"}, {"name": "Hp diff"}, {"name": "Fwd search"}, {"name": "Skip seed frac"}, {"name": "Min seq length"}, {"name": "Max seq length"}, {"name": "Memory for job"}], "output": [{"name": "TMAP Map3 mapped file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/TMAP/src/map/map3", "https://github.com/iontorrent/TS", "https://github.com/iontorrent/TS/blob/master/Analysis/TMAP/doc/tmap-book.pdf"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1495632249, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tmap-map4/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tmap-map4/7", "applicationCategory": "CommandLineTool", "name": "Torrent Mapping Alignment Program Map4", "description": "***Recommended** TMAP aligner, based on the SMEM algorithm, suited for Ion Torrent reads longer than 100 bases.*\n\nTMAP is an aligner specifically tuned for data from the Ion Torrent sequencers. It uses a series of algorithms to map data to an indexed reference sequence. \n\nThis tool is running TMAP's \"Map4\" algorithm. Map4 is used to map sequences to a reference genome. This algorithm is well suited for longer reads (\u2265 100bp) and is a variation of the super-maximal exact matching (SMEM) algorithm. SMEMs are equivalent to returning all longest common sub-strings between the read and reference.\n\nMap4 is the current recommended algorithm. This algorithm runs quickly while maintaining a high degree of sensitivity and specificity.\n\n**In order to run this tool you will need reference file's indices that were made using TMAP Indexer.**\n\n### Common issues\n\n* This tool accepts **RAW BAM** files coming from IonTorrent sequencer as these files contain **flow data** required for further analysis. FastQ files are not capable of storing such data.", "input": [{"name": "Index archive", "encodingFormat": "application/x-tar"}, {"name": "BED file for AmpliSeq", "encodingFormat": "text/x-bed"}, {"name": "Reads"}, {"name": "Reads format"}, {"name": "BAM start VFO (virtual file offset)"}, {"name": "BAM end VFO (virtual file offset)"}, {"name": "Score match"}, {"name": "Mismatch penalty"}, {"name": "Pen gap open"}, {"name": "Pen gap extension"}, {"name": "Pen gap long"}, {"name": "Gap long length"}, {"name": "Band width"}, {"name": "Softclip type"}, {"name": "Duplicate window"}, {"name": "Max seed band"}, {"name": "Unroll banding"}, {"name": "Long hit mult"}, {"name": "Score threshold"}, {"name": "Reads queue size"}, {"name": "Num threads"}, {"name": "Aln output mode"}, {"name": "SAM read group"}, {"name": "Bidirectional"}, {"name": "Use seq equal"}, {"name": "Ignore RG from SAM"}, {"name": "Rand read name"}, {"name": "Prefix exclude"}, {"name": "Suffix exclude"}, {"name": "New QV"}, {"name": "Input gz"}, {"name": "Input bz2"}, {"name": "Output type"}, {"name": "End repair"}, {"name": "End repair NR"}, {"name": "Min indel end repair"}, {"name": "Max adapter bases for soft clipping"}, {"name": "Shared memory key"}, {"name": "VSW type"}, {"name": "Do realign"}, {"name": "R mat"}, {"name": "R mis"}, {"name": "R gip"}, {"name": "R gep"}, {"name": "R bw"}, {"name": "R clip"}, {"name": "Stats"}, {"name": "Realignment log file", "encodingFormat": "text/plain"}, {"name": "Do repeat clip"}, {"name": "Context"}, {"name": "Gap scale"}, {"name": "C mat"}, {"name": "C mis"}, {"name": "C gip"}, {"name": "C gep"}, {"name": "C bw"}, {"name": "Context debug log"}, {"name": "Min al len"}, {"name": "Pen flow error"}, {"name": "Softclip key"}, {"name": "SAM flowspace tags"}, {"name": "Ignore flowgram"}, {"name": "Final flowspace"}, {"name": "Pairing"}, {"name": "Strandedness"}, {"name": "Positioning (for same strand strandedness)"}, {"name": "Ins size mean"}, {"name": "Ins size std"}, {"name": "Ins size std max num"}, {"name": "Ins size outlier bound"}, {"name": "Ins size min mapq"}, {"name": "Read rescue"}, {"name": "Read rescue std num"}, {"name": "Read rescue mapq thr"}, {"name": "Min seed length"}, {"name": "Max seed length"}, {"name": "Hit frac"}, {"name": "Seed step"}, {"name": "Max seed length adj coef"}, {"name": "Max iwidth"}, {"name": "Max repr"}, {"name": "Min seq length"}, {"name": "Max seq length"}, {"name": "Rand repr"}, {"name": "Use min"}, {"name": "Memory for job"}], "output": [{"name": "TMAP Map4 mapped file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/TMAP/src/map/map4", "https://github.com/iontorrent/TS", "https://github.com/iontorrent/TS/blob/master/Analysis/TMAP/doc/tmap-book.pdf"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1495632251, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tmap-mapvsw/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tmap-mapvsw/9", "applicationCategory": "CommandLineTool", "name": "Torrent Mapping Alignment Program MapVSW", "description": "*TMAP aligner, performs Smith Waterman alignment of Ion Torrent reads. Suitable for debugging small number of reads.*\n\nTMAP is an aligner specifically tuned for data from the Ion Torrent sequencers. It uses a series of algorithms to map data to an indexed reference sequence. \n\nThis tool is running TMAP's \"MapVSW\" algorithm. MapVSW performs the full *Smith Waterman* algorithm alignment of the read to the reference genome, using SSE2 (vectorized) programming instructions.\n\n**In order to run this tool you will need reference file's indices that were made using TMAP Indexer.**\n\n### Common issues\n\n* **Important:** This algorithm should not be used for large number of reads or large genomes, and instead should be used for debugging and investigating small numbers of reads. Otherwise, alignment will fail.\n\n* This tool accepts **RAW BAM** files coming from IonTorrent sequencer as these files contain **flow data** required for further analysis. FastQ files are not capable of storing such data.", "input": [{"name": "Index archive", "encodingFormat": "application/x-tar"}, {"name": "BED file for AmpliSeq", "encodingFormat": "text/x-bed"}, {"name": "Reads"}, {"name": "Reads format"}, {"name": "BAM start VFO (virtual file offset)"}, {"name": "BAM end VFO (virtual file offset)"}, {"name": "Score match"}, {"name": "Mismatch penalty"}, {"name": "Pen gap open"}, {"name": "Pen gap extension"}, {"name": "Pen gap long"}, {"name": "Gap long length"}, {"name": "Band width"}, {"name": "Softclip type"}, {"name": "Duplicate window"}, {"name": "Max seed band"}, {"name": "Unroll banding"}, {"name": "Long hit mult"}, {"name": "Score threshold"}, {"name": "Reads queue size"}, {"name": "Num threads"}, {"name": "Aln output mode"}, {"name": "SAM read group"}, {"name": "Bidirectional"}, {"name": "Use seq equal"}, {"name": "Ignore RG from SAM"}, {"name": "Rand read name"}, {"name": "Prefix exclude"}, {"name": "Suffix exclude"}, {"name": "New QV"}, {"name": "Input gz"}, {"name": "Input bz2"}, {"name": "Output type"}, {"name": "End repair"}, {"name": "End repair NR"}, {"name": "Min indel end repair"}, {"name": "Max adapter bases for soft clipping"}, {"name": "Shared memory key"}, {"name": "VSW type"}, {"name": "Do realign"}, {"name": "R mat"}, {"name": "R mis"}, {"name": "R gip"}, {"name": "R gep"}, {"name": "R bw"}, {"name": "R clip"}, {"name": "Stats"}, {"name": "Realignment log file", "encodingFormat": "text/plain"}, {"name": "Do repeat clip"}, {"name": "Context"}, {"name": "Gap scale"}, {"name": "C mat"}, {"name": "C mis"}, {"name": "C gip"}, {"name": "C gep"}, {"name": "C bw"}, {"name": "Context debug log"}, {"name": "Min al len"}, {"name": "Pen flow error"}, {"name": "Softclip key"}, {"name": "SAM flowspace tags"}, {"name": "Ignore flowgram"}, {"name": "Final flowspace"}, {"name": "Pairing"}, {"name": "Strandedness"}, {"name": "Positioning (for same strand strandedness)"}, {"name": "Ins size mean"}, {"name": "Ins size std"}, {"name": "Ins size std max num"}, {"name": "Ins size outlier bound"}, {"name": "Ins size min mapq"}, {"name": "Read rescue"}, {"name": "Read rescue std num"}, {"name": "Read rescue mapq thr"}, {"name": "Min seed length"}, {"name": "Max seed length"}, {"name": "Memory for job"}], "output": [{"name": "TMAP MapVSW mapped file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/TMAP/src/map/mapvsw", "https://github.com/iontorrent/TS", "https://github.com/iontorrent/TS/blob/master/Analysis/TMAP/doc/tmap-book.pdf"], "applicationSubCategory": ["Alignment"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1495632251, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tvc/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tvc/7", "applicationCategory": "CommandLineTool", "name": "Torrent Variant Caller", "description": "Torrent Variant Caller (TVC) is a genetic variant caller for Ion Torrent Sequencing platforms, and is specially optimized to exploit the underlying flow signal information in the statistical model to evaluate variants. Torrent Variant Caller is designed to call single-nucleotide polymorphisms (SNPs), multi-nucleotide polymorphisms (MNPs), insertions, deletions, and block substitutions.\n\n### Common issues\n\n* This tool expects BAM files mapped with one of the **Torrent Mapping Alignment Program** tools", "input": [{"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Num threads"}, {"name": "Num variants per thread"}, {"name": "Parameters file"}, {"name": "Indexed input BAM", "encodingFormat": "application/x-bam"}, {"name": "Sample name"}, {"name": "Force sample name"}, {"name": "Target file", "encodingFormat": "text/x-bed"}, {"name": "Trim ampliseq primers"}, {"name": "Downsample to coverage"}, {"name": "Model file", "encodingFormat": "text/plain"}, {"name": "Recal model hp thres"}, {"name": "Suppress reference genotypes"}, {"name": "Suppress no calls"}, {"name": "Suppress nocall genotypes"}, {"name": "Allow SNPs"}, {"name": "Allow indels"}, {"name": "Allow MNPs"}, {"name": "Allow complex"}, {"name": "Max complex gap"}, {"name": "Use best N alleles"}, {"name": "Min mapping qv"}, {"name": "Read SNP limit"}, {"name": "Read max mismatch fraction"}, {"name": "Gen min alt allele freq"}, {"name": "Gen min indel alt allele freq"}, {"name": "Gen min coverage"}, {"name": "Input VCF"}, {"name": "Process input positions only"}, {"name": "Use input allele only"}, {"name": "Min delta for flow"}, {"name": "Max flows to test"}, {"name": "Outlier probability"}, {"name": "Heavy tailed"}, {"name": "Suppress recalibration"}, {"name": "Do SNP realignment"}, {"name": "Do MNP realignment"}, {"name": "Realignment threshold"}, {"name": "Use SSE basecaller"}, {"name": "Resolve clipped bases"}, {"name": "Prediction precision"}, {"name": "Shift likelihood penalty"}, {"name": "Minimum sigma prior"}, {"name": "Slope sigma prior"}, {"name": "Sigma prior weight"}, {"name": "K zero"}, {"name": "SSE relative safety level"}, {"name": "Tune sbias"}, {"name": "Max detail level"}, {"name": "SNP min coverage"}, {"name": "SNP min cov each strand"}, {"name": "SNP min variant score"}, {"name": "SNP strand bias"}, {"name": "SNP strand bias pval"}, {"name": "SNP min allele freq"}, {"name": "MNP min coverage"}, {"name": "MNP min cov each strand"}, {"name": "MNP min variant score"}, {"name": "MNP strand bias"}, {"name": "MNP strand bias pval"}, {"name": "MNP min allele freq"}, {"name": "Indel min coverage"}, {"name": "Indel min cov each strand"}, {"name": "Indel min variant score"}, {"name": "Indel strand bias"}, {"name": "Indel strand bias pval"}, {"name": "Indel min allele freq"}, {"name": "Hotspot min coverage"}, {"name": "Hotspot min cov each strand"}, {"name": "Hotspot min variant score"}, {"name": "Hotspot strand bias"}, {"name": "Hotspot strand bias pval"}, {"name": "Hotspot min allele freq"}, {"name": "HP max length"}, {"name": "Error motifs", "encodingFormat": "text/plain"}, {"name": "SSE prob threshold"}, {"name": "Min ratio reads non SSE strand"}, {"name": "Indel as HPindel"}, {"name": "Use position bias"}, {"name": "Position bias"}, {"name": "Position bias pval"}, {"name": "Position bias ref fraction"}, {"name": "Data quality stringency"}, {"name": "Read rejection threshold"}, {"name": "Filter unusual predictions"}, {"name": "Filter deletion predictions"}, {"name": "Filter insertion predictions"}, {"name": "Heal SNPs"}, {"name": "Reference index"}, {"name": "Memory for job"}], "output": [{"name": "TVC VCF output and filtered output", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/iontorrent/TS/tree/master/Analysis/VariantCaller", "https://github.com/iontorrent/TS"], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Thermo Fisher Scientific", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649155968, "dateCreated": 1457032406, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-asapratiopeptideparser-5-0-0/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-asapratiopeptideparser-5-0-0/13", "applicationCategory": "CommandLineTool", "name": "TPP ASAPRatioPeptideParser", "description": "**ASAPRatioPeptideParser** is a tool for measuring relative expression levels of peptides from isotopically-labeled samples (e.g. ICAT, SILAC). It is an integral part of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_file: IProphet results file in pepXML format\n\n###Outputs\n\n1. output_file: file in XML format that contains the relative expression levels of peptides from isotopically-labeled samples (e.g. ICAT, SILAC)\n\n\n###Common Issues and Important Notes\n\nWhen **ASAPRatioPeptideParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access MZXML files (even though they are not originally specified as tool inputs). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **MZXML files** is created within the wrapper. The file PEP.XML from the **Input file** port is modified here so that it reflects paths to the MZXML files on the platform.", "input": [{"name": "Input file"}, {"name": "Labeled residues"}, {"name": "Heavy labeled peptide"}, {"name": "areaFlag"}, {"name": "Range"}, {"name": "Static modification quantification"}, {"name": "Fixed scan range"}, {"name": "Quantitate only where cid was made"}, {"name": "Return a ratio"}, {"name": "Set all background to zero"}, {"name": "Min PeptideProphet probability"}, {"name": "Minimum iProphet probability"}, {"name": "Label masses"}, {"name": "Wavelet smoothing"}, {"name": "MZXML files"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038174, "dateCreated": 1510940904, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-asapratioproteinratioparser-5-0-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-asapratioproteinratioparser-5-0-0/7", "applicationCategory": "CommandLineTool", "name": "TPP ASAPRatioProteinRatioParser", "description": "**ASAPRatioProteinRatioParser**  is a tool for measuring relative expression levels of proteins from isotopically-labeled samples (e.g. ICAT, SILAC). It is an integral part of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_file: file in protXML format that contains a list of all proteins corresponding to the peptide-spectrum matches (PSMs) along with protein probabilities and global false discovery rates (FDRs) at different thresholds\n\n###Outputs\n\n1. output_file: file in XML format that contains the relative abundances of proteins and the corresponding confidence intervals\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input file"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-asapratiopvalueparser-5-0-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-asapratiopvalueparser-5-0-0/10", "applicationCategory": "CommandLineTool", "name": "TPP ASAPRatioPvalueParser", "description": "**ASAPRatioPvalueParser** overwrites ASAPRatio protein p-value information into ProteinProphet XML format. It is an integral part of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_file: output file in XML format from ASAPRatioProteinRatioParser\n2. input_mzxml_files: files in mzXML format representing mass-spec data\n\n###Outputs\n\n1. output_file: file in XML format that contains reformatted p-values\n\n###Common Issues and Important Notes\n\nWhen **ASAPRatioPeptideParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access MZXML files (even though they are not originally specified as tool inputs). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **MZXML files** is created within the wrapper. The file PEP.XML from the **Input file** port is modified here so that it reflects paths to the MZXML files on the platform.\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input file"}, {"name": "Input mzxml files"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/idconvert-5-0-0/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/idconvert-5-0-0/5", "applicationCategory": "CommandLineTool", "name": "TPP IdConvert", "description": "**IdConvert** is a tool for converting peptide identification files between various formats. It is developed by ProteoWizard.\n\n###Required Inputs\n1. input_file: input peptide identification file\n2. output_format: output file format (pepXML, mzIdentML, or TXT)\n \n###Outputs\n1. output\\_file: peptide identification file in pepXML, mzIdentML, or TXT format\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input File"}, {"name": "Output file format"}], "output": [{"name": "Output file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038486, "dateCreated": 1510941512, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-interactparser-5-0-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-interactparser-5-0-0/10", "applicationCategory": "CommandLineTool", "name": "TPP InteractParser", "description": "**InteractParser** is tool that corrects some of the common formatting issues found in pepXML files written by various search engines. It is an optional component of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n###Required Inputs\n1. input_file: input file in pepXML format\n \n###Outputs\n1. output\\_file: resulting file in pepXML format\n\n\n###Common Issues and Important Notes\n\nWhen **InteractParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access MZXML files (even though they are not originally specified as tool inputs). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **Input mzxml files** is created within the wrapper. The file PEP.XML from the **Input File** port is modified here so that it reflects paths to the MZXML files on the platform.", "input": [{"name": "Input File"}, {"name": "Allow, don't reject X containing peptides"}, {"name": "Experiment label"}, {"name": "Minimum peptide length not rejected"}, {"name": "Maximum peptide rank in output"}, {"name": "Record retention_time_sec"}, {"name": "Record ion_injection_time"}, {"name": "Record precursor intensity"}, {"name": "Record collision energy"}, {"name": "Record compensation voltage"}, {"name": "Correct protein names"}, {"name": "Correct charge states"}, {"name": "Correct Pyro-Glu and Ammonia"}, {"name": "Don't write the input file reference"}, {"name": "Only warn don't skip file"}, {"name": "Enzyme"}, {"name": "Input mzxml files"}, {"name": "Database file", "encodingFormat": "application/x-fasta"}, {"name": "Database type"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-iprophet-5-0-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-iprophet-5-0-0/11", "applicationCategory": "CommandLineTool", "name": "TPP IProphet", "description": "**iProphet** is a tool that combines the evidence from multiple identifications of the same peptide sequence across different spectra, experiments, precursor ion charge states, and modified states. It also allows accurate and effective integration of the results from multiple database search engines applied to the same data. \n\niProphet is an integral part of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center. The use of iProphet in the TPP increases the number of correctly identified peptides at a constant false discovery rate (FDR) as compared to both PeptideProphet and a representative state-of-the-art tool Percolator. As the main outcome, iProphet permits the calculation of accurate posterior probabilities and FDR estimates at the level of unique peptide sequences, which in turn leads to more accurate probability estimates at the protein level. Fully integrated with the TPP, it supports all commonly used MS instruments, search engines, and computer platforms.\n\n###Required Inputs\n\n1. input_files: output file in pepXML format from several peptide search engines\n\n###Outputs\n\n1. output_file: file in pepXML format that contains improved peptide probability estimates\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input Files"}, {"name": "Threads"}, {"name": "Decoy"}, {"name": "Category File"}, {"name": "Minimum probability"}, {"name": "Length"}, {"name": "NOFPKM"}, {"name": "NONSS"}, {"name": "NONSE"}, {"name": "NONRS"}, {"name": "NONSM"}, {"name": "NONSP"}, {"name": "SHARPNSE"}, {"name": "NONSI"}], "output": [{"name": "Output File"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940906, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-librapeptideparser-5-0-0/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-librapeptideparser-5-0-0/14", "applicationCategory": "CommandLineTool", "name": "TPP LibraPeptideParser", "description": "**LibraPeptideParser** and **LibraProteinRatioParser** are tools that perform quantification on MS/MS spectra for samples with multi-reagent-labeled peptides, e.g. iTRAQ-labeled samples, by integrating the intensities of the reagent m/z lines and storing the values at the peptide level in the interact.xml file format. They are an integral part of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center.\n\nWhen a reagent m/z line is not found in the MS/MS spectrum, Libra specifies the intensity of that reagent m/z as the default value. When the intensity of a reagent m/z line is less than or equal to zero, its value is specified as zero. \n\nProtein quantification values are derived from the group of peptides associated with each protein. First, each peptide's integrated intensity is normalized by the sum of its channel intensities. The normalized intensities are averaged over all peptides of a protein, and the standard deviation of the mean is determined for each peptide. Normalized intensities more than 2 sigma from the mean are removed, and an average intensity for each  protein is calculated using those intensities that survive outlier removal. The 1-sigma standard error then is calculated using the standard deviation. If the user has specified a reference normalization channel, protein quantification values are normalized with respect to that channel, and the errors become the channel error and the reference channel error added in quadrature. \n\nA value of 99.99 indicates that protein quantity was calculated using only one peptide, so the standard error is infinite. A value of -9.0 indicates that no peptides of the protein survived threshold filtering and outlier removal, so the protein quantity is undefined.\n\n###Required Inputs\n1. input_mzxml_files: mass spectrometry spectral data file in mzXML format\n2. input_pepxml_file: peptide-spectrum match data file in pepXML format\n \n###Outputs\n1. output\\_file: peptide quantification data file in pepXML format\n\n\n###Common Issues and Important Notes\n\nWhen **LibraPeptideParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access MZXML files (even though they are not originally specified as tool inputs). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **Input mzxml files** is created within the wrapper. The file PEP.XML from the **Input pep.xml file** port is modified here so that it reflects paths to the MZXML files on the platform.", "input": [{"name": "Input pep.xml file"}, {"name": "Input mzxml files"}, {"name": "Condition file"}, {"name": "Reagent 1"}, {"name": "Reagent 2"}, {"name": "Reagent 3"}, {"name": "Reagent 4"}, {"name": "Contributing mz1, % of -2"}, {"name": "Contributing mz1, % of -1"}, {"name": "Contributing mz1, % of +1"}, {"name": "Contributing mz1, % of +2"}, {"name": "Contributing mz2, % of -2"}, {"name": "Contributing mz2, % of -1"}, {"name": "Contributing mz2, % of +1"}, {"name": "Contributing mz2, % of +2"}, {"name": "Contributing mz3, % of -2"}, {"name": "Contributing mz3, % of -1"}, {"name": "Contributing mz3, % of +1"}, {"name": "Contributing mz3, % of +2"}, {"name": "Contributing mz4, % of -2"}, {"name": "Contributing mz4, % of -1"}, {"name": "Contributing mz4, % of +1"}, {"name": "Contributing mz4, % of +2"}, {"name": "Mass tolerance"}, {"name": "Centroiding"}, {"name": "Normalization"}, {"name": "Minimum threshold intensity"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-libraproteinratioparser-5-0-0/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-libraproteinratioparser-5-0-0/12", "applicationCategory": "CommandLineTool", "name": "TPP LibraProteinRatioParser", "description": "**LibraPeptideParser** and **LibraProteinRatioParser** are tools that perform quantification on MS/MS spectra for samples with multi-reagent-labeled peptides, e.g. iTRAQ-labeled samples, by integrating the intensities of the reagent m/z lines and storing the values at the peptide level in the interact.xml file format. They are an integral part of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center.\n\nWhen a reagent m/z line is not found in the MS/MS spectrum, Libra specifies the intensity of that reagent m/z as the default value. When the intensity of a reagent m/z line is less than or equal to zero, its value is specified as zero. \n\nProtein quantification values are derived from the group of peptides associated with each protein. First, each peptide's integrated intensity is normalized by the sum of its channel intensities. The normalized intensities are averaged over all peptides of a protein, and the standard deviation of the mean is determined for each peptide. Normalized intensities more than 2 sigma from the mean are removed, and an average intensity for each  protein is calculated using those intensities that survive outlier removal. The 1-sigma standard error then is calculated using the standard deviation. If the user has specified a reference normalization channel, protein quantification values are normalized with respect to that channel, and the errors become the channel error and the reference channel error added in quadrature. \n\nA value of 99.99 indicates that protein quantity was calculated using only one peptide, so the standard error is infinite. A value of -9.0 indicates that no peptides of the protein survived threshold filtering and outlier removal, so the protein quantity is undefined.\n\n\n###Required Inputs\n1. input_iproph_file: iProphet output file in pepXML format\n2. input_prot_xml_file: ProteinProphet output file in pepXML format\n \n###Outputs\n1. output\\_file: protein quantification file in pepXML format\n2. output\\_tsv\\_file: protein quantification file in TSV format\n\n\n###Common Issues and Important Notes\n\nWhen **LibraProteinRatioParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access PEP.XML file (even though it is not originally specified as tool input). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **Input iprophet file** is created within the wrapper. The file PROT.XML from the **Input prot.xml file** port is modified here so that it reflects path to the PEP.XML file on the platform.", "input": [{"name": "Input prot.xml file"}, {"name": "Input iprophet file"}, {"name": "Condition file"}, {"name": "Reagent 1"}, {"name": "Reagent 2"}, {"name": "Reagent 3"}, {"name": "Reagent 4"}, {"name": "Contributing mz1, % of -2"}, {"name": "Contributing mz1, % of -1"}, {"name": "Contributing mz1, % of +1"}, {"name": "Contributing mz1, % of +2"}, {"name": "Contributing mz2, % of -2"}, {"name": "Contributing mz2, % of -1"}, {"name": "Contributing mz2, % of +1"}, {"name": "Contributing mz2, % of +2"}, {"name": "Contributing mz3, % of -2"}, {"name": "Contributing mz3, % of -1"}, {"name": "Contributing mz3, % of +1"}, {"name": "Contributing mz3, % of +2"}, {"name": "Contributing mz4, % of -2"}, {"name": "Contributing mz4, % of -1"}, {"name": "Contributing mz4, % of +1"}, {"name": "Contributing mz4, % of +2"}, {"name": "Mass tolerance"}, {"name": "Centroiding"}, {"name": "Normalization"}, {"name": "Minimum threshold intensity"}], "output": [{"name": "Output file"}, {"name": "Output tsv file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940904, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-peptideprophet-5-0-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-peptideprophet-5-0-0/10", "applicationCategory": "CommandLineTool", "name": "TPP PeptideProphet", "description": "**PeptideProphet** is a tool that automatically validates peptide assignments to MS/MS spectra made using database search programs such as SEQUEST. It is an integral part of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\nPeptideProphet learns from each dataset distributions of search scores and peptide properties among correct and incorrect peptides and uses those distributions to compute for each result a probability that it is correct. Relevant peptide properties include the number of termini compatible with enzymatic cleavage (for unconstrained searches), the number of missed enzyme cleavages, the mass difference with respect to the precursor ion, the presence of light or heavy cysteine (for ICAT experiments), and the presence of an N-glycosylation motif (for N-glycosylation capture experiments). PeptideProphet can be used as a second step following the analysis of MS/MS spectra generated from any type of mass spectrometer and assigned peptides using any number of database search programs.\n\n###Required Inputs\n\n1.  input_file: file in pepXML format that contains MS/MS spectra generated by database search programs\n2.  rtcat: catalog of measured peptide retention times based on Trans-Proteomic Pipeline-processed data\n\n###Outputs\n\n1. output_file: file in pepXML format that contains a probability for every peptide-spectrum match (PSM) as well as the results of the modeling, including a representation of the receiver operating characteristic (ROC) curves\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input File"}, {"name": "EXCLUDE"}, {"name": "LEAVE"}, {"name": "PERFECTLIB"}, {"name": "ICAT"}, {"name": "NOICAT"}, {"name": "ZERO"}, {"name": "ACCMASS"}, {"name": "PPM"}, {"name": "NOMASS"}, {"name": "PI"}, {"name": "RT"}, {"name": "GLYC"}, {"name": "PHOSPHO"}, {"name": "MALDI"}, {"name": "INSTRWARN"}, {"name": "DECOYPROBS"}, {"name": "NONTT"}, {"name": "NONMC"}, {"name": "EXPECTSCORE"}, {"name": "NONPARAM"}, {"name": "NEGGAMMA"}, {"name": "FORCEDISTR"}, {"name": "optimizefval"}, {"name": "CLEVEL"}, {"name": "MINPINTT"}, {"name": "MINPIPROB"}, {"name": "MINRTNTT"}, {"name": "MINRTPROB"}, {"name": "MINPROB"}, {"name": "RTCAT"}, {"name": "DECOY"}], "output": [{"name": "Output File"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-proteinprophet-5-0-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-proteinprophet-5-0-0/7", "applicationCategory": "CommandLineTool", "name": "TPP ProteinProphet", "description": "**ProteinProphet** is a tool for generating probabilities for protein identifications based on MS/MS data from the validation results for peptide sequence identifications generated by PeptideProphet. It is an integral part of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_files: file in pepXML format that contains validation results for peptide sequence identifications\n\n###Outputs\n\n1. output\\_prot\\_xml\\_file: file in protXML format that contains a list of all proteins corresponding to the peptide-spectrum matches (PSMs) along with protein probabilities and global false discovery rates (FDRs) at different thresholds\n2. png_file: results visualisation file in PNG format\n3. html_file: results visualisation file in HTML format\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input files"}, {"name": "IPROPHET"}, {"name": "XPRESS"}, {"name": "ASAP PROPHET"}, {"name": "Libra ratios"}, {"name": "Exclude zeros"}, {"name": "Do not report protein length"}, {"name": "Get protein mol weights"}, {"name": "Highlight peptide cysteines"}, {"name": "Highlight peptide N-glycosylation motif"}, {"name": "Generate plot png file"}, {"name": "Model protein FPKM values"}, {"name": "Do not use NSP model"}, {"name": "PeptideProphet probabilty threshold"}, {"name": "Accuracy"}, {"name": "Normalize NSP using Protein Length"}, {"name": "Check peptide's Protein weight against the threshold"}, {"name": "Instances"}, {"name": "Delude"}, {"name": "Nooccam"}, {"name": "Softoccam"}, {"name": "Confem"}, {"name": "Logprobs"}, {"name": "Allpeps"}, {"name": "Mufactor"}, {"name": "Unmapped"}, {"name": "Excelpeps"}, {"name": "Excel"}], "output": [{"name": "Output xml file"}, {"name": "Png file"}, {"name": "Html file", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-ptmprophetparser-5-0-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-ptmprophetparser-5-0-0/10", "applicationCategory": "CommandLineTool", "name": "TPP PTMProphetParser", "description": "**PTMProphetParser** is tool that models the confidence with which mass modifications are correctly localized for each peptide. All of the popular search engines can identify that mass modifications are present for a peptide, but it is difficult to know the confidence with which the assignments are made. PTMProphetParser considers all of the possible configurations and applies a statistical model to predict which modification sites are most probable based on the spectrum evidence. PTMProphetParser is an integral component of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_file: file in pepXML format that contains peptide probability estimates\n2. input_mzxml_files: MS/MS data file in mzXML format\n\n###Outputs\n\n1. output_file: file in pepXML format that contains the assigned mass modifications with probabilities\n\n\n###Common Issues and Important Notes\n\nWhen **PTMProphetParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access MZXML files (even though they are not originally specified as tool inputs). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **Input mzxml files** is created within the wrapper. The file PEP.XML from the **Input File** port is modified here so that it reflects paths to the MZXML files on the platform.", "input": [{"name": "NOUPDATE"}, {"name": "EM models"}, {"name": "KEEPOLD"}, {"name": "MS2 mz tolerance"}, {"name": "MS1 ppm tolerance"}, {"name": "Minimum probability"}, {"name": "MASSDIFFMODE"}, {"name": "Modifications"}, {"name": "Input File"}, {"name": "Input mzxml files"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940904, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-refreshparser-5-0-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-refreshparser-5-0-0/10", "applicationCategory": "CommandLineTool", "name": "TPP RefreshParser", "description": "**RefreshParser** is a tool that refreshes all peptide-protein associations against a user-supplied protein database. It is an optional component of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n###Required Inputs\n1. input_file: file in pepXML format that contains peptide-protein associations\n2. database: file in FASTA or FA format that contains a protein sequence database\n \n###Outputs\n1. output\\_file: file in pepXML format that contains the resulting peptide-protein associations\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input file"}, {"name": "Database", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940906, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-spectrast-create-5-0-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-spectrast-create-5-0-0/8", "applicationCategory": "CommandLineTool", "name": "TPP SpectraST Create", "description": "**SpectraST** (also known as \"Spectra Search Tool\", which rhymes with \"contrast\") is a spectral library building and searching tool designed primarily for shotgun proteomics applications. It is an integral part of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n\nTraditionally, the inference of peptide sequence from its characteristic tandem mass spectra is done by sequence (database) searching. In sequence searching, a target protein (or translated DNA) database is used as a reference to generate all possible putative peptide sequences by in silico digestion. Spectral searching is an alternative approach that promises to address some of the shortcomings of sequence searching. In spectral searching, a spectral library is meticulously compiled from a large collection of previously observed and identified peptide MS/MS spectra. An unknown spectrum then can by identified by comparing it to all the candidates in the spectral library for the best match. This approach has been employed for mass spectrometric analysis of small molecules with great success but has only become possible for proteomics very recently. The main difficulty of generating enough high-quality experimental spectra for compilation into spectral libraries has been overcome by the recent explosion of proteomics data and the availability of public data repositories. Several attempts at creating and searching spectral libraries in the context of proteomics demonstrate the tremendous improvement in search speed and the great potential of this method in complementing, if not replacing, sequence searching in many proteomics applications.\n\nSpectraST Create can create a searchable spectral library using data in the following formats: MSP (a simple peak list preceded by precursor information developed by the National Institute of Standards and Technology), HLF (developed by X!Hunter), and MS2 (developed by BiblioSpec).\nIf files in these formats are supplied, SpectraST simply converts those spectral libraries into the SPLIB format that is suitable for SpectraST searches. \n\nSpectraST Create produces 5 files. The BAR.SPLIB is the library itself in a binary (machine-readable) format. The BAR.SPTXT file is a text (human-readable) version of the BAR.SPLIB file. The BAR.SPIDX and BAR.PEPIDX files are indices on the precursor m/z values and peptides, respectively. Lastly, the SPECTRAST.LOG file documents the executed commands. Some useful information about the library also is printed at the beginning of the BAR.SPTXT and BAR.PEPIDX files.\n\n###Required Inputs\n1. input\\_spectra\\_files: spectral data file in MSP, HLF, pepXML, XML, MS2, or SPLIB format from which the library is to be created\n\n###Outputs\n1. library: library file in binary SPLIB format\n2. text_library: library file in text-based SPTXT format\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input spectra files"}, {"name": "Output file name"}, {"name": "Remark"}, {"name": "Format"}, {"name": "Probability table"}, {"name": "Protein list"}, {"name": "Probability"}, {"name": "FDR"}, {"name": "Dataset identifier"}, {"name": "Change dataset identifier"}, {"name": "Deamidated"}, {"name": "Instrument and acquisition"}, {"name": "Filter library"}, {"name": "Union"}, {"name": "Intersection"}, {"name": "Subtraction"}, {"name": "Subtraction of homologs"}, {"name": "Appending"}, {"name": "Best replicate"}, {"name": "Consensus"}, {"name": "Quality filter"}, {"name": "Decoy spectra"}, {"name": "Sort library entries"}, {"name": "Semi-empirical spectra"}, {"name": "Reduce spectra"}, {"name": "Refresh protein mappings", "encodingFormat": "application/x-fasta"}, {"name": "Delete entries that do not map"}, {"name": "Delete entries that have multiple mappings"}, {"name": "Minimum number of replicates"}, {"name": "Replicate quorum"}, {"name": "Stringency"}, {"name": "Level for removal"}, {"name": "Concatenate"}, {"name": "Decoy / real size ratio"}, {"name": "modification tokens"}], "output": [{"name": "Library"}, {"name": "Text library"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB, Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940905, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-spectrast-search-5-0-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-spectrast-search-5-0-0/8", "applicationCategory": "CommandLineTool", "name": "TPP SpectraST Search", "description": "**SpectraST** (also known as \"Spectra Search Tool\", which rhymes with \"contrast\") is a spectral library building and searching tool designed primarily for shotgun proteomics applications. It is an integral part of the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\nTraditionally, the inference of peptide sequence from its characteristic tandem mass spectra is done by sequence (database) searching. In sequence searching, a target protein (or translated DNA) database is used as a reference to generate all possible putative peptide sequences by in silico digestion.\nSpectral searching is an alternative approach that promises to address some of the shortcomings of sequence searching. In spectral searching, a spectral library is meticulously compiled from a large collection of previously observed and identified peptide MS/MS spectra. An unknown spectrum then can by identified by comparing it to all the candidates in the spectral library for the best match. This approach has been employed for mass spectrometric analysis of small molecules with great success but has only become possible for proteomics very recently. The main difficulty of generating enough high-quality experimental spectra for compilation into spectral libraries has been overcome by the recent explosion of proteomics data and the availability of public data repositories. Several attempts at creating and searching spectral libraries in the context of proteomics demonstrate the tremendous improvement in search speed and the great potential of this method in complementing, if not replacing, sequence searching in many proteomics applications.\n\nSpectraST Search can perform spectral searching using data in the following formats: mzML, mzXML (all versions), mzData , MGF (Mascot Generic), DTA (SEQUEST), and MSP (a simple peak list preceded by precursor information developed by the National Institute of Standards and Technology). \nThe spectral library must be in SpectraST\u2019s SPLIB format, which can be created in SpectraST Create Mode. \nThe results can be output to the following file formats: pepXML, TXT, XLS, and HTML.\n\n###Required Inputs\n1. spectra\\_files: file in mzML, mzData, DTA, or MSP format that contains input mass spectrometry data\n2. library: spectral library file in SPLIB format and secondary SPIDX and PEPIDX files\n\n###Outputs\n1. output_file: file in pepXML, TXT, XLS, or HTML format that contains the resulting peptide-spectrum matches\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Spectra files"}, {"name": "Library"}, {"name": "Database", "encodingFormat": "application/x-fasta"}, {"name": "Database type"}, {"name": "Cache all entries"}, {"name": "Number of threads"}, {"name": "Subset spectra"}, {"name": "Precursor m/z tolerance"}, {"name": "Isotopically averaged mass"}, {"name": "All charge states"}, {"name": "Output format"}], "output": [{"name": "Output file", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB, Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940906, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-tandem2xml-5-0-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-tandem2xml-5-0-0/7", "applicationCategory": "CommandLineTool", "name": "TPP Tandem2XML", "description": "**Tandem2XML** is a tool that converts result files in Tandem format to pepXML format files suitable for use in the Trans-Proteomic Pipeline developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_file: output file from X!Tandem in XML format\n\n###Outputs\n\n1. output_file: converted file in pepXML format\n\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Input XML File"}], "output": [{"name": "Output PEP.XML File"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038175, "dateCreated": 1510940903, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-xpresspeptideparser-5-0-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-xpresspeptideparser-5-0-0/10", "applicationCategory": "CommandLineTool", "name": "TPP XPressPeptideParser", "description": "**XPressPeptideParser** is a tool that calculates the relative abundances of peptides detected in isotopically labeled MS/MS samples. It is an integral component of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input\\_pep\\_xml\\_file: file in pepXML format that contains peptide probability estimates\n2. input\\_mzXML_files: MS/MS converted data file in mzXML format\n\n###Outputs\n\n1. output_file: file in pepXML format that contains the relative abundances of peptides\n\n\n###Common Issues and Important Notes\n\nWhen **XPressPeptideParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access MZXML files (even though they are not originally specified as tool inputs). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **Input mzXML files** is created within the wrapper. The file PEP.XML from the **Input pepXML file** port is modified here so that it reflects paths to the MZXML files on the platform.", "input": [{"name": "Input pepXML file"}, {"name": "Mass tolerance"}, {"name": "Ppm"}, {"name": "Multiple isotopic labels"}, {"name": "Heavy labeled peptide elutes before light labeled partner"}, {"name": "Fix elution peak from start"}, {"name": "Fix elution peak from identified peak"}, {"name": "Vary heavy"}, {"name": "Vary light"}, {"name": "For 15N metabolic labeling, quantify w/corresponding 15N heavy pair"}, {"name": "For 15Nmetabolic labeling, quantify w/corresponding 14N light pair"}, {"name": "For 13C metabolic labeling, quantify w/corresponding 13C heavy pair"}, {"name": "For 13C metabolic labeling, quantify w/corresponding 12C light pair"}, {"name": "Charge range"}, {"name": "Minimum number of chromatogram points"}, {"name": "Number of 13C isotopic peaks"}, {"name": "Minimum probability"}, {"name": "Export intensities and intensity based ratio"}, {"name": "Label free mode"}, {"name": "Force reported endpoints to correspond to MS1 scans"}, {"name": "Input mzXML files"}], "output": [{"name": "Output PEP.XML File"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940904, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/tpp-xpressproteinratioparser-5-0-0/11", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/tpp-xpressproteinratioparser-5-0-0/11", "applicationCategory": "CommandLineTool", "name": "TPP XPressProteinRatioParser", "description": "**XPressProteinRatioParser** is a tool that calculates the relative abundances of proteins detected in isotopically labeled MS/MS samples. It is an integral component of the Trans-Proteomic Pipeline (TPP) developed by the Seattle Proteome Center.\n\n###Required Inputs\n\n1. input_file:  output file from ProteinProphet in protXML format that contains a list of proteins\n2. input_pepXML_file: output file from RefreshParser in pepXML format that contains a list of identified peptides and proteins\n\n###Outputs\n\n1. output_file: file in pepXML format that contains protein relative abundances\n\n\n###Common Issues and Important Notes\n\nWhen **XPressProteinRatioParser** is run as part of the **TPP**, it expects a specific folder structure to be established in order to access PEP.XML file (even though it is not originally specified as tool input). Since the tool wrapped here is run on the platform, which has a different folder structure, additional input **Input pepXML file** is created within the wrapper. The file PROT.XML from the **Input file** port is modified here so that it reflects path to the PEP.XML file on the platform.", "input": [{"name": "Input file"}, {"name": "Mass tolerance"}, {"name": "Ppm"}, {"name": "Multiple isotopic labels"}, {"name": "Heavy labeled peptide elutes before light labeled partner"}, {"name": "Fix elution peak from start"}, {"name": "Fix elution peak from identified peak"}, {"name": "Vary heavy"}, {"name": "Vary light"}, {"name": "For 15N metabolic labeling, quantify w/corresponding 15N heavy pair"}, {"name": "For 15Nmetabolic labeling, quantify w/corresponding 14N light pair"}, {"name": "For 13C metabolic labeling, quantify w/corresponding 13C heavy pair"}, {"name": "For 13C metabolic labeling, quantify w/corresponding 12C light pair"}, {"name": "Charge range"}, {"name": "Minimum number of chromatogram points"}, {"name": "Number of 13C isotopic peaks"}, {"name": "Minimum probability"}, {"name": "Export intensities and intensity based ratio"}, {"name": "Label free mode"}, {"name": "Force reported endpoints to correspond to MS1 scans"}, {"name": "Input pepXML file"}], "output": [{"name": "Output protXML File"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "ISB,  Insilicos LLC, the University of Washington, Hong Kong University of Science and Technology, and Fred Hutch Cancer Research Center", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038176, "dateCreated": 1510940904, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/trim-galore-0-6-2/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/trim-galore-0-6-2/9", "applicationCategory": "CommandLineTool", "name": "Trim Galore!", "description": "**Trim Galore!** is a wrapper around adapter trimming and quality control tools **Cutadapt** and **FastQC**.\n\n**Trim Galore!** has extra functionality for RRBS data. It can be used as a standalone tool or as a part of any workflow.\n\n**Trim Galore!** has one required input:\n\n* **Input reads** are files which can be in the FASTQ format or gzipped (GZ). Trim Galore! expects single-end or paired-end files to be supplied.  \n\nResults of the **TrimGalore!** tool can be presented in seven possible outputs:\n\n* **Trimmed reads** are trimmed (and validated for paired-end mode) FQ or gzipped FASTQ sequencing files. When parameter **Keep** (`--keep`) is set to TRUE, trimmed reads files are also kept together with validated reads files.\n\n* **Trimming report** are files in TXT format which represent metrics of trimming process.\n\n* **FastQC HTML reports** are files in HTML format which produce a quality control (QC) report for each FASTQ input file. These reports contain a lot of different modules, each of them will help identify a different type of potential problem in your data.\n\n* **FastQC zipped reports** are ZIP archive file which contains files needed to run FastQC HTML format file locally.\n\n* **Unpaired reads** are files in FQ format which are generated if the parameter **Retain unpaired** (`--retain_unpaired`) is set to TRUE. This file contains longer reads if only one of the two paired-end reads became too short. The length cutoff for unpaired single-end reads is governed by the parameters **Length read 1** (`-r1/--length_1`) and **Length read 2** (`-r2/--length_2`).\n\n* **Hard trimmed reads** are files in FQ format, generated as the output files of the hard-trimming process of the **Input reads**. \n\n* **Clock trimmed reads** are files in FQ format, the output files after trimming in a specific way that is currently used for the Mouse Epigenetic Clock [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nTrimming process consists of three steps:\n\n* **Quality Trimming** where low-quality base calls are trimmed off from the 3' end of the reads before adapter removal (default Phred score 20).\n\n* **Adapter Trimming** where adapter sequences from the 3\u2019 end of reads are found and removed. If no sequence was supplied it will attempt to auto-detect the adapter which has been used. For this it will analyse the first 1 million sequences of the first specified file and attempt to find the first 12 or 13 bp of the following standard adapters: \n\n  * Illumina:   AGATCGGAAGAGC\n  * Small RNA:  TGGAATTCTCGG\n  * Nextera:    CTGTCTCTTATA\n\nIf no adapter can be detected within the first 1 million sequences Trim Galore! will use default **Predefined adapter** (Illumina - `--illumina`). \n\n* **Trimming in RRBS Mode** where Trim galore! has a parameter **RRBS** (`--rrbs`) for DNA material that was digested with the restriction enzyme **MspI**. In this mode, Trim Galore! identifies sequences that were adapter-trimmed and removes another 2 bp from the 3' end of Read 1, and for paired-end libraries also the first 2 bp of Read 2.\n\n* **Removing short sequences** where trimmed reads, based on their sequence length (default: 20 bp), are filtered to avoid crashes of alignment programs which require sequences with a certain minimum length.\n\n### Changes Introduced by Seven Bridges\n\n*  According to the tool documentation, the `--basename` option can be used only when analyzing one sample. Therefore, it is not supported by Seven Bridges.\n\n### Common Issues and Important Notes\n\n* If paired-end reads are provided it is necessary to set parameter **Paired** (`--paired`) to TRUE. For paired-end reads, it is crucial to set the metadata field 'Paired-end' as 1 for the paired-end1 input file, as 2 for the paired-end2 input file. (FASTQ or FASTQ.GZ). Also, if there are more than two paired-end samples, metadata 'Sample ID' should be set, same for both paired-end reads.\n\n* Default instance (AWS c4.2xlarge) is set. The user can assign other instance to this tool by setting parameters **Memory in MB per job** and **Number of CPUs** per job accordingly (you can use tables below as a hint).\n\n* For the reads that are prepared by RRBS protocol there are three RRBS specific option: **RRBS** (`--rrbs`), **Keep** (`--keep`) and **Non-directional** (`--non-directional`).\n\n* Owing to the fact that the NuGEN Ovation kit RRBS System attaches a varying number of nucleotides (0-3) after each MspI site Trim Galore! should be run WITHOUT the option **RRBS** (`--rrbs`). This trimming is accomplished in a subsequent diversity trimming step afterwards.\n\n* If your DNA material was digested with MseI (recognition motif: TTAA) instead of MspI it is NOT necessary to specify **RRBS** (`--rrbs`) or Non-directional (`--non_directional`) since virtually all reads should start with the sequence 'TAA', and this holds true for both directional and non-directional libraries. As the end-repair of 'TAA' restricted sites does not involve any cytosines it does not need to be treated specially. Instead, simply run Trim Galore! in the standard (i.e. non-RRBS) mode.\n\n* **FastQC** will be run automatically using either selected parameter **FastQC** (`--fastqc`) or any of FastQC parameters  (e.g. **FastQC casava** (`--fastqc_args` \"`--casava`\")).\n\n* The tool doesn't have an option for parallelization. To make faster execution our recommendation is to split FASTQ files (using the public tool on the platform **SBG FASTQ Split**) and then to scatter **Trim Galore!**.\n\n* In case a hard-trimming is needed, this option will simply hard-trim sequences to a predefined number of base pairs at the 5'- or the 3'-end, depending on the option chosen. However, only one end can be hard-trimmed at the time. Also, note that the only other option allowed in the run where hard trimming is performed is *--paired* option in case the input files are paired-end. \n\n* When using *--polyA* option, please have in mind this is still an experimental option in TrimGalore. However, this option is to be performed only on one FASTQ/FQ file at a time. Also, the poly-A trimming mode expects that sequences were both adapter and quality trimmed before looking for Poly-A tails, and it is the user's responsibility to carry out an initial round of trimming. *PolyA* option is not to be run with any other option.\n\n* If reads need to be trimmed in a specific way that is currently used for the Mouse Epigenetic Clock [1], the user is advised to run this option on paired-end files, with no other option but *--paired* turned on. Following clock trimming, the resulting files should be adapter- and quality trimmed with Trim Galore as usual. In addition, reads need to be trimmed by 15bp from their 3' end to get rid of potential UMI and fixed sequences.\n\n### Performance Benchmarking\n\nThe execution time of the tool depends on the size of provided FASTQ files. AWS c4.2xlarge is assigned to this tool by default because it doesn't have high memory requirements.\n\n| Library preparation| Input size | Duration | Cost | Instance (AWS)\n| --- | --- | --- | --- | --- |\n|RRBS|2x3.1GB|14 min|$0.1|c4.2xlarge (1024GB storage)|\n|WGBS|2x700MB(gzipped)|14 min|$0.12|c4.2xlarge (1024GB storage)|\n|WGBS|2x12GB(gzipped)|2h 50 min|$1.25|c4.2xlarge (1024GB storage)|\n|WGBS|2x12GB(gzipped)|2h 17 min|$1.85|c4.4xlarge (1024GB storage)|\n|WGBS|2x12GB(gzipped)|2h 24 min|$3.90|c4.8xlarge (1024GB storage)|\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*   \n\n### References\n\n[1] Stubbs et al., Genome Biology, 2017 18:68, \"Multi-tissue DNA methylation age predictor in mouse\".\n\nThere is no published research paper about this tool. All information can be found on the [Homepage](http://www.bioinformatics.babraham.ac.uk/projects/trim_galore/) and [Manual](https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md) links.", "input": [{"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Quality"}, {"name": "Phred64"}, {"name": "FastQC"}, {"name": "Adapter"}, {"name": "Adapter2"}, {"name": "Predefined adapter"}, {"name": "Max length"}, {"name": "Stringency"}, {"name": "Error rate"}, {"name": "Gzip"}, {"name": "Length"}, {"name": "Max N"}, {"name": "Trim N"}, {"name": "No report file"}, {"name": "Suppress warnings"}, {"name": "5' clip read 1"}, {"name": "5' clip read 2"}, {"name": "Paired"}, {"name": "3' clip read 1"}, {"name": "3' clip read 2"}, {"name": "RRBS"}, {"name": "Non-directional"}, {"name": "Keep"}, {"name": "Trim 1"}, {"name": "Retain unpaired"}, {"name": "Length read 1"}, {"name": "Length read 2"}, {"name": "FastQC casava"}, {"name": "FastQC nano"}, {"name": "FastQC no filter"}, {"name": "FastQC no group"}, {"name": "FastQC contaminants", "encodingFormat": "text/plain"}, {"name": "FastQC adapters", "encodingFormat": "text/plain"}, {"name": "FastQC limits", "encodingFormat": "text/plain"}, {"name": "FastQC kmers"}, {"name": "Memory in MB per job"}, {"name": "NextSeq"}, {"name": "Cores"}, {"name": "Hard trim 5'"}, {"name": "Hard trim 3'"}, {"name": "Poly A trimming"}, {"name": "Number of CPUs per job"}, {"name": "Mouse Epigenetic Clock"}], "output": [{"name": "Trimmed reads", "encodingFormat": "text/fastq"}, {"name": "Trimming report", "encodingFormat": "text/plain"}, {"name": "FastQC HTML reports", "encodingFormat": "text/html"}, {"name": "FastQC zipped reports", "encodingFormat": "application/zip"}, {"name": "Unpaired reads", "encodingFormat": "text/fastq"}, {"name": "Hard trimmed reads"}, {"name": "Clock trimmed reads"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/FelixKrueger/TrimGalore/releases", "https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md"], "applicationSubCategory": ["Methylation", "Read Trimming"], "project": "SBG Public Data", "creator": "Felix Krueger / Babraham Bioinformatics", "softwareVersion": ["v1.0"], "dateModified": 1648560771, "dateCreated": 1574857217, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/trimmomatic-0-39/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/trimmomatic-0-39/2", "applicationCategory": "CommandLineTool", "name": "Trimmomatic", "description": "**Trimmomatic** performs a variety of useful trimming tasks for illumina paired-end and single ended data. It works with FASTQ files (using phred + 33 or phred + 64 quality scores, depending on the Illumina pipeline used), either uncompressed or gzipped. Use of gzip format is determined based on the .gz extension automatically.\n\nThere are two major modes of the program: **paired-end** mode and **single-end** mode. The **paired-end** mode will maintain correspondence of read pairs and also use the additional information contained in paired reads to better find adapter or PCR primer fragments introduced by the library preparation process [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nFor single-ended data, one input FASTQ file is specified and one output file is returned. For paired-end data, two input FASTQ files are specified, and 4 output files are returned, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not.\n\nMost important trimming options are:\n\n* **Fasta with adapters**: Cut adapter and other illumina-specific sequences from the read.\n* **Sliding window**: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.\n* **Leading quality threshold**: Cut bases off the start of a read, if below a threshold quality\n* **Trailing quality threshold**: Cut bases off the end of a read, if below a threshold quality\n* **Crop reads**: Cut the read to a specified length\n* **Head crop reads**: Cut the specified number of bases from the start of the read\n* **Minimum length**: Drop the read if it is below a specified length\n* **Average Quality**: Drop the read if the average quality is below a specified level\n\n\n### Changes Introduced by Seven Bridges\n\n* Adapters could be supplied as a file (**File With Adapters**), list of strings (**Array of Adapters**) or selected from a list of known adapters(**Fasta files with Adapters**, `ILLUMINACLIP`). When the file is provided, the list of strings and the list of known adapters are ignored. When the list of strings is provided, the list of known adapters is ignored. By default, when no file, list of strings or any of the adapters from the list is selected, no adapter trimming will be performed. \n\n* Output filenames are the same as the input filenames with the addition of the '\\_trimmed' suffix prior to the file extension for single-end files. In case of paired-end files, the '\\_trimmed\\_paired' suffix is added to both filenames, while unpaired read files have the '\\_trimmed\\_unpaired' suffix added prior to the file extension.\n\n\n### Common Issues and Important Notes\n\n* For paired-end read files, it is important to properly set the **Paired-end** metadata field on your read files.\n\n### Performance Benchmarking \n\nTrimming is not a computationally heavy procedure (as alignment is). Therefore, all benchmarking was performed on the default c4.2xlarge (8 cpus, 15Gb RAM) instance (AWS), while using the maximum number of threads the instance allows, showing that the tool benefits from multithreading.  \n\n| Input size [Gb] | Paired-end | Duration [min] |  Cost [$] |   Instance  |\n|:---------------:|:----------:|:--------------:|:------------:|:-----------:|\n|       210      |     Yes    |       670       |     6     | c4.2xlarge  |\n|       17.4      |     Yes    |       54       |     0.5     | c4.2xlarge  |\n|       4.5       |     Yes    |       16       |     0.15     |  c4.2xlarge |\n|       0.2       |     Yes    |        3       |     0.04     |  c4.2xlarge |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] [Trimmomatic paper](https://www.ncbi.nlm.nih.gov/pubmed/24695404)", "input": [{"name": "Fastq Files", "encodingFormat": "text/fastq"}, {"name": "Leading quality threshold"}, {"name": "Trailing quality threshold"}, {"name": "Minimal length"}, {"name": "Crop reads"}, {"name": "Removes the specified number of bases"}, {"name": "Fasta with Adapters"}, {"name": "Sliding window"}, {"name": "Adaptive quality trim"}, {"name": "Quality Encoding"}, {"name": "Average Quality"}, {"name": "No. CPUs"}], "output": [{"name": "Output Files - Trimmed", "encodingFormat": "text/fastq"}, {"name": "Unpaired files", "encodingFormat": "text/fastq"}, {"name": "Log file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["Utilities", "FASTQ Processing", "Read Trimming"], "project": "SBG Public Data", "creator": "Bolger A. M., Lohse M.,  Usadel B /  Max Planck Institute of Molecular Plant Physiology, Am M\u00fchlenberg & RWTH Aachen & Forschungszentrum J\u00fclich", "softwareVersion": ["v1.0"], "dateModified": 1649156263, "dateCreated": 1612261870, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/trinity-assembler-2-4-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/trinity-assembler-2-4-0/8", "applicationCategory": "CommandLineTool", "name": "Trinity Assembler 2.4.0", "description": "**Trinity Assembler** assembles transcript sequences from Illumina RNA-Seq data.    \nThe tool takes RNA-Seq data either in FASTQ or FASTA format. FASTQ files can also be gzip-compressed, in which case they should have a .gz extension. The output of the tool are the assembled transcripts.\n\n**Trinity Assembler**, developed at [Broad Institute](https://www.broadinstitute.org/) and [Hebrew University of Jerusalem](http://www.cs.huji.ac.il/), represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-seq data. It combines three independent software modules: _Inchworm_, _Chrysalis_, and _Butterfly_, applied sequentially to process large volumes of RNA-seq reads. Briefly, the process works like so:\n\n+ _Inchworm_ assembles the RNA-seq data into the unique sequences of transcripts, often generating full-length transcripts for a dominant isoform, but then reports just the unique portions of alternatively spliced transcripts.\n+ _Chrysalis_ clusters the Inchworm contigs into clusters and constructs complete de Bruijn graphs for each cluster. Each cluster represents the full transcriptional complexity for a given gene (or sets of genes that share sequences). Chrysalis then partitions the full read set among these disjoint graphs.\n+ _Butterfly_ process those the individual graphs in parallel, tracing the paths that reads and pairs of reads take within the graph, ultimately reporting full-length transcripts for alternatively spliced isoforms, and teasing apart transcripts that correspond to paralogous genes. [1] \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*\n\n\n###Common Use Case\nThis tool is created for de novo reconstruction of a transcriptome. Required input is RNA-Seq data in FASTQ or FASTA format (FASTQ files can be gzip-compressed, in which case they should have a .gz extension).    \nThe tool outputs **Assembled Transcripts** - full-length and nearly full-length transcripts across a broad range of expression levels and sequencing depths.    \n\n**Trinity Assembler** also accepts **Genome Guided BAM** (`--genome_guided_bam`) as input to perform Genome-guided Trinity De novo Transcriptome Assembly. If a genome sequence is available, Trinity offers a method whereby reads are first aligned to the genome, partitioned according to locus, followed by de novo transcriptome assembly at each locus. In this use-case, the genome is only being used as a substrate for grouping overlapping reads into clusters that will then be separately fed into Trinity for de novo transcriptome assembly. [2]    \nThe tool can handle both pair-end and single-end reads. Large RNA-Seq data sets, such as those exceeding 300M pairs, are best suited for in silico normalization prior to running Trinity. This version of Trinity will perform in silico normalization by default. You can turn it off with **No Normalize Reads** (`--no_normalize_reads`) parameter.   \n**Trinity Assembler** works better with strand-specific, but it can also accommodate non-strand-specific data. **SS Lib Type** (`--ss_lib_type`) parameter is used to control this. If you have strand-specific data, specify the library type. There are four library types. For paired reads **RF** - first read (/1) of fragment pair is sequenced as antisense (reverse(R)), and second read (/2) is in the sense strand (forward(F)) and **FR** - first read (/1) of fragment pair is sequenced as sense (forward), and second read (/2) is in the antisense strand (reverse). For unpaired (single) reads **F** - the single read is in the sense (forward) orientation and **R** - the single read is in the antisense (reverse) orientation. By setting this parameter to one values from above, you are indicating that the reads are strand-specific. By default, reads are treated as not strand-specific. [3]   \nThe raw input reads needs to be of high quality: free from adapters, bar-codes, and other contaminating sub-sequences. To perform quality trimming of inputted FASTQ files directly in this workflow, use **Trimmomatic** (`--trimmomatic`) parameter. This will generate quality-trimmed reads that will be used for assembly.\n\n###Changes Introduced by Seven Bridges\n+ Long Reads (`--long_reads`) parameter of **Trinity Assembler** tool is not added since it is still under development and marked as experimental.\n+ Full Cleanup (`--full_cleanup`) parameter of **Trinity Assembler** tool is not added because it could not find it's usability on SBG Platform.\n+ At the end of the command line of **Trinity Assembler** a command was added (`rm -R -f -- */`) for removing unnecessary temporary files.\n\n###Common Issues and Important Notes\n+ All input FASTQ/FASTA files on the **Reads** input node must have **Sample ID** metadata field set and it should be unique for each pair of pair-end reads, and for each single-end read file. If **Paired-end** metadata field is empty, input will be treated as single-end reads file in **Trinity Assembler**.\n+ The raw input **reads** need to be of high quality: free from adapters, bar-codes, and other contaminating sub-sequences. Otherwise, it is highly recommended to perform pre-processing steps (quality trimming and read filtering). To perform quality trimming of inputted FASTQ files directly in this tool, use **Trimmomatic** (`--trimmomatic`) parameter. This will generate quality-trimmed reads that will be used for assembly.\n+ When using **Genome Guided BAM** as input file, **Genome Guided Max Intron** (`--genome_guided_max_intron`) is required parameter to set.\n\n###Performance Benchmarking\n\nMemory (RAM) parameter (`--max_memory`) is locked to 200 GB in order to work properly for every input file size. The recommended instance to run this tool is r4.8xlarge.\n\nIn the following table you can find estimates of the **Trinity Assembler** tool running time and cost. All tasks in the table perform RNA-Seq De Novo Assembly on the paired-end reads.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n| Experiment size | Input size |  # of reads | Duration | Cost | Instance (AWS) |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| Small     | 4 x 232 MB         | 2M            | 20min.   | $0.72          | r4.8xlarge    |\n| Medium     | 4 x 4.5 GB             | 40M             | 5h 18min.    | $11.45               | r4.8xlarge     |\n| Large     | 2x17.4, 2x19.1  2x22.0 GB         | 76,84,97Mx2            | 1d 3h 10min.   |  $58.68               | r4.8xlarge     |\n\n\n###References\n[1] [Trinity Documentation](https://github.com/trinityrnaseq/trinityrnaseq/wiki)    \n[2] [Genome-guided Trinity De novo Transcriptome Assembly](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Genome-Guided-Trinity-Transcriptome-Assembly)   \n[3] [Strand-Specific Assembly](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity#strand_specific_assembly)", "input": [{"name": "Input read FASTQ file(s)", "encodingFormat": "text/fastq"}, {"name": "Strand Specific RNA-Seq Read Orientation"}, {"name": "Minimum contig length"}, {"name": "Jaccard Clip"}, {"name": "Run single end file as paired"}, {"name": "Trimmomatic"}, {"name": "Quality Trimming Parameters"}, {"name": "Do *not* run in silico normalization of reads"}, {"name": "Normalize Maximum Read Coverage To This Value"}, {"name": "Minimum K-mer Coverage"}, {"name": "Maximum Reads Per Graph"}, {"name": "Minimum Reads To Glue Contigs"}, {"name": "Butterfly Options"}, {"name": "PASA-like Alternative Reconstruction Mode"}, {"name": "Cufflinks-like Alternative Reconstruction Mode"}, {"name": "Group Pairs Distance"}, {"name": "Path Reinforcement Distance"}, {"name": "No Path Merging"}, {"name": "Output Prefix For Assembled FASTA file"}, {"name": "Genome Guided BAM", "encodingFormat": "application/x-bam"}, {"name": "Maximum allowed intron length"}, {"name": "Minimum read coverage for identifying and expressed region of the genome"}, {"name": "Genome guided min reads per partition"}, {"name": "Do not run Trinity phase two"}], "output": [{"name": "Assembled transcripts", "encodingFormat": "application/x-fasta"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running%20Trinity", "https://github.com/trinityrnaseq/trinityrnaseq/blob/master/LICENSE"], "applicationSubCategory": ["RNA-Seq", "Assembly"], "project": "SBG Public Data", "creator": "Broad Institute, Hebrew University of Jerusalem", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648037020, "dateCreated": 1515511504, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/twosamplemr/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/twosamplemr/2", "applicationCategory": "CommandLineTool", "name": "TwoSampleMR", "description": "**TwoSampleMR** performs Mendelian randomization testing for a given exposure-outcome pair. It's a wrapper around the **TwoSampleMR** R package and uses summary statistics data for making causal inference. \nExposure and outcome data is specified via .csv files. The app is available on the BioData Catalyst platform. For more information on the content of these files please visit the TwoSampleMr [web page](https://mrcieu.github.io/TwoSampleMR/articles/index.html).\n\n\nA list of all **inputs and parameters** with corresponding descriptions can be found at the bottom of this page.\n\n**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**\n\nFor more information on the **TwoSampleMR** package please refer to [TwoSampleMR web page](https://mrcieu.github.io/TwoSampleMR/index.html).\n\n### Common Use Cases\n\nTwoSampleMR is a tool for testing Mendelian randomization. The tool does the following:\n - Data harmonization using 'harmonise_data()' function from the package\n - Mendelian randomization testing using 'mr()' function\n - Generate odds ratio using 'generate_odds_ratios()' function\n - Generate .html report using 'mr_report()' function \n \n\n### Changes introduced by Seven Bridges\n\nThis tool covers some functionalities of the respective R package. The details can be seen in the section above. \n\n### Common Issues and Important Notes\n   - Test type can be given at input. It's specified as an array of strings and any combination of available tests can be used. Please visit the TwoSampleMr [web page](https://mrcieu.github.io/TwoSampleMR/articles/perform_mr.html) for more information on testing methods. If this input is not provided, the tool will run the analysis with the default tests: Mr Egger, Weighted median, Inverse variance weighted, Simple mode, Weighted mode.\n - When specifying tests to be used in the analysis they should be given **without** quotes.\n - When specifying the separator, it should be given **with** quotes. E.g, \"\\t\".\n - Produced report always contains results of the 5 most common tests for Mendelian randomisation (Mr Egger, Weighted median, Inverse variance weighted, Simple mode, Weighted mode.) regardless of the tests specified at the input. It's obtained using a built-in function from the package and has the same shape in every analysis. \n - Results of analysis are stored in a .csv output file. \n\n### Benchmarking\n\n - Inputs with 79 SNPs after harmonization and 5 standard test types: 3 minutes, $0.02\n\nOn-demand c4.2xlarge AWS instance was used for performance benchmarking testing. \n\n\nCost can be significantly reduced by using spot instances. Visit the [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.\n\n### Portability \n\n\nThis App was tested with cwltool version 3.0.20210319143721. The inputs, were provided in the job.yaml file and used for testing.", "input": [{"name": "Exposure file"}, {"name": "Outcome file"}, {"name": "Test type for Mendelian randomization"}, {"name": "Prefix"}, {"name": "Separator in outcome and exposure files"}, {"name": "CPU per job"}, {"name": "Memory per job"}], "output": [{"name": "Mendelian randomization results"}, {"name": "Report", "encodingFormat": "text/html"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/MRCIEU/TwoSampleMR"], "applicationSubCategory": ["GWAS"], "project": "SBG Public Data", "creator": "Gibran Heman, Philip Haycock, Jie Zheng, Tom Gaunt, Ben Elsworth, Tom Palmer", "softwareVersion": ["v1.2"], "dateModified": 1649259636, "dateCreated": 1649259636, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/umi-tools-count-1-1-1-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/umi-tools-count-1-1-1-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "UMI-tools Count", "description": "**UMI-tools Count** counts reads per-gene using mapping and UMI information [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**UMI-tools Count** counts reads per-gene based on UMI and alignment information [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--version`, `--log`, `--error`, `--help`, `--help-extended` and `--temp-dir` have been omitted from the wrapper.\n* Input parameter **Compress TSV output** was added to allow for creating the output in TSV.GZ format.\n\n### Common Issues and Important Notes\n\n*  **Input alignments** input is required. If the input is a BAM file, the corresponding BAI index must be present in the project.\n\n### Performance Benchmarking\n\nTypical tasks take a few minutes (<$0.1) on an on-demand AWS c4.2xlarge instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**UMI-tools Count** was tested with cwltool version 3.1.20211107152837. The `in_alignments`, `gene_tag`, `skip_tags_regex` and `extract_umi_method` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [UMI-tools publication](https://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract)\n\n[2] [UMI-tools documentation](https://umi-tools.readthedocs.io/en/latest/)", "input": [{"name": "Send logging information to stderr"}, {"name": "GZIP compression level"}, {"name": "Timing information output file name"}, {"name": "Timing file jobs class name"}, {"name": "Add header for timing information"}, {"name": "Log level"}, {"name": "Random seed"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Output alignments in SAM format"}, {"name": "UMI and barcode encoding method"}, {"name": "UMI separator"}, {"name": "UMI tag"}, {"name": "Split UMI in tag"}, {"name": "UMI tag delimiter"}, {"name": "Cell barcode tag"}, {"name": "Cell tag split"}, {"name": "Cell tag delimiter"}, {"name": "UMI grouping method"}, {"name": "Edit distance threshold"}, {"name": "Treat spliced reads as unique"}, {"name": "Soft clip threshold"}, {"name": "Use read length"}, {"name": "Dedup per gene"}, {"name": "Gene tag"}, {"name": "Assigned status tag"}, {"name": "Skip tags regex"}, {"name": "Dedup per contig"}, {"name": "Gene transcript mapping file"}, {"name": "Dedup per cell"}, {"name": "Mapping quality"}, {"name": "Unmapped reads handling"}, {"name": "Chimeric pairs handling"}, {"name": "Unpaired reads handling"}, {"name": "Ignore UMI"}, {"name": "Ignore template length"}, {"name": "Chromosome to restrict to"}, {"name": "Subset of reads [fraction]"}, {"name": "Input file in SAM format"}, {"name": "Paired input BAM"}, {"name": "Do not sort the output"}, {"name": "Output the cell counts in a wide format"}, {"name": "Compress TSV output"}], "output": [{"name": "Counts", "encodingFormat": "application/x-sam"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/CGATOxford/UMI-tools", "https://github.com/CGATOxford/UMI-tools/tree/master/umi_tools", "https://github.com/CGATOxford/UMI-tools/releases/tag/1.1.1"], "applicationSubCategory": ["Single Cell", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ian Sudbery", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618330139, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/umi-tools-count-tab-1-1-1-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/umi-tools-count-tab-1-1-1-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "UMI-tools Count-tab", "description": "**UMI-tools Count_tab** counts reads per-gene using UMI information [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**UMI-tools Count-tab** counts reads per-gene based on UMI information from a plain text input [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--version`, `--log`, `--error`, `--help`, `--help-extended` and `--temp-dir` have been omitted from the wrapper.\n* Input parameter **Compress TSV output** was added to allow for creating the output in TSV.GZ format.\n\n### Common Issues and Important Notes\n\n*  **Input flatlife** input is required. \n\n### Performance Benchmarking\n\nTypical tasks take a few minutes (<$0.1) on an on-demand AWS c4.2xlarge instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**UMI-tools Count-tab** was tested with cwltool version 3.1.20211107152837. The `in_flatfile` input was provided in the job.yaml/job.json file and used for testing.\n\n\n### References\n\n[1] [UMI-tools publication](https://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract)\n\n[2] [UMI-tools documentation](https://umi-tools.readthedocs.io/en/latest/)", "input": [{"name": "Send logging information to stderr"}, {"name": "GZIP compression level"}, {"name": "Timing information output file name"}, {"name": "Timing file jobs class name"}, {"name": "Add header for timing information"}, {"name": "Log level"}, {"name": "Random seed"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Input flatfile"}, {"name": "UMI grouping method"}, {"name": "Edit distance threshold"}, {"name": "Treat spliced reads as unique"}, {"name": "Soft clip threshold"}, {"name": "Use read length"}, {"name": "Readname includes cell barcode as well as UMI"}, {"name": "Compress TSV output"}, {"name": "Barcode separator"}], "output": [{"name": "Counts"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/CGATOxford/UMI-tools", "https://github.com/CGATOxford/UMI-tools/tree/master/umi_tools", "https://github.com/CGATOxford/UMI-tools/releases/tag/1.1.1"], "applicationSubCategory": ["Single Cell", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ian Sudbery", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618330139, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/umi-tools-dedup-1-1-1-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/umi-tools-dedup-1-1-1-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "UMI-tools Dedup", "description": "**UMI-tools Dedup** deduplicates reads based on mapping and UMI information [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**UMI-tools Dedup** deduplicates UMI-tagged aligned reads [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--version`, `--log`, `--error`, `--help`, `--help-extended` and `--temp-dir` have been omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n*  **Input alignments** input is required. If the input is a BAM file, the corresponding BAI index must be present in the project.\n\n### Performance Benchmarking\n\nDeduplicating a 1 GB BAM file took 6 minutes ($0.04 + $0.01) on an on-demand AWS c4.2xlarge instance with 1024 GB EBS storage.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**UMI-tools Dedup** was tested with cwltool version 3.1.20211107152837. The `in_alignments` input was provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [UMI-tools publication](https://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract)\n\n[2] [UMI-tools documentation](https://umi-tools.readthedocs.io/en/latest/)", "input": [{"name": "Send logging information to stderr"}, {"name": "GZIP compression level"}, {"name": "Timing information output file name"}, {"name": "Timing file jobs class name"}, {"name": "Add header for timing information"}, {"name": "Log level"}, {"name": "Random seed"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Output alignments in SAM format"}, {"name": "Output statistics file name"}, {"name": "UMI and barcode encoding method"}, {"name": "UMI separator"}, {"name": "UMI tag"}, {"name": "Split UMI in tag"}, {"name": "UMI tag delimiter"}, {"name": "Cell barcode tag"}, {"name": "Cell tag split"}, {"name": "Cell tag delimiter"}, {"name": "UMI grouping method"}, {"name": "Edit distance threshold"}, {"name": "Treat spliced reads as unique"}, {"name": "Soft clip threshold"}, {"name": "Use read length"}, {"name": "Dedup per gene"}, {"name": "Gene tag"}, {"name": "Assigned status tag"}, {"name": "Skip tags regex"}, {"name": "Dedup per contig"}, {"name": "Gene transcript mapping file"}, {"name": "Dedup per cell"}, {"name": "Buffer whole contig"}, {"name": "Multimapping detection method"}, {"name": "Mapping quality"}, {"name": "Unmapped reads handling"}, {"name": "Chimeric pairs handling"}, {"name": "Unpaired reads handling"}, {"name": "Ignore UMI"}, {"name": "Ignore template length"}, {"name": "Chromosome to restrict to"}, {"name": "Subset of reads [fraction]"}, {"name": "Input file in SAM format"}, {"name": "Paired input BAM"}, {"name": "Do not sort the output"}], "output": [{"name": "Deduplicated alignments", "encodingFormat": "application/x-sam"}, {"name": "Dedup statistics"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/CGATOxford/UMI-tools", "https://github.com/CGATOxford/UMI-tools/tree/master/umi_tools", "https://github.com/CGATOxford/UMI-tools/releases/tag/1.1.1"], "applicationSubCategory": ["Single Cell", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ian Sudbery", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618330139, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/umi-tools-extract-1-1-1-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/umi-tools-extract-1-1-1-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "UMI-tools Extract", "description": "**UMI-tools Extract** extracts UMI sequences from FASTQ data [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**UMI-tools Extract**  can be used to extract UMI sequences from read sequences and add them to the read identifiers [1,2]. The tool takes FASTQ inputs (**Input reads**) and a **Barcode pattern** as inputs. For paired-end data, the wrapper requires that the metadata field **Paired-end** on **Input reads** is set to the appropriate values (1 and 2 for R1 and R2 reads, respectively). \nDepending on the **Output file name suffix** value, the tool can produce GZIP-compressed outputs or uncompressed FASTQ files.\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--version`, `--log`, `--error`, `--help`, `--help-extended` and `--temp-dir` have been omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n* **Input reads**, **Barcode pattern** and **Output file name suffix** inputs are required.\n* Paired-end data used as **Input reads** must have the **Paired-end** metadata field set to the appropriate values (1 and 2 for R1 and R2 reads, respectively).\n\n### Performance Benchmarking\n\nExtracting UMIs from a 1.2 GB FASTQ.GZ file took 6 minutes ($0.04 + $0.01) on an on-demand AWS c4.2xlarge instance with 1024 GB EBS storage.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**UMI-tools Extract** was tested with cwltool version 3.1.20211107152837. The `in_reads` input was provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [UMI-tools publication](https://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract)\n\n[2] [UMI-tools documentation](https://umi-tools.readthedocs.io/en/latest/)", "input": [{"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Output file name prefix"}, {"name": "Read2 to stdout"}, {"name": "Output file name suffix"}, {"name": "Quality filter threshold"}, {"name": "Quality filter mask"}, {"name": "Quality encoding"}, {"name": "Correct errors in the cell barcode"}, {"name": "Whitelist of accepted cell barcodes", "encodingFormat": "text/plain"}, {"name": "Blacklist of rejected cell barcodes", "encodingFormat": "text/plain"}, {"name": "Subset reads"}, {"name": "Reconcile pairs"}, {"name": "UMI on either read"}, {"name": "Either read resolving options"}, {"name": "Extraction method"}, {"name": "Barcode pattern"}, {"name": "Barcode pattern for paired reads"}, {"name": "Barcode is on the 3` read end"}, {"name": "Output file to write reads not matching regex"}, {"name": "Output file to write paired reads not matching regex"}, {"name": "Ignore read pair suffixes"}, {"name": "Send logging information to stderr"}, {"name": "GZIP compression level"}, {"name": "Timing information output file name"}, {"name": "Timing file jobs class name"}, {"name": "Add header for timing information"}, {"name": "Log level"}, {"name": "Random seed"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}], "output": [{"name": "Processed reads", "encodingFormat": "text/fastq"}, {"name": "Reads not matching the regex pattern"}, {"name": "Paired reads not matching the regex pattern"}, {"name": "Timing file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/CGATOxford/UMI-tools", "https://github.com/CGATOxford/UMI-tools/tree/master/umi_tools", "https://github.com/CGATOxford/UMI-tools/releases/tag/1.1.1"], "applicationSubCategory": ["Single Cell", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ian Sudbery", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618330138, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/umi-tools-group-1-1-1-cwl1-1/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/umi-tools-group-1-1-1-cwl1-1/3", "applicationCategory": "CommandLineTool", "name": "UMI-tools Group", "description": "**UMI-tools Group** groups reads based on UMI information [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**UMI-tools Group** groups UMI-tagged aligned reads and can be used to examine PCR duplicates in detail [1,2].\n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--version`, `--log`, `--error`, `--help`, `--help-extended` and `--temp-dir` have been omitted from the wrapper.\n\n### Common Issues and Important Notes\n\n*  **Input alignments** input is required. If the input is a BAM file, the corresponding BAI index must be present in the project.\n*  **Output a BAM file with UG-tagged read groups** input parameter must be used to create an alignments output file (BAM or SAM). To obtain the output in SAM format, **Output alignments in SAM format** input parameter should also be used.\n*  **Output mapping file name** parameter should be used in order to create a flatfile describing read groups.\n\n### Performance Benchmarking\n\nProcessing a 1 GB BAM file took 6 minutes ($0.04 + $0.01) on an on-demand AWS c4.2xlarge instance with 1024 GB EBS storage.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**UMI-tools Group** was tested with cwltool version 3.1.20211107152837. The `in_alignments` and `output_bam` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [UMI-tools publication](https://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract)\n\n[2] [UMI-tools documentation](https://umi-tools.readthedocs.io/en/latest/)", "input": [{"name": "Send logging information to stderr"}, {"name": "GZIP compression level"}, {"name": "Timing information output file name"}, {"name": "Timing file jobs class name"}, {"name": "Add header for timing information"}, {"name": "Log level"}, {"name": "Random seed"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Output file name prefix"}, {"name": "Input alignments", "encodingFormat": "application/x-sam"}, {"name": "Output alignments in SAM format"}, {"name": "UMI and barcode encoding method"}, {"name": "UMI separator"}, {"name": "UMI tag"}, {"name": "Split UMI in tag"}, {"name": "UMI tag delimiter"}, {"name": "Cell barcode tag"}, {"name": "Cell tag split"}, {"name": "Cell tag delimiter"}, {"name": "UMI grouping method"}, {"name": "Edit distance threshold"}, {"name": "Treat spliced reads as unique"}, {"name": "Soft clip threshold"}, {"name": "Use read length"}, {"name": "Dedup per gene"}, {"name": "Gene tag"}, {"name": "Assigned status tag"}, {"name": "Skip tags regex"}, {"name": "Dedup per contig"}, {"name": "Gene transcript mapping file"}, {"name": "Dedup per cell"}, {"name": "Buffer whole contig"}, {"name": "Multimapping detection method"}, {"name": "Mapping quality"}, {"name": "Unmapped reads handling"}, {"name": "Chimeric pairs handling"}, {"name": "Unpaired reads handling"}, {"name": "Ignore UMI"}, {"name": "Ignore template length"}, {"name": "Chromosome to restrict to"}, {"name": "Subset of reads [fraction]"}, {"name": "Input file in SAM format"}, {"name": "Paired input BAM"}, {"name": "Do not sort the output"}, {"name": "Output a BAM file with UG-tagged read groups"}, {"name": "UMI group tag"}, {"name": "Output mapping file name"}], "output": [{"name": "Grouped alignments", "encodingFormat": "application/x-sam"}, {"name": "Output file mapping read ids to read group"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/CGATOxford/UMI-tools", "https://github.com/CGATOxford/UMI-tools/tree/master/umi_tools", "https://github.com/CGATOxford/UMI-tools/releases/tag/1.1.1"], "applicationSubCategory": ["Single Cell", "RNA-Seq", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Ian Sudbery", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618330139, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/umi-tools-whitelist-1-1-1-cwl1-1/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/umi-tools-whitelist-1-1-1-cwl1-1/4", "applicationCategory": "CommandLineTool", "name": "UMI-tools Whitelist", "description": "**UMI-tools Whitelist** builds a whitelist of cell barcodes  [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**UMI-tools Whitelist**  can be used to create a whitelist of barcodes [1,2]. The tool takes FASTQ inputs (**Input reads**). For paired-end data, the wrapper requires that the metadata field **Paired-end** on **Input reads** is set to the appropriate values (1 and 2 for R1 and R2 reads, respectively). \n\n### Changes Introduced by Seven Bridges\n\n* Parameters `--version`, `--log`, `--error`, `--help`, `--help-extended` and `--temp-dir` have been omitted from the wrapper.\n* Plots will always be generated (`--plot-prefix` is always used).\n\n### Common Issues and Important Notes\n\n* **Input reads** and **Barcode pattern** inputs are required.\n* Paired-end data provided as **Input reads** must have **Paired-end** metadata field set to appropriate values (1 and 2 for R1 and R2 reads, respectively).\n\n### Performance Benchmarking\n\nTypical tasks take a few minutes (<$0.1) on an on-demand AWS c4.2xlarge instance.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**UMI-tools Whitelist** was tested with cwltool version 3.1.20211107152837. The `in_reads` and `pattern` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [UMI-tools publication](https://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract)\n\n[2] [UMI-tools documentation](https://umi-tools.readthedocs.io/en/latest/)", "input": [{"name": "Input reads", "encodingFormat": "text/fastq"}, {"name": "Output file name prefix"}, {"name": "Subset reads"}, {"name": "Extraction method"}, {"name": "Barcode pattern"}, {"name": "Barcode pattern for paired reads"}, {"name": "Barcode is on the 3` read end"}, {"name": "Output file to write reads not matching regex"}, {"name": "Output file to write paired reads not matching regex"}, {"name": "Ignore read pair suffixes"}, {"name": "Send logging information to stderr"}, {"name": "GZIP compression level"}, {"name": "Timing information output file name"}, {"name": "Timing file jobs class name"}, {"name": "Add header for timing information"}, {"name": "Log level"}, {"name": "Random seed"}, {"name": "CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "CBs above the threshold"}, {"name": "Plot prefix"}, {"name": "Hamming distance for correction of barcodes"}, {"name": "Method"}, {"name": "Knee method"}, {"name": "Prior expectation - upper limit of cells sequenced"}, {"name": "Allow threshold error"}, {"name": "Number of cell barcodes to accept"}], "output": [{"name": "Whitelist", "encodingFormat": "text/plain"}, {"name": "Reads not matching the regex pattern"}, {"name": "Paired reads not matching the regex pattern"}, {"name": "Timing file"}, {"name": "Output plots"}, {"name": "Plots data"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/CGATOxford/UMI-tools", "https://github.com/CGATOxford/UMI-tools/tree/master/umi_tools", "https://github.com/CGATOxford/UMI-tools/releases/tag/1.1.1"], "applicationSubCategory": ["Single Cell", "CWLtool Tested", "RNA-Seq"], "project": "SBG Public Data", "creator": "Ian Sudbery", "softwareVersion": ["v1.1"], "dateModified": 1648045277, "dateCreated": 1618330139, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/variant-comparison-v1-0/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/variant-comparison-v1-0/10", "applicationCategory": "CommandLineTool", "name": "Variant Comparison", "description": "**Variant Comparison tool** provides fast and accurate way to compare different representations of VCF files. \n\nRegions with several overlapping variants often have a number of different ways in which they can be represented, all of which conform to the widely accepted VCF standard. The same is true for most variants which are complex in nature (e.g. indels in homopolymers, dinucleotide repeats etc.) and even some simple indels. Variant Comparison tool uses advanced variant comparison to deal with ambiguities arising from different variant representations. \n\nVariant Comparison tool is written using the algorithm of **vcfeval** [1], which matches the baseline and the called variant sequences so as to maximize the number of true positive variants and minimize the numbers of false positive and false negative variants.\n\nIn VBT,  two improvements have been made on vcfeval implementation:\n* VBT is written in C++ and that results in a small performance (and memory usage) gain compared to vcfeval. VBT uses this performance gain to increase the search space size of the algorithm from 50,000 (vcfeval default size) combinations to 150,000 combinations. For some complex regions where vcfeval is unable to compare variants (marked as skipped due to search space size), VBT is able to identify TPs and FPs thanks to the increased searched space.\n\n* In VBT, -ref-overlap mode of vcfeval is improved for repetitive regions. In -ref-overlap mode of vcfeval, redundant suffix and prefix base pairs from variants are trimmed using a naive method. However in repetitive regions, there are cases where the naive trimming method does not produce the best result. More details can be found in VBT supplementary text, Section 2 [2].  \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\n* Variant Comparison tool requires two VCF files as inputs, the baseline VCF file which is ground truth set of variants, on its **Baseline VCF** input (`-base`) and the called VCF file on its  **Called VCF** input (`-called`). The tool can also process multi-sample VCF files. It also requires reference sequence on its **Reference** input (`-reference`). In addition to the VCF files and reference sequence, BED file can also be provided on **BED file** input (`-bed`). BED file dictates the genomic regions inside which the comparison will be performed.\n\n* Variant Comparison tool supports an optional parameter  **Output Mode** (`-output-mode`) which specifies the way output will be generated. Two output modes are available: **SPLIT** and **GA4GH**. **SPLIT** mode creates 4 VCF files: **TPBase.vcf** - contains those variants from the baseline VCF file which agree with the variants in the called VCF file, **TPCalled.vcf** - contains those variants from the called VCF file which agree with the variants in the baseline VCF file, **FP.vcf** - contains variants unique to the called VCF file and **FN.vcf** - contains variants unique to the baseline VCF file. **GA4GH** mode creates a single merged VCF file. Default mode is **SPLIT**.\n\n###Changes Introduced by Seven Bridges\n\n* All output files will be prefixed using the **Output Prefix** parameter. In case **Output Prefix** is not provided, output prefix will be identical to **Called Sample Name**. If **Called Sample Name** is not set, output prefix will be the same as the **Sample ID** metadata from the **Called VCF** file, if the **Sample ID** metadata exists. Otherwise, output prefix will be inferred from the **Called VCF** filename. This way, having identical names of the output files between runs is avoided.\n\n###Common Issues and Important Notes\n\n* When processing multi-sample VCF files, it is very important to specify different sample names for the baseline and the called variants sets via the following two input parameters: **Base Sample Name** (`-sample-base`) and **Called Sample Name** (`-sample-called`). Otherwise, the first sample is selected by default for both baseline and called variants sets.\n\n* If BED file is not provided, the comparison will be performed across the whole genome, and will compare all entries of each of the baseline and the called variant files. This is very important in cases such as when comparing an exome called set to a whole-genome baseline set: without a BED file, the comparison will report a lot of false negatives, as it expects the called test set to identify all the variants contained in the baseline set across the whole genome. Similarly, if a whole-genome called variant set is compared to an exome baseline variant set without a BED file, the comparison will report a lot of false positives, as it expects the called variant set not to report any variants in regions that are outside of the baseline set's scope.\n\n###Performance Benchmarking\n\n* The tool relies on the algorithm that uses dynamic programming techique to solve the maximization problem. Thus, it is not computationally challenging with reasonable running time, which is about 6 minutes on c4.2xlarge AWS instance for the whole humane genome. The task costs around 0.04 $.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n###References\n\n[1] [vcfeval paper](https://www.biorxiv.org/content/biorxiv/early/2015/08/02/023754.full.pdf)\n\n[2] [VBT supplementary text](https://www.biorxiv.org/content/biorxiv/suppl/2018/01/24/253492.DC1/253492-1.pdf)", "input": [{"name": "Baseline VCF", "encodingFormat": "application/x-vcf"}, {"name": "Called VCF", "encodingFormat": "application/x-vcf"}, {"name": "Reference", "encodingFormat": "application/x-fasta"}, {"name": "Output Mode"}, {"name": "Is Allele Match"}, {"name": "Base Sample Name"}, {"name": "Called Sample Name"}, {"name": "Filter Name"}, {"name": "BED File", "encodingFormat": "text/x-bed"}, {"name": "Consider Only SNPs"}, {"name": "Consider Only INDELs"}, {"name": "Disable Reference Overlapping"}, {"name": "Generate Sync Points"}, {"name": "Start Trimming From End"}, {"name": "Thread Count"}, {"name": "Maximal Path Size"}, {"name": "Max Iteration Count"}, {"name": "Maximum Variant Size"}, {"name": "Output Prefix"}], "output": [{"name": "Output Logs", "encodingFormat": "text/plain"}, {"name": "Output VCF File(s)", "encodingFormat": "application/x-vcf"}, {"name": "Sync Point List File", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/sbg/VBT-TrioAnalysis", "https://github.com/sbg/VBT-TrioAnalysis", "https://github.com/sbg/VBT-TrioAnalysis", "https://github.com/sbg/VBT-TrioAnalysis"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Berke Cagkan Toptas (berke.toptas@sbgdinc.com), Seven Bridges Genomics", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649156264, "dateCreated": 1520434434, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/variant-effect-predictor-83/54", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/variant-effect-predictor-83/54", "applicationCategory": "CommandLineTool", "name": "Variant Effect Predictor", "description": "The VEP determines the effect of your variants (SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions. Using it, you can find out the:\n\n* genes and transcripts affected by the variants\n\n* location of the variants (e.g. upstream of a transcript, in coding sequence, in non-coding RNA, in regulatory regions)\n\n* consequence of your variants on the protein sequence (e.g. stop gained, missense, stop lost, frameshift)\n\n* known variants that match yours, and associated minor allele frequencies from the 1000 Genomes Project\n\n* SIFT and PolyPhen scores for changes to protein sequence\n\n### Common issues\n\n* **Important note:** This tool is always working in offline mode, meaning that you will have to provide species cache file and you will not be able to establish any database connection. In addition, features (parameters) that require database connection were not added to the platform. Unpacking archived cache files is included in the tool.\n* Cache files can be found on **ftp://ftp.ensembl.org/pub/release-83/variation/VEP/**.\n* If you are running VEP on a large VCF file (for example a DBSNP VCF) please use **fork** (number of threads) parameter or allocate more memory for the job.\n* In order to use LOFTEE plugin you have to enable the parameter called LOFTEE plugin and provide the **LOFTEE conservation file**.", "input": [{"name": "Everything"}, {"name": "Fork"}, {"name": "Species for your data"}, {"name": "Species Cache File", "encodingFormat": "application/x-tar"}, {"name": "Assembly version"}, {"name": "Input file name"}, {"name": "Format"}, {"name": "Force overwrite"}, {"name": "No stats"}, {"name": "HTML"}, {"name": "Dont skip"}, {"name": "Fasta file(s) to use to look up reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Variant class"}, {"name": "Sift"}, {"name": "Polyphen"}, {"name": "Humdiv"}, {"name": "Gene phenotype"}, {"name": "Regulatory"}, {"name": "Cell type"}, {"name": "Custom annotations files", "encodingFormat": "application/x-vcf"}, {"name": "LOFTEE plugin"}, {"name": "LOFTEE Conservation File"}, {"name": "Individual"}, {"name": "Phased"}, {"name": "Allele number"}, {"name": "Total length"}, {"name": "Numbers"}, {"name": "Domains"}, {"name": "No escape"}, {"name": "Keep CSQ"}, {"name": "VCF info field"}, {"name": "VCF info field (other)"}, {"name": "Terms"}, {"name": "HGVS"}, {"name": "Shift HGVS"}, {"name": "Protein"}, {"name": "Symbol"}, {"name": "CCDS"}, {"name": "UniProt"}, {"name": "Add transcript support level"}, {"name": "APPRIS"}, {"name": "Canonical"}, {"name": "Biotype"}, {"name": "Xref refseq"}, {"name": "Check existing"}, {"name": "Check alleles"}, {"name": "GMAF"}, {"name": "MAF 1Kg"}, {"name": "MAF ESP"}, {"name": "MAF ExAC"}, {"name": "Old MAF"}, {"name": "Pubmed"}, {"name": "Failed"}, {"name": "Output as VCF file"}, {"name": "Output as JSON file"}, {"name": "Output as GVF file"}, {"name": "Fields"}, {"name": "Convert"}, {"name": "Minimal"}, {"name": "Check ref"}, {"name": "Coding only"}, {"name": "Chromosomes"}, {"name": "No intergenic"}, {"name": "Pick"}, {"name": "Pick allele"}, {"name": "Flag pick"}, {"name": "Flag pick allele"}, {"name": "Per gene"}, {"name": "Pick order"}, {"name": "Most severe"}, {"name": "Summary"}, {"name": "Filter common"}, {"name": "Frequency population"}, {"name": "Freq freq"}, {"name": "Frequency GT/LT"}, {"name": "Freq filter"}, {"name": "Allow non variant"}, {"name": "Gencode basic"}, {"name": "Refseq"}, {"name": "Merged"}, {"name": "All refseq"}, {"name": "No whole genome"}, {"name": "Buffer size"}, {"name": "LOFTEE filter position"}, {"name": "LOFTEE minimum intron size"}, {"name": "LOFTEE fast length calculation"}, {"name": "LOFTEE human ancestor FA"}, {"name": "Number of CPUs to use"}, {"name": "Memory for job"}], "output": [{"name": "VEP output file", "encodingFormat": "application/x-vcf"}, {"name": "HTML summary file", "encodingFormat": "text/html"}, {"name": "HTML output file", "encodingFormat": "text/html"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-tools/tree/release/83/scripts/variant_effect_predictor", "https://github.com/Ensembl/ensembl-tools/archive/release/83.zip"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "McLaren et. al.", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648045661, "dateCreated": 1457032404, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/ensembl-vep-94-2/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/ensembl-vep-94-2/4", "applicationCategory": "CommandLineTool", "name": "Variant Effect Predictor", "description": "**Variant Effect Predictor** predicts functional effects of genomic variants [1] and is used to annotate VCF files.\n\n**Variant Effect Predictor** determines the effect of your variants (SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Variant Effect Predictor** is a tool commonly used for variant and gene level annotation of VCF or VCF.GZ files. Running the tool on Seven Bridges platform requires using a VEP cache file. VEP cache files can be obtained from our Public Reference Files section (homo_sapiens_vep_9x_GRCh37.tar.gz and \thomo_sapiens_vep_9x_GRCh38.tar.gz) or imported as files to your project from [Ensembl ftp site](ftp://ftp.ensembl.org/pub/current_variation/VEP/) using the [FTP/HTTP import](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server) feature.\n\n### Changes Introduced by Seven Bridges\n\n- Additional boolean flags are introduced to activate the use of plugins included in the Seven Bridges version of the tool (CSN, MaxEntScan, and LoFtool plugins can be accessed with parameters **Use CSN plugin**, **Use MaxEntScan plugin**, and **Use LoFtool plugin**, respectively).\n- When using custom annotation sources (`--custom` flag) input files and parameters are specified separately and both must be provided to run the tool (inputs **Custom annotation sources** and **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**). Additionally, separate inputs have been provided for BigWig custom annotation sources and parameters, as these files do not require indexing before use (inputs **Custom annotation - BigWig sources only** and **Annotation parameters for custom BigWig annotation sources only**). Tabix TBI indices are required for other custom annotation sources.\n- The following parameters have been excluded from the Seven Bridges version of the tool:\n    * `--help`: Not present in the Seven Bridges version in general.\n    * `--quiet`: Warnings are desirable.\n    * `--species [species]`: Relevant only if **Variant Effect Predictor** is connecting to the Ensembl database online, which is not the case with the tool on the platform.\n    * `--force_overwrite`: Overwriting existing output, which is not likely to be found on the Seven Bridges Platform.\n    * `--dir_cache [directory]`, `--dir_plugins [directory]`: Covered with a more general flag (`--dir`).\n    * `--cache`: The `--offline` argument is always used instead.\n    * `--format:` argument with it's corresponding suboptions `hgvs`, and `id` These options require an Ensembl database connection.\n    * `--show_cache_info`: This option only shows cache info and quits.\n    * `--plugin [plugin name]`: Several plugins are supplied in the **Variant Effect Predictor** tool on the platform (e.g. dbNSFP [4], CSN, MaxEntScan, LoFtool). However, this option was not wrapped because, in order to use any plugin, it must be installed on the **Variant Effect Predictor** docker image. Additional plugins can be added upon request.\n    * `--phased`: Used with plugins requiring phased data. No such plugins are present in the wrapper.\n    * `--database`: Database access-only option\n    * `--host [hostname]`: Database access-only option\n    * `--user [username]`: Database access-only option\n    * `--port [number]`: Database access-only option\n    * `--password [password]`: Database access-only option\n    * `--genomes`: Database access-only option\n    * `--lrg`: Database access-only option\n    * `--check_svs`: Database access-only option\n    * `--db_version [number]`: Database access-only option\n    * `--registry [filename]`: Database access-only option\n\n### Common Issues and Important Notes\n \n* Inputs **Input VCF** (`--input_file`) and **Species cache** files are required. They represent a variant file containing variants to be annotated and a database cache file used for annotating the most common variants found in the particular species, respectively. The cache file reduces the need to send requests to an outside **Variant Effect Predictor** relevant annotation database, which is usually located online.   \n* **Fasta file(s) to use to look up reference sequences** (`--fasta`) is not required, however, it is highly recommended when using **Variant Effect Predictor** in offline mode which requires a FASTA file for several annotations.\n* Please see flag descriptions or official documentation [3] for detailed descriptions of limitations.\n* The **Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)** (`--af_exac` or `--af_gnomAD`) parameter should be set: Please note that ExAC data has been superseded by gnomAD data and is only accessible with older (<90) cache versions. The Seven Bridges version of the tool will automatically swap flags according to the cache version reported.\n* The **Include Ensembl identifiers when using RefSeq and merged caches** (`--all_refseq`) and **Exclude predicted transcripts when using RefSeq or merged cache** (`--exclude_predicted`) parameters should only be used with RefSeq or merged caches\n* The **Add APPRIS identifiers** (`--appris`) parameter - APPRIS is only available for GRCh38.\n* The **Add transcript support level** (`--tsl`) parameter is only available for GRCh38.\n* The **Fields to configure the output format (VCF or tab only) with** (`--fields`) parameter can only be used with VCF and TSV output.\n* Input parameters **Force overwrite supplied reference allele** (`--lookup_ref`) and **Check REF allele against provided reference sequence** (`--check_ref`) should not be used together.\n* Input parameter **Output only the most severe consequence per variant** (`--most_severe`) is incompatible with **Output format** `vcf`. Using this parameter produces a tab-separated output file.\n* The **Samples to annotate** (`--individual`) parameter requires that all samples of interest have proper genotype entries for all variant rows in the file. **Variant Effect Predictor** will not output multiple variant rows per sample if genotypes are missing in those rows.\n* If dbNSFP [4] is used for annotation, a preprocessed dbNSFP file (input **dbNSFP database file**) and dbNSFP column names (parameter **Columns of dbNSFP to report**) should be provided. dbNSFP column names should match the release of dbNSFP provided for annotation (for detailed list of column names, please consult the [readme files accompanying the dbNSFP release](https://sites.google.com/site/jpopgen/dbNSFP) used for annotation). If no dbNSFP column names are provided alongside a dbNSFP annotation file, the following example subset of columns applicable to dbNSFP versions 2.9.3 and 3.Xc will be used for annotation: `FATHMM_pred,MetaSVM_pred,GERP++_RS`.\n * If using dbscSNV for annotation, a dbscSNV file (input **dbscSNV database file**) should be provided.\n* The **Version of VEP cache if not default** parameter (`--cache_version`) must be supplied if not using a VEP 94 cache.\n* If using custom annotation sources (input **Custom annotation sources**) corresponding parameters (input **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**) must be set and match the order of supplied input files.\n\nThe input files **GFF annotation** (`--gff`) and **GTF annotation** (`--gtf`), which are used for transcript annotation should be bgzipped (using the **Tabix Bgzip** tool) and tabix-indexed (using the **Tabix Index** tool), and a FASTA file containing genomic sequences is required (input **Fasta file(s) to use to look up reference sequence**). If preprocessing these files locally, implement the following [1]:\n\n    grep -v \"#\" data.gff | sort -k1,1 -k4,4n -k5,5n | bgzip -c > data.gff.gz\n\n    tabix -p gff data.gff.gz\n\n\n### Performance Benchmarking\n\nPerformance of **Variant Effect Predictor** will vary greatly depending on the annotation options selected and input file size. Increasing the number of forks used with the parameter **Fork number** (`--fork`) and the number of processors will help. Additionally, tabix-indexing your supplied FASTA file, or setting the **Do not generate a stats file** (`--no_stats`) flag will speed up annotation. Preprocessing the VEP cache using the **convert_cache.pl** script included in the **ensembl-vep distribution** will also help if using **Check for co-located known variants** (`--check_existing`) flag or any of the allele frequency associated flags. VEP caches available on the Seven Bridges platform have been preprocessed in this way.\nUsing **Add HGVS identifiers** (`--hgvs`) parameter will slow down the annotation process.\n\nIn the following table you can find estimates of **Variant Effect Predictor** running time and cost. Sample that was annotated was NA12878 genome (~100 Mb, as VCF.GZ).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n                   \n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| All available annotations, all plugins, and dbSNP v150 as a custom annotation source   | 53 min   | $0.35            |c4.2xlarge      |\n| Basic annotations, without plugins and dbSNP v150  | 35 min    | $0.23                | c4.2xlarge     |\n\n\n### References\n\n[1] [Ensembl Variant Effect Predictor github page](https://github.com/Ensembl/ensembl-vep)\n\n[2] [Homepage](http://www.ensembl.org/info/docs/tools/vep/script/index.html)\n\n[3] [Running VEP - Documentation page](https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html)\n\n[4] [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP)", "input": [{"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Species cache file", "encodingFormat": "application/x-tar"}, {"name": "Number of CPUs"}, {"name": "Memory to use for the task"}, {"name": "Assembly version"}, {"name": "Fasta file(s) to use to look up reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Fork number"}, {"name": "Input file format"}, {"name": "Do not skip input variants that fail validation"}, {"name": "PolyPhen2 HumDiv"}, {"name": "Overlapping protein domains"}, {"name": "No url escaping HGSV strings"}, {"name": "Chromosome synonyms", "encodingFormat": "text/plain"}, {"name": "Include failed collocated variants"}, {"name": "Buffer size to use"}, {"name": "Version of VEP cache if not default"}, {"name": "dbNSFP database file"}, {"name": "Columns of dbNSFP to report"}, {"name": "Use LoFtool plugin"}, {"name": "Use MaxEntScan plugin"}, {"name": "Use CSN plugin"}, {"name": "Add APPRIS identifiers"}, {"name": "dbscSNV database file"}, {"name": "Specify whether the cache used is an Ensembl, RefSeq or merged VEP cache"}, {"name": "GFF annotation file"}, {"name": "GTF annotation file", "encodingFormat": "application/x-gtf"}, {"name": "NCBI BAM file for correcting transcript models", "encodingFormat": "application/x-bam"}, {"name": "Custom annotation sources"}, {"name": "Use user-provided ref allele with bam"}, {"name": "Distance"}, {"name": "Optional config file"}, {"name": "Compress output"}, {"name": "Output aligned RefSeq mRNA identifier"}, {"name": "Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)"}, {"name": "Custom annotation - BigWig sources only"}, {"name": "Annotation parameters for custom BigWig annotation sources only"}, {"name": "Output file name"}, {"name": "Samples to annotate [--individual]"}, {"name": "Do not generate a stats file [--no_stats]"}, {"name": "Output Sequence Ontology variant class"}, {"name": "SIFT prediction"}, {"name": "PolyPhen prediction"}, {"name": "Connect overlapped gene with phenotype"}, {"name": "Report overlaps with regulatory regions [--regulatory]"}, {"name": "Keep existing CSQ entries in the input VCF INFO field"}, {"name": "Type of consequence terms to report"}, {"name": "Add HGVS identifiers"}, {"name": "Add genomic HGVS identifiers"}, {"name": "Add Ensembl protein identifiers"}, {"name": "Add gene symbols where available"}, {"name": "Add CCDS transcript identifers"}, {"name": "Add UniProt-associated database identifiers"}, {"name": "Add transcript support level"}, {"name": "Add a flag indicating if the transcript is canonical"}, {"name": "Add biotype of transcript or regulatory feature"}, {"name": "Check for co-located known variants"}, {"name": "Output format"}, {"name": "Report Pubmed IDs for publications that cite an existing variant"}, {"name": "Add 1000 genomes phase 3 global allele frequency"}, {"name": "Add allele frequency from continental 1000 genomes populations"}, {"name": "Add allele frequency from NHLBI-ESP populations"}, {"name": "Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)"}, {"name": "Adds affected exon and intron numbering"}, {"name": "Add cDNA, CDS and protein positions (position/length)"}, {"name": "VCF info field name"}, {"name": "Enable or disable 3' shifting of HGVS notations"}, {"name": "Keep non-variant lines (null ALT) in the VEP VCF output"}, {"name": "Check REF allele against provided reference sequence"}, {"name": "Shortcut flag to turn on most commonly used annotations [--everything]"}, {"name": "Raw input data string"}, {"name": "Summary stats file name"}, {"name": "Generate plain text stats file instead of HTML"}, {"name": "Optional file name for warnings file output"}, {"name": "Override input reference allele with overlapped transcript ref allele"}, {"name": "Retrieve nearest transcript/gene"}, {"name": "Cell type(s) to report regulatory regions for"}, {"name": "Identify allele number from VCF input"}, {"name": "Do not write header lines to output files"}, {"name": "Exclude null alleles when checking co-located variants"}, {"name": "Do not check alleles of co-located variants"}, {"name": "Report highest allele frequency observed in 1000 genomes, ESP or gnomAD populations"}, {"name": "Fields to configure the output format (VCF or tab only) with"}, {"name": "Convert alleles to minimal representation before assigning consequences"}, {"name": "Limit analysis to GENCODE basic transcript set"}, {"name": "Include Ensembl identifiers when using RefSeq and merged caches"}, {"name": "Exclude predicted transcripts when using RefSeq or merged cache"}, {"name": "Filter transcripts according to arbitrary rules"}, {"name": "Select a subset of chromosomes to analyse"}, {"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Exclude intergenic consequences from the output"}, {"name": "Output only the most severe consequence per variant"}, {"name": "Output only a comma-separated list of all observed consequences per variant"}, {"name": "Exclude variants with a common (>1 % AF) co-located variant"}, {"name": "Use frequency filtering"}, {"name": "Population to use in the frequency filter"}, {"name": "Allele frequency to use for filtering"}, {"name": "Frequency cutoff operator"}, {"name": "Specify whether to exclude or include only variants that pass the frequency filter"}, {"name": "Pick one line or block of consequence data per variant, including transcript-specific columns"}, {"name": "Pick one line or block of consequence data per variant allele"}, {"name": "Output only the most severe consequence per gene"}, {"name": "Pick one line or block of consequence data per variant allele and gene combination"}, {"name": "Pick one line or block of consequence data per variant with PICK flag"}, {"name": "Pick one line or block of consequence data per variant allele, with PICK flag"}, {"name": "Pick one line or block of consequence data per variant allele and gene combination, with PICK flag"}, {"name": "Customise the order of criteria applied when picking a block of annotation data"}, {"name": "Add IUPAC allele ambiguity codes"}, {"name": "Add version numbers to Ensembl transcript identifiers"}, {"name": "Force overwrite supplied reference allele"}], "output": [{"name": "VEP output file", "encodingFormat": "text/plain"}, {"name": "Compressed (bgzip/gzip) output"}, {"name": "Output summary stats file", "encodingFormat": "text/plain"}, {"name": "Optional file with VEP warnings and errors", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-vep"], "applicationSubCategory": ["Annotation"], "project": "SBG Public Data", "creator": "Ensembl", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648040158, "dateCreated": 1539880016, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/variant-effect-predictor-101-0-cwl1-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/variant-effect-predictor-101-0-cwl1-0/7", "applicationCategory": "CommandLineTool", "name": "Variant Effect Predictor CWL1.0", "description": "**Variant Effect Predictor** predicts functional effects of genomic variants [1] and is used to annotate VCF files.\n\n**Variant Effect Predictor** determines the effect of your variants (SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions [2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the end of the page.*\n\n### Common Use Cases\n\n**Variant Effect Predictor** is a tool commonly used for variant and gene level annotation of VCF or VCF.GZ files. Running the tool on the Seven Bridges platform requires using a VEP cache file. VEP cache files can be obtained from our **Public Reference Files** section (`homo_sapiens_vep_101_GRCh37.indexed.tar.gz` and `homo_sapiens_vep_101_GRCh38.indexed.tar.gz`) or imported as files to your project from [Ensembl ftp site](ftp://ftp.ensembl.org/pub/release-101/variation/indexed_vep_cache/) using the [FTP/HTTP import](https://docs.sevenbridges.com/docs/upload-from-an-ftp-server) feature.\n\n### Changes Introduced by Seven Bridges\n\n- Additional boolean flags are introduced to activate the use of plugins included in the Seven Bridges version of the tool (CSN, MaxEntScan, and LoFtool plugins can be accessed with parameters **Use CSN plugin**, **Use MaxEntScan plugin**, and **Use LoFtool plugin**, respectively).\n- When using custom annotation sources (`--custom` flag) input files and parameters are specified separately and both must be provided to run the tool (inputs **Custom annotation sources** and **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**). Additionally, separate inputs have been provided for BigWig custom annotation sources and parameters, as these files do not require indexing before use (inputs **Custom annotation - BigWig sources only** and **Annotation parameters for custom BigWig annotation sources only**). Tabix TBI indices are required for other custom annotation sources.\n- The following parameters have been excluded from the Seven Bridges version of the tool:\n    * `--help`: Not present in the Seven Bridges version in general.\n    * `--quiet`: Warnings are desirable.\n    * `--species [species]`: Relevant only if **Variant Effect Predictor** is connecting to the Ensembl database online, which is not the case with the tool on the Platform.\n    * `--force_overwrite`: Overwriting existing output, which is not likely to be found on the Seven Bridges Platform.\n    * `--dir_cache [directory]`, `--dir_plugins [directory]`: Covered with a more general flag (`--dir`).\n    * `--cache`: The `--offline` argument is always used instead.\n    * `--format:` argument with its corresponding suboptions `hgvs`, and `id`. These options require an Ensembl database connection.\n    * `--show_cache_info`: This option only shows cache info and quits.\n    * `--plugin [plugin name]`: Several plugins are supplied in the **Variant Effect Predictor** tool on the Platform (e.g. dbNSFP [4], CSN, MaxEntScan, LoFtool). However, this option was not wrapped because, in order to use any plugin, it must be installed on the **Variant Effect Predictor** Docker image. Additional plugins can be added upon request.\n    * `--phased`: Used with plugins requiring phased data. No such plugins are present in the wrapper.\n    * `--database`: Database access-only option\n    * `--host [hostname]`: Database access-only option\n    * `--user [username]`: Database access-only option\n    * `--port [number]`: Database access-only option\n    * `--password [password]`: Database access-only option\n    * `--genomes`: Database access-only option\n    * `--lrg`: Database access-only option\n    * `--check_svs`: Database access-only option\n    * `--db_version [number]`: Database access-only option\n    * `--registry [filename]`: Database access-only option\n\n### Common Issues and Important Notes\n \n* Inputs **Input VCF** (`--input_file`) and **Species cache** files are required. They represent a variant file containing variants to be annotated and a database cache file used for annotating the most common variants found in the particular species, respectively. The cache file reduces the need to send requests to an outside **Variant Effect Predictor** relevant annotation database, which is usually located online.   \n* **Fasta file(s) to use to look up reference sequences** (`--fasta`) is not required, however, it is highly recommended when using **Variant Effect Predictor** in offline mode which requires a FASTA file for several annotations.\n* Please see flag descriptions or official documentation [3] for detailed descriptions of limitations.\n* The **Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)** (`--af_exac` or `--af_gnomAD`) parameter should be set: Please note that ExAC data has been superseded by gnomAD data and is only accessible with older (<90) cache versions. The Seven Bridges version of the tool will automatically swap flags according to the cache version reported.\n* The **Include Ensembl identifiers when using RefSeq and merged caches** (`--all_refseq`) and **Exclude predicted transcripts when using RefSeq or merged cache** (`--exclude_predicted`) parameters should only be used with RefSeq or merged caches\n* The **Add APPRIS identifiers** (`--appris`) parameter - APPRIS is only available for GRCh38.\n* The **Fields to configure the output format (VCF or tab only) with** (`--fields`) parameter \n can only be used with VCF and TSV output.\n* The **Samples to annotate** (`--individual`) parameter requires that all samples of interest have proper genotype entries for all variant rows in the file. **Variant Effect Predictor** will not output multiple variant rows per sample if genotypes are missing in those rows.\n* If dbNSFP [4] is used for annotation, a preprocessed dbNSFP file (input **dbNSFP database file**) and dbNSFP column names (parameter **Columns of dbNSFP to report**) should be provided. dbNSFP column names should match the release of dbNSFP provided for annotation (for a detailed list of column names, please consult the [readme files accompanying the dbNSFP release](https://sites.google.com/site/jpopgen/dbNSFP) used for annotation). If no dbNSFP column names are provided alongside a dbNSFP annotation file, the following example subset of columns applicable to dbNSFP versions 2.9.3, 3.Xc and 4.0c will be used for annotation: `FATHMM_pred,MetaSVM_pred,GERP++_RS`.\n * If using dbscSNV for annotation, a dbscSNV file (input **dbscSNV database file**) should be provided.\n* The **Version of VEP cache if not default** parameter (`--cache_version`) must be supplied if not using a VEP 101 cache.\n* If using custom annotation sources (input **Custom annotation sources**), corresponding parameters (input **Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)**) must be set and must match the order of supplied input files.\n* Input parameter **Output only the most severe consequence per variant** (`--most_severe`) is incompatible with **Output format** `vcf`. Using this parameter produces a tab-separated output file.\n\nThe input files **GFF annotation** (`--gff`) and **GTF annotation** (`--gtf`), which are used for transcript annotation, should be bgzipped (using the **Tabix Bgzip** tool) and tabix-indexed (using the **Tabix Index** tool), and a FASTA file containing genomic sequences is required (input **Fasta file(s) to use to look up reference sequence**). If preprocessing these files locally, implement the following [1]:\n\n    grep -v \"#\" data.gff | sort -k1,1 -k4,4n -k5,5n | bgzip -c > data.gff.gz\n\n    tabix -p gff data.gff.gz\n\n\n### Performance Benchmarking\n\nPerformance of **Variant Effect Predictor** will vary greatly depending on the annotation options selected and input file size. Increasing the number of forks used with the parameter **Fork number** (`--fork`) and the number of processors will help. Additionally, tabix-indexing your supplied FASTA file, or setting the **Do not generate a stats file** (`--no_stats`) flag will speed up annotation. Preprocessing the VEP cache using the **convert_cache.pl** script included in the **ensembl-vep distribution** will also help if using **Check for co-located known variants** (`--check_existing`) flag or any of the allele frequency associated flags. VEP caches available on the Seven Bridges Platform are indexed Ensembl VEP cache files obtained directly from their [ftp repository](ftp://ftp.ensembl.org/pub/release-100/variation/indexed_vep_cache/).\nUsing **Add HGVS identifiers** (`--hgvs`) parameter will slow down the annotation process.\n\nIn the following table you can find estimates of **Variant Effect Predictor** running time and cost. The sample that was annotated was NA12878 genome (~100 Mb, as VCF.GZ).\n\n*Cost can be significantly reduced by **spot instance** usage. Visit [knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n                   \n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| All available annotations, all plugins and dbNSFP4.0c   | 3h 28 min   | $1.79            |c4.2xlarge      |\n| Basic annotations, without plugins and dbNSFP4.0c  | 37 min    | $0.23                | c4.2xlarge     |\n\n\n### References\n\n[1] [Ensembl Variant Effect Predictor github page](https://github.com/Ensembl/ensembl-vep)\n\n[2] [Homepage](http://www.ensembl.org/info/docs/tools/vep/script/index.html)\n\n[3] [Running VEP - Documentation page](https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html)\n\n[4] [dbNSFP](https://sites.google.com/site/jpopgen/dbNSFP)", "input": [{"name": "Input VCF", "encodingFormat": "application/x-vcf"}, {"name": "Species cache file", "encodingFormat": "application/x-tar"}, {"name": "Number of CPUs"}, {"name": "Memory to use for the task"}, {"name": "Assembly version"}, {"name": "Fasta file(s) to use to look up reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "Fork number"}, {"name": "Buffer size to use"}, {"name": "dbNSFP database file"}, {"name": "Columns of dbNSFP to report"}, {"name": "dbscSNV database file"}, {"name": "GFF annotation file"}, {"name": "GTF annotation file", "encodingFormat": "application/x-gtf"}, {"name": "NCBI BAM file for correcting transcript models", "encodingFormat": "application/x-bam"}, {"name": "Custom annotation sources"}, {"name": "Optional config file"}, {"name": "Annotation parameters for custom annotation sources (comma separated values, ensembl-vep --custom flag format)"}, {"name": "Custom annotation - BigWig sources only"}, {"name": "Annotation parameters for custom BigWig annotation sources only"}, {"name": "Optional file name for warnings file output"}, {"name": "Samples to annotate [--individual]"}, {"name": "Shortcut flag to turn on most commonly used annotations [--everything]"}, {"name": "Version of VEP cache if not default"}, {"name": "Specify whether the cache used is an Ensembl, RefSeq or merged VEP cache"}, {"name": "Do not generate a stats file [--no_stats]"}, {"name": "Output Sequence Ontology variant class"}, {"name": "PolyPhen2 HumDiv"}, {"name": "SIFT prediction"}, {"name": "PolyPhen prediction"}, {"name": "Overlapping protein domains"}, {"name": "No url escaping HGSV strings"}, {"name": "Distance"}, {"name": "Connect overlapped gene with phenotype"}, {"name": "Keep existing CSQ entries in the input VCF INFO field"}, {"name": "Type of consequence terms to report"}, {"name": "Adds affected exon and intron numbering"}, {"name": "Add cDNA, CDS and protein positions (position/length)"}, {"name": "VCF info field name"}, {"name": "Retrieve nearest transcript/gene"}, {"name": "Identify allele number from VCF input"}, {"name": "Do not write header lines to output files"}, {"name": "Report overlaps with regulatory regions [--regulatory]"}, {"name": "Cell type(s) to report regulatory regions for"}, {"name": "Chromosome synonyms", "encodingFormat": "text/plain"}, {"name": "Add APPRIS identifiers"}, {"name": "Output aligned RefSeq mRNA identifier"}, {"name": "Add HGVS identifiers"}, {"name": "Add genomic HGVS identifiers"}, {"name": "Add Ensembl protein identifiers"}, {"name": "Add gene symbols where available"}, {"name": "Add CCDS transcript identifers"}, {"name": "Add UniProt-associated database identifiers"}, {"name": "Add transcript support level"}, {"name": "Add a flag indicating if the transcript is canonical"}, {"name": "Add biotype of transcript or regulatory feature"}, {"name": "Enable or disable 3' shifting of HGVS notations"}, {"name": "Include failed collocated variants"}, {"name": "Check for co-located known variants"}, {"name": "Report Pubmed IDs for publications that cite an existing variant"}, {"name": "Add 1000 genomes phase 3 global allele frequency"}, {"name": "Add allele frequency from continental 1000 genomes populations"}, {"name": "Add allele frequency from NHLBI-ESP populations"}, {"name": "Add gnomAD allele frequencies (or ExAc frequencies with cache < 90)"}, {"name": "Exclude null alleles when checking co-located variants"}, {"name": "Do not check alleles of co-located variants"}, {"name": "Report highest allele frequency observed in 1000 genomes, ESP or gnomAD populations"}, {"name": "Compress output"}, {"name": "Output format"}, {"name": "Fields to configure the output format (VCF or tab only) with"}, {"name": "Convert alleles to minimal representation before assigning consequences"}, {"name": "Do not skip input variants that fail validation"}, {"name": "Keep non-variant lines (null ALT) in the VEP VCF output"}, {"name": "Check REF allele against provided reference sequence"}, {"name": "Limit analysis to GENCODE basic transcript set"}, {"name": "Include Ensembl identifiers when using RefSeq and merged caches"}, {"name": "Exclude predicted transcripts when using RefSeq or merged cache"}, {"name": "Filter transcripts according to arbitrary rules"}, {"name": "Select a subset of chromosomes to analyse"}, {"name": "Only return consequences that fall in the coding regions of transcripts"}, {"name": "Exclude intergenic consequences from the output"}, {"name": "Output only the most severe consequence per variant"}, {"name": "Output only a comma-separated list of all observed consequences per variant"}, {"name": "Exclude variants with a common (>1 % AF) co-located variant"}, {"name": "Pick one line or block of consequence data per variant, including transcript-specific columns"}, {"name": "Pick one line or block of consequence data per variant allele"}, {"name": "Output only the most severe consequence per gene"}, {"name": "Pick one line or block of consequence data per variant allele and gene combination"}, {"name": "Pick one line or block of consequence data per variant with PICK flag"}, {"name": "Pick one line or block of consequence data per variant allele, with PICK flag"}, {"name": "Pick one line or block of consequence data per variant allele and gene combination, with PICK flag"}, {"name": "Customise the order of criteria applied when picking a block of annotation data"}, {"name": "Use frequency filtering"}, {"name": "Population to use in the frequency filter"}, {"name": "Allele frequency to use for filtering"}, {"name": "Frequency cutoff operator"}, {"name": "Specify whether to exclude or include only variants that pass the frequency filter"}, {"name": "Input file format"}, {"name": "Raw input data string"}, {"name": "Summary stats file name"}, {"name": "Generate plain text stats file instead of HTML"}, {"name": "Use user-provided ref allele with bam"}, {"name": "Override input reference allele with overlapped transcript ref allele"}, {"name": "Use LoFtool plugin"}, {"name": "Use MaxEntScan plugin"}, {"name": "Use CSN plugin"}, {"name": "Output file name"}, {"name": "Overwrite the supplied reference allele"}, {"name": "Adds MANE identifiers"}, {"name": "Extend the maximum Structural Variant size"}, {"name": "Permit the use of unsorted input files"}, {"name": "Transcript overlap"}, {"name": "Right align all variants"}, {"name": "Right aligns all variants including intergenic variants"}, {"name": "Shifted distance"}, {"name": "Add reference allele in the output"}, {"name": "Add transcript version"}, {"name": "Report known synonyms for colocated variants"}, {"name": "Species"}], "output": [{"name": "VEP output file", "encodingFormat": "text/plain"}, {"name": "Compressed (bgzip/gzip) output"}, {"name": "Output summary stats file", "encodingFormat": "text/plain"}, {"name": "Optional file with VEP warnings and errors", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Ensembl/ensembl-vep"], "applicationSubCategory": ["Annotation", "VCF Processing"], "project": "SBG Public Data", "creator": "Ensembl", "softwareVersion": ["v1.0"], "dateModified": 1648039728, "dateCreated": 1617277730, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/gatk-2-3-9-lite-variants-to-table/1", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/gatk-2-3-9-lite-variants-to-table/1", "applicationCategory": "CommandLineTool", "name": "VariantsToTable", "description": "**Overview**\n\nThis tool is designed to extract fields from the VCF to a table format that is more convenient to work with in downstream analyses.\n\nThe user specifies one or more fields to print with the -F NAME, each of which appears as a single column in the output file, with a header named NAME, and the value of this field in the VCF one per line. NAME can be any standard VCF column (CHROM, ID, QUAL) or any binding in the INFO field (AC=10). In addition, there are specially supported values like EVENTLENGTH (length of the event), TRANSITION (for SNPs), HET (count of het genotypes), HOM-REF (count of homozygous reference genotypes), HOM-VAR (count of homozygous variant genotypes), NO-CALL (count of no-call genotypes), TYPE (the type of event), VAR (count of non-reference genotypes), NSAMPLES (number of samples), NCALLED (number of called samples), GQ (from the genotype field; works only for a file with a single sample), and MULTI-ALLELIC (is the record from a multi-allelic site).\n\n**Input**\n\nA VCF file\nA list of -F fields to write\n\n**Output**\n\n\nA tab-delimited file containing the values of the requested fields in the VCF file\n\nUsage example\n     java -jar GenomeAnalysisTK.jar \\\n     -R reference.fasta\n     -T VariantsToTable \\\n     -V file.vcf \\\n     -F CHROM -F POS -F ID -F QUAL -F AC \\\n     -o results.table\n \nwould produce a file that looks like:\n\n     CHROM    POS ID      QUAL    AC\n     1        10  .       50      1\n     1        20  rs10    99      10\n     et cetera...\n \n**Caveat**\n\nIf a VCF record is missing a value, then the tool by default throws an error, but the special value NA can be emitted instead if requested at the command line using --allowMissingData.\n[LINK](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_variantutils_VariantsToTable.php)", "input": [{"name": "VCF input file"}, {"name": "List of fields"}, {"name": "Reference Genome"}], "output": [{"name": "Table"}], "softwareRequirements": ["ExpressionEngineRequirement"], "project": "SBG Public Data", "creator": "Broad", "softwareVersion": ["sbg:draft-2"], "dateModified": 1545926073, "dateCreated": 1495813438, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-copycaller-v2-3-9/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-copycaller-v2-3-9/5", "applicationCategory": "CommandLineTool", "name": "VarScan2 CopyCaller", "description": "As of VarScan v2.2.7, the copyCaller command can be used to process VarScan2 copynumber output. This command allows you to:\n1) Filter copynumber calls by minimum coverage and/or region size.\n2) Adjust raw copynumber (log2) values for GC content.\n3) Classify each region as amplification (gain), deletion (loss), or neutral based on your preferred log2 thresholds.\n4) Recenter raw copynumber data if neutral segments are not on the log2=0 axis.", "input": [{"name": "Raw output from the VarScan copynumber command", "encodingFormat": "text/plain"}, {"name": "Minimum normal read depth"}, {"name": "Minimum tumor read depth at a position to make a non-homdel call"}, {"name": "Maximum depth in tumor"}, {"name": "Lower bound for log ratio to call amplification"}, {"name": "Upper bound for log ratio to call deletion"}, {"name": "Minimum size (in bases) for a region to be counted"}, {"name": "Recenter data around an adjusted baseline > 0"}, {"name": "Java Xmx%m requirement"}, {"name": "Recenter data around an adjusted baseline < 0"}], "output": [{"name": "File with the somatic CNA calls", "encodingFormat": "text/plain"}, {"name": "Output file for candidate homozygous deletions", "encodingFormat": "text/plain"}, {"name": "GC adjustment information", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799490, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-copynumber-v2-3-9/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-copynumber-v2-3-9/8", "applicationCategory": "CommandLineTool", "name": "VarScan2 CopyNumber", "description": "This command calls variants and identifies their somatic status (Germline/LOH/Somatic) using pileup files from a matched tumor-normal pair.\n\nIn copy-number mode, VarScan reads the pileup files from normal and tumor samples simultaneously. Only positions that are present in both files and meet the minimum coverage at least in one of them will be compared. VarScan expects that positions on the same chromosome occur in ascending order, so the input should be position-sorted. \n\nPlease note, this kind of simultaneous parsing gets more complex when there are numerous reference sequences (e.g. unplaced contigs) to which reads from only one sample aligned. VarScan tries to obtain the maximum number of comparisons, even if this means closing and reopening the normal file to try to match contig and position. This action can lead to looping errors. \n\nNote: The data ratio is intended to help you account for overall differences in the amount of sequencing coverage between normal and tumor samples, which might otherwise give the appearance of global copy-number differences. If the normal sample has more data than the tumor sample, set this to something greater than 1. If the tumor sample has more data than the normal sample, adjust it to something below 1. A basic formula for data ratio might be something like ratio = normal_unique_bp / tumor_unique_bp where unique base pairs are computed as mapped_non_dup_reads * read_length.\n\nFor more about somatic copy-number alterations, please consult this link: \nhttp://varscan.sourceforge.net/copy-number-calling.html", "input": [{"name": "The SAMtools pileup file for normal", "encodingFormat": "text/plain"}, {"name": "The SAMtools pileup file for tumor"}, {"name": "Minimum coverage threshold"}, {"name": "Minimum base quality to count for coverage"}, {"name": "Minimum read mapping quality"}, {"name": "Minimum number of consecutive bases to report a segment"}, {"name": "Max size before a new segment is made"}, {"name": "P-value threshold"}, {"name": "The normal/tumor input data ratio for copynumber adjustment"}, {"name": "Java Xmx%m requirement"}], "output": [{"name": "Copy number file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799644, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-filter-v2-3-9/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-filter-v2-3-9/6", "applicationCategory": "CommandLineTool", "name": "VarScan2 Filter", "description": "VarScan2 Filter tool filters variants in a file by coverage, supporting reads, variant frequency, or average base quality. It is for use with output from pileup2snp or pileup2indel.\nMore advanced filtering strategies consider mapping quality, read mismatches, soft-trimming, and other factors when deciding whether or not to filter a variant. For more information consult publication or documentation from links section.\n\nAnother mode for scattering is already applied in CWL implementation of this tool. This mode is used for varscan pipeline from BAM and is designed only to work with this pipeline.", "input": [{"name": "VCF file for somatic SNPs", "encodingFormat": "text/plain"}, {"name": "Minimum # of strands on which variant observed"}, {"name": "Minimum supporting reads"}, {"name": "File of INDELs for filtering nearby SNPs"}, {"name": "Default p-value threshold"}, {"name": "Minimum variant allele frequency threshold"}, {"name": "Minimum read depth"}, {"name": "Java Xmx%m requirement"}, {"name": "Minimum average base quality for variant-supporting reads"}], "output": [{"name": "Filtered VCF file", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799761, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-processsomatic-v2-3-9/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-processsomatic-v2-3-9/6", "applicationCategory": "CommandLineTool", "name": "VarScan2 ProcessSomatic", "description": "The VarScan2 ProcessSomatic tool allow user to segregate different types of variants within the VCF file produced by the VarScan2 Somatic tool. Variants are segregated into three groups: germline, LOH (loss of heterozygosity), and somatic. Each of these groups will have two files on the tool output: one contains all variants from this group; the second is the subset of these variants that is considered high confidence. The output format of the tool is VCF by default.\n\n**The script will not work if the input is a list of \"called SNPs\" in a Varscan format.**", "input": [{"name": "The VarScan output file for SNPs or INDELs", "encodingFormat": "text/plain"}, {"name": "Minimum variant allele frequency in tumor"}, {"name": "Maximum variant allele frequency in normal"}, {"name": "P-value for high-confidence calling"}], "output": [{"name": "Loss of heterozygosity variants", "encodingFormat": "text/plain"}, {"name": "High confidence loss of heterozygosity variants", "encodingFormat": "text/plain"}, {"name": "Germline variants", "encodingFormat": "text/plain"}, {"name": "High confidence germline variants", "encodingFormat": "text/plain"}, {"name": "Somatic variants", "encodingFormat": "text/plain"}, {"name": "High confidence somatic variants", "encodingFormat": "text/plain"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799505, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-somatic-v2-3-9/12", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-somatic-v2-3-9/12", "applicationCategory": "CommandLineTool", "name": "VarScan2 Somatic", "description": "VarScan calls somatic variants (SNPs and INDELs) using a heuristic method and a statistical test based on the number of aligned reads supporting each allele. \n\nThe VarScan2 Somatic command calls variants and identifies their somatic status (Germline/LOH/Somatic) using pileup files from a matched tumor-normal pair generated using SAMtools mpileup. \n\nMore specific options (e.g. min-coverage-normal) will override the default or specified value of less specific options (e.g. min-coverage). \n\nThe normal and tumor purity values should be a value between 0 and 1. The default (1) implies that the normal is 100% pure with no contaminating tumor cells, and the tumor is 100% pure with no contaminating stromal or other non-malignant cells. Please change the tumor-purity to something less than 1 if you have a low-purity tumor sample and thus expect lower variant allele frequencies for mutations. Please change the normal-purity to something less than 1 only if it is possible that there will be some tumor content in your \"normal\" sample, e.g. adjacent normal tissue for a solid tumor, malignant blood cells in the skin punch normal for some liquid tumors, etc. \n\nThere are two p-value options: the first, p-value, is the significance threshold for the first-pass algorithm that determines, for each position, if either normal or tumor is variant at that position; the second, somatic-p-value, is more important, as this is the threshold below which read count differences between tumor and normal are deemed significant enough to classify the sample as a somatic mutation or an LOH event.\n\nAnother mode for scattering is already applied in CWL implementation of this tool. This mode is used for varscan pipeline from BAM and is designed only to work with this pipeline.", "input": [{"name": "The SAMtools pileup file for normal", "encodingFormat": "text/plain"}, {"name": "The SAMtools pileup file for tumor", "encodingFormat": "text/plain"}, {"name": "Minimum coverage in normal and tumor to call variant"}, {"name": "Minimum coverage in normal to call somatic"}, {"name": "Minimum coverage in tumor to call somatic"}, {"name": "Minimum variant frequency to call a heterozygote"}, {"name": "Minimum frequency to call homozygote"}, {"name": "Estimated purity for normal"}, {"name": "Estimated purity for tumor"}, {"name": "P-value threshold to call a heterozygote"}, {"name": "P-value threshold to call a somatic site"}, {"name": "Strand-filter: If set to 1, removes variants with >90% strand bias"}, {"name": "Java Xmx%m requirement"}, {"name": "Validation:If set to 1, outputs all compared positions even if non-variant"}, {"name": "Intervals files names used in creating of pileup files"}], "output": [{"name": "Output file for SNP calls", "encodingFormat": "application/x-vcf"}, {"name": "Output file for INDEL calls", "encodingFormat": "application/x-vcf"}, {"name": "Validation output"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453798761, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-somaticfilter-v2-3-9/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-somaticfilter-v2-3-9/6", "applicationCategory": "CommandLineTool", "name": "VarScan2 SomaticFilter", "description": "VarScan2 SomaticFilter filters somatic mutation calls to remove clusters of false positives and SNV calls near INDELs. Note that this is a basic filter. More advanced filtering strategies consider mapping quality, read mismatches, soft-trimming, and other factors when deciding whether or not to filter a variant. For more information consult publication or documentation from links section.", "input": [{"name": "Mutations file output from VarScan Somatic", "encodingFormat": "text/plain"}, {"name": "Minimum # of strands on which variant observed"}, {"name": "Minimum supporting reads"}, {"name": "File of INDELs for filtering nearby SNPs", "encodingFormat": "text/plain"}, {"name": "Default p-value threshold"}, {"name": "Minimum variant allele frequency threshold"}, {"name": "Minimum read depth"}, {"name": "Java Xmx%m requirement"}], "output": [{"name": "Filtered VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649151537, "dateCreated": 1453799340, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-somaticfilter-v2-3-9-parallel/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-somaticfilter-v2-3-9-parallel/8", "applicationCategory": "CommandLineTool", "name": "VarScan2 SomaticFilter parallel ", "description": "VarScan2 FilterSomatic filters somatic mutation calls to remove clusters of false positives and SNV calls near INDELs. Note that this is a basic filter. More advanced filtering strategies consider mapping quality, read mismatches, soft-trimming, and other factors when deciding whether or not to filter a variant. For more information consult publication or documentation from links section.\n\nIMPORTANT: This tool will work only with the input list of SNP - INDEL VCF pairs per contig provided by the tool SBG Pair Contigs. Standard version of the tool that receives SNP and INDEL VCF files on separate ports is also available in the Public apps. Purpose of this tool is to run the VarScan2 SomaticFilter tool in parallel mode that will utilise instance resources much more effectively.", "input": [{"name": "SNP - INDEL mutations file pair output from VarScan Somatic per contig", "encodingFormat": "text/plain"}, {"name": "Minimum # of strands on which variant observed"}, {"name": "Minimum supporting reads"}, {"name": "Default p-value threshold"}, {"name": "Minimum variant allele frequency threshold"}, {"name": "Minimum read depth"}, {"name": "Java Xmx%m requirement"}], "output": [{"name": "Filtered VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649415456, "dateCreated": 1453799106, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/varscan2-somatic-v2-3-9-parallel/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/varscan2-somatic-v2-3-9-parallel/5", "applicationCategory": "CommandLineTool", "name": "VarScan2 Somatic parallel", "description": "VarScan calls somatic variants (SNPs and INDELs) using a heuristic method and a statistical test based on the number of aligned reads supporting each allele. \n\nThe VarScan2 Somatic command calls variants and identifies their somatic status (Germline/LOH/Somatic) using pileup files from a matched tumor-normal pair generated using SAMtools mpileup. \n\nMore specific options (e.g. min-coverage-normal) will override the default or specified value of less specific options (e.g. min-coverage). \n\nThe normal and tumor purity values should be a value between 0 and 1. The default (1) implies that the normal is 100% pure with no contaminating tumor cells, and the tumor is 100% pure with no contaminating stromal or other non-malignant cells. Please change the tumor-purity to something less than 1 if you have a low-purity tumor sample and thus expect lower variant allele frequencies for mutations. Please change the normal-purity to something less than 1 only if it is possible that there will be some tumor content in your \"normal\" sample, e.g. adjacent normal tissue for a solid tumor, malignant blood cells in the skin punch normal for some liquid tumors, etc. \n\nThere are two p-value options: the first, p-value, is the significance threshold for the first-pass algorithm that determines, for each position, if either normal or tumor is variant at that position; the second, somatic-p-value, is more important, as this is the threshold below which read count differences between tumor and normal are deemed significant enough to classify the sample as a somatic mutation or an LOH event.\n\nIMPORTANT: This tool will work only with the input list of tumor-normal pileup pairs per contig provided by the tool SBG Pair Contigs. Standard version of the tool that receives tumor and normal pileup files on separate ports is also available in the Public apps. Purpose of this tool is to run the VarScan2 in parallel mode that will utilise instance resources much more effectively.", "input": [{"name": "Tumor-Normal pair of pileups", "encodingFormat": "text/plain"}, {"name": "Minimum coverage in normal and tumor to call variant"}, {"name": "Minimum coverage in normal to call somatic"}, {"name": "Minimum coverage in tumor to call somatic"}, {"name": "Minimum variant frequency to call a heterozygote"}, {"name": "Minimum frequency to call homozygote"}, {"name": "Estimated purity for normal"}, {"name": "Estimated purity for tumor"}, {"name": "P-value threshold to call a heterozygote"}, {"name": "P-value threshold to call a somatic site"}, {"name": "If set to 1, removes variants with >90% strand bias"}, {"name": "Java Xmx%m requirement"}], "output": [{"name": "Output file for SNP calls", "encodingFormat": "application/x-vcf"}, {"name": "Output file for INDEL calls", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": [], "applicationSubCategory": ["Variant Calling"], "project": "SBG Public Data", "creator": "Genome Institute at Washington University", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649415456, "dateCreated": 1453799478, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcf2db-89f3a30d-cwl1-1/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcf2db-89f3a30d-cwl1-1/7", "applicationCategory": "CommandLineTool", "name": "Vcf2db", "description": "**Vcf2db** converts an annotated VCF file into a Gemini-compatible database [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n\n### Common Use Cases\n\n**Vcf2db** can be used to prepare a Gemini-compatible database from a VCF file annotated with **VEP**, **SnpEff**, **Vcfanno** or **BCFtools annotate**. A pedigree file must be provided (**Pedigree file**).\n\n### Changes Introduced by Seven Bridges\n\n* Input **Output file name prefix** was added to specify the prefix for the output Gemini database file name.\n* Creation of a DB file is the only supported tool execution mode. Direct loading into mysql or postgres is not supported by this wrapper.\n\n### Common Issues and Important Notes\n\n* Inputs **Input variants** and **Pedigree file** are required.\n* Input **Number of CPUs per job** can be used to request additional CPUs when executing a task, but the tool is single-threaded and will not be parallelized with this parameter.\n\n### Performance Benchmarking\n\n**Vcf2db** runs on a single core. The performance of the tool depends on the size of the input data to convert (number of variants, samples and annotation fields).\n\n**Datasets**:\n* Dataset 1: 3619471 variants (WGS NA12878)\n* Dataset 2: 3619471 variants (WGS NA12878) - VEP-annotated (--everything)\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1 | 19 min | $0.13 + $0.05 | c4.2xlarge - 1000 GB EBS | \n| Dataset 2 | 102 min | $0.68 + $0.24 | c4.2xlarge - 1000 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**Vcf2db** was tested with cwltool version 3.1.20211107152837. The `in_variants` and `in_pedigree` inputs were provided in the job.yaml/job.json file and used for testing. \n\n### References\n\n[1] [Vcf2db documentation](https://github.com/quinlan-lab/vcf2db)", "input": [{"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Pedigree file", "encodingFormat": "text/plain"}, {"name": "Output file name prefix"}, {"name": "INFO fields with Number=A to include"}, {"name": "INFO fields to exclude"}, {"name": "Fields to propagate to variant impacts table"}, {"name": "Legacy compression"}, {"name": "Sample columns to expand into tables"}], "output": [{"name": "Gemini database"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/quinlan-lab/vcf2db", "https://github.com/quinlan-lab/vcf2db/blob/master/vcf2db.py", "https://github.com/quinlan-lab/vcf2db/archive/89f3a30d14550763488fdd1eec121f5b2dbf4f7e.zip"], "applicationSubCategory": ["Utilities", "File Format Conversion", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Quinlan Lab", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612352526, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcf2maf-1-6-21-vep-104-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcf2maf-1-6-21-vep-104-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "Vcf2maf", "description": "**Vcf2maf** converts VCF files to MAF files [1]. To obtain a MAF file from a VCF, each variant must be mapped to exactly one gene transcript/isoform that it might affect  [1] and be associated with exactly one effect. **Vcf2maf** invokes the **Variant Effect Predictor** to choose the transcript and effect associated with each variant in the output MAF file.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Vcf2maf** can be used to convert an uncompressed VCF file to a corresponding MAF file. A reference FASTA file (**Reference FASTA file**) corresponding to the input VCF must be supplied to the tool. Unless the annotation step is skipped (**Inhibit VEP** input parameter) a VEP cache file (**VEP cache**) is also required. The version of the VEP cache file should be specified using the **Version of VEP cache file** input parameter (104 by default).\n\n### Changes Introduced by Seven Bridges\n\n* This wrapper includes **Variant Effect Predictor** v104.\n* Parameters `--vep-path`, `--vep-data` and `--tmp-dir` have been hardcoded in the tool wrapper.\n* Parameters `--vep-overwrite`, `--help`, `--man` and `--online` have been omitted from the wrapper.\n* Parameter `--vep-forks` is set to match the number of CPUs requested for the task (8 by default) and has not been exposed separately.\n* Support for the MaxEntScan VEP plugin annotations (**Use MaxEntScan plugin**) has been added.\n\n### Common Issues and Important Notes\n\n* Inputs **VCF file to convert to MAF** and **Reference FASTA file** are required. Only uncompressed VCF files can be processed.\n* **IMPORTANT** In order to successfully process GRCh38 data, **NCBI reference assembly of variants** should be set to `GRCh38`.\n* **VEP cache** input file is required to annotate variants with the **Variant Effect Predictor**. If the annotation step is skipped (**Inhibit VEP**), the input VCF file should already contain the annotations of interest.\n* If using a VEP cache not corresponding to Ensembl release 104, the **Version of VEP cache** input parameter must be set to the correct cache version.\n* Input parameter **Set VEP --failed flag to 0** changes the VEP flag `--failed`, which may troubleshoot some task failures.\n* The default **VEP buffer size** (`--buffer-size`) is 5000. It may be necessary to reduce this value if the `--regulatory` VEP parameter is used on the command line.\n\n### Performance Benchmarking\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| WES Mutect2 (2 MB) | 11 mins | $0.07 + $0.03 | c4.2xlarge - 1024 GB EBS | \n| WGS Mutect2 (64 MB) | 16 mins | $0.11 + $0.04 | c4.2xlarge - 1024 GB EBS |\n| WGS Mutect2 (64 MB) 15 CPUs | 9 mins | $0.10 + $0.02 | c5.4xlarge - 1024 GB EBS | \n| WES Strelka2 (435 MB) 15 CPUs | 40 mins | $0.45 + $0.09 | c5.4xlarge - 1024 GB EBS | \n| WES Strelka2 (435 MB) 30 CPUs | 35 mins | $0.89 + $0.08 | c5.9xlarge - 1024 GB EBS |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### Portability\n\n**Vcf2maf** was tested with cwltool version 3.1.20211107152837. The `in_variants`, `in_reference`, `vep_cache`, `normal_id`, `tumor_id`, `ncbi_build` and `cache_version` inputs were provided in the job.yaml/job.json file and used for testing. Cwltool command line parameters `--no-read-only` and `--no-match-user` were required for successful tool execution.\n\n### References\n\n[1] [vcf2maf github page](https://github.com/mskcc/vcf2maf)", "input": [{"name": "Any allele"}, {"name": "Version of VEP cache file"}, {"name": "File with custom ENST IDs to use", "encodingFormat": "text/plain"}, {"name": "Set VEP --failed flag to 0"}, {"name": "VCF file to convert to MAF", "encodingFormat": "application/x-vcf"}, {"name": "Variant calling center to report in MAF"}, {"name": "Use MaxEntScan plugin"}, {"name": "Memory per job [MB]"}, {"name": "Minimum AF for homozygous variants"}, {"name": "NCBI reference assembly of variants"}, {"name": "Matched normal sample ID to report"}, {"name": "CPUs per job"}, {"name": "Output MAF file name"}, {"name": "Reference FASTA file", "encodingFormat": "application/x-fasta"}, {"name": "Chain file to remap variants", "encodingFormat": "text/plain"}, {"name": "INFO fields to retain as extra columns in MAF"}, {"name": "Ensembl-friendly species name"}, {"name": "Tumor sample ID to report in the MAF"}, {"name": "Matched normal ID in the VCF"}, {"name": "Tumor sample ID in the VCF"}, {"name": "VEP cache", "encodingFormat": "application/x-tar"}, {"name": "FORMAT fields to retain in the MAF file"}, {"name": "VEP buffer size"}, {"name": "Do not use --regulatory VEP flag"}, {"name": "VEP --custom option files", "encodingFormat": "application/x-gtf"}, {"name": "VEP --custom string"}, {"name": "VEP config file", "encodingFormat": "text/plain"}, {"name": "Inhibit VEP"}, {"name": "Maximum sub-population AF"}, {"name": "Retain annotations"}, {"name": "Verbose"}], "output": [{"name": "Output MAF file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/mskcc/vcf2maf", "https://github.com/mskcc/vcf2maf/blob/main/vcf2maf.pl", "https://github.com/mskcc/vcf2maf/releases/tag/v1.6.21", "https://github.com/mskcc/vcf2maf/blob/main/docs/vep_maf_readme.txt"], "applicationSubCategory": ["VCF Processing", "File Format Conversion", "Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Cyriac Kandoth", "softwareVersion": ["v1.2"], "dateModified": 1648217713, "dateCreated": 1648217713, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcfanno-0-3-2-cwl1-1/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcfanno-0-3-2-cwl1-1/6", "applicationCategory": "CommandLineTool", "name": "Vcfanno", "description": "**Vcfanno** can be used to annotate variants with information from VCF, BED or BAM sources [1,2].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Vcfanno** can be used to annotate variants provided in VCF format as **Input variants** with information from VCF, BED or BAM sources (**Annotation files**). The annotation rules should be specified in the **Config file**. During task executions, the annotation sources will be placed in the tool working directory (no paths should be used in the config file).\n\n### Changes Introduced by Seven Bridges\n\n* Input parameter `-base-path` was omitted from the wrapper as it does not apply to task executions on Seven Bridges platforms.\n* Input parameter **Compress output** was added to allow outputting BGZIP-compressed VCF outputs.\n\n### Common Issues and Important Notes\n\n* Inputs **Input variants**, **Annotation sources** and **Config file** are required.\n* The tool performance can be adjusted with **GO GC value**, **Maximum number of query intervals** and **Maximum gap size** input parameters.\n* For dense annotation sources/files, a CSI index may improve performance, if available. \n* Please note that the tool is currently configured to stage its input annotation sources (**Input annotation files**) in the current working directory of the task. As a result, heavy scattering of the tool is not recommended and the input stage of task executions may be prolonged for large annotation sources.\n\n### Performance Benchmarking\n\n**Vcfanno** performance depends on the selected inputs, annotation sources and parameters.\n\nAnnotations used for testing:\n* ExAC (r0.3) - \"AC_AFR\", \"AC_AMR\", \"AC_EAS\", \"ID\"\n* dbSNP (150) - \"ID\"\n* gnomAD exomes r2.0.1 - \"AC_NFE\", \"AC_SAS\", \"AC_Female\"\n* BAM annotations - NA12878 GRCh38 BAM 72.3 GB: \"mapq\", \"coverage\" \n* Input parameter **Compress output** was set to True for all tasks\n\n**Datasets**:\n* Dataset 1: 3619471 variants (WGS NA12878)\n* Dataset 2: 4783247 variants (WGS Simons HGDP01078)\n* Dataset 3: 325174796 variants (dbSNP v150)\n\n\n| Experiment type  | Duration | Cost | Instance (AWS on-demand)|\n|---------------------------|------------------------|-----------------------|--------------------------------|\n| Dataset 1 - 8 cores | 13 min (7 min) | $0.09 + $0.03 | c4.2xlarge - 1000 GB EBS | \n| Dataset 1 - 15 cores | 6 min (3 min) | $0.07 + $0.02 | c5.4xlarge - 1000 GB EBS |\n| Dataset 1 - 8 cores BAM | 71 min (46 min) | $0.48 + $0.17 | c4.2xlarge - 1000 GB EBS |\n| Dataset 2 - 8 cores | 16 min (7 min) | $0.11 + $0.04 | c4.2xlarge - 1000 GB EBS | \n| Dataset 3 - 15 cores | 31 min (28 min) | $0.35 + $0.08 |  c5.4xlarge - 1000 GB EBS |\n\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.* \n\n### Portability\n\n**Vcfanno** was tested with cwltool version 3.1.20211107152837. The `in_config`, `in_variants`, `in_annotation_files` and `in_lua_files` inputs were provided in the job.yaml/job.json file and used for testing.\n\n### References\n\n[1] [Vcfanno documentation](https://github.com/brentp/vcfanno/blob/master/docs/index.md)\n\n[2] [Vcfanno publication](https://pubmed.ncbi.nlm.nih.gov/27250555/)", "input": [{"name": "Config file"}, {"name": "Input variants", "encodingFormat": "application/x-vcf"}, {"name": "Input annotation files", "encodingFormat": "text/x-bed"}, {"name": "Output file name prefix"}, {"name": "Annotate the start and end of intervals"}, {"name": "Lua file"}, {"name": "Number of processes to use"}, {"name": "Number of CPUs per job"}, {"name": "Memory per job [MB]"}, {"name": "Annotate with overlapping variant position"}, {"name": "Compress output"}, {"name": "GO GC value"}, {"name": "Maximum number of query intervals"}, {"name": "Maximum gap size"}], "output": [{"name": "Annotated variants", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/brentp/vcfanno", "https://github.com/brentp/vcfanno/blob/master/vcfanno.go", "https://github.com/brentp/vcfanno/releases/tag/v0.3.2", "https://github.com/brentp/vcfanno/blob/master/docs/index.md"], "applicationSubCategory": ["VCF Processing", "Annotation", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Brent Pedersen", "softwareVersion": ["v1.1"], "dateModified": 1648045481, "dateCreated": 1612352526, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-concat-0-1-14/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-concat-0-1-14/10", "applicationCategory": "CommandLineTool", "name": "VCFtools Concat", "description": "VCFtools concat concatenates VCF files (for example, splitting by chromosome). Note that the input and output VCFs will have the same number of columns. The script does not merge VCFs by position (see also vcf-merge).\n\nIn the default mode, this tool only checks that all files have the same columns. When run with the -s option, it will perform a partial merge sort and look at a limited number of open files simultaneously.", "input": [{"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "List of files", "encodingFormat": "text/plain"}, {"name": "Pad missing"}, {"name": "Small overlap"}, {"name": "Check columns"}, {"name": "Compressed output"}], "output": [{"name": "Concatenated vcf file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools", "https://github.com/vcftools/vcftools/zipball/master"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453798898, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-convert/5", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-convert/5", "applicationCategory": "CommandLineTool", "name": "VCFTools Convert", "description": "VCFTools convert is used to convert a VCF file to another version. In general, the most useful use-case is when one tool can only process VCF-s up to a specific version, and this tool can then be used to downgrade the input VCF so it can be processed properly. It can also be used to upgrade a VCF to a higher version.\n\nThe input can be VCF, gzip compressed VCF or bgzip compressed VCF [VCF].\nThe output can be chosen to be either VCF or bgzip compressed and indexed VCF [VCF.GZ]. \nAnother required parameter is the reference fasta file, which is important when the VCF contains indel sites.", "input": [{"name": "Input VCF file for conversion.", "encodingFormat": "application/x-vcf"}, {"name": "Type of compression used for output file"}, {"name": "Version of output VCF"}, {"name": "Reference fasta", "encodingFormat": "application/x-fasta"}], "output": [{"name": "Output reversioned VCF file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1489512565, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-hardy-0-1-14/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-hardy-0-1-14/4", "applicationCategory": "CommandLineTool", "name": "VCFtools Hardy", "description": "VCFtools hardy reports a p-value for each site from a Hardy-Weinberg Equilibrium test, as defined by Wigginton, Cutler and Abecasis (2005). The resulting file (with suffix \".hwe\") also contains the observed numbers of homozygotes and heterozygotes and the corresponding expected numbers under HWE.", "input": [{"name": "Chromosomes to analyze"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Chromosomes to ommit"}], "output": [{"name": "Output file"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649687422, "dateCreated": 1453799219, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-hwe-0-1-14/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-hwe-0-1-14/6", "applicationCategory": "CommandLineTool", "name": "VCFtools Hwe", "description": "VCFtools hwe assesses sites for Hardy-Weinberg Equilibrium using an exact test, as defined by Wigginton, Cutler and Abecasis (2005). Sites with a p-value below the threshold defined by this option are taken to be out of HWE and therefore excluded.", "input": [{"name": "Chromosomes to omit"}, {"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Chromosomes to analyze"}, {"name": "Threshold p-value for HW equilibrium"}], "output": [{"name": "Analyzed output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453799828, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-isec-0-1-14/16", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-isec-0-1-14/16", "applicationCategory": "CommandLineTool", "name": "VCFtools Isec", "description": "VCFtools isec creates intersections and complements of two or more VCF files. Given multiple VCF files, it can output the list of positions which are shared by at least N files, at most N files, exactly N files, etc. The first example below outputs positions shared by at least two files and the second outputs positions present in file A but absent from files B and C.", "input": [{"name": "nfiles"}, {"name": "Force"}, {"name": "Debugging info"}, {"name": "Apply filters"}, {"name": "Complement"}, {"name": "One file only"}, {"name": "Prefix"}, {"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "Regions file", "encodingFormat": "text/plain"}, {"name": "Tab delimited file", "encodingFormat": "text/plain"}, {"name": "win"}, {"name": "Compressed output"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453799753, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-merge-0-1-14/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-merge-0-1-14/15", "applicationCategory": "CommandLineTool", "name": "VCFtools Merge", "description": "VCFtools merge merges two or more VCF files into one so that if two source files had one column each, the output will print a single file with two columns. \nNote that this script is not intended for concatenating VCF files. For this, use vcf-concat instead.", "input": [{"name": "Regions", "encodingFormat": "text/plain"}, {"name": "Collapse"}, {"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "Remove duplicates"}, {"name": "vcf header file", "encodingFormat": "application/x-vcf"}, {"name": "Silent"}, {"name": "Trim"}, {"name": "Ref for missing"}, {"name": "Compressed output"}], "output": [{"name": "Output file merged vcfs", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools/zipball/master", "https://github.com/vcftools/vcftools/blob/master/src/perl/vcf-merge"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453798776, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-sort-0-1-14/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-sort-0-1-14/9", "applicationCategory": "CommandLineTool", "name": "VCFtools Sort", "description": "VCFtools sort sorts a VCF file.", "input": [{"name": "Input file", "encodingFormat": "application/x-vcf"}, {"name": "Chromosomal order"}, {"name": "Parallel threads"}, {"name": "Memory in MB"}, {"name": "Compressed output"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453798863, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-subset-0-1-14/17", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-subset-0-1-14/17", "applicationCategory": "CommandLineTool", "name": "VCFtools Subset", "description": "VCFtools subset removes specified columns from the VCF file.", "input": [{"name": "Private"}, {"name": "Force execution"}, {"name": "Exclude rows not containing variants"}, {"name": "Trim alternate alleles"}, {"name": "Input files", "encodingFormat": "application/x-vcf"}, {"name": "Uncalled"}, {"name": "Criteria"}, {"name": "Compressed output"}], "output": [{"name": "Output file", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools", "https://github.com/vcftools/vcftools/zipball/master"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241601, "dateCreated": 1453798764, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-venn2-0-1-14/18", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-venn2-0-1-14/18", "applicationCategory": "CommandLineTool", "name": "VCFtools Venn2", "description": "VCFtools venn2 reports the intersection of two VCF files.\nVCF files should be gzipped and contain a header.", "input": [{"name": "Input file 1", "encodingFormat": "application/x-vcf"}, {"name": "Input file 2", "encodingFormat": "application/x-vcf"}, {"name": "Decompressed output"}], "output": [{"name": "Output stats", "encodingFormat": "text/plain"}, {"name": "Output intersect", "encodingFormat": "application/x-vcf"}, {"name": "Output unique 2", "encodingFormat": "application/x-vcf"}, {"name": "Output unique 1", "encodingFormat": "application/x-vcf"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453799517, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/vcftools-venn3-0-1-14/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/vcftools-venn3-0-1-14/7", "applicationCategory": "CommandLineTool", "name": "VCFtools Venn3", "description": "VCFtools venn3 reports the intersection of three VCF files.\nVCF files should be gzipped and contain a header.", "input": [{"name": "Input file 2", "encodingFormat": "application/x-vcf"}, {"name": "Input file 1", "encodingFormat": "application/x-vcf"}, {"name": "Input file 3", "encodingFormat": "application/x-vcf"}], "output": [{"name": "Intersection 1 and 2", "encodingFormat": "application/x-vcf"}, {"name": "Unique 2", "encodingFormat": "application/x-vcf"}, {"name": "Unique 1", "encodingFormat": "application/x-vcf"}, {"name": "Unique 3", "encodingFormat": "application/x-vcf"}, {"name": "Intersection 1 and 3", "encodingFormat": "application/x-vcf"}, {"name": "Intersection 2 and 3", "encodingFormat": "application/x-vcf"}, {"name": "Intersection 1,2 and 3", "encodingFormat": "application/x-vcf"}, {"name": "Statistics report", "encodingFormat": "text/plain"}], "softwareRequirements": ["CreateFileRequirement"], "codeRepository": ["https://github.com/vcftools/vcftools"], "applicationSubCategory": ["VCF Processing"], "project": "SBG Public Data", "creator": "Adam Auton, Petr Danecek, Anthony Marcketta", "softwareVersion": ["sbg:draft-2"], "dateModified": 1649241600, "dateCreated": 1453799852, "contributor": ["sevenbridges", "admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/velocyto-py-0-17-17/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/velocyto-py-0-17-17/10", "applicationCategory": "CommandLineTool", "name": "Velocyto.py", "description": "Velocyto  is a package for the analysis of expression dynamics in single cell RNAseq data. In particular, it enables estimations of RNA velocities of single cells by distinguishing unspliced and spliced mRNAs in standard single-cell RNA sequencing protocols [1].  **Velocyto.py** is a command line tool (distributed with the package) that is used to generate spliced/unspliced count matrices.\n\n_A list of  **all inputs and parameters**  with corresponding descriptions can be found at the bottom of the page._\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n### Common Use Cases\n\n**Velocyto.py** is used to generate spliced/unspliced count matrices from BAM files generated using single cell alignment & quantification tools.\n\nInstructions for running with popular single cell alignment & quantification tools are given bellow:\n\n1. **Cell ranger count** - 10x protocol (cell ranger tools are not publicly available on the Seven Bridges platform):\n   - Provide BAM file produced by Cell ranger count. Only a single BAM file generated by Cell ranger count can be processed in a single **Velocyto.py** run.\n   - Set **logic** to default.\n   - Provide tsv.gz filtered barcode file generated by Cell ranger count.\n   - Provide transcriptome gtf file. One way of doing this is by providing a \"genes.gtf\" file that can be obtained by decompressing Cell ranger count transcriptome reference by using tools such as SBG Decompressor.\n2. **Star solo** - multiple protocols:\n   - In order to run velocyto.py successfully with STAR Solo output BAM file, **Output sorting type** in STAR Solo should be set to \"SortedByCoordinate\", **SAM attributes** input should be set to \"NH HI CR UR CB UB GX GN\" and **Output format** should be set to \"BAM\". Only a single BAM file generated by STARsolo can be processed in a single **Velocyto.py** run.\n\t   -  Star solo can be executed on Seven Bridges platform  by using **STAR** CWL tool.\n   - Set **logic** to default.\n   - Provide tsv.gz filtered barcode file generated by STAR Solo.\n   - Provide transcriptome GTF file. Version must match the version of reference used by STAR Solo.\n3. **Smart-seq 2**  - smart-seq 2 protocol:\n   - Provide BAM files from  the **HISAT2 RSEM output bam** output of**Single-Cell Smart-seq2 Workflow v3.0.0**.  In order to generate a BAM file when running **Smart-Seq 2** on the Seven Bridges platform, set **HISAT2 RSEM - Save BAM/SAM files** flag to True.\n   - Provide transcriptome GTF file. Version must match the version of reference used by **Single-Cell Smart-seq2 Workflow v3.0.0**.\n   - Set **One file per cell** to True.\n   - Set **Without umi** to True.\n   - Set **logic** to \"SmartSeq2\".\n4. For running **Velocyto.py** on other formats and protocols, documentation is available on  the [velocyto webpage](http://velocyto.org/velocyto.py/tutorial/cli.html#run-run-on-any-technique-advanced-use).\n\n### Changes Introduced by Seven Bridges\n\nNo changes to the original tool were introduced by Seven Bridges.\n\n### Common Issues and Important Notes\n\n **Velocyto.py** implementation on the Seven Bridges platform contains only [\"Run on any technique\"](http://velocyto.org/velocyto.py/tutorial/cli.html#run-run-on-any-technique-advanced-use) mode of running. Running modes such as run10x, run-dropest or run-smartseq2 are not part of this wrapper. In order to run **velocyto.py** on files produced by a certain protocol/aligner all parameters need to be set according to instructions given above or instructions given on the [velocyto web page](http://velocyto.org/velocyto.py/tutorial/cli.html#run-run-on-any-technique-advanced-use) [2].\n\n### Performance benchmarking\n\n| # of cells | Quantifier used  | Bam file size  | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:----------:|:------------:|\n|       1.2k     |     Cell Ranger   |   3.9 GB  |   1 h 4 min   |   $0.35    |    m5.xlarge    |\n|       4.6k     |     Cell Ranger  |   27.3 GB   |   6 h 20 min   |   $2.09    |    m5.xlarge    |\n|       8.7k     |     Cell Ranger    |   60.3 GB   |   12 h 9 min   |   $4.06    |    m5.xlarge    |\n|       11.7k     |    Cell Ranger    |   36.4 GB   |   9h 49 min   |   $3.24    |    m5.xlarge    |\n|       12.8k     |    Cell Ranger    |   53.9 GB   |   8h 24min   |   $2.77    |    m5.xlarge    |\n|       546     |    Single-Cell Smart-seq2 Workflow v3.0.0    |   Multiple bams   |   6 h 12 min   |   $2.05    |    m5.xlarge    |\n|       200     |    Single-Cell Smart-seq2 Workflow v3.0.0    |   Multiple bams   |   2 h 28 min   |   $0.81    |    m5.xlarge    |\n\n_Cost can be significantly reduced by using  **spot instances**. Visit the  [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances)  for more details._\n\n### Portability\n\n**Velocyto.py**  was tested with cwltool 3.1.20220124184855.\n\n### References\n\n[1] [Velocyto publication](https://www.nature.com/articles/s41586-018-0414-6)\n\n[2] [Velocyto tutorial](http://velocyto.org/velocyto.py/tutorial/index.html#running-the-cli)", "input": [{"name": "Barcode file"}, {"name": "Mask gtf"}, {"name": "Genome annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Bam files", "encodingFormat": "application/x-bam"}, {"name": "Samtool threads"}, {"name": "Samtools memory"}, {"name": "Logic"}, {"name": "Without umi"}, {"name": "One file per cell"}, {"name": "Verbosity"}], "output": [{"name": "Loom file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InlineJavascriptRequirement"], "codeRepository": [], "applicationSubCategory": ["Single Cell", "Quantification"], "project": "SBG Public Data", "softwareVersion": ["v1.2"], "dateModified": 1647251862, "dateCreated": 1646679879, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/verifybamid2-2-0-1-cwl1-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/verifybamid2-2-0-1-cwl1-2/3", "applicationCategory": "CommandLineTool", "name": "VerifyBamID2", "description": "**VerifyBamID2** estimates contamination of DNA samples from read data, accounting for ancestry information [1].\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n_**Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.**_\n\n\n### Common Use Cases\n\n**VerifyBamID2** can be used to assess DNA contamination of a sample in sequencing studies, starting from alignments (BAM, CRAM for a pileup file) [1,2]. \nHGDP and 1000 genomes population resources are provided with the tool (**SVD preset**). Customized resources can be prepared with the tool starting from a **Reference panel VCF** file and later used via the **SVD files** and **SVD prefix** inputs. \n\n### Changes Introduced by Seven Bridges\n\n* The following input parameters were excluded from this wrapper as they were deprecated by the tool authors: `--UDPath`, `--MeanPath` and `--BedPath`.\n\n* Resource files distributed alongside the tool are available via the **SVD preset** input parameter. To use a custom file set please specify the files and their associated name prefix using the **SVD files** and  **SVD prefix** inputs.\n\n### Common Issues and Important Notes\n\n* Either **Input alignments** or **Input pileup file** input must be provided. **Input alignments** should be accompanied by an appropriate index file (BAI and CRAI, for BAM and CRAM files, respectively).\n* **Reference sequence** input is required. \n* If a preset of resource files (**SVD preset** input parameter) is not set, the users should provide their own resource files (**SVD files** input parameter, with the corresponding **SVD prefix**). The customized resource files can also be prepared using this tool, if a reference panel VCF file is available (**Reference panel VCF**).\n* Please note that, unlike the contigs in the WGS resources, chromosomes in the WES b38 files distributed with the tool (and available in this wrapper by specifying `EXOME_GRCh38_1000g_10k` as the value for the **SVD preset** input parameter) do not have the `chr` prefix.\n* The **Reference panel VCF** must not contain duplicate records.\n* The tool will fail if it detects insufficient overlap between the analyzed sample and the chosen reference markers. It is possible to force the tool to complete by using **Disable sanity check** input, however, this should be avoided if possible. One possible source of this issue is a mismatch in the reference genome assemblies between sample and reference data or a different representation of contigs (e.g., with and without the `chr` prefix).\n\n### Performance Benchmarking\n\n**VerifyBamID2** performance depends on the number of markers in the reference data. All tests were done with GRCh38 data and using the default number of threads (4), unless otherwise noted.\n\n| Experiment type  | Duration | Cost | Instance (AWS)|\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n| WES BAM file (16.2 GB) exome_1000g_10k | 8 min | $0.05 + $0.02 | c4.2xlarge - 1024 GB EBS | \n| 30x WGS BAM file (48.4 GB) 1000g_10k | 11 min | $0.07 + $0.03 | c4.2xlarge - 1024 GB EBS | \n| 30x WGS BAM file (48.4 GB) HGDP_10k | 11 min | $0.07 + $0.03 | c4.2xlarge - 1024 GB EBS | \n| 30x WGS BAM file (48.4 GB) 1000g_100k | 25 min | $0.17 + $0.06 | c4.2xlarge - 1024 GB EBS | \n| 30x WGS CRAM file (15.1 GB) HGDP_10k | 6 min | $0.04 + $0.02 | c4.2xlarge - 1024 GB EBS | \n| 30x WGS CRAM file (15.1 GB) HGDP_100k | 28 min | $0.19 + $0.07 | c4.2xlarge - 1024 GB EBS | \n| 30x WGS CRAM file (15.1 GB) HGDP_100k 15 CPUs | 18 min | $0.20 + $0.04 | c5.4xlarge - 1024 GB EBS | \n| 50x WGS BAM file (72.3 GB) 1000g_10k | 15 min | $0.10 + $0.04 | c4.2xlarge - 1024 GB EBS | \n| 50x WGS BAM file (72.3 GB) 1000g_100k | 32 min | $0.21 + $0.08 | c4.2xlarge - 1024 GB EBS | \n\n*Cost can be significantly reduced by **spot instance** usage. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*  \n\n### Portability\n\n**VerifyBamID2** was tested with cwltool version 3.1.20210628163208. The `in_alignments`, `in_reference`, `disable_sanity_check` and `svd_preset` inputs were provided in the job.yaml/job.json file and used for testing. Please note that, for local executions only, if using a compressed reference sequence (`in_reference`) the GZI index may have to be supplied in the inputs directory to avoid issues with writing permissions in the Docker staging directory during GZI index creation by the tool.\n\n\n### References\n\n[1] [VerifyBamID2 publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7050530/)\n\n[2] [VerifyBamID2 GitHub](https://github.com/Griffan/VerifyBamID)", "input": [{"name": "Input alignments", "encodingFormat": "application/x-bam"}, {"name": "Input pileup file"}, {"name": "Reference sequence", "encodingFormat": "application/x-fasta"}, {"name": "SVD prefix"}, {"name": "Output file name prefix"}, {"name": "Within ancestry"}, {"name": "Disable sanity check"}, {"name": "Number of principal components"}, {"name": "Input fixed PCs"}, {"name": "Fixed alpha"}, {"name": "Known AF file", "encodingFormat": "text/x-bed"}, {"name": "Number of threads for likelihood calculation"}, {"name": "SVD files", "encodingFormat": "text/x-bed"}, {"name": "Random number seed"}, {"name": "Minimization procedure convergence threshold"}, {"name": "Output temporary pileup file"}, {"name": "Verbose"}, {"name": "Reference panel VCF", "encodingFormat": "application/x-vcf"}, {"name": "SVD preset"}, {"name": "Memory per job [MB]"}, {"name": "CPUs per job"}, {"name": "Minimum BQ"}, {"name": "Minimum mapping quality"}, {"name": "Adjust mapping quality"}, {"name": "Maximum per-file depth"}, {"name": "Do not use anomalous read pairs"}, {"name": "Required read flags"}, {"name": "Filter read flags"}], "output": [{"name": "VerifyBamID selfSM file"}, {"name": "VerifyBamID2 ancestry file"}, {"name": "Customized resource set", "encodingFormat": "text/x-bed"}, {"name": "Pileup output file"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/Griffan/VerifyBamID", "https://github.com/Griffan/VerifyBamID/tree/a6328f6c7ecd22abfbdb318fc3b9058a983f33df", "https://github.com/Griffan/VerifyBamID/releases/tag/2.0.1", "https://github.com/Griffan/VerifyBamID/blob/master/README.md"], "applicationSubCategory": ["Quality Control", "CWLtool Tested"], "project": "SBG Public Data", "creator": "Hyun Min Kang", "softwareVersion": ["v1.2"], "dateModified": 1648040304, "dateCreated": 1636032414, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/xcell-analysis-1-3/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/xcell-analysis-1-3/6", "applicationCategory": "CommandLineTool", "name": "xCell Analysis", "description": "**xCell Analysis** is a tool for cell type enrichment analysis. It takes gene expression data and performs analysis for 64 immune and stromal cell types [1].\n\n**xCell Analysis** applies a gene signatures-based method developed from thousands of pure cell types from six sources. It uses a novel technique for reducing associations between closely related cell types, allowing researchers to reliably portray the cellular heterogeneity landscape of tissue expression profiles [2].\n\nThe tool has one required input:\n\n* **Gene expression data set** - a TXT format file with row names as gene symbols and samples as columns.\n\nThe tool can take four optional inputs: \n\n* **Genes** - a TXT format file with names of the genes for the analysis; each gene name is in a single row.\n\n* **Cell types** - a TXT format file with names of the cell types for the analysis; each cell type is in a single row.\n\n* **Signatures object** - an RDS format file containing an R GeneSetCollection object. \n\n* **Spillover object** - an RDS format file containing an R List object.\n\nThe tool produces the following output:\n\n* **Output enrichment scores** - a file with enrichment scores in CSV format by default.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\n**xCell Analysis** is a tool primarily designed for studying cellular heterogeneity in the tumor microenvironment [2]. It takes a file with gene expression data set as the input and produces cell type enrichment scores as the output. **xCell Analysis** applies an enrichment method (not deconvolution method), which means that the main usage is for comparing across samples, not across cell types. It uses the variability among the samples for the linear transformation and works properly only with heterogenous mixtures [1].  \n\n**xCell Analysis** loads the *xCell.data* object which is a list with the spill over and calibration parameters, the signatures and the list of genes it uses. **xCell Analysis** uses a gene signatures-based method learned from thousands of pure cell types from various sources. Even though **xCell Analysis** signatures were validated using extensive in-silico simulations and cytometry immunophenotyping, users can provide their signatures by adding a **Signatures object** RDS optional file. Furthermore, the spillover object for adjusting the scores can be changed as well [2].\nUsers can apply different signatures and spill over parameters to perform the analysis by providing optional inputs. Necessary objects with these parameters can be produced using *Dev_scripts* available at the [xCell GitHub](https://github.com/dviraran/xCell) page.\n\nBy default, **xCell Analysis** uses 64 cell types for the analysis. Cell types can be defined by giving an optional TXT input file **Cell types** with each cell type in a single row. Genes can be specified as well, providing the **Genes** TXT format input.  \n\nTo gain the best results, it is recommended to run the spill over compensation only on relevant cell types (e.g. if there are no macrophages in the mixtures, it is best to remove them from the analysis)[2]. \n\n\n### Changes Introduced by Seven Bridges\n\n* Type of the cluster architecture is set to FORK; Note that Fork type is faster, but not supported in Windows;\n\n* The **Prefix** parameter is added so that **xCell Analysis** will produce a CSV format output with non-rounded scores if **Name of the rounded output** is not provided. \n\n### Common Issues and Important Notes\n\n* The **Gene expression data set** should be a matrix with genes in rows and samples in columns; The row names should be gene symbols;\n\n* **xCell Analysis** uses the expression levels ranking, thus normalisation does not have an effect. However, normalising to gene length is required [2];\n\n* **xCell Analysis** performs best with a heterogenous dataset; It is recommended to use all data combined in one run, especially cases and control samples [1];\n\n* Please note that **xCell Analysis** produces enrichment scores, not percentages;\n\n* Scaling the scores by samples will inevitably result in false interpretations [1];\n\n* For the linear transformation, **xCell Analysis** uses a calibration parameter, which can be found in the *xCell.data* object (*xCell.data$spill$fv$V3*); If there is prior knowledge about the mixture, this could be very handy to get better results [1]; The spill over values can be modified using an optional input, **Spillover object**, in RSD format;\n\n* **xCell Analysis** is developed for use with bulk gene expression data; It is not a great solution when using it directly to infer cell types in scRNA-seq data [1];\n\n* Please note that the R version for this tool is **4.0**; \n\n* The **Genes** and **Cell types** optional TXT inputs must have strings with gene names/cell types, each in a single line; \n\n* The **Genes** optional input must have more then 5000 genes for the successful task execution [1];\n\n* If a **Cell types** optional input file is provided, note that all the cell types listed should be available in the *xCell.data* object, or an error will occur [1];\n\n* If specifying **Name of the rounded output**, only rounded scores will be produced; It is recommended to add a name with an extension (e.g. *file_name.csv*);\n\n* The **Save raw to TXT file** parameter works only when **Name of the rounded output** is specified;\n\n* If **Save raw to TXT file** is set to TRUE, raw enrichment scores per cell type are given as the output;\n\n* If using a large **Gene expression data set** input file (e.g. 4GB), please note that a larger instance is required (e.g. c5.9xlarge).\n\n\n### Performance Benchmarking\n\n**xCell Analysis** is a fast tool, set to run on the **c4.2xlarge** AWS instance by default. Running a task with large **Gene expression data set** input (e.g. 4GB) requires more RAM memory, therefore a larger instance (e.g c5.9xlarge).\n\nIn the following table you can find estimates of **xCell Analysis** running time and cost. Both examples were produced using 8 threads.\n                   \n| Reference file size | Running time | Cost  | Instance |\n|-----------------------|-----------------|------------|-----------------|-------------|--------------|------------------|-------------|---------------|\n|  9.3 MB |  2min|  $0.02  | c4.2xlarge (on-demand)  |\n| 4 GB    | 11min     |  $0.41  |c5.9xlarge (on-demand) |\n\n\n*The cost can be significantly reduced by **spot instance** usage. Visit [the knowledge center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*            \n\n\n\n### References\n\n[1] [xCell GitHub](https://github.com/dviraran/xCell)\n[2] [xCell Publication](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1349-1)", "input": [{"name": "Gene expression data set", "encodingFormat": "text/plain"}, {"name": "Name of the complete output"}, {"name": "Signatures object"}, {"name": "Genes", "encodingFormat": "text/plain"}, {"name": "Spillover object"}, {"name": "RNAseq inputs"}, {"name": "Name of the rounded output"}, {"name": "Scaling"}, {"name": "Alpha parameter"}, {"name": "Save raw to TXT file"}, {"name": "Threads"}, {"name": "Cell types", "encodingFormat": "text/plain"}, {"name": "Memory per job"}, {"name": "CPU per job"}], "output": [{"name": "Output enrichment scores", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/dviraran/xCell", "https://github.com/dviraran/xCell/releases", "https://github.com/dviraran/xCell/archive/1.3.tar.gz", "https://github.com/dviraran/xCell/blob/master/README.Md"], "applicationSubCategory": ["Cell Type Enrichment Analysis"], "project": "SBG Public Data", "creator": "Aran, Hu and Butte", "softwareVersion": ["v1.1"], "dateModified": 1648048381, "dateCreated": 1612284374, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/x-tandem-2013-06-15-1/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/x-tandem-2013-06-15-1/8", "applicationCategory": "CommandLineTool", "name": "X!Tandem", "description": "**X!Tandem** is a tool that can match tandem mass spectra with peptide sequences.\n\nUnlike some older search engines, the X! series search engines calculate statistical confidence (expectation values) for all of the individual spectrum-to-sequence assignments. They also reassemble all of the peptide assignments in a data-set onto the known protein sequences and assign the statistical confidence that this assembly and alignment are non-random. As a result, separate assembly and statistical analysis software tools, e.g. PeptideProphet and ProteinProphet, do not need to be used.\n\n###Required Inputs\n\n1. input_spectra: mass spectrometry data file in DTA, PKL, MGF, or MZML format\n2. protein_fasta: example FASTA protein sequence file \n3. list\\_path\\_default\\_parameters: file that contains a list of default parameters\n\n###Outputs\n\n1. output\\_XML\\_file: file in tandem format that contains matched spectra-sequence pairs\n\n###Common Issues and Important Notes\n\nThere are no known common issues.", "input": [{"name": "Test spectra"}, {"name": "Protein Fasta Database", "encodingFormat": "application/x-fasta"}, {"name": "List path, default parameters"}, {"name": "Parent monoisotopic mass error minus"}, {"name": "Parent monoisotopic mass error plus"}, {"name": "Parent monoisotopic mass error units"}, {"name": "Parent monoisotopic mass isotope error"}, {"name": "Modification mass"}, {"name": "Potential modification mass"}, {"name": "Protein cleavage semi"}, {"name": "Maximum missed cleavage sites"}, {"name": "Refine cleavage semi"}, {"name": "Refine maximum valid expectation value"}, {"name": "Refine modification mass"}, {"name": "Refine point mutations"}, {"name": "Refine potential modification mass"}, {"name": "Refine potential modification motif"}, {"name": "Refine potential N-terminus modifications"}, {"name": "Refine"}, {"name": "Refine saps"}, {"name": "Refine sequence path"}, {"name": "Refine spectrum synthesis"}, {"name": "Refine tic percent"}, {"name": "Refine unanticipated cleavage"}, {"name": "Refine use annotations"}, {"name": "Refine use potential modifications for full refinement"}, {"name": "Protein cleavage C-terminal mass change"}, {"name": "Protein, cleavage N-terminal mass change"}, {"name": "Protein cleavage site"}, {"name": "Protein C-terminal residue modification mass"}, {"name": "Protein N-terminal residue modification mass"}, {"name": "Protein modified residue mass file"}, {"name": "Protein quick acetyl"}, {"name": "Protein quick pyrolidone"}, {"name": "Protein stP bias"}, {"name": "Protein saps"}, {"name": "Protein taxon"}, {"name": "Spectrum contrast angle"}, {"name": "Spectrum dynamic range"}, {"name": "Spectrum fragment mass error"}, {"name": "Spectrum fragment mass error units"}, {"name": "Spectrum fragment mass type"}, {"name": "Spectrum fragment monoisotopic mass error"}, {"name": "Spectrum fragment monoisotopic mass error units"}, {"name": "Spectrum maximum parent charge"}, {"name": "Spectrum minimum fragment mz"}, {"name": "Spectrum minimum peaks"}, {"name": "Spectrum minimum parent m+h"}, {"name": "Spectrum neutral loss mass"}, {"name": "Spectrum neutral loss window"}, {"name": "Spectrum sequence batch size"}, {"name": "Spectrum threads"}, {"name": "Spectrum total peaks"}, {"name": "Spectrum use neutral loss window"}, {"name": "Spectrum use noise suppression"}, {"name": "Spectrum use contrast angle"}, {"name": "Spectrum use dynamic parent ion selection"}, {"name": "Spectrum dynamic parent ion selection ratio"}, {"name": "Scoring a ions"}, {"name": "Scoring b ions"}, {"name": "Scoring c ions"}, {"name": "Scoring cyclic permutation"}, {"name": "Scoring include reverse"}, {"name": "Scoring minimum ion count"}, {"name": "Scoring x ions"}, {"name": "Scoring y ions"}, {"name": "Scoring z ions"}, {"name": "Residue, potential modification motif"}, {"name": "Output histogram column width"}, {"name": "Output histograms"}, {"name": "Output, maximum valid expectation value"}, {"name": "Output one sequence copy"}, {"name": "Output proteins"}, {"name": "Output result"}, {"name": "Output sort results by"}, {"name": "Output sequences"}, {"name": "Output spectra"}], "output": [{"name": "Output XML File"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": [], "applicationSubCategory": ["Proteomics"], "project": "SBG Public Data", "creator": "The Global Proteome Machine Organization", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648038486, "dateCreated": 1510941470, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/zumis/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/zumis/9", "applicationCategory": "CommandLineTool", "name": "zUMIs", "description": "**zUMIs** is a fast and flexible pipeline for scRNA-seq analysis. This performance is made possible by the early introduction of sample-specific barcodes (BCs), while the PCR amplification bias is alleviated by unique molecular identifiers (UMIs). **zUMIs** can handle both known and\nrandom BCs and also efficiently collapses UMIs, either just for exon mapping reads or for both exon and intron mapping\nreads. Another unique feature of **zUMIs** is the adaptive downsampling function, that facilitates dealing with hugely varying\nlibrary sizes, but also allows evaluation of whether the library has been sequenced to saturation [1].\n\nThe tool takes following inputs for all three modes:\n\n- Paired-end fastq files, where one read contains the cDNA sequence (**cDNA read fastq**) and the other read contains UMI and Cell Barcode information (**Barcode read fastq**).\n- **STAR reference directory** a tar that contains **STAR** index of the genome.\n- **GTF annotation** of the genome. Matching the version of the **STAR** index.\n- **XC baserange** Base range of the barcode\n- **XM baserange** Base range of the UMI sequence \n- **Read length** of the cDNA reads.\n\nThe tool creates following outputs:\n\n\n- **Filtered FASTQs for barcodes and cDNAs** containing two filtered gziped FASTQ files, one for the barcode, one for the cDNA.\n- **Expression reads** containing generated reference annotation(*.annotationSAF.rds), list of count matrices as sparseMatrix (*.dgecounts.rds) and Intermediate tbl with the data in long format (*.tbl.rds).\n- **Stats plots and files** containing plots and data files with descriptive statistics.\n- **STARgenome files** containing STAR output.\n- Every step creates .err and .out files with the prefix of stage name. e.g. map.???.err and map.???.out files contain error messages and standard output from mapping, respectively.\n\n\n\n### Common Use Cases\n\n**zUMIs** takes two paired-end FASTQ files, **Barcode read fastq** and **DNA read fastq** as input. Additionally, **zUMIs** needs a **GTF annotation** file, as well as the path to the **STAR reference directory**. In the case of the STAR reference directory, it should be supplied as a TAR archive and the wrapper will handle folder creation and referencing.\n\nRequired settings needed for the run include:\n\n- **Study Name**, used for output file names and report generation.\n- **Read length** of the cDNA reads.\n- **XC baserange**, the base range for cell/sample barcode of the read (e.g. 1-6). \n- **XM baserange**, the base range for UMI barcode of the read (e.g. 7-16).\n\n\n\n### Changes Introduced by Seven Bridges\n\n- No changes have been made to the original tool.\n\n###Common Issues and Important Notes\n\n\n#### **STAR** index\n\nThe **STAR** index can be used in two ways, with a pre built overhang or without overhang, for details refer to **STAR** manual [2]. If you wish to use a single **STAR** index for all runs, it is possible to have one without overhangs, as  **zUMIs** can pass the GTF file to STAR while mapping to insert junctions on the fly. If using a **STAR** index with an overhang, make sure the length of the overhang is (**Read length** - 1), otherwise the tool wont produce a *.dge file. We have seen no performance differences in the two approaches.\n\n#### Barcodes\nControlled by the **Barcodes** setting.\n\n**zUMIs** provides three main options for selecting relevant barcodes controlled by the **Barcodes** setting: automatic detection (default), number of barcodes with most reads and barcode list annotation. For more information, see the manual [3].\n\n\n#### Downsampling\nControlled by the **Downsampling** setting.\n\n**zUMIs** has powerful downsampling capabilities. Independent of downsampling mode, the full data is always exported as well.\n- Adaptive downsampling: According to the recommendation of the Scater package [4] reads are downsampled to be within 3 times median absolute deviation. This is the default setting.\n- Downsampling to a fixed depth: Reads are downsampled to a user-specified depth. Any barcodes that do not reach the requested depth are omitted. Example: `10000`\n- Downsampling to a depth range: Barcodes with read depth above the maximum of the range are downsampled to this value. All barcodes within the range are reported without downsampling and barcodes below the minimum specified read depth are omitted. Example: `10000-20000`\n-  Downsampling to several depths: Several depths can be requested by comma separation. Combinations of fixed depth and depth ranges may be given. Example: `10000,10000-20000,30000`\nFor more information see [5].\n\n\n**zUMIs** can be started from any stage, which can be controlled by the **Stage choice** parameter. \n\nIf starting from the Mapping stage, you need to provide your own processed and filtered FASTQ file as **cDNA read fastq**.\nIf starting from counting stage you need to provide a BAM file in **Starting from Counting stage (your own BAM)** input.\n\n\n### Notes on different run modes for **zUMIs**\n\n#### STRT-seq\nSTR-seq data can be processed by zUMIs by switching on the **STRT-seq** setting to *yes*. Three additional settings need to be selected when running STRT-seq data:\n\n- In case dual index is used, provide the second cell barcode index to **Gel barcode2 fastq** input.\n- Base range for cell/sample barcode should be provided in the **Base range for cell/sample barcode in -F Barcode** setting.\n- Trimming of the TSO-derived G homopolymers is handled within the filtering step. Fixed number of bases(G) will be trimmed between UMI, while cDNA read for STRT-seq should be set in the **Base Trim** setting (Default: 3).\n\n#### InDrops mode\n\nInDrops data can be processed by **zUMIs** when generated by the \"v3\" protocol. Thus, after bcl2fastq you will get four FASTQ files from your Illumina run.\n\nThe Reads correspond to:\n\n* R1 = cDNA\n* R2 = first half of gel barcode (8 nt)\n* R3 = library barcode (6 nt)\n* R4 = second half of gel barcode + UMI + polyT (8 + 6 + 8)\n\nFirst, switch the **InDrops** enum to *yes* to enable the InDrops mode and provide the following additional settings:\n\n- Base range for cell/sample barcode should be provided in the **Base range for cell/sample barcode in -F Barcode** setting.\n- Provide the library barcode read to the **Library barcode fastq** input.\n\n###Performance Benchmarking\n\nThe speed and cost of the workflow depends on the size of the Barcode FASTQ and cDNA FASTQ size.\nFollowing table showcases the metrics on the default m4.16xlarge instance.\n\n| Barcode FASTQ size | cDNA FASTQ size | Duration | Cost | Instance (AWS) |\n|---------------|-----------------|-----------|--------|-----|\n| 1.9 GB | 4.3 GB | 48 min. | $1.10 | m4.16xlarge |\n| 2.9 GB | 6.9 GB | 56 min. | $1.27 | m4.16xlarge |\n| 3.5 GB | 7.7 GB | 1h 7 min. |$1.53 | m4.16xlarge |\n| 1.9 GB | 4.6 GB | 1h 30 min. | $1.59 | m4.16xlarge |\n| 7.5 GB | 16.8 GB | 2h 33 min. | $3.26| m4.16xlarge |\n\n\n\n### API Python Implementation\nThe tool's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = 'enter_your_token', 'enter_api_endpoint'\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = 'your_username/project', 'your_username/project/workflow'\n# Get file names from files in your project. Names of the files in this example are fictious.\ninputs = {\n    'bc_read_fastq': api.files.query(project=project_id, names=['bc_and_umi_reads.fastq.gz'])[0],\n    'cdna_read_fastq': api.files.query(project=project_id, names=['cdna_reads.fastq.gz'])[0],\n    'genomedir': api.files.query(project=project_id, names=['Homo_sapiens.GRCh38.star-2.5.1b-index-archive.tar'])[0],\n    'gtf_file': api.files.query(project=project_id, names=['Homo_sapiens.GRCh38.gtf'])[0],\n    'study_name': 'API_Example',\n    'xc_baserange': '1-16',\n    'xm_baserange': '17-26',\n    'readlength': 98,\n    'processors': 64\n}\ntask = api.tasks.create(name='zUMIs - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n[1] [zUMIs](https://www.biorxiv.org/content/early/2018/04/08/153940.full.pdf+html)\n\n[2] [STAR MANUAL](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)\n\n[3] [Barcodes](https://github.com/sdparekh/zUMIs/wiki/Barcodes)\n\n[4] [McCarthy et al., 2017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408845/)\n\n[5] [Downsampling](https://github.com/sdparekh/zUMIs/wiki/Downsampling)", "input": [{"name": "Barcode read fastq", "encodingFormat": "text/fastq"}, {"name": "cDNA read fastq", "encodingFormat": "text/fastq"}, {"name": "Study Name"}, {"name": "STAR reference directory"}, {"name": "GTF annotation", "encodingFormat": "application/x-gtf"}, {"name": "XC baserange"}, {"name": "XM baserange"}, {"name": "Read length"}, {"name": "Number of barcodes for quality thresholding"}, {"name": "Number of UMI barcodes for quality thresholding"}, {"name": "Minimum base quality required for barcode"}, {"name": "Minimum base quality required for umi"}, {"name": "Processors"}, {"name": "Strandedness"}, {"name": "Barcodes"}, {"name": "Number of reads per cell (lower limit)"}, {"name": "Downsampling"}, {"name": "STAR parameters"}, {"name": "Output directory"}, {"name": "SLURM"}, {"name": "Stats output creation"}, {"name": "STAR-executable"}, {"name": "Samtools-executable"}, {"name": "Pigz-executable"}, {"name": "Rscript-executable"}, {"name": "zUMIs directory"}, {"name": "Starting from Mapping stage (your own FASTQ)"}, {"name": "Starting from Counting stage (your own BAM)"}, {"name": "Stage choice"}, {"name": "STRT-seq"}, {"name": "Dual Index fastq"}, {"name": "Base range for cell/sample barcode in -F Barcode"}, {"name": "Base Trim"}, {"name": "InDrops"}, {"name": "Gel barcode2 fastq"}, {"name": "Library barcode fastq"}, {"name": "Hamming distance"}, {"name": "Barcode binning"}, {"name": "Plate barcode read", "encodingFormat": "text/fastq"}, {"name": "Barcode range for plate barcode read"}], "output": [{"name": "Expression reads"}, {"name": "Filtered FASTQs for barcodes and cDNAs", "encodingFormat": "text/fastq"}, {"name": "Stats plots and files", "encodingFormat": "text/plain"}, {"name": "Aligned sorted BAM", "encodingFormat": "application/x-bam"}, {"name": "Feature counts for aligned sorted BAM in exons", "encodingFormat": "application/x-bam"}, {"name": "Feature counts for aligned sorted BAM in introns", "encodingFormat": "application/x-bam"}, {"name": "Barcode list filtered sort SAM", "encodingFormat": "application/x-sam"}, {"name": "Final metrics"}, {"name": "Mapping metrics"}, {"name": "Mapping pass metrics"}, {"name": "SJ table"}, {"name": "STARgenome files", "encodingFormat": "text/plain"}, {"name": "STARpass1"}], "softwareRequirements": ["ExpressionEngineRequirement", "CreateFileRequirement"], "codeRepository": ["https://github.com/sdparekh/zUMIs"], "applicationSubCategory": ["Single Cell", "Quantification", "RNA-Seq"], "project": "SBG Public Data", "creator": "Swati Parekh, Christoph Ziegenhain and Ines Hellmann", "softwareVersion": ["sbg:draft-2"], "dateModified": 1648050523, "dateCreated": 1526907073, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/zumis-2-5-5/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/zumis-2-5-5/4", "applicationCategory": "CommandLineTool", "name": "zUMIs", "description": "**zUMIs** is a fast and flexible pipeline for scRNA-seq analysis. This performance is made possible by the early introduction of sample-specific barcodes (BCs), while the PCR amplification bias is alleviated by unique molecular identifiers (UMIs). **zUMIs** can handle both known and random BCs and also efficiently collapses UMIs, either just for exon mapping reads or for both exon and intron mapping reads. Another unique feature of **zUMIs** is the adaptive downsampling function, that facilitates dealing with hugely varying library sizes, but also allows evaluation of whether the library has been sequenced to saturation [1].\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of this page.*\n\n### Common Use Cases\n\n**zUMIs** takes between two and four FASTQ files as input. Additionally, **zUMIs** needs a **GTF annotation** file, as well as the path to the **STAR reference directory** supplied as a TAR archive. FASTQ barcodes, UMI and read sequence ranges should be determined based on protocol-specific setup and inserted for each FASTQ file provided on input. Specific setups for widely used protocols could be found [here](https://github.com/sdparekh/zUMIs/wiki/Protocol-specific-setup).\n\nThe **STAR** index can be used in two ways, with a pre built overhang or without overhang, for details refer to **STAR** manual [2]. To use a single **STAR** index in multiple runs, it is possible to have one without overhangs, as  **zUMIs** can pass the GTF file to STAR which will insert junctions on the fly while performing alignment. If using a **STAR** index with an overhang, make sure the length of the overhang is (**Read length** - 1). Otherwise, the tool won't produce a *.dge file. We have seen no performance differences in the two approaches.\n\n*Note:  We created a custom Python script for parsing arguments provided to the command line through CWL wrapper and writing these arguments into YAML template file required for running **zUMIs**.*\n\n\n### Changes Introduced by Seven Bridges\n\nNo changes have been made that would modify the functionality of the original tool.\n\n### Common Issues and Important Notes\n\nBecause instance type is chosen dynamically based only on CPU and RAM requirements, attached storage will always be set to default. This could cause issues when having large FASTQ files on input (8k_v2 is an example of an oversized dataset) because attached storage will not be sufficient to store all the intermediate files. If input files are too big, the solution would be to set the attached storage size manually by setting the instance hint within tool editor.\n\n### Performance Benchmarking\n\nThe speed and cost of the workflow depend on the size of the Barcode and cDNA FASTQ files. Following table showcases the metrics for the task running on the m4.16xlarge on demand AWS instance. The price can be significantly reduced by using spot instances (set by default). Visit the Knowledge Center for more details.\n\n\nProtocol | Dataset | Fastq file 1 size(.gz) | Fastq file 2 size(.gz) | Duration | Cost | Instance (AWS) |\n|--------|---------|---------------|-----------------|-----------|--------|-----|\n| 10x | 1k_v3 | 1.2 GB | 2.9 GB | 36 min. | $1.34 | m4.16xlarge |\n| 10x | 4k_v2 | 7.6 GB | 22.4 GB | 2h 47 min. | $5.77 | m4.16xlarge |\n| 10x | 8k_v2 | 12.9 GB | 49.8 GB | 6h 27 min. |$14.09 | m4.16xlarge |\n| 10x | 10k_v3 | 11.4 GB | 27.3 GB | 5h 4 min. | $10.48 | m4.16xlarge |\n| scrbSeq | - | 3.5 GB | 7.7 GB | 1h | $1.25 | m4.10xlarge |\n| marsSeq | - | 1.9 GB | 4.3 GB | 46 min. | $0.8 | m4.10xlarge |\n| dropSeq | - | 7.5 GB | 16.8 GB | 2h 31 min. | $2.82 | m4.10xlarge |\n\n\n\n### References\n\n[1] [zUMIs](https://www.biorxiv.org/content/early/2018/04/08/153940.full.pdf+html)\n\n[2] [STAR MANUAL](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)", "input": [{"name": "Fastq file 1"}, {"name": "Fastq file 2"}, {"name": "Fastq file 3"}, {"name": "Fastq file 4"}, {"name": "Project"}, {"name": "STAR index", "encodingFormat": "application/x-tar"}, {"name": "GTF annotation file", "encodingFormat": "application/x-gtf"}, {"name": "Additional files", "encodingFormat": "text/fastq"}, {"name": "Additional STAR params"}, {"name": "CPU per job"}, {"name": "Memory"}, {"name": "Barcode filter num bases"}, {"name": "Barcode filter min phred"}, {"name": "UMI filter num bases"}, {"name": "UMI filter min phred"}, {"name": "Barcode number"}, {"name": "Barcode whitelist file", "encodingFormat": "text/fastq"}, {"name": "Barcode automatic"}, {"name": "BarcodeBinning"}, {"name": "Barcode n reads per cell"}, {"name": "Barcode demultiplex"}, {"name": "Generate exon and intron count tables"}, {"name": "Downsampling"}, {"name": "Strand"}, {"name": "Colapsing Hamming distance"}, {"name": "Write Hamming colapsed"}, {"name": "Use velocyto"}, {"name": "Count primary hit"}, {"name": "Star two pass"}], "output": [{"name": "Formated configured yaml file"}, {"name": "zUMIs standard output logs", "encodingFormat": "text/plain"}, {"name": "Expression files"}, {"name": "Stats files"}, {"name": "Barcodes status", "encodingFormat": "text/plain"}, {"name": "Kept barcodes", "encodingFormat": "text/plain"}], "softwareRequirements": ["ShellCommandRequirement", "ResourceRequirement", "DockerRequirement", "InitialWorkDirRequirement", "InlineJavascriptRequirement"], "codeRepository": ["https://github.com/sdparekh/zUMIs/wiki", "https://github.com/sdparekh/zUMIs/archive/master.zip", "https://github.com/sdparekh/zUMIs/blob/master/zUMIs-master.sh", "https://github.com/sdparekh/zUMIs"], "applicationSubCategory": ["Single Cell", "RNA-Seq"], "project": "SBG Public Data", "creator": "Swati Parekh, Christoph Ziegenhain, Beate Vieth, Wolfgang Enard, Ines Hellmann", "softwareVersion": ["v1.0"], "dateModified": 1649687799, "dateCreated": 1575462876, "contributor": ["admin"], "sdPublisher": "sbg", "programmingLanguage": "CWL"}]