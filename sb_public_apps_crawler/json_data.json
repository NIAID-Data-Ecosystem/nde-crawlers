[{"_id": "SB_Public_Apps_admin/sbg-public-data/alignment-metrics-qc/6", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/alignment-metrics-qc/6", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "Alignment Metrics QC", "description": "Running this pipeline will provide you with useful statistics to help you judge the quality of your alignment. Provide aligned reads in the BAM format and the reference FASTA to which they were aligned as inputs. This pipeline will generate an easily-digestible table report with quality control information, including the proportion of your reads that could not be aligned and the percentage of reads that passed quality checks.\n\nThis pipeline works with output from genomic or transcriptomic aligners. Run this pipeline as part of your routine bioinformatic quality control or when optimizing alignment for your data by experimenting with aligner settings."}, {"_id": "SB_Public_Apps_jiewho/bd-public-project/bd-rhapsody-analysis-pipeline/15", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/jiewho/bd-public-project/bd-rhapsody-analysis-pipeline/15", "applicationCategory": "Workflow", "version": "v1.0", "name": "BD Rhapsody\u2122 Targeted Analysis Pipeline", "description": "The BD Rhapsody\u2122 assays are used to create sequencing libraries from single cell transcriptomes.\n\nAfter sequencing, the analysis pipeline takes the FASTQ files and a reference file for gene alignment. The pipeline generates molecular counts per cell, read counts per cell, metrics, and an alignment file."}, {"_id": "SB_Public_Apps_jiewho/bd-public-project/bd-rhapsody-wta-analysis-pipeline/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/jiewho/bd-public-project/bd-rhapsody-wta-analysis-pipeline/7", "applicationCategory": "Workflow", "version": "v1.0", "name": "BD Rhapsody\u2122 WTA Analysis Pipeline", "description": "The BD Rhapsody\u2122 WTA Analysis Pipeline is used to create sequencing libraries from single cell transcriptomes without having to specify a targeted panel.\n\nAfter sequencing, the analysis pipeline takes the FASTQ files, a reference genome file and a transcriptome annotation file for gene alignment. The pipeline generates molecular counts per cell, read counts per cell, metrics, and an alignment file."}, {"_id": "SB_Public_Apps_admin/sbg-public-data/bismark-analysis-0-19-0/28", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/bismark-analysis-0-19-0/28", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "Bismark Analysis", "description": "**Bismark Analysis 0.19.0** is a workflow for analyzing DNA methylation, a type of epigenetic modification, by processing reads which can be obtained by using two technologies for library preparation and sequencing, Whole Genome Bisulfite Sequencing (WGBS) and Reduced Representation Bisulfite Sequencing (RRBS). It can be used for an input file of any size, but it is optimized for larger files (more than 15 GB).\n\nThe pipeline contains four main parts: a quality control and adapter trimming performed by **Trim Galore!**, alignment of BS-Seq reads performed by the **Bismark**, deduplication performed by **Bismark Deduplicate**, and extraction of methylated cytosines performed by **Bismark Methylation Extractor**.\n\nThe workflow takes the following input files:\n\n* **FASTQ files** - compressed or decompressed BS-seq\u00a0reads\u00a0(FASTQ/FASTQ.GZ) with **Paired-end** and **Sample ID** metadata field set\n* **Reference FASTA** - a reference genome FASTA file\n* **Bisulfite genome reference** - gzipped in-silico bisulfite-treated and indexed reference genome (TAR.GZ), which can be created with **Bismark Genome Preparation** tool (the appropriate alignment algorithm Bowie1/Bowtie2 have to be selected). Suggested file for that input is **ucsc\\_Bisulfite\\_Genome\\_bowtie2.tar.gz**, which is created by **Bismark Genome Preparation** tool with provided reference genome file ucsc.hg19.fasta and selected Bowtie2 option\n\nOutput files are generated within the four abovementioned segments of the workflow:\n\n* In the quality control and adapter trimming segment, graphical interfaces with all necessary information about reads, their quality, and adapter contamination are generated before and after the tool Trim Galore! (**Pre trimming FASTQC Analysis** and **Post trimming FASTQC Analysis**). \n* An alignment BAM file (**Bismark alignment file**) is produced by the **Bismark** in the alignment part of the workflow. This file is sorted by coordinate and has its own index BAI file. For further processing, the direct output of the Bismark is sorted by the name to be acceptable for the tool **Bismark Deduplicate**.\n* A **Sorted deduplicated BAM** is created in the deduplication process and is sorted by coordinate and has its own index BAI file. In **RRBS** mode (no deduplication), this file should be the same as **Bismark alignment file**.\n* In the methylation call segment, **Bismark Methylation Extractor** operates on BAM file and generates **Coverage file** which is applicable to further analysis (differential methylation). \n* Graphical HTML reports generated by the tool **Bismark2Report** contains all information obtained from reports generated by Bismark, Bismark Deduplicate, and Bismark Methylation Extractor.\n\n*A list of all inputs and parameters with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThe workflow is designed to analyze one WGBS or RRBS sample (FASTQ, FASTQ.GZ) with properly set metadata fields.\n\n* The user should provide FASTQ files to input **FASTQ files** and set metadata fields  **Sample ID** for single-end read and **Sample ID** and **Paired-end** for paired-end reads. \n* All workflow settings are optimized for the use of larger input files with sequences (more than 15 GB). For other use cases, please contact Seven Bridges for support.\n* If paired-end reads are provided, the option **Paired** in the tool Trim Galore! should be set. The option **Single end** in Bismark and Bismark Deduplicate should be set No value or False.\n* The option **Bowtie aligner** in Bismark must be selected according to the selected alignment algorithm in the tool Bismark Genome Preparation.\n* The reference genome files which are used for generating the gzipped bisulfite indexed genome reference file should be used in the workflow. \n* After running task of workflow has successfully completed, it creates **Coverage file** which contains methylated cytosines and can be used for further analysis of differential methylation, HTML reports of FASTQ reads before and after trimming, HTML report of complete process of methylation calling (alignment, deduplication and extraction methylation call) and BAM files after alignment and deduplication. BAM file and report generated by Bismark,  Deduplication report and Splitting report can be used for comparison alignment and methylation parameters between samples in the tool **Bismark2Summary**.\n\n### Changes Introduced by Seven Bridges\n\nThe realization of workflow is based on the idea of optimizing the workflow in a better way. \n\nThe technical configuration of workflow is:\n\n* AWS instance c4.8xlarge (36 CPU, 60GB)\n* maximum parallel instances set to 6. \n\nThree parts of the pipeline which contribute to faster task executions are as follows:\n\n* Trim Galore! scattered by split FASTQ files. SBG FASTQ Split splits FASTQ files into 30 parts. Reads can't be divided to the maximum number of CPUs (36), because some tools run in parallel with Trim Galore! and execution time won't be optimized. The number of parts should be divisible by the maximum number of instances which is 6.\n* Scattered Bismark by trimmed FASTQ files which are merged before into 6 parts. This tool runs on six parallel instances (c4.8xlarge).\n* Generated BAM file are split by chromosome name and provided to Bismark Deduplicate and Bismark Methylation Extractor which are scattered by chromosome bam files. \n* Outputs generated by workflow are finally merged.\n\n### Common Issues and Important Notes\n\n* Metadata fields for FASTQ files **Paired-end** and **Sample ID** should be properly set, fields Sample ID for single-end read and Sample ID and Paired-end for paired-end reads.\n* For RRBS library, option **RRBS** should be set in the tool Trim Galore!. Also, author of the Bismark recommends that deduplication process should not be applied to RRBS reads. Option in Bismark Deduplicate **RRBS** should be set to TRUE for bypassing deduplication for RRBS reads.\n* Option **Bowtie Aligner** in the tool Bismark should be specified depending on which Bowtie is used for indexing in Bismark Genome Preparation. The default value is Bowtie2.\n* Options **Memory in MB per job** and **Number of CPUs per job** in Bismark are related to the instance on which this tool is running. Those options should be set to the maximum of memory and number of CPU of specified instance. E.g. AWS instance c4.8xlarge has 36 CPU and 60 GB, **Memory in MB per job** is set to 61440 MB and **Number of CPUs per job** set to 36.\n* The **Multicore** option in Bismark tool represents the number of parallel jobs (multicores) which are run concurrently. It takes value 8 by default and should not be changed in the case of the reads from directional sequencing library and human genome reference (3GB). This value is set so the pipeline can be run on AWS instance c4.8xlarge (36CPUs, 60GB). If reference genome is a bigger size or more references are provided, this parameter should be set to less value (e.g. for the reference of 4.5GB multicore is set to 3 in order to fit on c4.8xlarge).\n* All the above settings are optimized for the case of larger input files with sequences (more than 15 GB). For other cases, please contact SBG support.\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\n### Performance Benchmarking\n\nAmong all tools in workflow, Bismark has the highest memory demands. Memory and CPU requirements for Bismark strongly correlate with **Sequencing library** (directional or non-directional), a number of multicores (parameter **Multicore**) and the size of the reference file. If reference genome file has the bigger size or there are more reference files provided, the multicore parameter should be set to a lower value. We tested for the human genome (3 GB), multicore was set to 6, it means that 10 GB per one multicore, for three genome reference files (4.5GB in total), multicore was 3, 20 GB per one multicore.\n\nBased on our experience we choose AWS instance c4.8xlarge (36 CPU, 60 GB) which was enough for high memory requirement.\nIn the following table, you can find estimates of Bismark Analysis 0.19.0 running time and cost.\n\n| FASTQ (size) | Genome Reference (size) | Multicore (Bismark) | Instance (AWS) | Duration | Cost |\n| --- | --- | --- | --- | --- | --- |\n| 2 x 700 MB (GZ)| 3 GB | 6 | c4.8xlarge | 1h 9m | $6.78 |\n| 2 x 12 GB (GZ) | 3 GB | 6 | c4.8xlarge | 5h 11m | $26.67 |\n| 2 x 220 GB | 4.5 GB | 3 | c4.8xlarge | 20h 44m | $114.29 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### References\n\n[1] Krueger F. and Andrews S.R. (2011) [Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3102221/). Bioinformatics. 2011 Jun 1;27(11):1571-2. doi: 10.1093/bioinformatics/btr167. Epub 2011 Apr 14"}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/bms-bam-prep/4", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/bms-bam-prep/4", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "BMS BAM prep", "description": "**BMS BAM prep** is the obligatory first phase that must precede all variant discovery. It involves pre-processing the raw sequence data (provided in FASTQ format) to produce analysis-ready BAM files. This involves alignment to a reference genome, as well as some data cleanup operations to correct for technical biases and make the data suitable for analysis [1].\n\nThis workflow is a part of the project to transfer BMS variant calling procedures from custom bash scripts to the Platform, which started in October 2017. \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nThis workflow contains three main steps:\n* **Alignment** of raw reads to the human reference genome with **BWA MEM Bundle** (0.7.17). Besides the standard **BWA MEM** SAM output file, **BWA MEM** package has been extended to support two additional output options: a BAM file obtained by piping through **Sambamba view** while filtering out the secondary alignments, as well as a **Coordinate Sorted BAM** option that additionally pipes the output through **Sambamba sort**, along with an accompanying BAI file produced by **Sambamba sort**. This sorted BAM is the default output of BWA MEM.\n* **Deduplication** with **Picard MarkDuplicates** tool (2.9.2). This second processing step consists of identifying read pairs that are likely to have originated from duplicates of the same original DNA fragments through some artifactual processes [1].\n* **Base recalibration** consists of two tools: **GATK BaseRecalibrator** and **GATK apply BQSR**. The first tool is applying machine learning to detect patterns of systematic errors in the base quality scores, which are confidence scores emitted by the sequencer for each base, while the second one is applying recalibration to the deduplicated BAM file [1].\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named by sample IDs e.g.: *SampleID.sorted.dedup.realigned.recal.bam*\n* **Picard BEDToIntervalsList** tools are added for conversion of BED files into INTERVALS_LIST.\n* Alignment can be performed in batch mode. Batching is most often performed by sample ID metadata.\n\n### Common Issues and Important Notes\n\n* In order to successfully work, **BWA MEM Bundle** requires a FASTA reference file accompanied with BWA FASTA indices in a TAR file. \n* There is a known issue with **Samblaster** - it does not support processing when the number of sequences in the reference FASTA is larger than 32768. \n* If the reference file is provided in FASTA format instead of a TAR file (which already contains all the necessary indices), the runtime will be significantly increased.\n* Pipeline provides suggested reference files when running it for the first time in a project. These files are aimed for use with BMS kit, and should only be used for testing purposes. If using this pipeline for production, these files should be replaced with references corresponding to the input reads.\n\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of **BMS BAM prep** run times and costs. All samples are aligned against **hg19 human reference**. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:| ------------- | ------------- | ------------- |\n|16|30000| 2 X 150 MB     | WES | 10 minutes| $0.13 |    c4.4xlarge |\n|16|30000| 2 X 3.9 GB     | WES | 1 hour and 35 minutes| $1.26 |    c4.4xlarge |\n|16|30000| 2 X 8.3 GB     | WES | 4 hour 15 minute| $3.38 |    c4.4xlarge |\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"bait_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_reads\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"]))}\n# Creates draft task\ntask = api.tasks.create(name=\"BMS BAM prep - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/).\n\n\n### References\n[1 - GATK best practices](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165)"}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-normal-pipeline-hg19/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-normal-pipeline-hg19/2", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "BMS WES Tumor-Normal pipeline hg19", "description": "**BMS WES TN pipeline hg19** is a workflow made around the _Strelka 1.0.15_ somatic variant caller (SNVs and INDELs) and _GATK Mutect 1.1.6_ (SNVs) somatic variant callers, which perform variant calling on matched tumor - normal BAM files.\n\nThis workflow is a part of the project to transfer BMS variant calling procedures from custom bash scripts to the Platform, which started in October 2017. This workflow utilizes human reference genome hg19.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n###Common Use Cases\n\nBAM files are used as inputs. **Tumor BAM file** and **Normal BAM file**, should be sorted and indexed. This can be done by using the **BMS BAM prep** workflow. The **Input tar with reference** input should contain the reference genome with all necessary indexes. For every VCF output that this workflow produces, Variant Calling Metrics are calculated and a MAF file is produced.\n\nThere are tools that performs annotation of output VCF files in this workflow. Annotation is done with: \n* _dbSNP_ \n* _SNPEff database_ \n* _1000G snps_ \n* _1000G indels_\n* _Mills and 1000G indels_ \n* _Cosmic database_\n* _ExAC database_\n\n\n### Changes Introduced by Seven Bridges\n\n* Every output has its own subset which contains only variants that are in BED file regions. These output are prefixed with **intersected**.\n\n* Only primary chromosomes are kept in VCF/MAF files.\n\n* Outputs are named by sample IDs e.g.: TumorID-NormalID.passed.somatic.strelka.snvs.vcf\n\n\n### Common Issues and Important Notes\n\n#### **IMPORTANT LICENSING INFORMATION**\n* MuTect is licensed by the Broad Institute and is made available for free to academic users for non-commercial use only pursuant to the licensing terms below, and to other authorized licensees pursuant to the terms of their respective licenses, in each case for use within this pipeline only. The full text of the academic license for non-commercial use of MuTect is available on [github](https://github.com/broadinstitute/mutect/blob/master/mutect.LICENSE.TXT). \n\n*   For commercial licensing information, please email [Broad institute](softwarelicensing@broadinstitute.org). \n \n*  For more information about Mutect, please visit the [Mutect website](https://software.broadinstitute.org/cancer/cga/mutect).\n  \n#### MuTect documentation resources and support\n\nGeneral MuTect documentation can be found on the [MuTect website](https://software.broadinstitute.org/cancer/cga/mutect). Users of this pipeline are welcome to ask MuTect-related questions and report problems that are not specific to this pipeline in the [MuTect forum](http://gatkforums.broadinstitute.org/categories/mutect).\n\n#### Strelka common issues\n\n- According to the [Strelka FAQ](https://sites.google.com/site/strelkasomaticvariantcaller/home/faq), the following BAM records are incompatible with Strelka (quoted):\n  - Alignments which use the match/mismatch (\"=\"/\"X\") CIGAR notation\n  - Records where the \"=\" character is used in the SEQ field\n  - BAM records with basecall quality values greater than 70 (not supported assuming that this indicates an offset error)\n- Unless input BAM files **Tumor BAM file** and **Normal BAM file** are supplied to Strelka (both sorted and indexed), Strelka will not finish successfully. \n\n###Performance Benchmarking\n\nIn the following table you can find estimates of **Matched TN pipeline hg19** run times and costs. All samples are aligned against **hg19 human reference**. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:| ------------- | ------------- | ------------- |\n|36|60000| 5.2 GB + 4.9 GB     | WES | 42 minutes| $1.11 |    c4.8xlarge |\n|36|60000| 6.6 GB + 6.9 GB     | WES | 48 minutes| $1.27 |    c4.8xlarge |\n|36|60000| 14 GB + 14 GB     | WES | 1 hour 21 minutes| $2.14 |    c4.8xlarge |\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"tumor_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"normal_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"kgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"mgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"snpEff_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"cosmic_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"cache_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"annotation_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"ExAC_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"BMS WES TN pipeline hg19 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/)."}, {"_id": "SB_Public_Apps_bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-only-pipeline-hg19/2", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/bristol-myers-squibb-publishin/bms-public-apps/bms-wes-tumor-only-pipeline-hg19/2", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "BMS WES tumor only pipeline hg19", "description": "BMS WES tumor only pipeline hg19 is a workflow which performs somatic variant calling on a tumor BAM file.\n\nIt is made around the _Strelka 1.0.15_ (SNVs and INDELs) and _GATK Mutect 1.1.6_ (SNVs) somatic variant callers which perform variant calling on a tumor BAM file.\n\nThis workflow is a part of the project to transfer BMS variant calling procedures from custom bash scripts to the Platform, which started in October 2017. The workflow utilizes human reference genome hg19.\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\nBAM file is used as input. **Tumor BAM file**, should be sorted and indexed. This can be done by using the **BMS BAM prep** workflow. The **Input tar with reference** input should contain the reference genome file with all necessary indexes. For every output that this workflow produces, Variant Calling Metrics are calculated and a MAF file is produced.\n\nThere are also tools that perform annotation of output VCF files in this workflow. Annotation is done with:\n* _dbSNP_ \n* _SNPEff database_ \n* _1000G snps_ \n* _1000G indels_\n* _Mills and 1000G indels_ \n* _Cosmic database_\n* _ExAC database_\n\n\n### Changes Introduced by Seven Bridges\n\n* **Strelka1.0.15** doesn't perform somatic calling in tumor-only mode. We used **NA12878** WES BAM sample aligned against hg19 human reference as  normal sample.\n\n* Every output has its own subset, which contains only variants that are in BED file regions. These outputs are prefixed with **intersected**.\n\n* Only primary chromosomes are kept in VCF/MAF files.\n\n* Outputs are named by sample IDs e.g.: TumorID.passed.somatic.strelka.snvs.vcf\n\n\n### Common Issues and Important Notes\n\n#### **IMPORTANT LICENSING INFORMATION**\n* MuTect is licensed by the Broad Institute and is made available for free to academic users for non-commercial use only pursuant to the licensing terms below, and to other authorized licensees pursuant to the terms of their respective licenses, in each case for use within this pipeline only. The full text of the academic license for non-commercial use of MuTect is available on [github](https://github.com/broadinstitute/mutect/blob/master/mutect.LICENSE.TXT). \n\n*   For commercial licensing information, please email [Broad institute](softwarelicensing@broadinstitute.org). \n \n*  For more information about Mutect, please visit the [Mutect website](https://software.broadinstitute.org/cancer/cga/mutect).\n  \n#### MuTect documentation resources and support\n\nGeneral MuTect documentation can be found on the [MuTect website](https://software.broadinstitute.org/cancer/cga/mutect). Users of this pipeline are welcome to ask MuTect-related questions and report problems that are not specific to this pipeline in the [MuTect forum](http://gatkforums.broadinstitute.org/categories/mutect).\n\n#### Strelka common issues\n\n- According to the [Strelka FAQ](https://sites.google.com/site/strelkasomaticvariantcaller/home/faq) the following BAM records are incompatible with Strelka (quoted):\n  - Alignments which use the match/mismatch (\"=\"/\"X\") CIGAR notation\n  - Records where the \"=\" character is used in the SEQ field\n  - BAM records with basecall quality values greater than 70 (not supported assuming that this indicates an offset error)\n- Unless input BAM files **Tumor BAM file** and **Normal BAM file** are supplied to Strelka (both sorted and indexed), Strelka will not finish successfully.\n\n### Performance Benchmarking\n\nIn the following table you can find estimates of **Tumor only pipeline hg19** run times and costs. All samples are aligned against **hg19 human reference**. \n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*     \n\n| Threads and CPUs        | Memory          | Input size        | WGS/WES           | Duration       | Cost          | Instance (AWS)  |\n| -------------:|:-------------:| -----:|:-------------:| ------------- | ------------- | ------------- |\n|36|60000| 4.4 GB     | WES | 48 minutes| $1.27 |    c4.8xlarge |\n|36|60000| 5.4 GB     | WES | 50 minutes| $1.32 |    c4.8xlarge |\n|36|60000| 14 GB     | WES | 1 hour 1 minute| $1.62 |    c4.8xlarge |\n\n\n### API Python Implementation\n\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"cosmic_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"annotation_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_tar_with_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"tumor_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"kgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"mgindel_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"snpEff_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"ExAC_database\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"cache_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"normal_reads\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"BMS WES tumor only pipeline hg19 - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) client is available. To learn more about using this API client please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/)."}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practices-rna-seq-variant-calling-4-1-0-0/8", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practices-rna-seq-variant-calling-4-1-0-0/8", "applicationCategory": "Workflow", "version": "v1.0", "name": "BROAD Best Practices RNA-Seq Variant Calling 4.1.0.0", "description": "This workflow represents the GATK Best Practices for SNP and INDEL calling on RNA-Seq data. \n\nStarting from an unmapped BAM file, it performs alignment to the reference genome, followed by marking duplicates, reassigning mapping qualities, base recalibration, variant calling and variant filtering. On the [GATK website](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891), you can find more detailed information about calling variants in RNA-Seq.\n\n###Common Use Cases\n- If you have raw sequencing reads in FASTQ format, you should convert them to an unmapped BAM file using the **Picard FastqToSam** app before running the workflow.\n- **BaseRecalibrator** uses **Known indels** and **Known SNPs** databases to mask out polymorphic sites when creating a model for adjusting quality scores. Also, the **HaplotypeCaller** uses the **Known SNPs** database to populate the ID column of the VCF output.\n- The **HaplotypeCaller** app uses **Intervals list** to restrict processing to specific genomic intervals. You can set the **Scatter count** value in order to split **Intervals list** into smaller intervals. **HaplotypeCaller** processes these intervals in parallel, which will significantly reduce workflow execution time  in some cases.\n- You can provide a pre-generated **STAR** reference index file or a genome reference file to the **Reference or STAR index** input.\n- **Running a batch task**: Batching is performed by **Sample ID** metadata field on the **Unmapped BAM** input port. For running analyses in batches, it is necessary to set **Sample ID** metadata for each unmapped BAM file.\n\n\n###Changes Introduced by Seven Bridges\nThis workflow represents the GATK Best Practices for SNP and indel calling on RNA-Seq data, and there are no modifications to the original workflow.\n\n\n###Common Issues and Important Notes\n- As the *(--known-sites)* is the required option for GATK BaseRecalibrator tool, it is necessary to provide at least one database file to the **Known INDELs** or **Known SNPs** input port.\n- If you are providing pre-generated STAR reference index make sure it is created using the adequate version of STAR (check the STAR version in the original [WDL file](https://github.com/gatk-workflows/gatk3-4-rnaseq-germline-snps-indels/blob/master/rna-germline-variant-calling.wdl)).\n- When converting FASTQ files to an unmapped BAM file using **Picard FastqToSam**, it is required to set the **Platform** (`PLATFORM=`) parameter.\n- This workflow allows you to process one sample per task execution. If you are planning to process more than one sample, it is required to run multiple task executions in batch mode. More about batch analyses can be found [here](https://docs.sevenbridges.com/docs/about-batch-analyses).\n \n\n###Performance Benchmarking\nThe default memory and CPU requirements for each app in the workflow are the same as in the original [GATK Best Practices WDL](https://github.com/gatk-workflows/gatk3-4-rnaseq-germline-snps-indels/blob/master/rna-germline-variant-calling.wdl). You can change the default runtime requirements for **STAR GenomeGenerate** and **STAR Align** apps. \n\n| Experiment type |  Input size | Paired-end | # of reads | Read length | Duration |  AWS Instance Cost (spot) | AWS Instance Cost (on-demand) | \n|:--------------:|:------------:|:--------:|:-------:|:---------:|:----------:|:------:|:------:|\n|     RNA-Seq     |  1.3 GB |     Yes    |     16M     |     101     |   2h44min   | 0.79$ | 1.79$ | \n|     RNA-Seq     |  3.9 GB |     Yes    |     50M     |     101     |   4h38min   | 1.29$ | 2.71$ | \n|     RNA-Seq     | 6.5 GB |     Yes    |     82M    |     101     |  6h44min  | 1.85$ | 3.84$ | \n|     RNA-Seq     | 12.9 GB |     Yes    |     164M    |     101     |  12h4min  | 3.30$ | 6.99$ |\n\n\n###API Python Implementation\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id = \"your_username/project\"\nworkflow_id = \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n        \"input\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.whole_genome.interval_list']),\n        \"in_alignments\": api.files.query(project=project_id, names=['G26234.HCC1187_1Mreads.bam'])[0],\n        \"in_reference\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.fasta'])[0],\n        \"in_gene_annotation\": api.files.query(project=project_id, names=['star.gencode.v19.transcripts.patched_contigs.gtf'])[0],\n        \"in_reference_or_index\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.star.gencode.v19.transcripts.patched_contigs.star-2.5.3a_modified-index-archive.tar'])[0],\n        \"known_indels\": api.files.query(project=project_id, names=['Mills_and_1000G_gold_standard.indels.b37.sites.vcf',\n                                                                   'Homo_sapiens_assembly19_1000genomes_decoy.known_indels.vcf']),\n        \"known_snps\": api.files.query(project=project_id, names=['Homo_sapiens_assembly19_1000genomes_decoy.dbsnp138.vcf']),\n}\n\ntask = api.tasks.create(name='GATK4 RNA-Seq Workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False)\n#For running a batch task\ntask = api.tasks.create(name='GATK4 RNA-Seq Workflow - API Batch Example', project=project_id, app=workflow_id, inputs=inputs, run=False, batch_input='in_alignments', batch_by = { 'type': 'CRITERIA', 'criteria': [ 'metadata.sample_id'] })\n```\n\nInstructions for installing and configuring the API Python client are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart)."}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practices-somatic-cnv-pair-workflow-4-1-0-0/7", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practices-somatic-cnv-pair-workflow-4-1-0-0/7", "applicationCategory": "Workflow", "version": "v1.0", "name": "BROAD Best Practices Somatic CNV Pair Workflow 4.1.0.0", "description": "BROAD Best Practices Somatic CNV Pair is used for detecting copy number variants (CNVs) in a single sample.\n\n### Common Use Cases\n\nThe workflow denoises case sample alignment data against a panel of normals (PON), created by **GATK CNV Panel Workflow**, to obtain copy ratios and models segments from the copy ratios and allelic counts. The latter modeling incorporates data from a matched control sample. The same workflow steps apply to targeted exome and whole genome sequencing data [1].\n\nThe basis of copy number variant detection is formed by collecting coverage counts, while the resolution of the analysis is defined by the genomic intervals list. In the case of whole genome data, the reference genome is divided into equally sized intervals or bins, while for exome data, the target regions of the capture kit should be padded. In either case the **PreprocessIntervals** tool is used for preparing the intervals list which is then used for collecting the raw integer counts. For this step,**CollectReadCounts** is utilized, which counts reads that overlap the interval. Read counts are standardized and denoised against the PON with the **DenoiseReadCounts** tool. Standardized and denoised copy ratios are plotted using the **PlotDenoisedCopyRatios** tool [2].\n\nNext step in the workflow is segmentation, performed by the **ModelSegments** tool [3]. In segmentation, contiguous copy ratios are grouped together into segments. The tool performs segmentation for both copy ratios and for allelic copy ratios, given allelic counts. **CollectAllelicCounts** will tabulate counts of the reference allele and counts of the dominant alternate allele for each site in a given genomic intervals list (**Common sites**). Modeled copy ratio and allelic fraction segments are plotted using the **PlotModeledSegments** tool.\n\nThe **CallCopyRatioSegments** tool allows for systematic calling of copy-neutral, amplified and deleted segments. The **Neutral segment copy ratio lower bound** (default 0.9) and **Neutral segment copy ratio upper bound** (default 1.1) parameters together set the copy ratio range for copy-neutral segments [4].\n\nSome of the common input parameters are listed below:\n* **Input reads - tumor** - Tumor BAM/SAM/CRAM file. In case of BAM and CRAM formats BAI and CRAI index files are required.\n* **Input reads - normal** - Matched normal BAM/SAM/CRAM file. In case of BAM and CRAM formats BAI and CRAI index files are required.\n* **Panel of normals** - CNV panel of normals (PON) file in HDF5 format.\n* **Reference** - Reference genome in FASTA format along with FAI and DICT secondary files.\n* **Intervals** - Required for both WGS and WES cases. Accepted formats must be compatible with the GATK `-L` argument. For WGS, the intervals should simply cover the autosomal chromosomes (sex chromosomes may be included, but care should be taken to avoid creating panels of mixed sex, and to denoise case samples only with panels containing only individuals of the same sex as the case samples) [5].\n* **Bin length** - This argument is used by the **PreprocessIntervals** tool and must be set to the same value that was used to create the PON file. If intervals in PON do not exactly match the ones used to collect read counts for case sample, the workflow will produce an error. For WES analysis, this parameter should be set to 0.\n* **Common sites** - Sites at which allelic counts will be collected, used in the **CollectAllelicCounts** tool. The file must be compatible with the GATK -L argument. This is usually a dbsnp VCF or Mills gold standard (SNPs only) VCF file. In case of WES analysis, we advise using subset of this file with variants contained in target intervals. This would reduce execution time of **CollectAllelicCounts** tool and would require less resources (see *Common Issues and Important Notes*).\n\n### Changes Introduced by Seven Bridges\n* Outputs of several tools in the workflow are grouped together using the **SBG Group Outputs** tool. This does not affect the contents of the files nor execution performance, it is introduced with the purpose of keeping output files neatly organized.\n\n### Common Issues and Important Notes\n* For WGS and some cases of WES samples **CollectAllelicCounts** will require more memory than the default 13000 MB. If the entire set of variants from dbsnp is used as input for this tool, we advise allocating at least 100000 MB (100GB) of memory through the **Memory per job** parameter.\n* For WGS analysis, **ModelSegments** may require more memory than the default 13000 MB. We advise allocating at least 32000 MB (32GB) of memory through the **Memory per job** parameter.\n* This workflow is set to run on two instances in parallel to reduce execution time. This is achieved through the `sbg:maxNumberOfParallelInstances` instance hint. \n\n### Performance Benchmarking\n| Input Size | Experimental Strategy | Coverage | Duration | Cost (on demand) |\n| --- | --- | --- | --- | --- | --- |\n| 2 x 45GB | WGS | 8x | 1h 34min | $3.27 | \n| 2 x 120GB | WGS | 25x | 3h 23min | $7.08 |\n| 2 x 210GB | WGS | 40x | 4h 57min | $10.56 |\n| 2 x 420GB | WGS | 80x | 8h 58min | $19.96 |\n\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"intervals\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"sequence_dictionary\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"input_alignments_tumor\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"common_sites\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"count_panel_of_normals\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"GATK CNV Somatic Pair Workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n\n### References\n* [1] [https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_pair_workflow.wdl](https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_pair_workflow.wdl)\n* [2] [https://gatkforums.broadinstitute.org/dsde/discussion/11682](https://gatkforums.broadinstitute.org/dsde/discussion/11682)\n* [3] [https://gatkforums.broadinstitute.org/dsde/discussion/11683#6](https://gatkforums.broadinstitute.org/dsde/discussion/11683#6)\n* [4] [https://gatkforums.broadinstitute.org/dsde/discussion/11683#6](https://gatkforums.broadinstitute.org/dsde/discussion/11683#6)\n* [5] [https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists](https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/broad-best-practices-somatic-cnv-panel-workflow-4-1-0-0/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/broad-best-practices-somatic-cnv-panel-workflow-4-1-0-0/14", "applicationCategory": "Workflow", "version": "v1.0", "name": "BROAD Best Practices Somatic CNV Panel Workflow 4.1.0.0", "description": "BROAD Best Practices Somatic CNV Panel is used for creating a panel of normals (PON) given a set of normal samples.\n\n### Common Use Cases\n\nFor CNV discovery, the PON is created by running the initial coverage collection tools individually on a set of normal samples and combining the resulting copy ratio data using a dedicated PON creation tool [1]. This produces a binary file that can be used as a PON. It is very important to use normal samples that are as technically similar as possible to the tumor samples (same exome or genome preparation methods, sequencing technology etc.) [2].\n \nThe basis of copy number variant detection is formed by collecting coverage counts, while the resolution of the analysis is defined by the genomic intervals list. In the case of whole genome data, the reference genome is divided into equally sized intervals or bins, while for exome data, the target regions of the capture kit should be padded. In either case, the **PreprocessIntervals** tool is used for preparing the intervals list which is then used for collecting raw integer counts. For this step **CollectReadCounts** is utilized, which counts reads that overlap the interval. Finally a CNV panel of normals is generated using the **CreateReadCountPanelOfNormals** tool. \n\nIn creating a PON, **CreateReadCountPanelOfNormals** abstracts the counts data for the samples and the intervals using Singular Value Decomposition (SVD), a type of Principal Component Analysis. The normal samples in the PON should match the sequencing approach of the case sample under scrutiny. This applies especially to targeted exome data because the capture step introduces target-specific noise [3].\n\nSome of the common input parameters are listed below:\n*  **Input reads** (`--input`) - BAM/SAM/CRAM file containing reads. In the case of BAM and CRAM files, secondary BAI and CRAI index files are required.\n* **Intervals** (`--intervals`) - required for both WGS and WES cases. Formats must be compatible with the GATK `-L` argument. For WGS, the intervals should simply cover the autosomal chromosomes (sex chromosomes may be included, but care should be taken to avoid creating panels of mixed sex, and to denoise case samples only with panels containing only individuals of the same sex as the case samples)[4].\n* **Bin length** (`--bin-length`). This parameter is passed to the **PreprocessIntervals** tool. Read counts will be collected per bin and final PON file will contain information on read counts per bin. Thus, when calling CNVs in Tumor samples, **Bin length** parameter has to be set to the same value used when creating the PON file.\n* **Padding** (`--padding`). Also used in the **PreprocessIntervals** tool, defines number of base pairs to pad each bin on each side.\n* **Reference** (`--reference`) - Reference sequence file along with FAI and DICT files.\n* **Blacklisted Intervals** (`--exclude_intervals`) will be excluded from coverage collection and all downstream steps.\n* **Do Explicit GC Correction** - Annotate intervals with GC content using the **AnnotateIntervals** tool.\n\n### Changes Introduced by Seven Bridges\n*The workflow in its entirety is per [best practice](https://github.com/gatk-workflows/gatk4-somatic-cnvs/blob/master/cnv_somatic_panel_workflow.wdl) specification.* \n\n### Performance Benchmarking\n\n| Input Size | Experimental Strategy | Coverage | Duration | Cost (on demand) | AWS Instance Type |\n| --- | --- | --- | --- | --- | --- | --- |\n| 2 x 45GB | WGS | 8x | 33min | $0.59 | c4.4xlarge 2TB EBS |\n| 2 x 120GB | WGS | 25x | 1h 22min | $1.47 | c4.4xlarge 2TB EBS |\n| 2 x 210GB | WGS | 40x | 2h 19min | $2.48 | c4.4xlarge 2TB EBS |\n| 2 x 420GB | WGS | 80x | 4h 15min | $4.54 | c4.4xlarge 2TB EBS |\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"sequence_dictionary\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"intervals\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"in_alignments\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])), \n\t\"in_reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"output_prefix\": \"sevenbridges\"}\n# Creates draft task\ntask = api.tasks.create(name=\"GATK CNV Somatic Panel Workflow - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client, are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n* [1] [https://github.com/gatk-workflows/gatk4-somatic-cnvs](https://github.com/gatk-workflows/gatk4-somatic-cnvs)\n* [2] [https://gatkforums.broadinstitute.org/gatk/discussion/11053/panel-of-normals-pon](https://gatkforums.broadinstitute.org/gatk/discussion/11053/panel-of-normals-pon)\n* [3] [https://gatkforums.broadinstitute.org/dsde/discussion/11682](https://gatkforums.broadinstitute.org/dsde/discussion/11682)\n* [4] [https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists](https://gatkforums.broadinstitute.org/gatk/discussion/11009/intervals-and-interval-lists)"}, {"_id": "SB_Public_Apps_lizhang/adk-resources/bwa-mem-multi-lane/9", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/lizhang/adk-resources/bwa-mem-multi-lane/9", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "BWA MEM Multi-Lane", "description": "[Beta] BWA MEM workflow aligning and merging multiple pairs of FASTq files into single BAM output.\n\nThis workflow is used as part of the \"Sample QC\" automation example available on [GitHub](https://github.com/sbg/adk-examples/tree/master/examples/sample-qc). It takes multi-lane FASTQ files as input, uses BWA MEM to align reads to given reference genome, and then merges all per-lane BAM files into a single BAM file using \"Sambamba Merge\". Please note this workflow is still under development and not yet recommended for use in production pipelines."}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-macs2/21", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-macs2/21", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "ChIP-seq BWA Alignment and Peak Calling MACS2 2.1.1", "description": "ChIP-seq (Chromatin immunoprecipitation followed by high-throughput sequencing technology) allows researchers to study the landscape of chromatin modifications or the binding patterns of transcription factors and other chromatin-associated proteins like RNA-polymerases.\n\nThis pipeline takes you all the way from unaligned raw sequencing reads (FASTQ) to ChIP-seq peak detection.\n\nSequenced reads are aligned with the BWA-backtrack (preferably for short reads). After alignment, BAM files are filtered (to remove low quality and unpaired reads) and de-duplicated. A number of QC metrics that evaluate the library complexity and signal enrichment are calculated. Peak calling is performed on the processed BAM files using MACS2 peak caller.\n\nRequired inputs:\n\n* Treatment FASTQ files (ID: treatment_files) - one FASTQ file for single-end data, or two files for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata **Paired-end** field as 1 for one input file and as 2 for the other input file.\n\n* Reference/Index files (ID: reference_index_tar) - TAR bundle of already generated BWA index files.\n\nOptional inputs:\n\n* Control FASTQ files (ID: control_files) - one FASTQ file for single-end data, or two files per for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata **Paired-end** field as 1 for one input file and as 2 for the other input file.\n\n* Blacklist file (ID: blacklist_file) - BED file containing genomic regions that have anomalous, unstructured, high signal/read counts in next-generation sequencing experiments. It could be used by BEDTools intersect to increase the accuracy of peak calling and downstream ChIP-seq analysis.\n\n###**Common Use Cases**\nBefore executing the workflow you should choose between narrow and broad peak analysis by setting the **Analysis type** parameter and organism to calculate effective genome size by setting **Organism** parameter within MACS2 app in Define App Settings. \n \nBlacklisting:\n\nWhen provided with adequate blacklist BED file, BEDTools intersect within this pipeline, will remove blacklisted regions from MACS2 *narrowPeak/broadPeak* output file. The exclusion of these regions, called blacklist regions, aims to remove sources of artifact signal caused by biases from chromatin accessibility and ambiguous alignment to increase the accuracy of both peak calling and downstream ChIP-seq analysis.\n\n###**Changes Introduced by Seven Bridges**\n\n* MACS2 **Analysis type** parameter is used for choosing between narrow and broad peak analysis. If narrow peak is selected, **QVALUE** (`--qvalue`) parameter will be set to 0.01. If broad peak is selected, **BROAD** (`--broad`) parameter will be set to True and **QVALUE** (`--qvalue`) parameter to 0.05. \n\n* MACS2 uses the estimated fragment length to extend reads in 5'->3' direction to fix-sized fragments. By default, the fragment length is estimated by MACS2 during the model building step. However, for this specific workflow (following ENCODE pipeline), the predominant fragment length is estimated in a previous step (*JSON output; from 'SBG ChIP-seq filter and QC') and used as an input via the parameter **EXTSIZE** (`--extsize`). The parameter **NOMODEL** (`--nomodel`) is also set to force MACS2 to bypass the building of the shifting model.\n\n* **KEEP_DUPS** (`--keep-dup`) parameter is set to 'all' to prevent MACS2 from searching for duplicated reads because we expect the input BAM files to be de-duplicated. If this is not the case, this parameter should be set to the default option '1' which is to keep one tag at the same location.\n\n* **spmr** (`--SPMR`) and **BDG** (`--bdg`) parameters are set to True to perform normalization per million reads for fragment pileup and save extended fragment pileup, and local lambda tracks into a bedGraph file.\n\n###**Common Issues and Important Notes**\n\n* **When providing treatment and control files to the input, a \"Case ID\" and \"Sample ID\" metadata should be properly set for each file. PE files should have the same \"Sample ID\" and \"Case ID\". Treatment FASTQ files (PE or SE) should have the same \"Case ID\" as corresponding control FASTQ files.**\n\n* In some situations read trimming is necessary, for instance, if read ends display poor quality values (most generally the right end) or if treatment and control FASTQ files have inconsistent read lengths. In these cases, read-trimming should be applied.\n\n*  Preferred values for library complexity are NRF > 0.9, PBC1 \u2265 0.9, and PBC2 \u2265 10.\n\n* Preferred values for NSC and RSC metric when analysing sharp histone marks are: NSC > 1.05 and RSC > 0.8 (Even if ChIP-seq data does not meet these guidelines, there could still be significant biological information. The users should evaluate the profiles of the cross-correlation plot to further assess the quality of their data).\n\n* Input (control) and negative control samples should have low NSC and RSC scores.\n\n* SBG ChIP-seq Set Metadata tool adds values 'control' and 'sample' to the metadata field **chip-seq** of the input files.\n\n* Along with suggested input files for Human genome, Mus musculus Reference/index TAR file and Blacklist file could be obtained from Public Reference Files.\n\n* If the total size of input FASTQ files in a single task exceeds 20GB a larger AWS instance should be allocated due to memory requirements.\n\n* In cases when analysing two biological replicates, outputs between replicates should be compared using IDR app. IDR is publicly available app which could be used to combine outputs of ChIP-seq pipeline to perform the comparative analysis between biological replicates in the separate task.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n###**Performance Benchmarking**\n\nThe  run time and cost strongly depend on the input file sizes and available resources. Based on our testing, BWA Aligment and Filtering tool has increasing demand for RAM memory with the increase of sequencing reads within FASTQ input files. We set c4.4xlarge AWS instance with 30GB of RAM as a default instance but in some cases where more memory is requred our suggestion is using c4.8xlarge AWS instance. Here are some examples of run times and costs for tasks running on AWS spot instances:\n\n|Input FASTQ file size|AWS Instance| Duration | Cost |\n| --- | --- | --- | --- |\n| 3.4 GB (SE, fastq.gz, treatment)  | c4.4xlarge | 1h27m | $0.49 |\n| 3.4 GB + 410.4 MB (SE, fastq.gz, treatment + control) | c4.4xlarge | 1h36m | $0.54 |\n| 6 GB + 6.4 GB (PE, fastq.gz, treatment) | c4.4xlarge | 6h50m | $2.31 |\n| 6 GB + 6.4 GB + 3.4 GB + 3.7 GB  (PE, fastq.gz, treatment + control) | c4.4xlarge | 9h51m | $3.33 |\n| 3.4 GB (SE, fastq.gz, treatment)  | c4.8xlarge | 1h24m | $0.83 |\n| 3.4 GB + 410.4 MB (SE, fastq.gz, treatment + control) | c4.8xlarge | 1h23m | $0.83 |\n| 6 GB + 6.4 GB (PE, fastq.gz, treatment) | c4.8xlarge | 6h15m | $3.73 |\n| 6 GB + 6.4 GB + 3.4 GB + 3.7 GB  (PE, fastq.gz, treatment + control) | c4.8xlarge | 6h41m | $3.98 |\n\n###**API Python Implementation**\n\nThe workflow's draft task can also be submitted via the API. To learn how to get your Authentication token and API endpoint for corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n```python\nfrom sevenbridges import Api\n\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n# Get file names from files in your project.\ninputs = {\n    'reference_index_tar': api.files.query(project=project_id, names=reference_index_tar_file)[0],\n    'treatment_files': list(api.files.query(project=project_id, names=treatment_files)),\n    'control_files': list(api.files.query(project=project_id, names=control_files)),\n    'blacklist_file': api.files.query(project=project_id, names=blacklist_file)[0],\n    'analysis_type' : \"narrow peaks\",\n    'Organism' : \"hs\" \n    }\ntask = api.tasks.create(name='ChIP-seq BWA Alignment and Peak Calling MACS2 - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False)\n\n#For running a batch task\ntask = api.tasks.create(name='ChIP-seq BWA Alignment and Peak Calling MACS2 - API Example', project=project_id, app=workflow_id, inputs=inputs, run=False, batch_input='treatment_files', batch_by = { 'type': 'CRITERIA', 'criteria': [ 'metadata.sample_id'] })\n```\nwhere `reference_index_tar`, `treatment_files`, `control_files` and `blacklist_file` are list objects containing the input files. \n\nInstructions for installing and configuring the API Python client, are provided on GitHub. For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). More examples are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n**Running a batch task:**\nBatching is performed by **Sample ID** metadata on the treatment_files port. For running analysis in batches it is necessary to set **Sample ID** metadata for each file and for paired-end data both files must have the same **Sample ID**."}, {"_id": "SB_Public_Apps_admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-spp-1-14/10", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/chip-seq-bwa-alignment-and-peak-calling-spp-1-14/10", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "ChIP-seq BWA Alignment and Peak Calling SPP 1.14", "description": "**Description:**\n\nChIP-seq (Chromatin immunoprecipitation followed by high-throughput sequencing technology) allows researchers to study the landscape of chromatin modifications or the binding patterns of transcription factors (TF) and other chromatin-associated proteins like RNA-polymerases or chromatin modulators.\n\nThis pipeline takes you all the way from unaligned raw sequencing reads (FASTQ) to ChIP-seq peaks detection.\n\nSequenced reads are aligned with the BWA-backtrack tool (specific for read size < 100bp). After alignment, BAM files are filtered (to remove low quality and unpaired reads) and de-duplicated. A number of QC metrics that evaluate the library complexity and signal enrichment are calculated. Peak calling is performed on the processed BAM files using SPP which is suggested peak caller for analysing binding patterns of TF.\n\n**Blacklisting**\n\nPrior to further ChIP-seq analysis, genomic regions that are associated with artifact signal may be removed. The exclusion of these regions, called blacklist regions, aims to remove sources of artifact signal caused by biases from chromatin accessibility and ambiguous alignment in order to increase the accuracy of both peak calling and comparative ChIP analysis. BEDTools intersect tool within this pipeline, when provided with adequate blacklist file, will remove blacklisted regions from peak SPP output file. \n\n**Required inputs:**\n\n* Input FASTQ reads (ID: Sample FASTQ) - one fastq file per sample for single-end data, or two files per sample for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata 'paired-end' field as 1 for one input file, as 2 for the other input file.\n\n* Reference/Index files (ID: Reference index) - TAR bundle of already generated index files.\n\n**Optional input:**\n\n* Control FASTQ reads (ID: Control FASTQ) - one fastq file per sample for single-end data, or two files per sample for paired-end data. NOTE: For paired-end reads, it is crucial to set the metadata 'paired-end' field as 1 for one input file, as 2 for the other input file.\n\n* Blacklist file (ID: Blacklist file) - BED file containing genomic regions that have anomalous, unstructured, high signal/ read counts in next-generation sequencing experiments. It could be used by BEDTools intersect in order to increase the accuracy of peak calling and comparative ChIP analysis.\n\n**Outputs:**\n\n* Sample_FASTQ.qc.b64html: B64HTML file with a table of all QC metrics.\n\n* Sample_FASTQ_deduped.filter.srt.bam: BAM files that are filtered (to remove low quality and unpaired reads) and de-duplicated. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Control_FASTQ_deduped.filter.srt.bam: BAM files that are filtered (to remove low quality and unpaired reads) and de-duplicated. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Sample_FASTQ_deduped.filter.srt.bam.bai: BAI index files for processed BAM files. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Control_FASTQ_deduped.filter.srt.bam.bai: BAI index files for processed BAM files. Could be used independently in other analyses for peak calling with different peak-callers.\n\n* Sample_FASTQ_SPPpeaks.narrowPeak: a BED6+4 format file which contains the peak locations together with pvalue and qvalue. This output contains fixed width peaks.\n\n* Sample_FASTQ_SPPpeaks.regionPeak: a BED6+4 format file which contains the peak locations together with pvalue and qvalue.  This output contains variable width peaks with regions of enrichment around peak summits.\n\n* Sample_FASTQ_SPPmodel.Rdata: Rdata object which you can use to access the model and output results produced by SPP. \n\n* Sample_FASTQ_SPPxcorplot.pdf: The cross-correlation of stranded read density profiles plot saved in a PDF file.\n\n* Sample_FASTQ_SPPpeaks.blacklisted.narrowPeak: Sample FASTQ_SPPpeaks.narrowPeak file without blacklist regions.(This file is not created if Blacklist file is not provided on input).\n\n**Important notes:**\n\n* **When providing sample and control files to the input, a \"Case ID\" and \"Sample ID\" metadata should be properly set for each file. PE files should have the same \"Sample ID\" and \"Case ID\". Sample files (PE or SE) should have the same \"Case ID\" as corresponding control files.**\n\n* In some situations read trimming is necessary, for instance, if read ends display poor quality values (most generally the right end) or if control and ChIP-seq samples have inconsistent read lengths. In these cases, read-trimming should be applied.\n\n* Preferred values for library complexity are NRF>0.9, PBC1 \u2265 0.9, and PBC2 \u2265 10.\n\n* Preferred values for NSC and RSC metric are: NSC > 1.05 and RSC > 0.8 (Even if ChIP-seq data does not meet these guidelines, there could still be significant biological information. The users should evaluate the profiles of the cross-correlation plot to further assess the quality of their data).\n\n* Input and negative control samples should have low NSC and RSC scores.\n\n* SBG ChIP-seq Set Metadata tool adds values 'control' and 'sample' to the metadata field 'chip-seq' of the input files.\n\n**Important notes about SPP options:**\n\n* **run_spp_nodups:** In this workflow we set \u2018run_spp_nodups\u2019 because we expect the input BAM files to be de-duplicated (after 'SBG ChIP-seq Filter and QC' workflow). If this is not the case, this parameter should not be set (un-select it).\n\n* **fragLen**: SPP requires the user to provide the fragment-length cross-correlation peak strandshift value to the \u2014speak parameter. In this workflow, this argument is passed to SPP using the fragment length estimated in the previous step (*json output; from 'SBG ChIP-seq filter and QC').\n\n* **npeak**: The threshold on a number of peaks to call is set to 300,000 to allow for 'relaxed' peak calling. Peak sets thresholded in this way are not meant to be interpreted as definitive binding events, but are rather intended to be used as input for subsequent statistical comparison of replicates. Please use the fdr parameter and leave the npeak option in blank if a more stringent analysis is needed.\n\n* **call_narrowPeaks**: Select this option for \u2018narrowPeak\u2019 output if you require that fixed width peaks are outputted.\n\n* **call_regionPeaks**: Select this option for \u2018regionPeak\u2019 output if you require that variable width peaks with regions of enrichment around peak summits outputted (recommended).\n\n**Common issues and limitations of the workflow:**\n\n* If the total size of the input files in a single task exceeds 20GB perhaps a larger instance should be allocated due to memory requirements.\n\n* In cases when analysing two biological replicates, outputs between replicates should be compared using IDR app. IDR is publicly available app which could be used to combine outputs of chip-seq pipeline in order to perform the comparative analysis between biological replicates in the separate task.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**."}, {"_id": "SB_Public_Apps_admin/sbg-public-data/clustering-and-gene-marker-identification-with-seurat-3-2-2/3", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/clustering-and-gene-marker-identification-with-seurat-3-2-2/3", "applicationCategory": "Workflow", "version": "v1.2", "name": "Clustering and Gene Marker Identification with Seurat 3.2.2", "description": "The workflow performs clustering and gene marker identification analysis starting from gene-cell UMI or read counts.\n\nThe **Clustering and Gene Marker Identification with Seurat 3.2.2** workflow is based on the **Seurat 3.2.2** R package [1] and it can be used to process gene-cell UMI or read counts produced with the following tools available on the Seven Bridges Platform: **Cell Ranger counts**, **Salmon Alevin**, **Kallisto BUStools Workflow**, **zUMIs**, **Single-Cell Smart-seq2 Workflow v3.0.0**, and **STAR** (STARsolo option). \n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of this page.*  \n***Please note that any cloud infrastructure costs resulting from app and pipeline executions, including the use of public apps, are the sole responsibility of you as a user. To avoid excessive costs, please read the app description carefully and set the app parameters and execution settings accordingly.***\n\n### Common Use Cases\n\nDepending on the quantification method used, the **Input type** parameter needs to be specified adequately and the **Gene-cell count matrices** input port needs to be provided with one of the following files:\n\n- Cell Ranger counts:  filtered or unfiltered feature_bc_matrix.tar.gz file (from the *Feature-barcode matrices TAR* output port); \n- Salmon Alevin:  alevin_output.tar.gz file (from the *Compressed output directory* output port);\n- Kallisto BUStools Workflow: Rdata file (from the *Counts table* output port);\n- zUMIs: RDS file (from the *Expression files* output port);\n- STAR: filtered or unfiltered .tar.gz file (from the *STARsolo matrix TAR* output port);\n- Single-Cell Smart-seq2 Workflow v3.0.0: gene_expression.results.mtx file (from the *RSEM gene expression matrix* output port).\n\nRemoving low quality cells and unexpressed genes from the data can significantly improve the quality of the clustering analysis. Some of the parameters that can be used for filtering are: **Minimum number of cells**, **Minimum number of genes**, **Minimal number of UMI counts detected per cell**, **Minimal number of genes detected per cell**, **Maximum percentage of mitochondrial genes** etc. For more details and the default values, please read the parameters description. \n\nThe quality of the clustering can be further improved by restricting the variation due to uninteresting (unwanted) sources. By default, the Seurat method `SCTransform` will regress out variation due to number of UMIs (sequencing depth), number of genes, mitochondrial expression, and cell cycle phase (the last mentioned can be avoided by setting the **Cell cycle regression** parameter to **FALSE**). \n\nTo overcome the extensive technical noise in the expression of any single gene for scRNA-seq data, the Seurat assigns cells to clusters based on their PCA scores derived from the expression of the integrated most variable genes. With the Seurat `SCTransform` method, selection of PCs is no longer as important as it used to be. In theory, the more PCs we choose, the more variation is accounted for when performing the clustering, but it takes longer to perform the clustering. The **Number of principal components** parameter defines the number of PCs that will be used for clustering (default = **30**).\n\nThe parameter of particular importance for this analysis is the **Clustering resolutions** parameter specifying which resolution values should be used by the clustering algorithm to determine the number of clusters (higher resolution value will result in higher number of clusters). Multiple values for the parameter can be set and the results will be produced for each provided value (default = **0.5**). \n\nIn addition, a clustering tree, showing how samples move as the number of clusters increases, may help in deciding what resolution to use. The **Clustree resolutions** parameter specifies the resolutions that will be used for making the clustering tree (default = **0.1**, **0.2**, **0.3**, **0.4**, **0.5**, **0.6**, **0.7**, **0.8**, **0.9**, and **1** + resolutions chosen using the **Clustering resolutions** parameter, if not already in the list).\n \n To identify gene markers of a single cluster (compared to all other cells), Seurat performs differential expression. The testing method can be set using the **Differential expression test** parameter (default = **wilcox**, a.k.a. The Mann Whitney U test). Two parameters are important for gene marker selection: the **Minimum percentage of expressed cells** parameter that requires a gene to be detected at a minimum percentage in either of the two groups of cells (default = **0.1**) and the **Minimum log fold change** parameter that requires a gene to be differentially expressed (on average) by certain amount between the two groups (default = **0.25**).\n\nBy default, only positive markers will be considered. If the **Only positive markers** parameter is set to **FALSE**, both positive and negative markers will be included in the analysis.\n\nAs a result, the workflow produces: \n- HTML report, containing quality metric plots, the UMAP plot showing the results of clustering, the clustering tree plot, the table with top 10 markers per cluster, and the heatmap plot showing the expression pattern of marker genes among clusters.\n- TSV table with detected biomarkers (one table for each resolution).\n- PDF report, containing multiple violin and feature plots showing the top 6 marker genes identified in each cluster (one PDF file for each resolution). This output can be omitted by setting the **Biomarker plots** parameter to **FALSE**.\n- Result Seurat object, allowing further exploration of the data and results locally.\n\n### Changes Introduced by Seven Bridges\nThe Clustering tree plot is produced using the **Clustree** R package.\n\n### Common Issues and Important Notes\n- Currently, only human and mouse samples are supported.\n- In most of the cases, it is hard to tell the optimal parameter values for best clustering results in advance. Thus, after analysing all the QC and clustering plots, re-running of the analysis with corrected (optimal) parameter values is recommended.\n- The **Minimum number of genes** parameter is applied before the Seurat object is created, thus removed cells won\u2019t be visible in the QC plots of the html report. Using the **Minimal number of genes detected per cell** and the **Minimal number of UMI counts detected per cell** parameters would then be a safer option for removing low quality cells because the result of filtering using these parameters will be visible.\n- While running the PCA, the total number of PCs to compute and store is 50 by default. Thus, the total number of cells after filtering should be greater than 50, otherwise the task will fail.\n- In some cases, the workflow won't be able to detect mitochondrial counts, thus the percent of mitochondrial counts won't be used as a criterium for filtering. This may happen due to differences in labelling of the mitochondrial genes comparing different model organisms and reference files. Currently, the workflow should be able to detect human and mouse mitochondrial gene ids.\n- If the size of the html or pdf report exceeds 10 MiB, the preview of the file's content will not be supported on the Seven Bridges Platform. In that case, the files can be downloaded and observed locally.\n\n\n### Performance Benchmarking\n\nRuntime and task cost for different number of analysed cells, using the default options:\n\n| # of cells | Quantifier used  | Duration |  Cost | Instance (AWS) | \n|:-------------:|:----------:|:----------:|:----------:|:------------:|\n|       1.2 k     |     Cell Ranger   |   5 min   |   $0.05    |    c4.2xlarge    |\n|       4.6 k     |     Cell Ranger  |   11 min   |   $0.10    |    c4.2xlarge    |\n|       8.7 k     |     Cell Ranger    |   18 min   |   $0.17    |    c4.2xlarge    |\n|       11.7 k     |    Cell Ranger    |   30 min   |   $0.28    |    c4.2xlarge    |\n\n      \n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n\n### API Python Implementation\nThe workflow's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\nfrom sevenbridges import Api\n\n# Enter api credentials\nauthentication_token, api_endpoint = \"enter_your_token\", \"enter_api_endpoint\"\napi = Api(token=authentication_token, url=api_endpoint)\n\n# Get project_id/workflow_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/workflow\nproject_id, workflow_id = \"your_username/project\", \"your_username/project/workflow\"\n\n# Get file names from files in your project. File names below are just as an example.\ninputs = {\n        'in_counts': list(api.files.query(project=project_id, names=['sample_kallisto_gene_counts.Rdata'])), \n        'input_type': \"kallisto_bus\"\n        }\n\n# Run the task\ntask = api.tasks.create(name='Seurat 3.2.2 workflow - API Example', project=project_id, app=workflow_id, inputs=inputs, run=True)\n```\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [sevenbridges-python documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n### References\n\n[1] [Seurat documentation](https://satijalab.org/seurat/)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-pipeline-from-baseline-093/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-pipeline-from-baseline-093/13", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "CNVkit CNV Calling", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **CNVkit CNV Calling** workflow determines copy number alterations and variations.\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* For proper CNV detection in sex chromosomes,  corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n* For a deeper CNV analysis (eg. determining loss of heterozygosity regions), an additional step of calling the germline mutations in a tumor sample should be performed. This pipeline, unlike the **CNVkit CNV Calling with GATK HaplotypeCaller** doesn't have a variant caller implemented. Instead, a VCF created by another caller can be provided to the **SNP VCF** input.\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named based on the **Tumor BAM Sample ID** metadata field.\n\n### Common Issues and Important Notes\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* For proper CNV detection in sex chromosomes,  corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n### Performance Benchmarking\n\nThe instance set for this workflow is the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM and 256 GB of EBS (disk space).\n\n\n| BAM size (GB) | Type | Duration | Cost  |\n|---------------|------|----------|-------|\n|       12      | WES  | 9m       | $0.06 |\n|      144      | WGS  | 1h 26m       | $0.57 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"antitarget_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference_coverage_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit CNV Calling - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc/14", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc/14", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "CNVkit CNV Calling with GATK HaplotypeCaller", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **CNVkit CNV Calling with GATK HaplotypeCaller** workflow determines copy number alterations and variations.\n\n* For a deeper CNV analysis (eg. determining loss of heterozygosity regions), a VCF containing germline mutations in a tumor must be provided. The germline mutations in a tumor are determined using the two GATK HaplotypeCallers. The first GATK HaplotypeCaller outputs a VCF with germline mutations in a normal sample. The created VCF is an input for the second GATK HaplotypeCaller, based on which the germline mutations are confirmed in a tumor sample.\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* For a proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named based on the **Tumor BAM** and **Normal BAM Sample ID** metadata field.\n\n### Common Issues and Important Notes\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this pipeline.\n\n* Matched Tumor-Normal BAMs are highly recommended as Tumor BAM and Normal BAM inputs, respectively.\n\n* A db SNP can be provided as an input in order to annotate known SNPs. \n\n* For a proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n### Performance Benchmarking\n\nThe instances set for this workflow are the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM with 1 TB of EBS (disk) and the c5.9xlarge instance with 36 vCPUs, 72 GiB of RAM with EBS set to 1 TB.\n\n\n| BAM size (GB) | Type | Duration | Cost  |\n|---------------|------|----------|-------|\n|       12      |  WES |    31m   | $0.79 |\n|      144      |  WGS |  8h 26m  | $12.9 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"antitarget_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference_coverage_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"normal_bam\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])), \n\t\"reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"bed_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit CNV Calling with GATK HaplotypeCaller - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-reference/13", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-reference/13", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "CNVkit Reference", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n* **CNVkit Reference** creates a **Reference coverage file** from the normal samples. Also, the workflow creates a **Target BED file** of baited regions and an **Antitarget BED file** of regions not included in the Target BED file. All three files are prerequisites for further CNV Calling workflows.\n\n* A **Reference coverage file** should be constructed specifically for each target capture panel, using a BED file that lists the genomic coordinates of the baited regions. Ideally, the control or \u201cnormal\u201d samples that are used to build the reference should match the type of sample (e.g. FFPE-extracted or fresh DNA) and library preparation protocol or kit used for the test (e.g. tumor) samples.[2]\n\n* A **Reference coverage file** needs to be created for both genders in separate runs; one run with only normal female, the other one with only male normal samples. The parameter **Male reference** should be set to *True* or *False* in accordance with the gender of the input samples.\n\n* In order to obtain annotated CNV (name of the genes that are located in the CNV regions), an **Annotation File** input must be provided (eg. hg19 annotation - [refFlat19](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refFlat.txt.gz)).\n\n* If the sequencing type is **Amplicon**, parameter **Split** for the CNVkit-target tool and parameter **No edge** in the CNVkit reference tool  should be set to *True*.\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made, apart from the automation of file naming.\n\n### Common Issues and Important Notes\n\n* The number of samples should be under 40, because exceeding this number would not make any statistical difference.\n\n* The total memory of samples can not exceed 4 TB (important when samples are high-coverage WES or WGS).\n\n\n### Performance Benchmarking\n\nThe instance set for this workflow is the AWS c5.9xlarge instance with 36 vCPUs, 72 GiB of RAM and EBS (disk space) set to 4 TB.\n\n| Number of BAM files  (average size in GB) | Type | Duration | Cost  |\n|-------------------------------------------|------|----------|-------|\n|                  12 (11)                  |  WES |    29m   | $0.73 |\n|                   18 (11)                 |  WES |    38m   | $0.97 |\n|                  36 (11)                  |  WES |  1h 12m  | $1.87 |\n|                  10 (159)                 |  WGS |  5h 15m  | $8 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n    \"fa_fname\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"data_type\": \"WXS\",\n    \"bam_files\": list(api.files.query(project=project_id, names=[\"enter_filename\", \"enter_filename\"])),\n    \"targets_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit Reference - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)\n\n[2] [CNVkit reference](https://cnvkit.readthedocs.io/en/stable/pipeline.html#reference)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc-to/22", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-pipeline-from-baseline-093-hc-to/22", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "CNVkit Tumor-Only CNV Calling with GATK HaplotypeCaller", "description": "CNVkit analyses copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n### Common Use Cases\n\n* The **CNVkit CNV Calling** workflow determines copy number alterations and variations. \n\n* For a deeper CNV analysis (eg. determining loss of heterozygosity regions), an additional step of calling the germline mutations in a tumor sample must be performed. This pipeline employs **GATK HaplotypeCaller** as germline variant caller.\n\n* This workflow is to be run in the case where no matched normal sample for a tumor sample is available.\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this workflow.\n\n* For proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n### Changes Introduced by Seven Bridges\n\n* Outputs are named based on the **Tumor BAM Sample ID** metadata field.\n\n### Common Issues and Important Notes\n\n* **CNVkit Reference** must be run prior to running this workflow. **CNVkit Reference** output files, **Reference coverage file**, **Target** and **Antitarget BED** are the required inputs for this workflow.\n\n* For proper CNV detection in sex chromosomes, corresponding inputs must be set in the **Gender** and the **Male reference** parameters (eg. for female sample: **Gender**: *female*, **Male reference**: *False*).\n\n* A db SNP file must be provided in order to exclude somatic mutations in a tumor sample.\n\n* This workflow might not work with filenames containing some special characters e.g. **#**, **~**.\n\n\n### Performance Benchmarking\n\nThe instances set for this workflow are the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM with 1 TB of EBS (disk space) and the c5.9xlarge instance with 36 vCPUs, 72 GiB of RAM with EBS set to 1 TB.\n\n\n| BAM size (GB) | Type | Duration | Cost  |\n|---------------|------|----------|-------|\n|       12      | WES  | 21m       | $0.53 |\n|      144      | WGS  | 3h 42m       | $5.66 |\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*\n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform, visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n\t\"tumor_bam\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"target_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"antitarget_bed\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference_coverage_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"reference\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"db_snp\": api.files.query(project=project_id, names=[\"enter_filename\"])[0], \n\t\"bed_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit Tumor-Only CNV Calling with GATK HaplotypeCaller - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvkit-reference-wf-no-normals/30", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvkit-reference-wf-no-normals/30", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "CNVkit Tumor-only Reference", "description": "CNVkit analyzes copy number variants in exome, targeted, amplicon, hybrid and whole-genome DNA sequencing. \n\nThis toolkit takes advantage of both on\u2013 and off-target sequencing reads and applies a series of corrections to improve accuracy in copy number calling.[1]\n\n*A list of **all inputs and parameters** with corresponding descriptions can be found at the bottom of the page.*\n\n### Common Use Cases\n\n* Ideally, the control or \u201cnormal\u201d samples used to build the reference should match the type of sample (e.g. FFPE-extracted or fresh DNA) and library preparation protocol or kit used for the test (e.g. tumor) samples[1]. If this is not the case, **CNVkit Tumor-only Reference** creates a \u201cflat\u201d **Reference coverage file** of neutral copy number for each probe from the target and antitarget interval files. \n\n* In order to obtain annotated CNV (name of the genes that are located in the CNV regions), an annotation file for the **Annotation File** input must be provided (eg. hg19 annotation - [refFlat19](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refFlat.txt.gz)).\n\n### Changes Introduced by Seven Bridges\n\n* No modifications to the original tool representation have been made, apart from the automation of file naming.\n\n### Common Issues and Important Notes\n\n* The **Reference FASTA** has to be indexed. We recommend using the **SBG FASTA Indices** app to index the reference file if needed.\n\n### Performance Benchmarking\n\nThe instance set for this workflow is the AWS c4.2xlarge instance with 8vCPUs, 15 GiB of RAM and 50 GB of EBS (disk space). The running time is around 5 minutes with the cost of $0.03.\n\n*Cost can be significantly reduced by using **spot instances**. Visit the [Knowledge Center](https://docs.sevenbridges.com/docs/about-spot-instances) for more details.*   \n\n### API Python Implementation\nThe app's draft task can also be submitted via the **API**. In order to learn how to get your **Authentication token** and **API endpoint** for the corresponding platform visit our [documentation](https://github.com/sbg/sevenbridges-python#authentication-and-configuration).\n\n```python\n# Initialize the SBG Python API\nfrom sevenbridges import Api\napi = Api(token=\"enter_your_token\", url=\"enter_api_endpoint\")\n# Get project_id/app_id from your address bar. Example: https://igor.sbgenomics.com/u/your_username/project/app\nproject_id = \"your_username/project\"\napp_id = \"your_username/project/app\"\n# Replace inputs with appropriate values\ninputs = {\n    \"interval_file\": api.files.query(project=project_id, names=[\"enter_filename\"])[0],\n    \"reference_fasta\": api.files.query(project=project_id, names=[\"enter_filename\"])[0]}\n# Creates draft task\ntask = api.tasks.create(name=\"CNVkit Tumor-only Reference - API Run\", project=project_id, app=app_id, inputs=inputs, run=False)\n```\n\nInstructions for installing and configuring the API Python client are provided on [github](https://github.com/sbg/sevenbridges-python#installation). For more information about using the API Python client, consult [the client documentation](http://sevenbridges-python.readthedocs.io/en/latest/). **More examples** are available [here](https://github.com/sbg/okAPI).\n\nAdditionally, [API R](https://github.com/sbg/sevenbridges-r) and [API Java](https://github.com/sbg/sevenbridges-java) clients are available. To learn more about using these API clients, please refer to the [API R client documentation](https://sbg.github.io/sevenbridges-r/), and [API Java client documentation](https://docs.sevenbridges.com/docs/java-library-quickstart).\n\n###References\n\n[1] [CNVkit: Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839673/)"}, {"_id": "SB_Public_Apps_admin/sbg-public-data/cnvnator-analysis/35", "includedInDataCatalog": {"name": "PublicApps@SevenBridges"}, "@type": "Dataset", "url": "https://igor.sbgenomics.com/public/apps/admin/sbg-public-data/cnvnator-analysis/35", "applicationCategory": "Workflow", "version": "sbg:draft-2", "name": "CNVnator Analysis", "description": "CNVnator Analysis workflow performs copy number variation (CNVs) calling by doing read-depth (RD) analysis of the input BAM files. \n###Methods\nCNVnator tool has **five** major steps: \n\n1. Reads extraction \n\n2. Histogram generation \n\n3. Statistics calculation\n\n4. RD signal partition \n4'. Calculating average RD signal per bin \n\n5. CNVs identification\n###Inputs\nIn order to execute the workflow properly, a user should set the following input files: \n\n1. Input BAM files (bam_files) - one or more BAM files \n\n2. Reference genome file (ref_genome) - FASTA file matching the provided BAM files\n###Parameter settings\nFor the correct execution of the workflow it is **necessary** for the following settings to be provided: Evaluate RD, Histogram, Identifying CNVs, RD signal partitioning and Calculate statistics. Recommended value for these settings (bin size) is 100.\n\nThe No GC correction (calculate results without GC correction) can be set, but is not mandatory.\n###Outputs\nOn the output **three** files are generated: \n\n1. average_rd_output_file - contains average RD signal per bin. These results are calculated after the RD signal partition step. \n\n2. vcf file - This is the result of the conversion of the cnv_result_file.\n\n3. cnv_result_file - This file contains the result of the CNV calling in **nine** columns, they are: \n\ncolumn 1: CNV_type \ncolumn 2: coordinates\ncolumn 3: CNV_size \ncolumn 4: normalized_RD - normalized to 1  \ncolumn 5: e-val1 - is calculated using t-test statistics.  \ncolumn 6: e-val2 - is from the probability of RD values within the region to be in the tails of a gaussian distribution describing frequencies of RD values in bins. \ncolumn 7: e-val3 - same as e-val1 but for the middle of CNV  \ncolumn 8: e-val4 - same as e-val2 but for the middle of CNV \ncolumn 9: q0 - fraction of reads mapped with q0 quality\n###Common issues\nThere aren't any known common issues."}]